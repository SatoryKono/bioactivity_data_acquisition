[build-system]
requires = ["setuptools>=65", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "bioactivity-data-acquisition"
version = "0.1.0"
description = "ETL pipeline for ChEMBL bioactivity data"
readme = "README.md"
authors = [{ name = "Bioactivity Team" }]
requires-python = ">=3.10"
dependencies = [
    # Core HTTP and retry logic - stable APIs
    "backoff>=2.2,<3.0",
    "requests>=2.31,<3.0",
    
    # Data processing - major version bounds for stability
    "pandas>=2.1,<3.0",
    "pandera[io]>=0.18,<1.0",
    "scipy>=1.10,<2.0",
    "scikit-learn>=1.3,<2.0",
    "psutil>=5.9,<6.0",
    
    # Configuration and validation - stable APIs
    "pydantic>=1.10,<3.0",
    "pydantic-settings>=2.0,<3.0",
    "python-dotenv>=1.0,<2.0",
    "pyyaml>=6.0,<7.0",
    "jsonschema>=4.0,<5.0",
    
    # CLI and logging - stable APIs
    "typer[all]>=0.9,<1.0",
    "structlog>=24.1,<26.0",
    "rich>=13.0,<14.0",
    
    # Security - critical for safety
    "defusedxml>=0.7,<1.0",
    
    # Observability - stable APIs
    "opentelemetry-api>=1.20,<2.0",
    "opentelemetry-sdk>=1.20,<2.0",
    "opentelemetry-instrumentation-requests>=0.42,<1.0",
    "opentelemetry-exporter-jaeger>=1.17,<2.0",
]

[project.optional-dependencies]
dev = [
    # Code formatting and linting - stable APIs
    "black>=24.4,<25.0",
    "ruff>=0.4,<1.0",
    "pre-commit>=3.7,<4.0",
    
    # Type checking - stable APIs
    "mypy>=1.8,<2.0",
    "pandera[mypy]>=0.18,<1.0",
    "types-PyYAML>=6.0,<7.0",
    "types-requests>=2.31,<3.0",
    "types-jsonschema>=4.0,<5.0",
    
    # Testing - stable APIs
    "pytest>=7.4,<8.0",
    "pytest-cov>=4.1,<5.0",
    "pytest-benchmark>=4.0,<5.0",
    "responses>=0.25,<1.0",
    
    # Security - critical for safety
    "safety>=3.0,<4.0",
    "bandit>=1.7,<2.0",
]

[project.scripts]
bioactivity-data-acquisition = "library.cli:main"

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
addopts = "--cov=library --cov=tests --cov-report=term-missing --cov-fail-under=90"
testpaths = ["tests"]
markers = [
    "benchmark: mark test as a benchmark",
    "integration: mark test as integration test requiring real API access",
    "slow: mark test as slow running",
    "network: mark test as requiring network access",
    "api: mark test as requiring API access",
]
filterwarnings = [
    "ignore::DeprecationWarning",
    "ignore::PendingDeprecationWarning",
]

[tool.coverage.run]
omit = [
    "src/library/io_/normalize.py",
]

[tool.black]
target-version = ["py310"]
line-length = 180

[tool.ruff]
line-length = 180

[tool.ruff.lint]
select = ["E", "F", "I", "B", "UP", "S"]
ignore = ["S101"]

[tool.ruff.lint.per-file-ignores]
"tests/**" = ["E501", "E712"]
"src/scripts/**" = ["E501", "E712", "E402"]
"scripts/**" = ["E501", "E712", "E402"]
"src/library/etl/qc.py" = ["E402"]

[tool.mypy]
python_version = "3.10"
warn_unused_configs = true
strict = true
plugins = ["pandera.mypy"]
show_error_codes = true
explicit_package_bases = true

[[tool.mypy.overrides]]
module = ["pandas.*", "pandera.*", "typer", "structlog"]
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "yaml"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "library.config"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "library.documents.config"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "library.documents.pipeline"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "library.etl.run"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "library.logging_setup"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "library.telemetry"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "library.clients.health"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "library.utils.graceful_shutdown"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "requests"
ignore_missing_imports = true

[tool.pytest.benchmark]
save = "benchmark_results.json"
save_data = true
autosave = true
format = "json"
sort = "mean"
compare = "benchmark_results.json"
compare_fail = "mean:20%"
warmup = true
warmup_iterations = 1
min_rounds = 5
max_time = 10.0
verbose = true
disable_gc = true
