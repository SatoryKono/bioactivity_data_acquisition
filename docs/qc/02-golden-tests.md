# Golden Test and Snapshot Policy

This document defines the policy for using "Golden" (also known as Snapshot or Characterization) tests to ensure the stability and deterministic behavior of the ChEMBL ETL pipelines.

## 1. Overview and Purpose

Golden testing is a form of regression testing where the output of a pipeline run is compared against a previously approved "golden" version stored in the repository. The purpose is **not** to verify the correctness of the business logic itself (which is the job of unit and integration tests), but rather to detect **unintended changes** in the output.

This approach is particularly effective for complex ETL pipelines where defining explicit `assert` statements for every possible output variation is impractical. For more background, see articles on [Snapshot Testing][snapshot-kent] and [Characterization Testing][snapshot-wiki].

## 2. Golden Artifacts

For each ChEMBL pipeline, a set of golden artifacts **MUST** be stored in a dedicated directory.

-   **Location**: `[ref: repo:tests/bioetl/golden/<pipeline_name>/@refactoring_001]`
    -   Example: `[ref: repo:tests/bioetl/golden/activity_chembl/@refactoring_001]`

-   **Required Artifacts**:
    1.  **Primary Dataset**: The main output of the pipeline (e.g., `activity_chembl.parquet`). This is the most critical artifact.
    2.  **Metadata File**: The `meta.yaml` file generated by the run.
    3.  **File Manifest**: A text file (`manifest.txt`) listing the names and checksums of all files generated by the pipeline run.

## 3. Deterministic Serialization

To ensure that comparisons are meaningful, the serialization of artifacts **MUST** be deterministic.

-   **Column Order**: The output dataset **MUST** have a fixed, explicitly defined column order, enforced by the Pandera schema.
-   **Sort Keys**: The rows in the dataset **MUST** be sorted by a stable, unique business key (e.g., `activity_id`).
-   **Serialization Parameters**: The parameters for the output format (e.g., Parquet or CSV) **MUST** be fixed.
-   **Localization**: All timestamps **MUST** be in `UTC`, and all locale-dependent settings **MUST** be disabled.

## 4. Comparison Methods

The comparison between the new output and the golden artifacts can be performed using one of three methods, depending on the nature of the data.

### 4.1. Byte-wise Comparison

This is the strictest and most preferred method. The new artifact is considered a match if and only if it is byte-for-byte identical to the golden version.

-   **Usage**: This method **SHOULD** be used for the primary dataset file and the file manifest.
-   **Pseudocode**: `sha256(new_artifact) == sha256(golden_artifact)`

### 4.2. Structural Comparison with Masking

This method is used when artifacts contain volatile metadata that changes with every run (e.g., timestamps, run IDs). The content is compared for structural equivalence after masking (ignoring) the volatile fields.

-   **Usage**: This method **MUST** be used for comparing the `meta.yaml` file.
-   **Volatile Fields to Mask**:
    -   `run_id`
    -   `config_hash` (if run-specific overrides are used)
    -   `execution.start_time_utc`
    -   `execution.end_time_utc`
    -   `execution.duration_seconds`
    -   `execution.stage_durations_ms`

-   **Pseudocode**:
    ```python
    new_meta = load_yaml('meta.yaml')
    golden_meta = load_yaml('golden/meta.yaml')

    # Mask volatile fields
    for field in VOLATILE_FIELDS:
        new_meta.pop(field, None)
        golden_meta.pop(field, None)

    # Compare the remaining content
    assert new_meta == golden_meta
    ```

### 4.3. Comparison with Tolerances

This method is used for datasets that contain floating-point numbers or other numerical data that may have minor, acceptable variations between runs.

-   **Usage**: This **MAY** be used for numerical columns in the primary dataset if byte-wise comparison is not feasible.
-   **Pseudocode**:
    ```python
    new_df = pd.read_parquet('output.parquet')
    golden_df = pd.read_parquet('golden/output.parquet')

    # Compare with a tolerance for numeric columns
    pd.testing.assert_frame_equal(
        new_df,
        golden_df,
        check_dtype=True,
        atol=1e-6,  # Absolute tolerance
        rtol=1e-6   # Relative tolerance
    )
    ```

## 5. Snapshot Update Policy

Golden snapshots are a critical backstop against regressions and **MUST NOT** be updated casually.

-   **When to Update**: A snapshot **SHOULD** only be updated when the change in the output is **intentional and verified**. This typically happens when:
    -   The business logic of the `transform` stage has been purposefully modified.
    -   The Pandera schema has evolved (e.g., a column was added or a data type changed).
    -   A bug in the pipeline has been fixed, and the new output is now correct.

-   **Update Procedure**:
    1.  Run the pipeline and generate the new, correct output.
    2.  Verify that the new output passes all other tests (especially Pandera validation).
    3.  Delete the old golden artifacts.
    4.  Copy the new artifacts into the `tests/bioetl/golden/<pipeline_name>/` directory.
    5.  Commit the updated snapshots with a clear commit message explaining *why* the output changed.

-   **Golden-Driven Development**: Golden tests can be used to guide development. A developer can intentionally write a failing golden test by creating the desired output manually, and then work on the pipeline until the test passes.

[snapshot-kent]: https://kentcdodds.com/blog/effective-snapshot-testing
[snapshot-wiki]: https://en.wikipedia.org/wiki/Characterization_test
