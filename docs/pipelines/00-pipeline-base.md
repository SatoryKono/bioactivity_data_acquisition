# PipelineBase Orchestrator Specification

> **Note**: Implementation status: **planned**. All file paths referencing `src/bioetl/` in this document describe the intended architecture and are not yet implemented in the codebase.

This document provides a comprehensive technical specification for the `PipelineBase` abstract class, which serves as the core orchestrator for all ETL pipelines within the `bioetl` framework.

[ref: repo:README.md@refactoring_001]

## 1. Overview and Goals

The `PipelineBase` class standardizes the ETL process by defining a fixed, five-stage lifecycle: `extract` → `transform` → `validate` → `write` → `run`. Its primary purpose is to abstract away the boilerplate of pipeline orchestration, allowing developers to focus on source-specific business logic.

**Boundaries of Responsibility:**

-   **`PipelineBase` (Framework)** is responsible for:
    -   Orchestrating the sequence of stages.
    -   Injecting configuration (`PipelineConfig`).
    -   Managing the logging context and stage timers.
    -   Handling exceptions at a high level.
    -   Performing the final validation step against a provided schema.
    -   Atomically writing the output dataset and all associated metadata artifacts.
    -   Enforcing determinism through sorting and hashing.

-   **Concrete Implementation (Developer)** is responsible for:
    -   Implementing the logic to connect to and extract data from a specific source (e.g., API clients, database connections).
    -   Implementing the business logic to parse, clean, and transform the raw data.
    -   Providing the Pandera schema for the final data structure.

## 2. Public API (Signatures and Types)

The public API is defined by the `PipelineBase` abstract class and a set of dataclasses for structured results. The implementation is based on Python's `abc` module.

[ref: repo:src/bioetl/pipelines/base.py@refactoring_001]

### Result Types

These dataclasses define the expected structure of the results returned by the `write` and `run` stages. They are based on the `OutputArtifacts` class found in the codebase.

```python
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Dict

@dataclass(frozen=True)
class WriteResult:
    """Core artifacts from a successful write operation."""
    dataset: Path
    quality_report: Path
    metadata: Path

@dataclass(frozen=True)
class RunResult:
    """All artifacts generated by a complete pipeline run."""
    write_result: WriteResult
    run_directory: Path
    manifest: Path
    additional_datasets: Dict[str, Path]
    qc_summary: Optional[Path]
    debug_dataset: Optional[Path]
```

### `PipelineBase` Abstract Class

All pipelines **must** inherit from this class.

```python
import pandas as pd
from abc import ABC, abstractmethod
from typing import Any
from bioetl.config import PipelineConfig

class PipelineBase(ABC):
    """
    The abstract contract for all ETL pipelines.
    """
    def __init__(self, config: PipelineConfig, run_id: str):
        """Initializes the pipeline with its configuration and a unique run ID."""
        ...

    @abstractmethod
    def extract(self, *args: Any, **kwargs: Any) -> pd.DataFrame:
        """
        Extracts data from the source and returns it as a DataFrame.
        """
        raise NotImplementedError

    @abstractmethod
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Applies business logic to transform the raw DataFrame.
        """
        raise NotImplementedError

    def validate(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        A framework-managed method that validates the DataFrame against a Pandera schema.
        This method should not be overridden.
        """
        ...

    def write(self, df: pd.DataFrame, output_path: Path, extended: bool = False) -> RunResult:
        """
        A framework-managed method that writes the DataFrame and all metadata artifacts.
        This maps to the `export` method in the current implementation.
        This method should not be overridden.
        """
        ...

    def run(self, output_path: Path, extended: bool = False, *args: Any, **kwargs: Any) -> RunResult:
        """

        Orchestrates the entire pipeline lifecycle.
        This method should not be overridden.
        """
        ...
```

## 3. Lifecycle and `run()` Pseudocode

The `run()` method executes the ETL stages in a fixed sequence. It provides robust error handling, logging, and timing for each stage.

### Full `run()` Pseudocode

This pseudocode is a realistic representation of the orchestration logic within `PipelineBase.run()`.

```python
import time
from bioetl.core.logger import UnifiedLogger, set_run_context

# This pseudocode resides within the PipelineBase class.

def run(self, output_path: Path, extended: bool = False, *args: Any, **kwargs: Any) -> "RunResult":
    """
    Orchestrates the full pipeline lifecycle: extract → transform → validate → write.
    """
    log = UnifiedLogger.get(__name__)
    set_run_context(
        run_id=self.run_id,
        stage="bootstrap",
        actor="system", # Or could be derived from user context
        source=self.config.pipeline.entity
    )
    log.info("pipeline_started", pipeline_name=self.config.pipeline.name)

    stage_durations = {}

    try:
        # --- EXTRACT STAGE ---
        set_run_context(stage="extract")
        log.info("extract_started")
        extract_start = time.perf_counter()

        raw_df = self.extract(*args, **kwargs)

        stage_durations["extract"] = (time.perf_counter() - extract_start) * 1000.0
        log.info("extract_completed", duration_ms=stage_durations["extract"], rows=len(raw_df))

        # --- TRANSFORM STAGE ---
        set_run_context(stage="transform")
        log.info("transform_started")
        transform_start = time.perf_counter()

        transformed_df = self.transform(raw_df)

        stage_durations["transform"] = (time.perf_counter() - transform_start) * 1000.0
        log.info("transform_completed", duration_ms=stage_durations["transform"], rows=len(transformed_df))

        # --- VALIDATE STAGE ---
        set_run_context(stage="validate")
        log.info("validate_started")
        validate_start = time.perf_counter()

        validated_df = self.validate(transformed_df)

        stage_durations["validate"] = (time.perf_counter() - validate_start) * 1000.0
        log.info("validate_completed", duration_ms=stage_durations["validate"], rows=len(validated_df))

        # --- WRITE STAGE (EXPORT) ---
        set_run_context(stage="load") # 'load' is the conventional name for the write stage
        log.info("write_started")
        write_start = time.perf_counter()

        # The 'write' method in this spec maps to 'export' in the codebase.
        # It handles file writing and artifact generation.
        artifacts: "OutputArtifacts" = self.export(validated_df, output_path, extended=extended)

        stage_durations["load"] = (time.perf_counter() - write_start) * 1000.0
        log.info("write_completed", duration_ms=stage_durations["load"], artifacts_path=str(artifacts.run_directory))

        # Construct the final RunResult object from the generated artifacts
        write_result = WriteResult(
            dataset=artifacts.dataset,
            quality_report=artifacts.quality_report,
            metadata=artifacts.metadata
        )
        run_result = RunResult(
            write_result=write_result,
            run_directory=artifacts.run_directory,
            manifest=artifacts.manifest,
            additional_datasets=artifacts.additional_datasets,
            qc_summary=artifacts.qc_summary,
            debug_dataset=artifacts.debug_dataset
        )

        log.info("pipeline_completed_successfully")
        return run_result

    except Exception as e:
        # Enrich the exception with context and re-raise to ensure failure
        log.error("pipeline_failed", error=str(e), exc_info=True)
        # In a real implementation, you might wrap the exception with a custom PipelineError
        raise e

    finally:
        # --- CLEANUP STAGE ---
        set_run_context(stage="cleanup")
        log.info("cleanup_started")
        self.close_resources() # A method for closing connections, etc.
        log.info("cleanup_completed")

```

## 4. Configuration and DI

The pipeline receives its configuration via Dependency Injection (DI) in the constructor. A `PipelineConfig` object, loaded and validated from a YAML file, is passed in during initialization.

[ref: repo:src/bioetl/configs/models.py@refactoring_001]

-   **Priority and Overlays**: The configuration system supports profiles via the `extends` key, allowing a pipeline-specific YAML file (e.g., `activity.yaml`) to inherit from and override values in base files (e.g., `base.yaml`, `determinism.yaml`).
-   **Strictness**: The Pydantic models are configured with `extra="forbid"`. This is a critical feature that causes the configuration loading to fail if the YAML file contains any keys that are not explicitly defined in the models, preventing typos and "silent" configuration errors.

## 5. Logging and Telemetry

The framework provides a unified, structured logging system based on `structlog`.

[ref: repo:src/bioetl/core/logger.py@refactoring_001]

-   **Mandatory Fields**: Every log event emitted during a `run()` is guaranteed to be a JSON object containing these fields: `run_id`, `stage`, `actor`, `source`, `level`, and `generated_at` (UTC timestamp). Stage-specific timers (`duration_ms`) are added at the completion of each stage.
-   **Output Formats**: The logger is configured to output human-readable `key=value` text to the console during development and deterministic, sorted-key JSON to files.
-   **Secret Redaction**: A multi-layer redaction filter automatically finds and masks sensitive keywords (e.g., `token`, `api_key`, `Authorization`) in both structured fields and string messages, replacing their values with `[REDACTED]`.
-   **Telemetry Correlation**: If telemetry is enabled in the configuration, a processor automatically injects the active `trace_id` and `span_id` from OpenTelemetry into every log record, enabling seamless correlation between logs and traces.

## 6. Determinism and Artifacts

The framework guarantees that a pipeline run with the same configuration will produce a bit-for-bit identical output.

-   **Stable Sorting**: Before writing, the final DataFrame is sorted by the columns specified in the `write.sort_by` key in the configuration.
-   **Integrity Hashes**: The `write` stage calculates two critical hashes that are stored in `meta.yaml`:
    -   `hash_business_key`: A hash of the columns forming the unique business identifier.
    -   `hash_row`: A hash of all columns specified, ensuring row-level integrity.
-   **`meta.yaml`**: This artifact is the run's "birth certificate," recording the `run_id`, configuration hash, source versions, `row_count`, all hashes, and stage timings.
-   **Invariant**: A repeated run with an identical configuration against an identical source state **must** produce identical `meta.yaml` hashes and an identical primary dataset file hash.

## 7. Validation Contracts

Data validation is a non-negotiable stage of the pipeline, enforced by Pandera schemas.

-   **Mandatory Checks**: Every output schema **must** enforce:
    -   **Fixed Column Order**: `class Config: ordered = True`.
    -   **Strict Data Types**: All columns must have a specific type (e.g., `Series[Int64]`). Coercion is allowed (`pa.Field(coerce=True)`).
    -   **Business Key Uniqueness**: The primary business key column must be marked with `unique=True`.
-   **Schema Evolution**: Schemas must be versioned. Any backward-incompatible changes require a major version bump. The schema version is recorded in the `meta.yaml` file.

## 8. CLI Integration

Pipelines are automatically discovered and exposed as commands in the Typer-based CLI.

[ref: repo:src/bioetl/cli/app.py@refactoring_001]

-   **Registration**: Placing a `PipelineBase` subclass in the `src/bioetl/pipelines/` directory is sufficient to register it as a new CLI command.
-   **Required Flags**: All pipeline commands accept a standard set of flags:
    -   `--config <path>`: Path to the pipeline's YAML configuration.
    -   `--output-dir <path>`: **Required**. The destination directory for all output artifacts.
    -   `--input-file <path>`: (Optional) Path to a local input file.
    -   `--dry-run`: A critical flag that runs the pipeline through the `validate` stage but writes no files. It is used to verify configuration and logic.
-   **Exit Codes**: The CLI returns a `0` exit code on success and a non-zero exit code on failure.

**Example Invocation:**
```bash
python -m bioetl.cli.main activity \
  --config src/bioetl/configs/pipelines/chembl/activity.yaml \
  --output-dir data/output/activity \
  --dry-run
```

## 9. Test Plan

-   **Unit Tests**:
    -   Verify that the `run()` orchestrator calls the stages in the correct order (`extract` -> `transform` -> `validate` -> `write`).
    -   Test that an exception raised in any stage is caught, logged with `exc_info`, and correctly re-raised.
    -   Test that the `RunResult` object is constructed correctly with all expected paths.
-   **Golden Test**:
    -   Create a test that runs a pipeline and captures the primary dataset file and the `meta.yaml` file.
    -   Compare these artifacts against a pre-approved "golden" version committed to the repository. The test fails if there is any byte-level difference, ensuring determinism.
-   **Integration Test**:
    -   Write a test that invokes the pipeline via the CLI (`subprocess.run`).
    -   Assert that a run with the `--dry-run` flag completes with exit code `0`, produces no files in the output directory, and emits valid log messages.

## 10. Minimal Example

### Minimal `PipelineBase` Subclass

```python
# file: src/bioetl/pipelines/minimal_example.py

import pandas as pd
from bioetl.pipelines.base import PipelineBase

class MinimalPipeline(PipelineBase):

    @abstractmethod
    def close_resources(self) -> None:
        """Required by ABC, even if empty."""
        pass

    def extract(self) -> pd.DataFrame:
        print("Extracting data...")
        # In a real pipeline, this would connect to a source.
        return pd.DataFrame([{"id": 1, "data": "raw"}])

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        print("Transforming data...")
        # In a real pipeline, this would apply business logic.
        df["data"] = df["data"].str.upper()
        df["transformed"] = True
        return df
```

### Stage Mapping Table

| Stage       | Extension Points (Developer Implements) | Invariants (Framework Guarantees)                               |
|-------------|-----------------------------------------|-----------------------------------------------------------------|
| **`extract`**   | `extract()` method, API clients         | A `pd.DataFrame` is produced; stage is timed and logged.        |
| **`transform`** | `transform()` method, normalizers       | A `pd.DataFrame` is produced; stage is timed and logged.        |
| **`validate`**  | Pandera schema (`schema_out.py`)        | Fails fast on any schema violation; enforces column order.      |
| **`write`**     | (None)                                  | Output is atomic; `meta.yaml` is generated; data is sorted.     |
| **`run`**       | (None)                                  | Stages run in fixed order; exceptions are handled.              |
| **`cleanup`**   | `close_resources()` method              | Always called, even on failure.                                 |
