# PipelineBase Orchestrator Specification

> **Note**: Implementation status: **planned**. All file paths referencing `src/bioetl/` in this document describe the intended architecture and are not yet implemented in the codebase.

This document provides a comprehensive technical specification for the `PipelineBase` abstract class, which serves as the core orchestrator for all ETL pipelines within the `bioetl` framework.

[ref: repo:README.md@refactoring_001]

## 1. Overview and Goals

The `PipelineBase` class standardizes the ETL process by defining a fixed, four-stage lifecycle: `extract` → `transform` → `validate` → `write`, orchestrated by the `run()` method. Its primary purpose is to abstract away the boilerplate of pipeline orchestration, allowing developers to focus on source-specific business logic.

**Boundaries of Responsibility:**

-   **`PipelineBase` (Framework)** is responsible for:
    -   Orchestrating the sequence of stages.
    -   Injecting configuration (`PipelineConfig`).
    -   Managing the logging context and stage timers.
    -   Handling exceptions at a high level.
    -   Performing the final validation step against a provided schema.
    -   Atomically writing the output dataset and all associated metadata artifacts.
    -   Enforcing determinism through sorting and hashing.

-   **Concrete Implementation (Developer)** is responsible for:
    -   Implementing the logic to connect to and extract data from a specific source (e.g., API clients, database connections).
    -   Implementing the business logic to parse, clean, and transform the raw data.
    -   Providing the Pandera schema for the final data structure.

## 2. Public API (Signatures and Types)

The public API is defined by the `PipelineBase` abstract class and a set of dataclasses for structured results. The implementation is based on Python's `abc` module.

[ref: repo:src/bioetl/pipelines/base.py@refactoring_001]

### Result Types

These dataclasses define the expected structure of the results returned by the `write` and `run` stages. They are based on the `OutputArtifacts` class found in the codebase.

```python
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Dict

@dataclass(frozen=True)
class WriteResult:
    """Core artifacts from a successful write operation."""
    dataset: Path
    quality_report: Path
    metadata: Path

@dataclass(frozen=True)
class RunResult:
    """All artifacts generated by a complete pipeline run."""
    write_result: WriteResult
    run_directory: Path
    manifest: Path
    additional_datasets: Dict[str, Path]
    qc_summary: Optional[Path]
    debug_dataset: Optional[Path]
```

### `PipelineBase` Abstract Class

All pipelines **must** inherit from this class.

```python
import pandas as pd
from abc import ABC, abstractmethod
from typing import Any
from bioetl.config import PipelineConfig

class PipelineBase(ABC):
    """
    The abstract contract for all ETL pipelines.
    """
    def __init__(self, config: PipelineConfig, run_id: str):
        """Initializes the pipeline with its configuration and a unique run ID."""
        ...

    @abstractmethod
    def extract(self, *args: Any, **kwargs: Any) -> pd.DataFrame:
        """
        Extracts data from the source and returns it as a DataFrame.
        """
        raise NotImplementedError

    @abstractmethod
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Applies business logic to transform the raw DataFrame.
        """
        raise NotImplementedError

    def validate(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        A framework-managed method that validates the DataFrame against a Pandera schema.
        This method should not be overridden.
        """
        ...

    def write(self, df: pd.DataFrame, output_path: Path, extended: bool = False) -> RunResult:
        """
        A framework-managed method that writes the DataFrame and all metadata artifacts.
        This maps to the `export` method in the current implementation.
        This method should not be overridden.
        """
        ...

    def run(self, output_path: Path, extended: bool = False, *args: Any, **kwargs: Any) -> RunResult:
        """

        Orchestrates the entire pipeline lifecycle.
        This method should not be overridden.
        """
        ...
```

## 3. Lifecycle and `run()` Pseudocode

The `run()` method executes the ETL stages in a fixed sequence. It provides robust error handling, logging, and timing for each stage.

### Full `run()` Pseudocode

This pseudocode is a realistic representation of the orchestration logic within `PipelineBase.run()`.

```python
import time
from bioetl.core.logger import UnifiedLogger, set_run_context

# This pseudocode resides within the PipelineBase class.

def run(self, output_path: Path, extended: bool = False, *args: Any, **kwargs: Any) -> "RunResult":
    """
    Orchestrates the full pipeline lifecycle: extract → transform → validate → write.
    """
    log = UnifiedLogger.get(__name__)
    set_run_context(
        run_id=self.run_id,
        stage="bootstrap",
        actor="system", # Or could be derived from user context
        source=self.config.pipeline.entity
    )
    log.info("pipeline_started", pipeline_name=self.config.pipeline.name)

    stage_durations = {}

    try:
        # --- EXTRACT STAGE ---
        set_run_context(stage="extract")
        log.info("extract_started")
        extract_start = time.perf_counter()

        raw_df = self.extract(*args, **kwargs)

        stage_durations["extract"] = (time.perf_counter() - extract_start) * 1000.0
        log.info("extract_completed", duration_ms=stage_durations["extract"], rows=len(raw_df))

        # --- TRANSFORM STAGE ---
        set_run_context(stage="transform")
        log.info("transform_started")
        transform_start = time.perf_counter()

        transformed_df = self.transform(raw_df)

        stage_durations["transform"] = (time.perf_counter() - transform_start) * 1000.0
        log.info("transform_completed", duration_ms=stage_durations["transform"], rows=len(transformed_df))

        # --- VALIDATE STAGE ---
        set_run_context(stage="validate")
        log.info("validate_started")
        validate_start = time.perf_counter()

        validated_df = self.validate(transformed_df)

        stage_durations["validate"] = (time.perf_counter() - validate_start) * 1000.0
        log.info("validate_completed", duration_ms=stage_durations["validate"], rows=len(validated_df))

        # --- WRITE STAGE ---
        set_run_context(stage="write")
        log.info("write_started")
        write_start = time.perf_counter()

        # The 'write' method handles file writing and artifact generation.
        # Note: In some implementations, this method may be internally named 'export',
        # but the public API consistently uses 'write'.
        artifacts: "OutputArtifacts" = self.write(validated_df, output_path, extended=extended)

        stage_durations["write"] = (time.perf_counter() - write_start) * 1000.0
        log.info("write_completed", duration_ms=stage_durations["write"], artifacts_path=str(artifacts.run_directory))

        # Construct the final RunResult object from the generated artifacts
        write_result = WriteResult(
            dataset=artifacts.dataset,
            quality_report=artifacts.quality_report,
            metadata=artifacts.metadata
        )
        run_result = RunResult(
            write_result=write_result,
            run_directory=artifacts.run_directory,
            manifest=artifacts.manifest,
            additional_datasets=artifacts.additional_datasets,
            qc_summary=artifacts.qc_summary,
            debug_dataset=artifacts.debug_dataset
        )

        log.info("pipeline_completed_successfully")
        return run_result

    except Exception as e:
        # Enrich the exception with context and re-raise to ensure failure
        log.error("pipeline_failed", error=str(e), exc_info=True)
        # In a real implementation, you might wrap the exception with a custom PipelineError
        raise e

    finally:
        # --- CLEANUP STAGE ---
        set_run_context(stage="cleanup")
        log.info("cleanup_started")
        self.close_resources() # A method for closing connections, etc.
        log.info("cleanup_completed")

```

### Stage Contracts and Guarantees

The real implementation in `src/bioetl/pipelines/base.py` formalises the public
contract of each lifecycle stage. The table below summarises what the
orchestrator guarantees, what implementers must provide, and the determinism or
idempotency expectations that are enforced at runtime.【F:docs/pipelines/00-pipeline-base.md†L109-L170】

| Stage | Framework responsibilities | Implementer responsibilities | Determinism & idempotency requirements |
| --- | --- | --- | --- |
| `extract` | Sets logging context to `stage="extract"`, records wall-clock
timing, and preserves any CLI `limit`/`sample` options for deterministic
sampling. Re-raises exceptions with full tracebacks. | Implement a
side-effect-free extractor that returns a Pandas `DataFrame`. Use
`PipelineBase.read_input_table` for file inputs to inherit consistent logging,
deterministic limiting, and empty-file handling. | For deterministic re-runs,
the extractor must respect the same configuration and runtime options. When
using runtime limits, the same subset must be returned for the same seed order
and upstream source state.【F:docs/pipelines/00-pipeline-base.md†L172-L213】
| `transform` | Switches the logging context to `stage="transform"`, captures
duration, and propagates the DataFrame returned by `extract`. | Implement all
transformations as pure functions (no mutation of external state). May call
`execute_enrichment_stages` to run registered enrichment hooks. | Transformations
must be deterministic for identical inputs—avoid reliance on wall-clock time or
random ordering unless a seeded random generator is used.【F:docs/pipelines/00-pipeline-base.md†L215-L255】
| `validate` | Fetches the schema from the Unified Schema registry, enforces
column order, runs Pandera validation, records QC metrics, and accumulates
issues through `record_validation_issue`. | Do not override unless absolutely
required. Register additional schemas through configuration and call
`run_schema_validation` for secondary datasets. | Validation is deterministic
for identical input frames and schema versions. Non-fatal severities return the
input frame unchanged but still log issues for auditability.【F:docs/pipelines/00-pipeline-base.md†L257-L322】
| `export` (write) | Applies determinism rules (column order, stable sort,
hash policies), writes the dataset and all QC artefacts via the configured
output writer, and persists `stage_durations_ms`. | Optional overrides should
call `PipelineBase.export` to benefit from the shared behaviour. Use
`set_export_metadata_from_dataframe` and `finalize_with_standard_metadata` to
standardise metadata prior to writing. | Output files must be reproducible: the
same input DataFrame and determinism settings produce byte-identical CSV files
and metadata manifests.【F:docs/pipelines/00-pipeline-base.md†L324-L392】
| `cleanup` | Resets logging context to `stage="cleanup"`, removes transient
runtime options, closes registered API clients, and calls `close_resources`. |
Release any additional resources inside `close_resources` without raising
exceptions. | Cleanup has no external side effects beyond releasing resources,
preserving idempotency for subsequent runs.【F:docs/pipelines/00-pipeline-base.md†L394-L428】

### Retry and Backoff Expectations

`PipelineBase` centralises HTTP/client creation through helper methods that
inject the project-wide retry and backoff policies. Pipelines should:

1. Call `init_chembl_client` to obtain a `ChemblClientContext`, which wraps the
   shared `UnifiedAPIClient` so that the default retry/backoff, throttling, and
   observability policies are applied consistently.
2. Register any instantiated clients via `register_client` so that the
   orchestrator can dispose of them safely during cleanup (even on failure).
3. Prefer `read_input_table` for disk inputs—`limit`/`sample` runtime options are
   honoured automatically, missing files become structured warnings, and the
   resolved path is logged for traceability.

Custom retry loops should compose with the defaults (for example by decorating
client calls with additional `backoff.on_exception` policies) rather than
replacing them. This keeps failure semantics and observability consistent across
pipelines.【F:docs/pipelines/00-pipeline-base.md†L430-L470】

## 4. Configuration and DI

The pipeline receives its configuration via Dependency Injection (DI) in the constructor. A `PipelineConfig` object, loaded and validated from a YAML file, is passed in during initialization.

[ref: repo:src/bioetl/configs/models.py@refactoring_001]

-   **Priority and Overlays**: The configuration system supports profiles via the `extends` key, allowing a pipeline-specific YAML file (e.g., `activity.yaml`) to inherit from and override values in base files (e.g., `base.yaml`, `determinism.yaml`).
-   **Strictness**: The Pydantic models are configured with `extra="forbid"`. This is a critical feature that causes the configuration loading to fail if the YAML file contains any keys that are not explicitly defined in the models, preventing typos and "silent" configuration errors.

## 5. Logging and Telemetry

The framework provides a unified, structured logging system based on
`structlog`. `PipelineBase.run()` enriches every record with mandatory context
fields (`run_id`, `stage`, `actor`, `source`) and captures wall-clock durations
for extract/transform/validate/write stages in milliseconds. Additional
artifacts such as the QC summary and metadata files embed the same timings via
`stage_durations_ms`.【F:docs/pipelines/00-pipeline-base.md†L472-L522】

### Stage Event Catalogue

The table below enumerates the core log events emitted by the orchestrator. All
events inherit the mandatory fields described above, and many attach additional
payload fields as shown.

| Stage | Event | Additional fields |
| --- | --- | --- |
| Bootstrap | `pipeline_initialized` | `pipeline`, `run_id`
| Bootstrap | `pipeline_started` | `pipeline`
| Extract | `reading_input` | `path`, optional `limit`
| Extract | `input_file_not_found` | `path`
| Extract | `input_limit_active` | `limit`, `rows`
| Extract | `extraction_completed` | `rows`, `duration_ms`
| Transform | `transformation_completed` | `rows`, `duration_ms`
| Transform | `enrichment_stage_*` | `stage`, plus `reason`, `rows`, or `error`
| Validate | `schema_validation_error` | `dataset`, `column`, `check`, `count`, `severity`
| Validate | `schema_validation_failed` | `dataset`, `errors`, `error`
| Validate | `validation_completed` | `rows`, `duration_ms`
| Export | `exporting_data` | `path`, `rows`
| Export | `pipeline_completed` | `artifacts`, `load_duration_ms`
| Cleanup | `pipeline_resource_cleanup_failed` | `error`
| Any | `pipeline_failed` | `error`, `exc_info`

These events, combined with the logger's redaction processors, form the minimum
telemetry contract. Pipelines may emit additional structured logs (for example,
per-API-call retries) provided they do not remove the baseline context.

### Extension Hooks

Implementers can extend `PipelineBase` behaviour without reimplementing the
orchestrator by using the following hook surface:

- **Abstract methods** – `extract`, `transform`, and `close_resources` must be
  implemented by every concrete pipeline. `validate` should only be overridden
  to customise schema loading.
- **Stage utilities** – `read_input_table`, `execute_enrichment_stages`,
  `run_schema_validation`, and `finalize_with_standard_metadata` encapsulate
  shared orchestration logic and should be preferred over bespoke
  implementations.
- **QC helpers** – `set_stage_summary`, `add_qc_summary_section(s)`,
  `set_qc_metrics`, `record_validation_issue`, `refresh_validation_issue_summary`
  keep validation and enrichment telemetry consistent.
- **Metadata helpers** – `set_export_metadata_from_dataframe`,
  `set_export_metadata`, and `add_additional_table` feed the writer with the
  required context for deterministic artefact generation.
- **Client lifecycle** – `init_chembl_client`, `register_client`, and
  `reset_stage_context` provide safe resource management and per-stage state.

All hooks are idempotent when invoked with identical inputs and configuration,
supporting reproducible pipeline runs.【F:docs/pipelines/00-pipeline-base.md†L524-L586】

## 6. Determinism and Artifacts

The framework guarantees that a pipeline run with the same configuration will produce a bit-for-bit identical output.

-   **Stable Sorting**: Before writing, the final DataFrame is sorted by the columns specified in the `write.sort_by` key in the configuration.
-   **Integrity Hashes**: The `write` stage calculates two critical hashes that are stored in `meta.yaml`:
    -   `hash_business_key`: A hash of the columns forming the unique business identifier.
    -   `hash_row`: A hash of all columns specified, ensuring row-level integrity.
-   **`meta.yaml`**: This artifact is the run's "birth certificate," recording the `run_id`, configuration hash, source versions, `row_count`, all hashes, and stage timings.
-   **Invariant**: A repeated run with an identical configuration against an identical source state **must** produce identical `meta.yaml` hashes and an identical primary dataset file hash.

## 7. Validation Contracts

Data validation is a non-negotiable stage of the pipeline, enforced by Pandera schemas.

-   **Mandatory Checks**: Every output schema **must** enforce:
    -   **Fixed Column Order**: `class Config: ordered = True`.
    -   **Strict Data Types**: All columns must have a specific type (e.g., `Series[Int64]`). Coercion is allowed (`pa.Field(coerce=True)`).
    -   **Business Key Uniqueness**: The primary business key column must be marked with `unique=True`.
-   **Schema Evolution**: Schemas must be versioned. Any backward-incompatible changes require a major version bump. The schema version is recorded in the `meta.yaml` file.

## 8. CLI Integration

Pipelines are automatically discovered and exposed as commands in the Typer-based CLI.

[ref: repo:src/bioetl/cli/app.py@refactoring_001]

-   **Registration**: Placing a `PipelineBase` subclass in the `src/bioetl/pipelines/` directory is sufficient to register it as a new CLI command.
-   **Required Flags**: All pipeline commands accept a standard set of flags:
    -   `--config <path>`: Path to the pipeline's YAML configuration.
    -   `--output-dir <path>`: **Required**. The destination directory for all output artifacts.
    -   `--input-file <path>`: (Optional) Path to a local input file.
    -   `--dry-run`: A critical flag that runs the pipeline through the `validate` stage but writes no files. It is used to verify configuration and logic.
-   **Exit Codes**: The CLI returns a `0` exit code on success and a non-zero exit code on failure.

**Example Invocation:**
```bash
python -m bioetl.cli.main activity \
  --config src/bioetl/configs/pipelines/chembl/activity.yaml \
  --output-dir data/output/activity \
  --dry-run
```

## 9. Test Plan

-   **Unit Tests**:
    -   Verify that the `run()` orchestrator calls the stages in the correct order (`extract` -> `transform` -> `validate` -> `write`).
    -   Test that an exception raised in any stage is caught, logged with `exc_info`, and correctly re-raised.
    -   Test that the `RunResult` object is constructed correctly with all expected paths.
-   **Golden Test**:
    -   Create a test that runs a pipeline and captures the primary dataset file and the `meta.yaml` file.
    -   Compare these artifacts against a pre-approved "golden" version committed to the repository. The test fails if there is any byte-level difference, ensuring determinism.
-   **Integration Test**:
    -   Write a test that invokes the pipeline via the CLI (`subprocess.run`).
    -   Assert that a run with the `--dry-run` flag completes with exit code `0`, produces no files in the output directory, and emits valid log messages.

## 10. Minimal Example

### Minimal `PipelineBase` Subclass

```python
# file: src/bioetl/pipelines/minimal_example.py

import pandas as pd
from bioetl.pipelines.base import PipelineBase

class MinimalPipeline(PipelineBase):

    @abstractmethod
    def close_resources(self) -> None:
        """Required by ABC, even if empty."""
        pass

    def extract(self) -> pd.DataFrame:
        print("Extracting data...")
        # In a real pipeline, this would connect to a source.
        return pd.DataFrame([{"id": 1, "data": "raw"}])

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        print("Transforming data...")
        # In a real pipeline, this would apply business logic.
        df["data"] = df["data"].str.upper()
        df["transformed"] = True
        return df
```

### Stage Mapping Table

| Stage       | Extension Points (Developer Implements) | Invariants (Framework Guarantees)                               |
|-------------|-----------------------------------------|-----------------------------------------------------------------|
| **`extract`**   | `extract()` method, API clients         | A `pd.DataFrame` is produced; stage is timed and logged.        |
| **`transform`** | `transform()` method, normalizers       | A `pd.DataFrame` is produced; stage is timed and logged.        |
| **`validate`**  | Pandera schema (`schema_out.py`)        | Fails fast on any schema violation; enforces column order.      |
| **`write`**     | (None)                                  | Output is atomic; `meta.yaml` is generated; data is sorted.     |
| **`run`**       | (None)                                  | Stages run in fixed order; exceptions are handled.              |
| **`cleanup`**   | `close_resources()` method              | Always called, even on failure.                                 |
