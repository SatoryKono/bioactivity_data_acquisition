"""Core pipeline orchestration utilities.

This module provides the abstract base class PipelineBase that defines the
contract and lifecycle for all ETL pipelines in the bioetl framework.
"""

from __future__ import annotations

import hashlib
import json
import os
import subprocess
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass
from dataclasses import field as dataclass_field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

import pandas as pd
import pandera as pa
import yaml

from bioetl.configs.models import CacheConfig, PipelineConfig
from bioetl.core.api_client import UnifiedAPIClient
from bioetl.core.client_factory import APIClientFactory
from bioetl.core.logger import bind_global_context, get_logger
from bioetl.core.schema_registry import get_registry


@dataclass(frozen=True)
class WriteResult:
    """Core artifacts from a successful write operation."""

    dataset: Path
    quality_report: Path
    metadata: Path


@dataclass(frozen=True)
class RunResult:
    """All artifacts generated by a complete pipeline run."""

    write_result: WriteResult
    run_directory: Path
    manifest: Path
    additional_datasets: dict[str, Path] = dataclass_field(default_factory=dict)
    qc_summary: Path | None = None
    debug_dataset: Path | None = None


class PipelineBase(ABC):
    """Abstract base class defining the contract for all ETL pipelines.

    The PipelineBase class standardizes the ETL process by defining a fixed,
    four-stage lifecycle: extract → transform → validate → write, orchestrated
    by the run() method.
    """

    def __init__(self, config: PipelineConfig, run_id: str) -> None:
        """Initialize the pipeline with its configuration and a unique run ID.

        Args:
            config: Pipeline configuration validated from YAML.
            run_id: Unique identifier for this pipeline run (UUID format).
        """
        self.config = config
        self.run_id = run_id
        self.logger = get_logger(__name__)

        # Determine source identifier from config
        source = self._determine_source()

        # Bind initial logging context
        bind_global_context(
            run_id=self.run_id,
            pipeline=self.config.pipeline.name,
            stage="bootstrap",
            actor=self.config.pipeline.name,
            source=source,
            dataset=self.config.pipeline.name,
            component="pipeline_base",
        )

        # Initialize client registry for cleanup
        self._registered_clients: list[Any] = []

        # Initialize API client factory
        self._client_factory = APIClientFactory(config)

        # Initialize schema registry
        self._schema_registry = get_registry()

        # Stage durations will be tracked here
        self._stage_durations: dict[str, float] = {}

        self.logger.info("pipeline_initialized", pipeline=self.config.pipeline.name, run_id=self.run_id)

    def _determine_source(self) -> str:
        """Determine the source identifier from configuration."""
        # Try to get source from first enabled source in config
        for source_name, source_config in self.config.sources.items():
            if source_config.enabled:
                return source_name
        # Fallback to pipeline name if no source found
        return self.config.pipeline.name

    @abstractmethod
    def extract(self, *args: Any, **kwargs: Any) -> pd.DataFrame:
        """Extract data from the source and return it as a DataFrame.

        This method MUST be implemented by the developer. It is responsible
        for all source interaction, including API calls, database queries,
        or reading from files.

        Returns:
            DataFrame containing raw, unmodified data from the source.
        """
        raise NotImplementedError

    @abstractmethod
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform the raw extracted data into the target schema.

        This method MUST be implemented by the developer. It includes all
        business logic: cleaning, normalization, type casting, column renaming,
        and enrichment.

        Args:
            df: Raw DataFrame from extract stage.

        Returns:
            DataFrame conforming to the structure expected by the output
            Pandera schema.
        """
        raise NotImplementedError

    def validate(self, df: pd.DataFrame) -> pd.DataFrame:
        """Validate the transformed data against the output Pandera schema.

        This method is managed by the framework and SHOULD NOT be overridden.
        It enforces strict data types, column order, and custom checks.

        Args:
            df: Transformed DataFrame to validate.

        Returns:
            Validated DataFrame with enforced column order and types.

        Raises:
            pandera.errors.SchemaError: If validation fails.
        """
        bind_global_context(stage="validate")
        self.logger.info("validation_started", rows=len(df))

        validate_start = time.perf_counter()

        try:
            if not self.config.validation.schema_out:
                self.logger.warning("no_schema_configured", message="No schema_out specified, skipping validation")
                return df

            # Load schema from dotted path
            schema = self._load_schema(self.config.validation.schema_out)

            # Apply validation (strict and coerce are schema-level settings)
            validated_df_raw: pd.DataFrame = schema.validate(df, lazy=False)
            validated_df = pd.DataFrame(validated_df_raw)

            # Ensure column order matches config if specified
            if self.config.determinism.column_order:
                validated_df = validated_df[self.config.determinism.column_order]

            duration_ms = (time.perf_counter() - validate_start) * 1000.0
            self._stage_durations["validate"] = duration_ms
            self.logger.info("validation_completed", rows=len(validated_df), duration_ms=duration_ms)

            return validated_df

        except pa.errors.SchemaError as e:
            duration_ms = (time.perf_counter() - validate_start) * 1000.0
            self.logger.error(
                "validation_failed",
                errors=str(e),
                duration_ms=duration_ms,
                exc_info=True,
            )
            raise

    def _load_schema(self, dotted_path: str) -> Any:
        """Load a Pandera schema from a dotted module path.

        Uses SchemaRegistry for caching and improved error handling.

        Args:
            dotted_path: Dotted path like 'bioetl.schemas.chembl.activity_out.ActivitySchema'

        Returns:
            Pandera DataFrameSchema instance or SchemaModel class.

        Raises:
            ImportError: If module cannot be imported.
            AttributeError: If schema class cannot be found.
            ValueError: If schema path format is invalid.
        """
        try:
            schema = self._schema_registry.load_schema(dotted_path)
            return schema
        except (ImportError, AttributeError, ValueError) as e:
            self.logger.error(
                "schema_load_failed",
                schema_path=dotted_path,
                error=str(e),
                exc_info=True,
            )
            raise

    def write(
        self,
        df: pd.DataFrame,
        output_path: Path,
        extended: bool = False,
    ) -> WriteResult:
        """Write the final dataset and metadata to the output directory.

        This method is managed by the framework and SHOULD NOT be overridden.
        It handles deterministic sorting, hashing, atomic file writes, and
        generation of all standard artifacts.

        Args:
            df: Validated DataFrame to write.
            output_path: Base output directory path.
            extended: Whether to generate extended artifacts.

        Returns:
            WriteResult containing paths to generated artifacts.
        """
        bind_global_context(stage="write")
        self.logger.info("write_started", path=str(output_path), rows=len(df))

        write_start = time.perf_counter()

        # Ensure output directory exists
        output_path.mkdir(parents=True, exist_ok=True)

        # Apply determinism: sorting
        if self.config.determinism.enabled and self.config.determinism.sort.by:
            ascending: bool | list[bool]
            if self.config.determinism.sort.ascending:
                if len(self.config.determinism.sort.ascending) == len(self.config.determinism.sort.by):
                    ascending = self.config.determinism.sort.ascending
                else:
                    ascending = [True] * len(self.config.determinism.sort.by)
            else:
                ascending = True

            # Type ignore needed because pandas accepts both bool and list[bool] for ascending
            df = df.sort_values(  # type: ignore[call-overload]
                by=self.config.determinism.sort.by,
                ascending=ascending,
                na_position=self.config.determinism.sort.na_position,
            )

        # Ensure column order
        if self.config.determinism.column_order:
            df = df[self.config.determinism.column_order]

        # Generate artifact paths
        pipeline_name = self.config.pipeline.name
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
        stem = f"{pipeline_name}_{timestamp}"

        dataset_path = output_path / f"{stem}.csv"
        quality_report_path = output_path / f"{stem}_quality_report.csv"
        metadata_path = output_path / f"{stem}_meta.yaml"

        # Compute hashes
        hash_business_key = self._compute_business_key_hash(df)
        hash_row_sample = self._compute_row_hash_sample(df)

        # Generate quality report
        quality_report = self._generate_quality_report(df)

        # Write dataset atomically
        self._write_csv_atomic(df, dataset_path)

        # Write quality report atomically
        self._write_csv_atomic(quality_report, quality_report_path)

        # Generate and write metadata
        metadata = self._generate_metadata(df, dataset_path, quality_report_path, hash_business_key, hash_row_sample)
        self._write_yaml_atomic(metadata, metadata_path)

        duration_ms = (time.perf_counter() - write_start) * 1000.0
        self._stage_durations["write"] = duration_ms

        self.logger.info(
            "write_completed",
            duration_ms=duration_ms,
            artifacts_path=str(output_path),
        )

        return WriteResult(
            dataset=dataset_path,
            quality_report=quality_report_path,
            metadata=metadata_path,
        )

    def _compute_business_key_hash(self, df: pd.DataFrame) -> str:
        """Compute hash for business key columns."""
        if not self.config.determinism.hashing.business_key_fields:
            return ""

        hash_algo = hashlib.new(self.config.determinism.hashing.algorithm)

        # Concatenate business key values in deterministic order
        for _, row in df.iterrows():
            key_values = []
            for hash_field in self.config.determinism.hashing.business_key_fields:
                if hash_field in df.columns:
                    value = row[hash_field]
                    # Canonicalize value
                    canonical = self._canonicalize_value(value)
                    key_values.append(str(canonical))
            key_str = "|".join(key_values)
            hash_algo.update(key_str.encode("utf-8"))

        return f"{self.config.determinism.hashing.algorithm}:{hash_algo.hexdigest()}"

    def _compute_row_hash_sample(self, df: pd.DataFrame) -> str:
        """Compute hash for sample row (first row after sorting)."""
        if df.empty or not self.config.determinism.hashing.row_fields:
            return ""

        # Use first row as sample
        row = df.iloc[0]
        row_dict = {}

        for hash_field in self.config.determinism.hashing.row_fields:
            if hash_field in df.columns and hash_field not in self.config.determinism.hashing.exclude_fields:
                value = row[hash_field]
                if pd.notna(value):
                    canonical = self._canonicalize_value(value)
                    row_dict[hash_field] = canonical

        # Serialize to JSON with sorted keys
        json_str = json.dumps(row_dict, sort_keys=True, ensure_ascii=False)
        hash_algo = hashlib.new(self.config.determinism.hashing.algorithm)
        hash_algo.update(json_str.encode("utf-8"))

        return f"{self.config.determinism.hashing.algorithm}:{hash_algo.hexdigest()}"

    def _canonicalize_value(self, value: Any) -> Any:
        """Canonicalize a value for deterministic hashing."""
        if pd.isna(value):
            return ""
        if isinstance(value, str):
            return value.strip().lower()
        if isinstance(value, (int, float)):
            if isinstance(value, float):
                return round(value, self.config.determinism.float_precision)
            return value
        if isinstance(value, pd.Timestamp):
            return value.tz_convert(timezone.utc).isoformat()
        if isinstance(value, (list, dict)):
            return json.dumps(value, sort_keys=True, ensure_ascii=False)
        return str(value)

    def _generate_quality_report(self, df: pd.DataFrame) -> pd.DataFrame:
        """Generate quality report DataFrame."""
        report_data = {
            "metric": [
                "row_count",
                "column_count",
                "null_count",
                "duplicate_count",
            ],
            "value": [
                len(df),
                len(df.columns),
                df.isnull().sum().sum(),
                df.duplicated().sum(),
            ],
        }
        return pd.DataFrame(report_data)

    def _generate_metadata(
        self,
        df: pd.DataFrame,
        dataset_path: Path,
        quality_report_path: Path,
        hash_business_key: str,
        hash_row_sample: str,
    ) -> dict[str, Any]:
        """Generate metadata dictionary for meta.yaml."""
        # Get git commit hash if available
        git_commit = self._get_git_commit()

        # Compute config hash
        config_hash = self._compute_config_hash()

        metadata: dict[str, Any] = {
            "pipeline": {
                "name": self.config.pipeline.name,
                "version": self.config.pipeline.version,
            },
            "run_id": self.run_id,
            "generated_at_utc": datetime.now(timezone.utc).isoformat(),
            "row_count": len(df),
            "artifacts": {
                "dataset": str(dataset_path),
                "quality_report": str(quality_report_path),
            },
            "hash_business_key": hash_business_key,
            "hash_row_sample": hash_row_sample,
            "hash_algo": self.config.determinism.hashing.algorithm,
            "stage_durations_ms": self._stage_durations.copy(),
        }

        if git_commit:
            metadata["git_commit"] = git_commit

        if config_hash:
            metadata["config_hash"] = config_hash

        if self.config.determinism.column_order:
            metadata["column_order"] = self.config.determinism.column_order

        return metadata

    def _get_git_commit(self) -> str | None:
        """Get current git commit hash."""
        try:
            result = subprocess.run(
                ["git", "rev-parse", "HEAD"],
                capture_output=True,
                text=True,
                check=False,
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception:
            pass
        return None

    def _compute_config_hash(self) -> str | None:
        """Compute hash of configuration."""
        try:
            # Serialize config to JSON and hash
            config_json = self.config.model_dump_json()
            hash_algo = hashlib.new("sha256")
            hash_algo.update(config_json.encode("utf-8"))
            return f"sha256:{hash_algo.hexdigest()}"
        except Exception:
            return None

    def _write_csv_atomic(self, df: pd.DataFrame, path: Path) -> None:
        """Write DataFrame to CSV atomically."""
        tmp_path = path.with_suffix(path.suffix + ".tmp")

        # Prepare CSV writing parameters
        csv_config = self.config.determinism.serialization.csv
        quoting_map = {
            "ALL": 1,
            "MINIMAL": 0,
            "NONNUMERIC": 2,
            "NONE": 3,
        }
        quoting = quoting_map.get(csv_config.quoting.upper(), 1)

        # Type ignore needed because pandas to_csv accepts string paths and mypy has issues with overloads
        df.to_csv(  # type: ignore[call-overload]
            str(tmp_path),
            index=False,
            sep=csv_config.separator,
            quoting=quoting,
            na_rep=csv_config.na_rep,
            encoding="utf-8",
            lineterminator="\n",
        )

        # Flush and sync
        tmp_path.parent.mkdir(parents=True, exist_ok=True)
        with tmp_path.open("rb") as f:
            os.fsync(f.fileno())

        # Atomic rename
        os.replace(tmp_path, path)

    def _write_yaml_atomic(self, data: dict[str, Any], path: Path) -> None:
        """Write dictionary to YAML atomically."""
        tmp_path = path.with_suffix(path.suffix + ".tmp")

        path.parent.mkdir(parents=True, exist_ok=True)

        with tmp_path.open("w", encoding="utf-8") as f:
            yaml.dump(data, f, default_flow_style=False, sort_keys=True, allow_unicode=True)

        # Flush and sync
        with tmp_path.open("rb") as f:
            os.fsync(f.fileno())

        # Atomic rename
        os.replace(tmp_path, path)

    def run(self, output_path: Path, extended: bool = False, *args: Any, **kwargs: Any) -> RunResult:
        """Orchestrate the full pipeline lifecycle: extract → transform → validate → write.

        This method is managed by the framework and SHOULD NOT be overridden.
        It ensures the strict execution order and manages logging, timing,
        and exception handling.

        Args:
            output_path: Base output directory for artifacts.
            extended: Whether to generate extended artifacts.
            *args: Additional arguments passed to extract().
            **kwargs: Additional keyword arguments passed to extract().

        Returns:
            RunResult containing all generated artifacts.

        Raises:
            Exception: Re-raises any exception from pipeline stages after logging.
        """
        bind_global_context(stage="bootstrap")
        self.logger.info("pipeline_started", pipeline=self.config.pipeline.name)

        run_result: RunResult | None = None

        try:
            # --- EXTRACT STAGE ---
            bind_global_context(stage="extract")
            self.logger.info("extract_started")
            extract_start = time.perf_counter()

            raw_df = self.extract(*args, **kwargs)

            extract_duration = (time.perf_counter() - extract_start) * 1000.0
            self._stage_durations["extract"] = extract_duration
            self.logger.info("extract_completed", duration_ms=extract_duration, rows=len(raw_df))

            # Apply limit if configured
            if self.config.cli.limit and len(raw_df) > self.config.cli.limit:
                raw_df = raw_df.head(self.config.cli.limit)
                self.logger.info("input_limit_active", limit=self.config.cli.limit, rows=len(raw_df))

            # --- TRANSFORM STAGE ---
            bind_global_context(stage="transform")
            self.logger.info("transform_started")
            transform_start = time.perf_counter()

            transformed_df = self.transform(raw_df)

            transform_duration = (time.perf_counter() - transform_start) * 1000.0
            self._stage_durations["transform"] = transform_duration
            self.logger.info("transform_completed", duration_ms=transform_duration, rows=len(transformed_df))

            # --- VALIDATE STAGE ---
            validated_df = self.validate(transformed_df)

            # --- WRITE STAGE ---
            if not self.config.cli.dry_run:
                write_result = self.write(validated_df, output_path, extended=extended)
            else:
                self.logger.info("write_skipped", reason="dry_run")
                # Create dummy paths for dry run
                pipeline_name = self.config.pipeline.name
                timestamp = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
                stem = f"{pipeline_name}_{timestamp}"
                write_result = WriteResult(
                    dataset=output_path / f"{stem}.csv",
                    quality_report=output_path / f"{stem}_quality_report.csv",
                    metadata=output_path / f"{stem}_meta.yaml",
                )

            # Construct RunResult
            manifest_path = output_path / f"{self.config.pipeline.name}_manifest.json"
            run_result = RunResult(
                write_result=write_result,
                run_directory=output_path,
                manifest=manifest_path,
                additional_datasets={},
            )

            self.logger.info("pipeline_completed_successfully")
            return run_result

        except Exception as e:
            # Enrich exception with context and re-raise
            self.logger.error("pipeline_failed", error=str(e), exc_info=True)
            raise

        finally:
            # --- CLEANUP STAGE ---
            bind_global_context(stage="cleanup")
            self.logger.info("cleanup_started")

            try:
                # Close registered clients
                for client in self._registered_clients:
                    try:
                        if hasattr(client, "close"):
                            client.close()
                        elif hasattr(client, "__exit__"):
                            client.__exit__(None, None, None)
                    except Exception as cleanup_error:
                        self.logger.warning(
                            "client_cleanup_failed",
                            error=str(cleanup_error),
                            client_type=type(client).__name__,
                        )

                # Close client factory
                if hasattr(self, "_client_factory"):
                    self._client_factory.close_all()

                # Call close_resources hook
                self.close_resources()

            except Exception as cleanup_error:
                self.logger.warning("pipeline_resource_cleanup_failed", error=str(cleanup_error))

            self.logger.info("cleanup_completed")

    def register_client(self, client: Any) -> None:
        """Register a client for automatic cleanup during pipeline shutdown.

        Args:
            client: Client object with close() method or context manager.
        """
        self._registered_clients.append(client)

    def init_http_client(
        self,
        source_name: str,
        base_url: str,
        cache_config: CacheConfig | None = None,
    ) -> UnifiedAPIClient:
        """Initialize HTTP client for a source.

        Args:
            source_name: Name of the source (must exist in config.sources).
            base_url: Base URL for the API.
            cache_config: Optional cache configuration (uses config.cache if not provided).

        Returns:
            UnifiedAPIClient instance registered for cleanup.
        """
        client = self._client_factory.create_client(
            source_name=source_name,
            base_url=base_url,
            cache_config=cache_config,
        )

        # Register for automatic cleanup
        self.register_client(client)

        self.logger.info(
            "http_client_initialized",
            source=source_name,
            base_url=base_url,
        )

        return client

    @abstractmethod
    def close_resources(self) -> None:
        """Close any additional resources opened by the pipeline.

        This method is called during cleanup stage. Override to release
        resources specific to your pipeline implementation.
        """
        pass

    def read_input_table(self, path: Path) -> pd.DataFrame:
        """Read input table from file with consistent logging.

        Args:
            path: Path to input file.

        Returns:
            DataFrame read from file.

        Raises:
            FileNotFoundError: If file does not exist.
        """
        if not path.exists():
            self.logger.warning("input_file_not_found", path=str(path))
            raise FileNotFoundError(f"Input file not found: {path}")

        self.logger.info("reading_input", path=str(path))

        # Simple CSV reading - can be extended for other formats
        return pd.read_csv(path)

    def execute_enrichment_stages(self, df: pd.DataFrame) -> pd.DataFrame:
        """Execute registered enrichment hooks.

        This is a placeholder for future enrichment stage execution.
        Override in subclasses if needed.

        Args:
            df: DataFrame to enrich.

        Returns:
            Enriched DataFrame.
        """
        return df

    def set_export_metadata_from_dataframe(self, df: pd.DataFrame) -> dict[str, Any]:
        """Prepare metadata dictionary from DataFrame for export.

        Args:
            df: DataFrame to extract metadata from.

        Returns:
            Dictionary with metadata fields.
        """
        return {
            "row_count": len(df),
            "column_count": len(df.columns),
            "columns": list(df.columns),
        }

