"""Shared utilities and base class for ChEMBL pipelines.

This module provides common functionality for all ChEMBL-based pipelines,
including configuration resolution, API client management, pagination handling,
and data extraction utilities.
"""

from __future__ import annotations

import time
from collections.abc import Callable, Iterable, Mapping, Sequence
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any, Generic, Literal, TypeVar, cast
from urllib.parse import urlparse

import pandas as pd
from pandera import DataFrameSchema
from structlog.stdlib import BoundLogger
from typing_extensions import Self

from bioetl.clients.base import (
    build_filters_payload,
    merge_select_fields,
    normalize_select_fields,
)
from bioetl.clients.chembl_entity_factory import ChemblClientBundle, ChemblEntityClientFactory
from bioetl.config.models.source import SourceConfig
from bioetl.core import APIClientFactory
from bioetl.core.common import ChemblReleaseMixin
from bioetl.core.http import UnifiedAPIClient
from bioetl.core.logging import LogEvents, UnifiedLogger
from bioetl.schemas import SchemaRegistryEntry
from bioetl.schemas.pipeline_contracts import get_out_schema

from bioetl.core.pipeline import PipelineBase

PipelineT = TypeVar("PipelineT", bound="ChemblPipelineBase", contravariant=True)


@dataclass(slots=True)
class ChemblExtractionContext:
    """Holds runtime state for a descriptor-driven extraction run."""

    source_config: Any
    iterator: Any
    chembl_client: Any | None = None
    select_fields: Sequence[str] | None = None
    page_size: int | None = None
    chembl_release: str | None = None
    metadata: dict[str, Any] = field(default_factory=dict)
    extra_filters: dict[str, Any] = field(default_factory=dict)
    iterate_all_kwargs: dict[str, Any] = field(default_factory=dict)
    stats: dict[str, Any] = field(default_factory=dict)


@dataclass(slots=True)
class ChemblExtractionDescriptor(Generic[PipelineT]):
    """Descriptor describing how to execute a ``run_extract_all`` operation."""

    name: str
    source_name: str
    source_config_factory: Callable[[SourceConfig[Any]], Any]
    build_context: Callable[[PipelineT, Any, BoundLogger], ChemblExtractionContext]
    id_column: str
    summary_event: str
    must_have_fields: Sequence[str] = ()
    default_select_fields: Sequence[str] | None = None
    record_transform: Callable[
        [PipelineT, Mapping[str, Any], ChemblExtractionContext],
        Mapping[str, Any],
    ] | None = None
    post_processors: Sequence[
        Callable[[PipelineT, pd.DataFrame, ChemblExtractionContext, BoundLogger], pd.DataFrame]
    ] = ()
    sort_by: Sequence[str] | str | None = None
    empty_frame_factory: Callable[[PipelineT, ChemblExtractionContext], pd.DataFrame] | None = None
    dry_run_handler: Callable[
        [PipelineT, ChemblExtractionContext, BoundLogger, float],
        pd.DataFrame,
    ] | None = None
    hard_page_size_cap: int | None = 25
    summary_extra: Callable[
        [PipelineT, pd.DataFrame, ChemblExtractionContext],
        Mapping[str, Any],
    ] | None = None


@dataclass(slots=True)
class BatchExtractionStats:
    """Summary information generated by :meth:`run_batched_extraction`."""

    requested: int
    rows: int = 0
    batches: int = 0
    api_calls: int | None = None
    cache_hits: int | None = None
    duration_ms: float | None = None
    extra: dict[str, Any] = field(default_factory=dict)

    def as_dict(self) -> dict[str, Any]:
        """Return a dictionary representation suitable for persistence/logging."""

        payload: dict[str, Any] = {
            "requested": self.requested,
            "rows": self.rows,
            "batches": self.batches,
        }
        if self.api_calls is not None:
            payload["api_calls"] = self.api_calls
        if self.cache_hits is not None:
            payload["cache_hits"] = self.cache_hits
        if self.duration_ms is not None:
            payload["duration_ms"] = self.duration_ms
        if self.extra:
            payload.update(self.extra)
        return payload

    def for_logging(self) -> dict[str, Any]:
        """Return keyword arguments appropriate for structured logging."""

        return self.as_dict()

    def set_extra(self, **kwargs: Any) -> None:
        """Merge additional diagnostic metadata into the summary payload."""

        if kwargs:
            self.extra.update(kwargs)


@dataclass(slots=True)
class BatchExtractionContext:
    """Shared state passed to hooks during batch extraction."""

    ids: tuple[str, ...]
    id_column: str
    select_fields: tuple[str, ...]
    limit: int | None
    batch_size: int
    chunk_size: int
    stats: BatchExtractionStats
    log: BoundLogger
    metadata: dict[str, Any] = field(default_factory=dict)
    extra: dict[str, Any] = field(default_factory=dict)

    def increment_batches(self) -> None:
        """Increment processed batch counter."""

        self.stats.batches += 1

    def increment_api_calls(self, *, count: int = 1) -> None:
        """Increment API call counter for diagnostics."""

        if self.stats.api_calls is None:
            self.stats.api_calls = 0
        self.stats.api_calls += count

    def set_cache_hits(self, value: int | None) -> None:
        """Record cache hit counter for diagnostics."""

        self.stats.cache_hits = value

    def set_extra(self, **kwargs: Any) -> None:
        """Merge arbitrary diagnostic metadata into the stats payload."""

        if kwargs:
            self.stats.extra.update(kwargs)

# ChemblClient is dynamically loaded in __init__.py at runtime
# Type checking uses Any for client parameters to avoid circular dependencies


class ChemblPipelineBase(ChemblReleaseMixin, PipelineBase):
    """Base class for ChEMBL-based ETL pipelines.

    This class provides common functionality for all ChEMBL pipelines,
    including configuration resolution, API client management, pagination,
    and data extraction utilities.
    """

    #: Canonical identifier column used when reading CLI-supplied input files.
    id_column: str | None = None

    #: Structured logging event emitted when the extraction mode is resolved.
    extract_event_name: str | None = None

    #: Source label used when identifiers are provided via legacy hooks.
    legacy_extract_source: str = "legacy"

    def build_descriptor(self: Self) -> ChemblExtractionDescriptor[Self]:
        """Return the descriptor used by :meth:`extract_all`."""

        msg = f"{type(self).__name__} must implement build_descriptor()"
        raise NotImplementedError(msg)

    def extract_all(self) -> pd.DataFrame:
        """Extract all records according to the pipeline descriptor."""

        descriptor = self.build_descriptor()
        return self.run_extract_all(descriptor)

    def __init__(self, config: Any, run_id: str) -> None:
        """Initialize the ChEMBL pipeline base.

        Parameters
        ----------
        config
            Pipeline configuration object.
        run_id
            Unique identifier for this pipeline run.
        """
        super().__init__(config, run_id)
        self._client_factory = APIClientFactory(config)
        self._chembl_entity_factory = ChemblEntityClientFactory(
            config,
            api_client_factory=self._client_factory,
        )
        self._api_version: str | None = None
        self._output_schema_entry: SchemaRegistryEntry | None = None
        self._output_schema: DataFrameSchema | None = None
        self._output_column_order: tuple[str, ...] = ()
        self._output_schema_cache: dict[str, Any] = {}

    def configure_output_schema(
        self,
        schema_entry: SchemaRegistryEntry,
        *,
        extra_cache: dict[str, Any] | None = None,
    ) -> None:
        """Configure runtime caches bound to the pipeline output schema."""

        self._output_schema_entry = schema_entry
        self._output_schema = schema_entry.schema
        self._output_column_order = tuple(schema_entry.column_order)
        self._output_schema_cache = dict(extra_cache) if extra_cache is not None else {}

    def resolve_output_schema_entry(self) -> SchemaRegistryEntry:
        """Return the schema registry entry associated with this pipeline.

        Subclasses may override this method when they need to fetch the
        registry entry from a non-standard location. The default
        implementation consults :func:`bioetl.schemas.pipeline_contracts.get_out_schema`
        using the pipeline actor (when available) and finally falls back to the
        configured :attr:`pipeline_code`.
        """

        candidates: list[str] = []

        actor = getattr(self, "actor", None)
        if isinstance(actor, str) and actor.strip():
            actor_code = actor.strip()
        else:
            actor_code = None

        pipeline_code = self.pipeline_code.strip()

        if actor_code and actor_code != pipeline_code:
            candidates.append(actor_code)

        candidates.append(pipeline_code)

        last_error: KeyError | None = None

        for candidate in candidates:
            try:
                return get_out_schema(candidate)
            except KeyError as exc:
                last_error = exc
                continue

        if last_error is not None:
            raise last_error

        msg = "Unable to resolve pipeline output schema"
        raise KeyError(msg)

    def initialize_output_schema(
        self,
        schema_entry: SchemaRegistryEntry | None = None,
        *,
        extra_cache: dict[str, Any] | None = None,
    ) -> None:
        """Resolve and configure the pipeline output schema.

        Parameters
        ----------
        schema_entry
            Optional pre-resolved schema registry entry. When omitted the
            registry entry is obtained via :meth:`resolve_output_schema_entry`.
        extra_cache
            Optional mapping propagated to :meth:`configure_output_schema` for
            callers that need to seed schema-level caches.
        """

        resolved_entry = schema_entry or self.resolve_output_schema_entry()
        self.configure_output_schema(resolved_entry, extra_cache=extra_cache)

    # ------------------------------------------------------------------
    # Normalization helpers
    # ------------------------------------------------------------------

    def _normalize_identifiers(self, df: pd.DataFrame, log: BoundLogger) -> pd.DataFrame:
        """Normalize identifier columns.

        Subclasses are expected to override this method with domain specific
        logic. The default implementation is a no-op so tests can provide
        lightweight pipeline stubs without having to implement normalization.
        """

        return df

    def _normalize_string_fields(self, df: pd.DataFrame, log: BoundLogger) -> pd.DataFrame:
        """Normalize free-text columns.

        Subclasses are expected to override this method. The default
        implementation is intentionally a no-op to avoid forcing all call sites
        to provide custom behaviour.
        """

        return df

    def _normalize_and_enforce_schema(
        self,
        df: pd.DataFrame,
        column_order: Sequence[str],
        log: BoundLogger,
        *,
        normalize_identifiers: bool = True,
        normalize_strings: bool = True,
        order_columns: bool = False,
        copy: bool = True,
    ) -> pd.DataFrame:
        """Apply shared normalization steps and ensure schema columns.

        Parameters
        ----------
        df
            DataFrame to normalize.
        column_order
            Target schema column order used for enforcement and optional
            ordering.
        log
            Logger used for diagnostic output.
        normalize_identifiers
            When True (default) invoke :meth:`_normalize_identifiers`.
        normalize_strings
            When True (default) invoke :meth:`_normalize_string_fields`.
        order_columns
            When True reorder columns using :meth:`_order_schema_columns`.
        copy
            Control whether the DataFrame should be copied before mutation.

        Returns
        -------
        pd.DataFrame
            Normalized DataFrame with all schema columns present.
        """

        working_df = df.copy() if copy else df

        working_df = self._ensure_schema_columns(working_df, column_order, log)

        if normalize_identifiers:
            working_df = self._normalize_identifiers(working_df, log)

        if normalize_strings:
            working_df = self._normalize_string_fields(working_df, log)

        working_df = self._ensure_schema_columns(working_df, column_order, log)

        if order_columns:
            working_df = self._order_schema_columns(working_df, column_order)

        return working_df

    @property
    def api_version(self) -> str | None:
        """Return the cached API version captured during extraction."""
        return self._get_optional_string_value(
            "_api_version", field_name="api_version"
        )

    def _set_api_version(self, value: str | None) -> None:
        """Update the cached API version used by the pipeline."""
        self._set_optional_string_value(
            "_api_version", value, field_name="api_version"
        )

    def extract(self, *args: object, **kwargs: object) -> pd.DataFrame:
        """Dispatch between batch and full extraction modes."""

        log = UnifiedLogger.get(__name__).bind(
            component=self._component_for_stage("extract")
        )

        event_name = self.extract_event_name or f"{self.pipeline_code}.extract_mode"
        id_column_name = self.id_column or self._get_id_column_name()

        legacy_resolver: Callable[[BoundLogger], Sequence[str] | None] | None = None
        if self.has_legacy_extract_support():

            def _legacy(bound_log: BoundLogger) -> Sequence[str] | None:
                return self.resolve_legacy_extract_ids(
                    bound_log, *args, **kwargs
                )

            legacy_resolver = _legacy

        return self._dispatch_extract_mode(
            log,
            event_name=event_name,
            batch_callback=self.extract_by_ids,
            full_callback=self.extract_all,
            id_column_name=id_column_name,
            legacy_id_resolver=legacy_resolver,
            legacy_source=self.legacy_extract_source,
        )

    def has_legacy_extract_support(self) -> bool:
        """Return whether the pipeline exposes a legacy ID resolver."""

        return (
            type(self).resolve_legacy_extract_ids
            is not ChemblPipelineBase.resolve_legacy_extract_ids
        )

    def resolve_legacy_extract_ids(
        self,
        log: BoundLogger,
        *args: object,
        **kwargs: object,
    ) -> Sequence[str] | None:
        """Resolve identifiers supplied via deprecated inputs."""

        return None

    # ------------------------------------------------------------------
    # Configuration resolution methods
    # ------------------------------------------------------------------

    def _resolve_source_config(self, name: str) -> SourceConfig[Any]:
        """Resolve source configuration by name.

        Parameters
        ----------
        name
            Name of the source configuration to resolve.

        Returns
        -------
        SourceConfig
            The resolved source configuration.

        Raises
        ------
        KeyError
            If the source is not configured for this pipeline.
        """
        try:
            return self.config.sources[name]
        except KeyError as exc:
            msg = f"Source '{name}' is not configured for pipeline '{self.pipeline_code}'"
            raise KeyError(msg) from exc

    @staticmethod
    def _stringify_mapping(mapping: Mapping[object, Any]) -> dict[str, Any]:
        """Return mapping with stringified keys preserving values."""

        return {str(key): value for key, value in mapping.items()}

    @staticmethod
    def _normalize_parameters(parameters: Any) -> dict[str, Any]:
        """Return parameters as a plain mapping.

        Parameters
        ----------
        parameters:
            Source configuration parameters represented as Mapping, Pydantic
            model, or arbitrary object with attributes.

        Returns
        -------
        dict[str, Any]
            Normalised mapping with stringified keys preserving deterministic
            ordering semantics for later processing.
        """
        parameters_mapping = getattr(parameters, "parameters_mapping", None)
        if callable(parameters_mapping):
            mapping_candidate = parameters_mapping()
            if isinstance(mapping_candidate, Mapping):
                return {str(key): value for key, value in mapping_candidate.items()}

        if isinstance(parameters, Mapping):
            mapping = cast(Mapping[object, Any], parameters)
            return {str(key): value for key, value in mapping.items()}

        model_dump = getattr(parameters, "model_dump", None)
        if callable(model_dump):
            dumped = model_dump()
            if isinstance(dumped, Mapping):
                mapping = cast(Mapping[object, Any], dumped)
                return {str(key): value for key, value in mapping.items()}

        as_dict = getattr(parameters, "dict", None)
        if callable(as_dict):
            dumped = as_dict()
            if isinstance(dumped, Mapping):
                mapping = cast(Mapping[object, Any], dumped)
                return {str(key): value for key, value in mapping.items()}

        attrs = getattr(parameters, "__dict__", None)
        if isinstance(attrs, dict):
            attr_mapping = cast(dict[str, Any], attrs)
            return {key: value for key, value in attr_mapping.items() if not key.startswith("_")}

        return {}

    @staticmethod
    def _resolve_base_url(parameters: Any) -> str:
        """Resolve base URL from source configuration parameters.

        Parameters
        ----------
        parameters
            Source configuration parameters mapping.

        Returns
        -------
        str
            The resolved base URL, normalized (trailing slash removed).

        Raises
        ------
        ValueError
            If base_url is not a non-empty string.
        """
        params = ChemblPipelineBase._normalize_parameters(parameters)
        base_url = params.get("base_url") or "https://www.ebi.ac.uk/chembl/api/data"
        if not isinstance(base_url, str) or not base_url.strip():
            msg = "sources.chembl.parameters.base_url must be a non-empty string"
            raise ValueError(msg)
        return base_url.rstrip("/")

    @staticmethod
    def _resolve_page_size(batch_size: int, limit: int | None, *, hard_cap: int = 25) -> int:
        """Return deterministic page size respecting limit and API hard cap."""

        effective = min(max(int(batch_size), 1), hard_cap)
        if limit is not None:
            effective = min(effective, max(int(limit), 0))
        return max(effective, 1)

    @staticmethod
    def _resolve_batch_size(source_config: SourceConfig[Any]) -> int:
        """Resolve batch size from source configuration.

        Parameters
        ----------
        source_config
            Source configuration object.

        Returns
        -------
        int
            The resolved batch size (default: 25).
        """
        batch_size: int | None = getattr(source_config, "batch_size", None)
        if batch_size is None:
            parameters_mapping = source_config.parameters_mapping()
            candidate: Any = parameters_mapping.get("batch_size")
            if isinstance(candidate, int) and candidate > 0:
                batch_size = candidate
        if batch_size is None or batch_size <= 0:
            batch_size = 25
        return batch_size

    def _resolve_select_fields(
        self,
        source_config: SourceConfig[Any],
        default_fields: Sequence[str] | None = None,
    ) -> list[str]:
        """Resolve select_fields from config or use default.

        Parameters
        ----------
        source_config
            Source configuration object.
        default_fields
            Optional default field list to use if not configured.

        Returns
        -------
        list[str]
            List of field names to select from the API.
        """
        parameters = source_config.parameters_mapping()
        normalized = normalize_select_fields(
            parameters.get("select_fields"),
            default=default_fields,
        )
        if normalized is None:
            return []
        return list(normalized)

    @staticmethod
    def _merge_select_fields(
        select_fields: Sequence[str] | None,
        required_fields: Sequence[str] | None = None,
    ) -> list[str] | None:
        """Return a deterministic list merging configured and required fields."""

        merged = merge_select_fields(select_fields, required_fields)
        if merged is None:
            return None
        return list(merged)

    def _dispatch_extract_mode(
        self,
        log: BoundLogger,
        *,
        event_name: str,
        batch_callback: Callable[[Sequence[str]], pd.DataFrame],
        full_callback: Callable[[], pd.DataFrame],
        id_column_name: str | None = None,
        legacy_id_resolver: Callable[[BoundLogger], Sequence[str] | None] | None = None,
        legacy_source: str = "legacy",
    ) -> pd.DataFrame:
        """Dispatch extraction mode between batch and full strategies.

        This helper centralises reading identifiers from the CLI configuration,
        logging the selected execution mode, and invoking the provided
        callbacks for batch (ID-based) or full extraction.

        Parameters
        ----------
        log
            Logger bound to the caller's execution context.
        event_name
            Structured logging event emitted whenever the extraction mode is
            resolved.
        batch_callback
            Callback executed when identifiers are available. Receives the
            resolved list of identifiers.
        full_callback
            Callback executed when no identifiers are supplied via CLI or
            legacy hooks.
        id_column_name
            Optional override for the identifier column expected within the
            CLI input file. Defaults to the pipeline-derived value.
        legacy_id_resolver
            Optional callable that can provide identifiers from legacy inputs
            (for example deprecated keyword arguments). The callable receives
            the same logger so it can emit warnings prior to returning
            identifiers.
        legacy_source
            Source label used in structured logging when identifiers originate
            from the legacy resolver.
        """

        column_name = id_column_name or self._get_id_column_name()

        if self.config.cli.input_file:
            ids = self._read_input_ids(
                id_column_name=column_name,
                limit=self.config.cli.limit,
                sample=self.config.cli.sample,
            )
            if ids:
                log.info(
                    event_name,
                    mode="batch",
                    source="cli_input",
                    ids_count=len(ids),
                )
                return batch_callback(ids)

        if legacy_id_resolver is not None:
            legacy_ids = legacy_id_resolver(log)
            if legacy_ids:
                if isinstance(legacy_ids, (str, bytes)):
                    normalized_ids = [str(legacy_ids)]
                else:
                    normalized_ids = [str(item) for item in legacy_ids]
                if normalized_ids:
                    log.info(
                        event_name,
                        mode="batch",
                        source=legacy_source,
                        ids_count=len(normalized_ids),
                    )
                    return batch_callback(normalized_ids)

        log.info(event_name, mode="full")
        return full_callback()

    def run_extract_all(
        self: PipelineT, descriptor: ChemblExtractionDescriptor[PipelineT]
    ) -> pd.DataFrame:
        """Execute a descriptor-driven extraction loop with uniform metadata."""

        log = UnifiedLogger.get(__name__).bind(component=f"{self.pipeline_code}.extract")
        stage_start = time.perf_counter()

        source_raw = self._resolve_source_config(descriptor.source_name)
        source_config = descriptor.source_config_factory(source_raw)

        context = descriptor.build_context(self, source_config, log)
        context.source_config = source_config

        limit = self.config.cli.limit

        configured_select: Sequence[str] | None = context.select_fields
        if configured_select is None and descriptor.default_select_fields is not None:
            configured_select = descriptor.default_select_fields
        merged_select = merge_select_fields(configured_select, descriptor.must_have_fields)
        select_fields_list = list(merged_select) if merged_select else None
        context.select_fields = select_fields_list

        batch_size_candidate: int | None = getattr(source_config, "batch_size", None)
        if batch_size_candidate is None:
            batch_size_candidate = getattr(source_config, "page_size", None)
        if batch_size_candidate is None:
            batch_size_candidate = self._resolve_batch_size(source_raw)

        hard_cap = descriptor.hard_page_size_cap
        if hard_cap is None and batch_size_candidate is not None:
            hard_cap = max(int(batch_size_candidate), 1)

        page_size = context.page_size
        if page_size is None:
            base_size = batch_size_candidate if batch_size_candidate is not None else 25
            cap = hard_cap if hard_cap is not None else base_size
            page_size = self._resolve_page_size(base_size, limit, hard_cap=cap)
        context.page_size = page_size

        normalised_parameters = self._normalize_parameters(source_config.parameters)

        filters_payload, compact_filters = build_filters_payload(
            limit=limit,
            page_size=page_size,
            select_fields=select_fields_list,
            extra_filters=context.extra_filters,
            parameters=normalised_parameters,
            mode="all",
        )

        metadata_kwargs = dict(context.metadata)
        self.record_extract_metadata(
            chembl_release=context.chembl_release,
            filters=compact_filters,
            requested_at_utc=datetime.now(timezone.utc),
            **metadata_kwargs,
        )

        if self.config.cli.dry_run:
            if descriptor.dry_run_handler is not None:
                return descriptor.dry_run_handler(self, context, log, stage_start)

            duration_ms = (time.perf_counter() - stage_start) * 1000.0
            if descriptor.empty_frame_factory is not None:
                dataframe = descriptor.empty_frame_factory(self, context)
            else:
                dataframe = pd.DataFrame()

            dry_run_summary: dict[str, Any] = {
                "rows": int(dataframe.shape[0]),
                "duration_ms": duration_ms,
                "dry_run": True,
            }
            if context.chembl_release is not None:
                dry_run_summary["chembl_release"] = context.chembl_release
            if descriptor.summary_extra is not None:
                dry_run_summary.update(descriptor.summary_extra(self, dataframe, context))
            if context.stats:
                dry_run_summary.update(context.stats)
            log.info(descriptor.summary_event, **dry_run_summary)
            return dataframe

        iterator_kwargs = dict(context.iterate_all_kwargs)
        records: list[dict[str, Any]] = []

        for payload in context.iterator.iterate_all(
            limit=limit,
            page_size=page_size,
            select_fields=select_fields_list,
            **iterator_kwargs,
        ):
            if descriptor.record_transform is not None:
                record_mapping = descriptor.record_transform(self, payload, context)
            else:
                record_mapping = self._coerce_mapping(payload)
            records.append(dict(record_mapping))

        if records:
            dataframe = pd.DataFrame.from_records(records)
        elif descriptor.empty_frame_factory is not None:
            dataframe = descriptor.empty_frame_factory(self, context)
        else:
            dataframe = pd.DataFrame({descriptor.id_column: pd.Series(dtype="object")})

        if descriptor.sort_by and not dataframe.empty:
            if isinstance(descriptor.sort_by, Sequence) and not isinstance(descriptor.sort_by, (str, bytes)):
                sort_columns = list(descriptor.sort_by)
            else:
                sort_columns = [str(descriptor.sort_by)]
            dataframe = dataframe.sort_values(sort_columns).reset_index(drop=True)

        for processor in descriptor.post_processors:
            dataframe = processor(self, dataframe, context, log)

        duration_ms = (time.perf_counter() - stage_start) * 1000.0

        summary_payload: dict[str, Any] = {
            "rows": int(dataframe.shape[0]),
            "duration_ms": duration_ms,
        }
        if context.chembl_release is not None:
            summary_payload["chembl_release"] = context.chembl_release
        if context.stats:
            summary_payload.update(context.stats)
        if descriptor.summary_extra is not None:
            summary_payload.update(descriptor.summary_extra(self, dataframe, context))

        log.info(descriptor.summary_event, **summary_payload)
        return dataframe

    # ------------------------------------------------------------------
    # API client management
    # ------------------------------------------------------------------

    def prepare_chembl_client(
        self,
        source_name: str = "chembl",
        *,
        base_url: str | None = None,
        client_name: str | None = None,
    ) -> tuple[UnifiedAPIClient, str]:
        """Prepare and register a ChEMBL API client.

        Parameters
        ----------
        source_name
            Name of the source configuration (default: "chembl").
        base_url
            Optional base URL override. If not provided, resolved from config.
        client_name
            Optional client registration name. If not provided, uses default.

        Returns
        -------
        tuple[UnifiedAPIClient, str]
            The prepared API client and resolved base URL.
        """
        source_config = self._resolve_source_config(source_name)
        options = {"base_url": base_url} if base_url else None
        client, resolved_base_url, _ = self._chembl_entity_factory.build_http_client(
            source_name=source_name,
            source_config=source_config,
            options=options,
        )
        if client_name:
            self.register_client(client_name, client)
        return client, resolved_base_url

    def build_chembl_entity_bundle(
        self,
        entity_name: str,
        *,
        source_name: str = "chembl",
        source_config: SourceConfig[Any] | None = None,
        options: Mapping[str, Any] | None = None,
        chembl_client_kwargs: Mapping[str, Any] | None = None,
        fresh_http_client: bool = False,
    ) -> ChemblClientBundle:
        """Создать сущностный клиент ChEMBL с общими параметрами пайплайна."""

        effective_source = source_config or self._resolve_source_config(source_name)
        kwargs = self._default_chembl_client_kwargs()
        if chembl_client_kwargs:
            kwargs.update(dict(chembl_client_kwargs))
        return self._chembl_entity_factory.build(
            entity_name,
            source_name=source_name,
            source_config=effective_source,
            options=options,
            chembl_client_kwargs=kwargs,
            fresh_http_client=fresh_http_client,
        )

    def _default_chembl_client_kwargs(self) -> dict[str, Any]:
        """Вернуть контекст по умолчанию для инициализации ChemblClient."""

        return {
            "load_meta_store": self.load_meta_store,
            "job_id": self.run_id,
            "operator": self.pipeline_code,
        }

    # ------------------------------------------------------------------
    # ChEMBL release fetching
    # ------------------------------------------------------------------

    def fetch_chembl_release(
        self,
        client: UnifiedAPIClient | Any,  # pyright: ignore[reportAny]
        log: BoundLogger | None = None,
    ) -> str | None:
        """Fetch ChEMBL release version from status endpoint.

        Supports both UnifiedAPIClient (direct HTTP) and ChemblClient
        (wrapped with handshake) interfaces.

        Parameters
        ----------
        client
            API client (UnifiedAPIClient or ChemblClient).
        log
            Optional logger instance. If not provided, creates one.

        Returns
        -------
        str | None
            The ChEMBL release version, or None if unavailable.
        """
        if log is None:
            log = UnifiedLogger.get(__name__).bind(component=f"{self.pipeline_code}.extract")

        release_value: str | None = None

        # Check if client is ChemblClient by checking for handshake method
        handshake_candidate = getattr(client, "handshake", None)
        if callable(handshake_candidate):
            handshake = cast(Callable[..., Any], handshake_candidate)
            request_timestamp = datetime.now(timezone.utc)
            try:
                status = handshake()
                if isinstance(status, Mapping):
                    status_mapping = cast(Mapping[str, Any], status)
                    candidate = status_mapping.get("chembl_db_version") or status_mapping.get("chembl_release")
                    if isinstance(candidate, str):
                        release_value = candidate
                        log.info(LogEvents.CHEMBL_DESCRIPTOR_STATUS, chembl_release=release_value)
            except Exception as exc:
                log.warning(LogEvents.CHEMBL_DESCRIPTOR_STATUS_FAILED, error=str(exc))
            finally:
                self.record_extract_metadata(
                    chembl_release=release_value,
                    requested_at_utc=request_timestamp,
                )
            return release_value

        # Use direct HTTP for UnifiedAPIClient
        get_candidate = getattr(client, "get", None)
        if callable(get_candidate):
            client_get = cast(Callable[..., Any], get_candidate)
            request_timestamp = datetime.now(timezone.utc)
            try:
                response = client_get("/status.json")
                json_candidate = getattr(response, "json", None)
                if callable(json_candidate):
                    status_payload_raw = json_candidate()
                    status_payload = self._coerce_mapping(status_payload_raw)
                    release_value = self._extract_chembl_release(status_payload)
                    log.info(LogEvents.CHEMBL_DESCRIPTOR_STATUS, chembl_release=release_value)
            except Exception as exc:
                log.warning(LogEvents.CHEMBL_DESCRIPTOR_STATUS_FAILED, error=str(exc))
            finally:
                self.record_extract_metadata(
                    chembl_release=release_value,
                    requested_at_utc=request_timestamp,
                )
            return release_value
        self.record_extract_metadata(requested_at_utc=datetime.now(timezone.utc))
        return None

    def _fetch_chembl_release(
        self,
        client: UnifiedAPIClient | Any,  # pyright: ignore[reportAny]
        log: BoundLogger | None = None,
    ) -> str | None:
        """Backward compatible wrapper for tests expecting private method."""

        return self.fetch_chembl_release(client, log)

    @staticmethod
    def _extract_chembl_release(payload: Mapping[str, Any] | None) -> str | None:
        """Extract ChEMBL release version from status payload.

        Parameters
        ----------
        payload
            Status response payload mapping.

        Returns
        -------
        str | None
            The release version string, or None if not found.
        """
        if not payload:
            return None

        for key in ("chembl_release", "chembl_db_version", "release", "version"):
            value = payload.get(key)
            if isinstance(value, str) and value.strip():
                return value
            if value is not None:
                return str(value)
        return None

    # ------------------------------------------------------------------
    # Response processing utilities
    # ------------------------------------------------------------------

    @staticmethod
    def _coerce_mapping(payload: Any) -> dict[str, Any]:
        """Coerce payload to dictionary mapping.

        Parameters
        ----------
        payload
            Response payload (may be dict, Mapping, or other).

        Returns
        -------
        dict[str, Any]
            Dictionary representation of the payload.
        """
        if isinstance(payload, Mapping):
            mapping = cast(Mapping[object, Any], payload)
            return ChemblPipelineBase._stringify_mapping(mapping)
        return {}

    @staticmethod
    def _extract_page_items(
        payload: Mapping[str, Any],
        items_keys: Sequence[str] | None = None,
    ) -> list[dict[str, Any]]:
        """Extract items from paginated response payload.

        Parameters
        ----------
        payload
            Paginated response payload mapping.
        items_keys
            Optional sequence of keys to check for items. If not provided,
            uses common defaults: ("data", "items", "results").

        Returns
        -------
        list[dict[str, Any]]
            List of extracted item dictionaries.
        """
        if items_keys is None:
            items_keys = ("data", "items", "results")

        for key in items_keys:
            value = payload.get(key)
            if isinstance(value, Sequence) and not isinstance(value, (str, bytes, bytearray)):
                candidates: list[dict[str, Any]] = []
                sequence_items = cast(Sequence[object], value)
                for item in sequence_items:
                    if isinstance(item, Mapping):
                        mapping = cast(Mapping[object, Any], item)
                        candidates.append(ChemblPipelineBase._stringify_mapping(mapping))
                if candidates:
                    return candidates

        # Fallback: iterate all keys except page_meta
        for key, value in payload.items():
            if key == "page_meta":
                continue
            if isinstance(value, Sequence) and not isinstance(value, (str, bytes, bytearray)):
                candidates = []
                sequence_items = cast(Sequence[object], value)
                for item in sequence_items:
                    if isinstance(item, Mapping):
                        mapping = cast(Mapping[object, Any], item)
                        candidates.append(ChemblPipelineBase._stringify_mapping(mapping))
                if candidates:
                    return candidates
        return []

    @staticmethod
    def _next_link(payload: Mapping[str, Any], base_url: str) -> str | None:
        """Extract next page link from paginated response.

        Parameters
        ----------
        payload
            Paginated response payload mapping.
        base_url
            Base URL for the API (used to normalize full URLs to relative paths).

        Returns
        -------
        str | None
            Relative path for the next page, or None if no next page.
        """
        page_meta = payload.get("page_meta")
        if not isinstance(page_meta, Mapping):
            return None

        page_meta_mapping = cast(Mapping[str, Any], page_meta)
        next_link_raw = page_meta_mapping.get("next")
        if not isinstance(next_link_raw, str):
            return None

        next_link = next_link_raw.strip()
        if not next_link:
            return None

        # If next_link is a full URL, extract only the relative path
        if next_link.startswith("http://") or next_link.startswith("https://"):
            parsed = urlparse(next_link)
            base_parsed = urlparse(base_url)

            # Normalize paths: remove trailing slashes for comparison
            path = parsed.path.rstrip("/")
            base_path = base_parsed.path.rstrip("/")

            # If paths match, return just the path with query
            if path == base_path or path.startswith(f"{base_path}/"):
                relative_path = path[len(base_path) :] if path.startswith(base_path) else path
                if parsed.query:
                    return f"{relative_path}?{parsed.query}"
                return relative_path

            # If base paths don't match, return full URL path + query
            if parsed.query:
                return f"{parsed.path}?{parsed.query}"
            return parsed.path

        # Already a relative path
        return next_link

    # ------------------------------------------------------------------
    # Batch extraction utilities
    # ------------------------------------------------------------------

    def run_batched_extraction(
        self,
        ids: Sequence[Any],
        *,
        id_column: str,
        fetcher: Callable[[Sequence[str], BatchExtractionContext], Any],
        select_fields: Sequence[str] | None = None,
        required_fields: Sequence[str] | None = None,
        limit: int | None = None,
        batch_size: int | None = None,
        chunk_size: int | None = None,
        max_batch_size: int | None = 25,
        metadata_filters: Mapping[str, Any] | None = None,
        chembl_release: str | None = None,
        id_normalizer: Callable[[Any], tuple[str | None, Any]] | None = None,
        sort_key: Callable[[tuple[str, Any]], Any] | None = None,
        transform_item: Callable[[Mapping[str, Any], BatchExtractionContext], Mapping[str, Any]]
        | None = None,
        finalize: Callable[[pd.DataFrame, BatchExtractionContext], pd.DataFrame] | None = None,
        finalize_context: Callable[[BatchExtractionContext], None] | None = None,
        empty_frame_factory: Callable[[], pd.DataFrame] | None = None,
        stats_attribute: str | None = None,
        fetch_mode: Literal["default", "delegated"] = "default",
    ) -> tuple[pd.DataFrame, BatchExtractionStats]:
        """Execute a reusable batch extraction routine with deterministic semantics."""

        log = UnifiedLogger.get(__name__).bind(component=f"{self.pipeline_code}.extract")
        method_start = time.perf_counter()

        normalizer: Callable[[Any], tuple[str | None, Any]]

        if id_normalizer is not None:
            normalizer = id_normalizer
        else:

            def _default_normalizer(raw: Any) -> tuple[str | None, Any]:
                if raw is None:
                    return None, None
                candidate = str(raw).strip()
                if not candidate:
                    return None, None
                return candidate, None

            normalizer = _default_normalizer

        canonical_pairs: list[tuple[str, Any]] = []
        seen: set[str] = set()
        for raw in ids:
            canonical, metadata = normalizer(raw)
            if canonical is None:
                continue
            if canonical in seen:
                continue
            seen.add(canonical)
            canonical_pairs.append((canonical, metadata))

        if sort_key:
            canonical_pairs.sort(key=sort_key)
        else:
            canonical_pairs.sort(key=lambda pair: pair[0])

        if limit is not None:
            effective_limit = max(int(limit), 0)
            canonical_pairs = canonical_pairs[:effective_limit]

        unique_ids = [identifier for identifier, _ in canonical_pairs]
        id_metadata_map = dict(canonical_pairs)

        stats = BatchExtractionStats(requested=len(unique_ids))

        if batch_size is None:
            try:
                source_config = self._resolve_source_config("chembl")
                batch_size = self._resolve_batch_size(source_config)
            except Exception:
                batch_size = 25

        effective_batch_size = max(int(batch_size or 1), 1)
        if max_batch_size is not None:
            effective_batch_size = min(effective_batch_size, int(max_batch_size))

        if chunk_size is None:
            effective_chunk_size = effective_batch_size
        else:
            effective_chunk_size = max(int(chunk_size), 1)
            if max_batch_size is not None:
                effective_chunk_size = min(effective_chunk_size, int(max_batch_size))

        merged_select_fields = self._merge_select_fields(select_fields, required_fields)
        select_fields_tuple: tuple[str, ...] = tuple(merged_select_fields or ())

        extra_filter_payload: dict[str, Any] = {
            "requested_ids": unique_ids,
            "batch_size": effective_batch_size,
        }
        if metadata_filters:
            extra_filter_payload.update(dict(metadata_filters))
        filters_payload, compact_filters = build_filters_payload(
            mode="ids",
            limit=limit,
            page_size=effective_batch_size,
            select_fields=merged_select_fields,
            extra_filters=extra_filter_payload,
        )

        self.record_extract_metadata(
            chembl_release=chembl_release or self.chembl_release,
            filters=compact_filters,
            requested_at_utc=datetime.now(timezone.utc),
        )

        context = BatchExtractionContext(
            ids=tuple(unique_ids),
            id_column=id_column,
            select_fields=select_fields_tuple,
            limit=int(limit) if limit is not None else None,
            batch_size=effective_batch_size,
            chunk_size=effective_chunk_size,
            stats=stats,
            log=log,
            metadata={identifier: id_metadata_map.get(identifier) for identifier in unique_ids},
        )
        context.extra.setdefault("id_metadata", context.metadata)

        records: list[dict[str, Any]] = []
        delegated_summary: Mapping[str, Any] | None = None

        try:
            if not unique_ids:
                dataframe = (
                    empty_frame_factory()
                    if empty_frame_factory is not None
                    else pd.DataFrame({id_column: pd.Series(dtype="string")})
                )
                stats.rows = int(dataframe.shape[0])
                stats.duration_ms = (time.perf_counter() - method_start) * 1000.0
                return dataframe, stats

            if fetch_mode == "delegated":
                fetch_result = fetcher(tuple(unique_ids), context)
                delegated_payload: Any | None = None
                items_iter: Iterable[Mapping[str, Any]]
                if isinstance(fetch_result, tuple) and fetch_result:
                    items_iter = cast(Iterable[Mapping[str, Any]], fetch_result[0])
                    if len(fetch_result) > 1:
                        delegated_payload = fetch_result[1]
                else:
                    items_iter = cast(Iterable[Mapping[str, Any]], fetch_result)

                for item in items_iter:
                    record = dict(item)
                    if transform_item is not None:
                        record = dict(transform_item(record, context))
                    records.append(record)

                if isinstance(delegated_payload, BatchExtractionStats):
                    stats.batches = delegated_payload.batches
                    stats.api_calls = delegated_payload.api_calls
                    stats.cache_hits = delegated_payload.cache_hits
                    stats.set_extra(**delegated_payload.extra)
                elif isinstance(delegated_payload, Mapping):
                    delegated_summary = cast(Mapping[str, Any], delegated_payload)
            else:
                for start_idx in range(0, len(unique_ids), effective_chunk_size):
                    batch_ids = tuple(unique_ids[start_idx : start_idx + effective_chunk_size])
                    context.increment_batches()
                    fetch_output = fetcher(batch_ids, context)
                    batch_items_iter: Iterable[Mapping[str, Any]]
                    if isinstance(fetch_output, tuple) and fetch_output:
                        batch_items_iter = cast(Iterable[Mapping[str, Any]], fetch_output[0])
                        if len(fetch_output) > 1 and isinstance(fetch_output[1], Mapping):
                            delegated_summary = cast(Mapping[str, Any], fetch_output[1])
                    else:
                        batch_items_iter = cast(Iterable[Mapping[str, Any]], fetch_output)

                    for item in batch_items_iter:
                        record = dict(item)
                        if transform_item is not None:
                            record = dict(transform_item(record, context))
                        records.append(record)
                        if limit is not None and len(records) >= int(limit):
                            break
                    if limit is not None and len(records) >= int(limit):
                        break

            if limit is not None and len(records) > int(limit):
                records = records[: int(limit)]

            dataframe = pd.DataFrame.from_records(records)  # pyright: ignore[reportUnknownMemberType]
            if dataframe.empty:
                dataframe = (
                    empty_frame_factory()
                    if empty_frame_factory is not None
                    else pd.DataFrame({id_column: pd.Series(dtype="string")})
                )
            elif id_column in dataframe.columns:
                dataframe = dataframe.sort_values(id_column).reset_index(drop=True)

            if finalize is not None:
                dataframe = finalize(dataframe, context)

            duration_ms = (time.perf_counter() - method_start) * 1000.0
            stats.rows = int(dataframe.shape[0])
            stats.duration_ms = duration_ms

            if delegated_summary is not None:
                context.extra.setdefault("delegated_summary", dict(delegated_summary))
                if "batches" in delegated_summary and isinstance(
                    delegated_summary.get("batches"), int
                ):
                    stats.batches = int(delegated_summary["batches"])
                if "api_calls" in delegated_summary and isinstance(
                    delegated_summary.get("api_calls"), int
                ):
                    stats.api_calls = int(delegated_summary["api_calls"])
                if "cache_hits" in delegated_summary and isinstance(
                    delegated_summary.get("cache_hits"), int
                ):
                    stats.cache_hits = int(delegated_summary["cache_hits"])
                extra_payload = {
                    key: value
                    for key, value in delegated_summary.items()
                    if key not in {"batches", "api_calls", "cache_hits"}
                }
                if extra_payload:
                    stats.set_extra(**extra_payload)

            return dataframe, stats
        finally:
            if finalize_context is not None:
                finalize_context(context)

            if stats_attribute and hasattr(self, stats_attribute):
                override = context.extra.get("stats_attribute_override")
                if override is not None:
                    setattr(self, stats_attribute, override)
                else:
                    setattr(self, stats_attribute, stats.as_dict())

    def extract_ids_paginated(
        self,
        ids: Sequence[str],
        endpoint: str,
        id_column: str,
        id_param_name: str,
        client: UnifiedAPIClient,
        *,
        batch_size: int | None = None,
        limit: int | None = None,
        select_fields: Sequence[str] | None = None,
        items_keys: Sequence[str] | None = None,
        process_item: Any | None = None,
    ) -> pd.DataFrame:
        """Extract records by batching ID values with pagination support.

        Parameters
        ----------
        ids
            Sequence of ID values to extract.
        endpoint
            API endpoint path (e.g., "/activity.json", "/document.json").
        id_column
            Name of the ID column in the resulting DataFrame.
        id_param_name
            API parameter name for ID filtering (e.g., "activity_id__in", "document_chembl_id__in").
        client
            Unified API client instance.
        batch_size
            Optional batch size override. If not provided, resolved from config.
        limit
            Optional limit on total number of records to extract.
        select_fields
            Optional list of fields to select from the API.
        items_keys
            Optional keys to check for items in response (passed to _extract_page_items).
        process_item
            Optional callable to process each item before adding to results.

        Returns
        -------
        pd.DataFrame
            DataFrame containing extracted records, sorted by ID column.
        """
        log = UnifiedLogger.get(__name__).bind(component=f"{self.pipeline_code}.extract")
        method_start = time.perf_counter()

        if batch_size is None:
            source_config = self._resolve_source_config("chembl")
            batch_size = self._resolve_batch_size(source_config)

        # Ensure batch_size does not exceed ChEMBL API limit
        batch_size = min(batch_size, 25)
        batch_size = max(batch_size, 1)

        # Extract unique IDs, filter out NaN, sort for determinism
        unique_ids = sorted({str(id_val) for id_val in ids if id_val and str(id_val).strip()})

        if not unique_ids:
            log.debug(LogEvents.EXTRACT_IDS_PAGINATED_NO_VALID_IDS)
            return pd.DataFrame({id_column: pd.Series(dtype="string")})

        # Process in batches
        all_records: list[dict[str, Any]] = []
        batches = 0
        api_calls = 0

        for i in range(0, len(unique_ids), batch_size):
            batch_ids = unique_ids[i : i + batch_size]
            batches += 1

            params: dict[str, Any] = {
                id_param_name: ",".join(batch_ids),
                "limit": batch_size,
            }
            if select_fields:
                params["only"] = ",".join(select_fields)

            try:
                response = client.get(endpoint, params=params)
                api_calls += 1
                payload = self._coerce_mapping(response.json())
                page_items = self._extract_page_items(payload, items_keys=items_keys)

                for item in page_items:
                    if process_item:
                        processed_item = process_item(dict(item))
                    else:
                        processed_item = dict(item)
                    all_records.append(processed_item)

                if limit is not None and len(all_records) >= limit:
                    all_records = all_records[:limit]
                    break

            except Exception as exc:
                log.warning(LogEvents.EXTRACT_IDS_PAGINATED_BATCH_ERROR,
                    batch_ids=batch_ids,
                    error=str(exc),
                    exc_info=True,
                )

        dataframe = pd.DataFrame.from_records(all_records)  # pyright: ignore[reportUnknownMemberType]
        if dataframe.empty:
            dataframe = pd.DataFrame({id_column: pd.Series(dtype="string")})
        elif id_column in dataframe.columns:
            dataframe = dataframe.sort_values(id_column).reset_index(drop=True)

        duration_ms = (time.perf_counter() - method_start) * 1000.0
        log.info(LogEvents.EXTRACT_IDS_PAGINATED_SUMMARY,
            rows=int(dataframe.shape[0]),
            requested=len(unique_ids),
            batches=batches,
            api_calls=api_calls,
            duration_ms=duration_ms,
        )

        return dataframe
