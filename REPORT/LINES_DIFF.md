# Сравнительный отчёт по пайплайнам ChEMBL

## Пара: activity ↔ assay

- AST hash: 978d4152ad77000361c21b7f2051d3d1 ↔ 4e65bc8b94391cb8c868cf6ad530c2c6

- Jaccard по токенам: 0.212

### Модуль run.py

Определение                                                      | activity сигнатура                                                                                                                                                         | assay сигнатура                                                   | Побочные эффекты                                                                                                                                                                                                                                                                                                                                                                                             | Исключения                                                | Статус           
-----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------|------------------
ChemblActivityPipeline                                           | —                                                                                                                                                                          | —                                                                 | activity: {'logging': ['UnifiedLogger.bind', 'UnifiedLogger.get', 'bound_log.warning', 'errors_to_log.iterrows', 'log.debug', 'log.error', 'log.info', 'log.warning', 'pipeline._log_validity_comments_metrics', 'self._log_detailed_validation_errors', 'self._log_validity_comments_metrics'], 'io': ['cache_file.read_text', 'json.dumps', 'json.loads', 'payload.get', 'tmp_path.write_text']}
assay: {} | activity: ['TypeError(msg)', 'ValueError(msg)']
assay: [] | только в activity
ChemblActivityPipeline.__init__                                  | self, config: PipelineConfig, run_id: str                                                                                                                                  | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._add_row_metadata                         | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.debug'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._build_activity_descriptor                | self                                                                                                                                                                       | —                                                                 | activity: {'logging': ['pipeline._log_validity_comments_metrics'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                       | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._cache_directory                          | self, release: str | None                                                                                                                                                  | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._cache_file_path                          | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._cache_key                                | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                 | activity: {'logging': [], 'io': ['json.dumps']}
assay: {}                                                                                                                                                                                                                                                                                                                                                    | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._check_activity_id_uniqueness             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.debug', 'log.error', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                         | activity: ['ValueError(msg)']
assay: []                   | только в activity
ChemblActivityPipeline._check_cache                              | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                 | activity: {'logging': [], 'io': ['cache_file.read_text', 'json.loads']}
assay: {}                                                                                                                                                                                                                                                                                                                            | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._check_foreign_key_integrity              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.debug', 'log.error', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                         | activity: ['ValueError(msg)']
assay: []                   | только в activity
ChemblActivityPipeline._coerce_activity_dataset                  | self, dataset: object                                                                                                                                                      | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: ['TypeError(msg)']
assay: []                    | только в activity
ChemblActivityPipeline._coerce_mapping                           | payload: Any                                                                                                                                                               | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._collect_records_by_ids                   | self, normalized_ids: Sequence[tuple[int, str]], activity_iterator: ChemblActivityClient, *, select_fields: Sequence[str] | None                                           | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                    | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._create_fallback_record                   | self, activity_id: int, error: Exception | None                                                                                                                            | —                                                                 | activity: {'logging': [], 'io': ['json.dumps']}
assay: {}                                                                                                                                                                                                                                                                                                                                                    | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._deduplicate_activity_properties          | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                                 | activity: {'logging': ['log.debug'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._enrich_assay                             | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                              | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._enrich_compound_record                   | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                              | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._enrich_data_validity                     | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                  | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._enrich_molecule                          | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                  | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._ensure_comment_fields                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.debug'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._extract_activity_properties_fields       | self, record: dict[str, Any]                                                                                                                                               | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.warning'], 'io': ['json.loads']}
assay: {}                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._extract_assay_fields                     | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                                 | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                          | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._extract_chembl_release                   | payload: Mapping[str, Any]                                                                                                                                                 | —                                                                 | activity: {'logging': [], 'io': ['payload.get']}
assay: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._extract_data_validity_descriptions       | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                                 | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                          | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._extract_from_chembl                      | self, dataset: object, chembl_client: ChemblClient | Any, activity_iterator: ChemblActivityClient, *, limit: int | None = None, select_fields: Sequence[str] | None = None | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'self._log_validity_comments_metrics'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                          | activity: ['ValueError(msg)']
assay: []                   | только в activity
ChemblActivityPipeline._extract_nested_fields                    | self, record: dict[str, Any]                                                                                                                                               | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._extract_page_items                       | payload: Mapping[str, Any], items_keys: Sequence[str] | None                                                                                                               | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._filter_invalid_required_fields           | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._finalize_identifier_columns              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._finalize_output_columns                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._get_data_validity_comment_whitelist      | self                                                                                                                                                                       | —                                                                 | activity: {'logging': ['UnifiedLogger.get'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                             | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._harmonize_identifier_columns             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.debug'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._log_detailed_validation_errors           | self, failure_cases: pd.DataFrame, payload: pd.DataFrame, log: BoundLogger                                                                                                 | —                                                                 | activity: {'logging': ['errors_to_log.iterrows', 'log.error', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                            | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._log_validity_comments_metrics            | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.info', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._materialize_activity_record              | self, payload: Mapping[str, Any], *, activity_id: int | None = None                                                                                                        | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._next_link                                | payload: Mapping[str, Any], base_url: str                                                                                                                                  | —                                                                 | activity: {'logging': [], 'io': ['payload.get']}
assay: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._normalize_activity_ids                   | self, input_frame: pd.DataFrame, *, limit: int | None, log: BoundLogger                                                                                                    | —                                                                 | activity: {'logging': ['log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._normalize_activity_properties_items      | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                                 | activity: {'logging': ['log.warning'], 'io': ['json.loads']}
assay: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._normalize_data_types                     | self, df: pd.DataFrame, schema: Any, log: BoundLogger                                                                                                                      | —                                                                 | activity: {'logging': ['log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._normalize_identifiers                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.debug'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._normalize_measurements                   | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._normalize_nested_structures              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.warning'], 'io': ['json.dumps', 'json.loads']}
assay: {}                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._normalize_string_fields                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._prepare_activity_iteration               | self, *, client_name: str = 'chembl_activity_client'                                                                                                                       | —                                                                 | activity: {'logging': ['UnifiedLogger.get'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                             | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._sanitize_cache_component                 | value: str                                                                                                                                                                 | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._schema_column_specs                      | self                                                                                                                                                                       | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._serialize_activity_properties            | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                                 | activity: {'logging': ['log.warning'], 'io': ['json.dumps']}
assay: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._should_enrich_assay                      | self                                                                                                                                                                       | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._should_enrich_compound_record            | self                                                                                                                                                                       | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._should_enrich_data_validity              | self                                                                                                                                                                       | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._should_enrich_molecule                   | self                                                                                                                                                                       | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._store_cache                              | self, batch_ids: Sequence[str], batch_data: Mapping[str, Mapping[str, Any]], release: str | None                                                                           | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'log.debug'], 'io': ['json.dumps', 'tmp_path.write_text']}
assay: {}                                                                                                                                                                                                                                                                                             | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._validate_activity_properties_truv        | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                                 | activity: {'logging': ['log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._validate_data_validity_comment_soft_enum | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline._validate_foreign_keys                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: {'logging': ['log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline.build_quality_report                      | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline.chembl_release                            | self                                                                                                                                                                       | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline.extract                                   | self, *args, **kwargs                                                                                                                                                      | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'bound_log.warning'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                        | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline.extract_all                               | self                                                                                                                                                                       | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline.extract_by_ids                            | self, ids: Sequence[str]                                                                                                                                                   | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning', 'self._log_validity_comments_metrics'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                           | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline.transform                                 | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                    | activity: []
assay: []                                    | только в activity
ChemblActivityPipeline.validate                                  | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.info', 'self._log_detailed_validation_errors'], 'io': []}
assay: {}                                                                                                                                                                                                                                                               | activity: ['ValueError(msg)']
assay: []                   | только в activity
ChemblActivityPipeline.write                                     | self, df: pd.DataFrame, output_path: Path, *, extended: bool = False, include_correlation: bool | None = None, include_qc_metrics: bool | None = None                      | —                                                                 | activity: {'logging': ['UnifiedLogger.bind', 'UnifiedLogger.get', 'log.debug'], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                          | activity: []
assay: []                                    | только в activity
ChemblAssayPipeline                                              | —                                                                                                                                                                          | —                                                                 | activity: {}
assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                     | activity: []
assay: ['TypeError(msg)']                    | только в assay   
ChemblAssayPipeline.__init__                                     | —                                                                                                                                                                          | self, config: PipelineConfig, run_id: str                         | activity: {}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline._add_row_metadata                            | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: {}
assay: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline._build_assay_descriptor                      | —                                                                                                                                                                          | self: SelfChemblAssayPipeline                                     | activity: {}
assay: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                                          | activity: []
assay: ['TypeError(msg)']                    | только в assay   
ChemblAssayPipeline._check_missing_columns                       | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any, select_fields: list[str] | None | activity: {}
assay: {'logging': ['log.debug', 'log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                                                      | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline._enrich_with_related_data                    | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: {}
assay: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                                          | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline._harmonize_identifier_columns                | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: {}
assay: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline._normalize_data_types                        | —                                                                                                                                                                          | self, df: pd.DataFrame, schema: Any, log: Any                     | activity: {}
assay: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline._normalize_identifiers                       | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: {}
assay: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline._normalize_nested_structures                 | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: {}
assay: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline._normalize_string_fields                     | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: {}
assay: {'logging': ['log.debug', 'log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                                                      | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline._serialize_array_fields                      | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: {}
assay: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline.chembl_release                               | —                                                                                                                                                                          | self                                                              | activity: {}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline.extract                                      | —                                                                                                                                                                          | self, *args, **kwargs                                             | activity: {}
assay: {'logging': ['UnifiedLogger.get'], 'io': []}                                                                                                                                                                                                                                                                                                                                             | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline.extract_all                                  | —                                                                                                                                                                          | self                                                              | activity: {}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline.extract_by_ids                               | —                                                                                                                                                                          | self, ids: Sequence[str]                                          | activity: {}
assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                     | activity: []
assay: []                                    | только в assay   
ChemblAssayPipeline.transform                                    | —                                                                                                                                                                          | self, df: pd.DataFrame                                            | activity: {}
assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                                                                                                                                                                                                                                                                                                                    | activity: []
assay: []                                    | только в assay   
__module_block_0                                                 | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_1                                                 | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_10                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_11                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_12                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_13                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_14                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_15                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_16                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_17                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_18                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_19                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_2                                                 | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_20                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_21                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_22                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_23                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
__module_block_24                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
__module_block_25                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
__module_block_26                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_27                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
__module_block_28                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
__module_block_29                                                | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в activity
__module_block_3                                                 | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_4                                                 | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | совпадает        
__module_block_5                                                 | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_6                                                 | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | совпадает        
__module_block_7                                                 | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
__module_block_8                                                 | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | совпадает        
__module_block_9                                                 | —                                                                                                                                                                          | —                                                                 | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
assay: []                                    | отличается       
_extract_bao_ids_from_classifications                            | —                                                                                                                                                                          | node: Any                                                         | activity: {}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в assay   
_iter_classification_mappings                                    | —                                                                                                                                                                          | node: Any                                                         | activity: {}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в assay   
_normalize_bao_identifier                                        | —                                                                                                                                                                          | raw_value: Any                                                    | activity: {}
assay: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
assay: []                                    | только в assay   

_Показаны первые 20 горячих участков из 110._

#### Горячий участок 1

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:106-3476
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,1684 +0,0 @@

-class ChemblActivityPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting activity records from the ChEMBL API."
-    actor ="activity_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-        self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch activity payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        def legacy_activity_ids (bound_log :BoundLogger )->Sequence [str ]|None :
-            payload_activity_ids =kwargs .get ("activity_ids")
-            if payload_activity_ids is None :
-                return None
-            bound_log .warning ("chembl_activity.deprecated_kwargs",message ="Using activity_ids in kwargs is deprecated. Use --input-file instead.")
-            if isinstance (payload_activity_ids ,Sequence )and (not isinstance (payload_activity_ids ,(str ,bytes ))):
-                sequence_ids :Sequence [str |int ]=cast (Sequence [str |int ],payload_activity_ids )
-                return [str (id_val )for id_val in sequence_ids ]
-            return [str (payload_activity_ids )]
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_activity.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="activity_id",legacy_id_resolver =legacy_activity_ids ,legacy_source ="deprecated_kwargs")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all activity records from ChEMBL using the shared iterator."
-        return self .run_extract_all (self ._build_activity_descriptor ())
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract activity records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of activity_id values to extract (as strings or integers).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted activity records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_config ,chembl_client ,activity_iterator ,select_fields =self ._prepare_activity_iteration ()
-        limit =self .config .cli .limit
-        invalid_ids :list [Any ]=[]
-        def normalize_activity_id (raw :Any )->tuple [str |None ,Any ]:
-            if pd .isna (raw ):
-                return (None ,None )
-            try :
-                if isinstance (raw ,str ):
-                    candidate =raw .strip ()
-                    if not candidate :
-                        return (None ,None )
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw ,(int ,float )):
-                    numeric_id =int (raw )
-                else :
-                    numeric_id =int (raw )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw )
-                return (None ,None )
-            return (str (numeric_id ),int (numeric_id ))
-        def delegated_fetch (canonical_ids :Sequence [str ],context :BatchExtractionContext )->tuple [Sequence [Mapping [str ,Any ]],Mapping [str ,Any ]]:
-            numeric_map =context .metadata
-            normalized_ids :list [tuple [int ,str ]]=[]
-            for identifier in canonical_ids :
-                numeric_value =numeric_map .get (identifier )
-                if numeric_value is None :
-                    continue
-                normalized_ids .append ((int (numeric_value ),identifier ))
-            if not normalized_ids :
-                summary ={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-                context .extra ["delegated_summary"]=summary
-                return ([],summary )
-            records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =context .select_fields or None )
-            context .extra ["delegated_summary"]=summary
-            return (records ,summary )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            dataframe =self ._ensure_comment_fields (dataframe ,log )
-            dataframe =self ._extract_data_validity_descriptions (dataframe ,chembl_client ,log )
-            dataframe =self ._extract_assay_fields (dataframe ,chembl_client ,log )
-            self ._log_validity_comments_metrics (dataframe ,log )
-            return dataframe
-        def empty_activity_frame ()->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def finalize_context (context :BatchExtractionContext )->None :
-            summary =context .extra .get ("delegated_summary")
-            if isinstance (summary ,Mapping ):
-                summary_dict =dict (summary )
-                context .extra ["stats_attribute_override"]=summary_dict
-                self ._last_batch_extract_stats =summary_dict
-            else :
-                context .extra ["stats_attribute_override"]=context .stats .as_dict ()
-                self ._last_batch_extract_stats =context .stats .as_dict ()
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="activity_id",fetcher =delegated_fetch ,select_fields =select_fields ,batch_size =source_config .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release ,id_normalizer =normalize_activity_id ,sort_key =lambda pair :int (pair [0 ]),finalize =finalize_dataframe ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats",fetch_mode ="delegated",empty_frame_factory =empty_activity_frame )
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        batch_stats =self ._last_batch_extract_stats or {}
-        log .info ("chembl_activity.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =batch_stats .get ("batches"),api_calls =batch_stats .get ("api_calls"),cache_hits =batch_stats .get ("cache_hits"))
-        return dataframe
-    def _prepare_activity_iteration (self ,*,client_name :str ="chembl_activity_client")->tuple [ActivitySourceConfig ,ChemblClient ,ChemblActivityClient ,list [str ]]:
-        "Construct reusable ChEMBL clients and iterator for activity extraction."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name =client_name )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =self ._resolve_select_fields (source_raw ,default_fields =API_ACTIVITY_FIELDS )
-        return (source_config ,chembl_client ,activity_iterator ,select_fields )
-    def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-        "Construct the descriptor driving the shared extraction template."
-        def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-            http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-            chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-            typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-            activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-            select_fields =source_config .parameters .select_fields
-            return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-        def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            df =pipeline ._ensure_comment_fields (df ,log )
-            chembl_client =cast (ChemblClient ,context .chembl_client )
-            if chembl_client is not None :
-                df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-            pipeline ._log_validity_comments_metrics (df ,log )
-            return df
-        def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            return pipeline ._materialize_activity_record (payload )
-        return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
-    def _materialize_activity_record (self ,payload :Mapping [str ,Any ],*,activity_id :int |None =None )->dict [str ,Any ]:
-        "Normalize nested fields within an activity payload."
-        record =dict (payload )
-        record =self ._extract_nested_fields (record )
-        record =self ._extract_activity_properties_fields (record )
-        if activity_id is not None :
-            record .setdefault ("activity_id",activity_id )
-        return record
-    def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-        "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-        if isinstance (dataset ,pd .Series ):
-            return dataset .to_frame (name ="activity_id")
-        if isinstance (dataset ,pd .DataFrame ):
-            return dataset
-        if isinstance (dataset ,Mapping ):
-            mapping =cast (Mapping [str ,Any ],dataset )
-            return pd .DataFrame ([dict (mapping )])
-        if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-            dataset_list :list [Any ]=list (dataset )
-            return pd .DataFrame ({"activity_id":dataset_list })
-        msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-        raise TypeError (msg )
-    def _normalize_activity_ids (self ,input_frame :pd .DataFrame ,*,limit :int |None ,log :BoundLogger )->list [tuple [int ,str ]]:
-        "Normalize raw identifier values into deduplicated integer/string pairs."
-        normalized_ids :list [tuple [int ,str ]]=[]
-        invalid_ids :list [Any ]=[]
-        seen :set [str ]=set ()
-        for raw_id in input_frame ["activity_id"].tolist ():
-            if pd .isna (raw_id ):
-                continue
-            try :
-                if isinstance (raw_id ,str ):
-                    candidate =raw_id .strip ()
-                    if not candidate :
-                        continue
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw_id ,(int ,float )):
-                    numeric_id =int (raw_id )
-                else :
-                    numeric_id =int (raw_id )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw_id )
-                continue
-            key =str (numeric_id )
-            if key not in seen :
-                seen .add (key )
-                normalized_ids .append ((numeric_id ,key ))
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        if limit is not None :
-            normalized_ids =normalized_ids [:max (int (limit ),0 )]
-        return normalized_ids
-    def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-        "Iterate over IDs using the shared iterator while preserving cache semantics."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        records :list [dict [str ,Any ]]=[]
-        success_count =0
-        fallback_count =0
-        error_count =0
-        cache_hits =0
-        api_calls =0
-        total_batches =0
-        key_order =[key for _ ,key in normalized_ids ]
-        key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-        for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-            total_batches +=1
-            batch_start =time .perf_counter ()
-            from_cache =False
-            chunk_records :dict [str ,dict [str ,Any ]]={}
-            try :
-                cached_records =self ._check_cache (chunk ,self ._chembl_release )
-                if cached_records is not None :
-                    from_cache =True
-                    cache_hits +=len (chunk )
-                    chunk_records =cached_records
-                else :
-                    api_calls +=1
-                    fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                    for item in fetched_items :
-                        if not isinstance (item ,Mapping ):
-                            continue
-                        activity_value =item .get ("activity_id")
-                        if activity_value is None :
-                            continue
-                        chunk_records [str (activity_value )]=dict (item )
-                    self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-                success_in_batch =0
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    record =chunk_records .get (key )
-                    if record and (not record .get ("error")):
-                        materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                        records .append (materialized )
-                        success_count +=1
-                        success_in_batch +=1
-                    else :
-                        fallback_record =self ._create_fallback_record (numeric_id )
-                        records .append (fallback_record )
-                        fallback_count +=1
-                        error_count +=1
-                batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-                log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-            except CircuitBreakerOpenError as exc :
-                log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except RequestException as exc :
-                log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except Exception as exc :
-                log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-        total_records =len (normalized_ids )
-        success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-        summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-        return (records ,summary )
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw activity data by normalizing measurements, identifiers, and data types."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_measurements (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,ActivitySchema ,log )
-        if "curated"in df .columns or "curated_by"in df .columns :
-            if "curated"not in df .columns :
-                df ["curated"]=pd .NA
-            if "curated_by"in df .columns :
-                mask =df ["curated"].isna ()
-                df .loc [mask ,"curated"]=df .loc [mask ,"curated_by"].notna ()
-            df ["curated"]=df ["curated"].astype ("boolean")
-        df =self ._validate_foreign_keys (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if self ._should_enrich_compound_record ():
-            df =self ._enrich_compound_record (df )
-        if self ._should_enrich_assay ():
-            df =self ._enrich_assay (df )
-        if self ._should_enrich_molecule ():
-            df =self ._enrich_molecule (df )
-        if self ._should_enrich_data_validity ():
-            df =self ._enrich_data_validity (df )
-        df =self ._finalize_identifier_columns (df ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        df =self ._finalize_output_columns (df ,log )
-        df =self ._filter_invalid_required_fields (df ,log )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against ActivitySchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        if "target_tax_id"in df .columns :
-            dtype_name :str =str (df ["target_tax_id"].dtype .name )
-            if dtype_name !="Int64":
-                numeric_series :pd .Series [Any ]=pd .to_numeric (df ["target_tax_id"],errors ="coerce")
-                df ["target_tax_id"]=numeric_series .astype ("Int64")
-        self ._check_activity_id_uniqueness (df ,log )
-        self ._check_foreign_key_integrity (df ,log )
-        self ._validate_data_validity_comment_soft_enum (df ,log )
-        original_coerce =self .config .validation .coerce
-        try :
-            self .config .validation .coerce =False
-            validated =super ().validate (df )
-            log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce )
-            return validated
-        except pandera .errors .SchemaErrors as exc :
-            failure_cases_df :pd .DataFrame |None =None
-            if hasattr (exc ,"failure_cases"):
-                failure_cases_df =cast (pd .DataFrame ,exc .failure_cases )
-            error_count =len (failure_cases_df )if failure_cases_df is not None else 0
-            error_summary =summarize_schema_errors (exc )
-            log .error ("validation_failed",error_count =error_count ,schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce ,error_summary =error_summary ,exc_info =True )
-            if failure_cases_df is not None and (not failure_cases_df .empty ):
-                failure_cases_summary =format_failure_cases (failure_cases_df )
-                log .error ("validation_failure_cases",failure_cases =failure_cases_summary )
-                self ._log_detailed_validation_errors (failure_cases_df ,df ,log )
-            msg =f'Validation failed with {error_count } error(s) against schema {self .config .validation .schema_out }: {error_summary }'
-            raise ValueError (msg )from exc
-        except Exception as exc :
-            log .error ("validation_error",error =str (exc ),schema =self .config .validation .schema_out ,exc_info =True )
-            raise
-        finally :
-            self .config .validation .coerce =original_coerce
-    def _should_enrich_compound_record (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 compound_record \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            compound_record_section :Any =enrich_section .get ("compound_record")
-            if not isinstance (compound_record_section ,Mapping ):
-                return False
-            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-            enabled :Any =compound_record_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        compound_record_section :Any =enrich_section .get ("compound_record")
-                        if isinstance (compound_record_section ,Mapping ):
-                            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                            enrich_cfg =dict (compound_record_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_assay (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 assay \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            assay_section :Any =enrich_section .get ("assay")
-            if not isinstance (assay_section ,Mapping ):
-                return False
-            assay_section =cast (Mapping [str ,Any ],assay_section )
-            enabled :Any =assay_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        assay_section :Any =enrich_section .get ("assay")
-                        if isinstance (assay_section ,Mapping ):
-                            assay_section =cast (Mapping [str ,Any ],assay_section )
-                            enrich_cfg =dict (assay_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_assay (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_data_validity (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 data_validity \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            data_validity_section :Any =enrich_section .get ("data_validity")
-            if not isinstance (data_validity_section ,Mapping ):
-                return False
-            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-            enabled :Any =data_validity_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        data_validity_section :Any =enrich_section .get ("data_validity")
-                        if isinstance (data_validity_section ,Mapping ):
-                            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                            enrich_cfg =dict (data_validity_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        if "data_validity_description"in df .columns :
-            non_na_count =int (df ["data_validity_description"].notna ().sum ())
-            if non_na_count >0 :
-                log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_molecule (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 molecule \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            molecule_section :Any =enrich_section .get ("molecule")
-            if not isinstance (molecule_section ,Mapping ):
-                return False
-            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-            enabled :Any =molecule_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        molecule_section :Any =enrich_section .get ("molecule")
-                        if isinstance (molecule_section ,Mapping ):
-                            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                            enrich_cfg =dict (molecule_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-        if "molecule_name"in df_join .columns :
-            if "molecule_pref_name"not in df .columns :
-                df ["molecule_pref_name"]=pd .NA
-            mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-            if mask .any ():
-                df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-                df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-                log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-        return df
-    def _extract_from_chembl (self ,dataset :object ,chembl_client :ChemblClient |Any ,activity_iterator :ChemblActivityClient ,*,limit :int |None =None ,select_fields :Sequence [str ]|None =None )->pd .DataFrame :
-        "Extract activity records by delegating batching to ``ChemblActivityClient``."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        method_start =time .perf_counter ()
-        self ._last_batch_extract_stats =None
-        input_frame =self ._coerce_activity_dataset (dataset )
-        if "activity_id"not in input_frame .columns :
-            msg ="Input dataset must contain an 'activity_id' column"
-            raise ValueError (msg )
-        normalized_ids =self ._normalize_activity_ids (input_frame ,limit =limit ,log =log )
-        if not normalized_ids :
-            summary :dict [str ,Any ]={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-            self ._last_batch_extract_stats =summary
-            log .info ("chembl_activity.batch_summary",**summary )
-            empty_frame =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-            return self ._ensure_comment_fields (empty_frame ,log )
-        records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =select_fields )
-        duration_ms =(time .perf_counter ()-method_start )*1000.0
-        summary ["duration_ms"]=duration_ms
-        self ._last_batch_extract_stats =summary
-        log .info ("chembl_activity.batch_summary",**summary )
-        result_df :pd .DataFrame =pd .DataFrame .from_records (records )
-        if result_df .empty :
-            result_df =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        elif "activity_id"in result_df .columns :
-            result_df =result_df .sort_values ("activity_id").reset_index (drop =True )
-        result_df =self ._ensure_comment_fields (result_df ,log )
-        result_df =self ._extract_data_validity_descriptions (result_df ,chembl_client ,log )
-        result_df =self ._extract_assay_fields (result_df ,chembl_client ,log )
-        self ._log_validity_comments_metrics (result_df ,log )
-        return result_df
-    def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-        cache_config =self .config .cache
-        if not cache_config .enabled :
-            return None
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        if not cache_file .exists ():
-            return None
-        try :
-            stat =cache_file .stat ()
-        except OSError :
-            return None
-        ttl_seconds =int (cache_config .ttl )
-        if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        try :
-            payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-        except (OSError ,json .JSONDecodeError ):
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        if not isinstance (payload ,dict ):
-            return None
-        missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-        if missing :
-            return None
-        return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
-    def _store_cache (self ,batch_ids :Sequence [str ],batch_data :Mapping [str ,Mapping [str ,Any ]],release :str |None )->None :
-        cache_config =self .config .cache
-        if not cache_config .enabled or not batch_ids or (not batch_data ):
-            return
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        normalized_set =set (normalized_ids )
-        data_to_store ={key :batch_data [key ]for key in normalized_set if key in batch_data }
-        if not data_to_store :
-            return
-        try :
-            cache_file .parent .mkdir (parents =True ,exist_ok =True )
-            tmp_path =cache_file .with_suffix (cache_file .suffix +".tmp")
-            tmp_path .write_text (json .dumps (data_to_store ,sort_keys =True ,default =str ),encoding ="utf-8")
-            tmp_path .replace (cache_file )
-        except Exception as exc :
-            log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-            log .debug ("chembl_activity.cache_store_failed",error =str (exc ))
-    def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-        directory =self ._cache_directory (release )
-        cache_key =self ._cache_key (batch_ids ,release )
-        return directory /f'{cache_key }.json'
-    def _cache_directory (self ,release :str |None )->Path :
-        cache_root =Path (self .config .paths .cache_root )
-        directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-        release_component =self ._sanitize_cache_component (release or "unknown")
-        pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-        version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-        return cache_root /directory_name /pipeline_component /release_component /version_component
-    def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-        payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-        raw =json .dumps (payload ,sort_keys =True )
-        return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
-    @staticmethod
-    def _sanitize_cache_component (value :str )->str :
-        sanitized =re .sub ("[^0-9A-Za-z_.-]","_",value )
-        return sanitized or "default"
-    def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-        "Create fallback record enriched with error metadata."
-        base_message ="Fallback: ChEMBL activity unavailable"
-        message =f'{base_message } ({error })'if error else base_message
-        timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-        metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-        if isinstance (error ,RequestException ):
-            response =getattr (error ,"response",None )
-            status_code =getattr (response ,"status_code",None )
-            if status_code is not None :
-                metadata ["http_status"]=status_code
-            metadata ["error_message"]=str (error )
-        elif error is not None :
-            metadata ["error_message"]=str (error )
-        fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-        return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
-    def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-        if df .empty :
-            return df
-        required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-        if missing_fields :
-            for field in missing_fields :
-                df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            log .debug ("comment_fields_ensured",fields =missing_fields )
-        return df
-    def _extract_data_validity_descriptions (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c data_validity_description \u0438\u0437 DATA_VALIDITY_LOOKUP \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f data_validity_comment \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_data_validity_lookup() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c data_validity_comment.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 data_validity_description.\n        "
-        if df .empty :
-            return df
-        if "data_validity_comment"not in df .columns :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="data_validity_comment_column_missing")
-            return df
-        validity_comments :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            comment =row .get ("data_validity_comment")
-            if pd .isna (comment )or comment is None :
-                continue
-            comment_str =str (comment ).strip ()
-            if comment_str :
-                validity_comments .append (comment_str )
-        if not validity_comments :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="no_valid_comments")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        unique_comments =list (set (validity_comments ))
-        log .info ("extract_data_validity_descriptions_fetching",comments_count =len (unique_comments ))
-        try :
-            records_dict =client .fetch_data_validity_lookup (comments =unique_comments ,fields =["data_validity_comment","description"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_data_validity_descriptions_fetch_error",error =str (exc ),exc_info =True )
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for comment in unique_comments :
-            record =records_dict .get (comment )
-            if record :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":record .get ("description")})
-            else :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":None })
-        if not enrichment_data :
-            log .debug ("extract_data_validity_descriptions_no_records")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        original_index =df .index .copy ()
-        df_result =df .merge (df_enrich ,on =["data_validity_comment"],how ="left",suffixes =("","_enrich"))
-        if "data_validity_description"not in df_result .columns :
-            df_result ["data_validity_description"]=pd .Series ([pd .NA ]*len (df_result ),dtype ="string")
-        else :
-            df_result ["data_validity_description"]=df_result ["data_validity_description"].astype ("string")
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_data_validity_descriptions_complete",comments_requested =len (unique_comments ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _extract_assay_fields (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c assay_organism \u0438 assay_tax_id \u0438\u0437 ASSAYS \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f assay_chembl_id \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_assays_by_ids() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438 assay_organism \u0438 assay_tax_id.\n        "
-        if df .empty :
-            return df
-        if "assay_chembl_id"not in df .columns :
-            log .debug ("extract_assay_fields_skipped",reason ="assay_chembl_id_column_missing")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        assay_ids :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            assay_id =row .get ("assay_chembl_id")
-            if pd .isna (assay_id )or assay_id is None :
-                continue
-            assay_id_str =str (assay_id ).strip ().upper ()
-            if assay_id_str :
-                assay_ids .append (assay_id_str )
-        if not assay_ids :
-            log .debug ("extract_assay_fields_skipped",reason ="no_valid_assay_ids")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        unique_assay_ids =list (set (assay_ids ))
-        log .info ("extract_assay_fields_fetching",assay_ids_count =len (unique_assay_ids ))
-        try :
-            records_dict =client .fetch_assays_by_ids (ids =unique_assay_ids ,fields =["assay_chembl_id","assay_organism","assay_tax_id"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_assay_fields_fetch_error",error =str (exc ),exc_info =True )
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for assay_id in unique_assay_ids :
-            record =records_dict .get (assay_id )if records_dict else None
-            if record :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":record .get ("assay_organism"),"assay_tax_id":record .get ("assay_tax_id")})
-            else :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":None ,"assay_tax_id":None })
-        if not enrichment_data :
-            log .debug ("extract_assay_fields_no_records")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        df_enrich ["assay_chembl_id"]=df_enrich ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        original_index =df .index .copy ()
-        df_normalized =df .copy ()
-        df_normalized ["assay_chembl_id_normalized"]=df_normalized ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        df_result =df_normalized .merge (df_enrich ,left_on ="assay_chembl_id_normalized",right_on ="assay_chembl_id",how ="left",suffixes =("","_enrich"))
-        df_result =df_result .drop (columns =["assay_chembl_id_normalized"])
-        for col in ["assay_organism","assay_tax_id"]:
-            if f'{col }_enrich'in df_result .columns :
-                if col not in df_result .columns :
-                    df_result [col ]=df_result [f'{col }_enrich']
-                else :
-                    base_series :pd .Series [Any ]=df_result [col ]
-                    enrich_series :pd .Series [Any ]=df_result [f'{col }_enrich']
-                    missing_mask =base_series .isna ()
-                    if bool (missing_mask .any ()):
-                        df_result .loc [missing_mask ,col ]=enrich_series .loc [missing_mask ]
-                df_result =df_result .drop (columns =[f'{col }_enrich'])
-        for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-            if col not in df_result .columns :
-                df_result [col ]=pd .Series ([pd .NA ]*len (df_result ),dtype =dtype )
-        df_result ["assay_organism"]=df_result ["assay_organism"].astype ("string")
-        df_result ["assay_tax_id"]=pd .to_numeric (df_result ["assay_tax_id"],errors ="coerce").astype ("Int64")
-        mask_valid =df_result ["assay_tax_id"].notna ()
-        if mask_valid .any ():
-            invalid_mask =mask_valid &(df_result ["assay_tax_id"]<1 )
-            if invalid_mask .any ():
-                log .warning ("invalid_assay_tax_id_range",count =int (invalid_mask .sum ()))
-                df_result .loc [invalid_mask ,"assay_tax_id"]=pd .NA
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_assay_fields_complete",assay_ids_requested =len (unique_assay_ids ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _log_validity_comments_metrics (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "\u041b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442:\n        - \u0414\u043e\u043b\u044e NA \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0442\u0440\u0435\u0445 \u043f\u043e\u043b\u0435\u0439\n        - \u0422\u043e\u043f-10 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment\n        - \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment (\u043d\u0435 \u0432 whitelist)\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity.\n        log:\n            Logger instance.\n        "
-        if df .empty :
-            return
-        metrics :dict [str ,Any ]={}
-        comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        for field in comment_fields :
-            if field in df .columns :
-                na_count =int (df [field ].isna ().sum ())
-                total_count =len (df )
-                na_rate =float (na_count )/float (total_count )if total_count >0 else 0.0
-                metrics [f'{field }_na_rate']=na_rate
-                metrics [f'{field }_na_count']=na_count
-                metrics [f'{field }_total_count']=total_count
-        non_null_comments_series :pd .Series [str ]|None =None
-        if "data_validity_comment"in df .columns :
-            series_candidate =df ["data_validity_comment"].dropna ()
-            if len (series_candidate )>0 :
-                typed_series :pd .Series [str ]=series_candidate .astype ("string")
-                non_null_comments_series =typed_series
-                value_counts =typed_series .value_counts ().head (10 )
-                top_10 ={str (key ):int (value )for key ,value in value_counts .items ()}
-                metrics ["top_10_data_validity_comments"]=top_10
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if whitelist and non_null_comments_series is not None :
-            whitelist_set :set [str ]=set (whitelist )
-            def _is_unknown (value :str )->bool :
-                return value not in whitelist_set
-            unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-            unknown_count =int (unknown_mask .sum ())
-            if unknown_count >0 :
-                unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-                metrics ["unknown_data_validity_comments_count"]=unknown_count
-                metrics ["unknown_data_validity_comments_samples"]=unknown_values
-                log .warning ("unknown_data_validity_comments_detected",unknown_count =unknown_count ,samples =unknown_values ,whitelist =whitelist )
-        if metrics :
-            log .info ("validity_comments_metrics",**metrics )
-    def _get_data_validity_comment_whitelist (self )->list [str ]:
-        "\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c whitelist \u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0434\u043b\u044f data_validity_comment \u0438\u0437 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\n\n        Raises\n        ------\n        RuntimeError\n            \u0415\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d \u0438\u043b\u0438 \u043f\u0443\u0441\u0442.\n        "
-        try :
-            values =sorted (self ._required_vocab_ids ("data_validity_comment"))
-        except RuntimeError as exc :
-            UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validation',run_id =self .run_id ).error ("data_validity_comment_whitelist_unavailable",error =str (exc ))
-            raise
-        return values
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested assay and molecule objects."
-        if "assay"in record and isinstance (record ["assay"],Mapping ):
-            assay =cast (Mapping [str ,Any ],record ["assay"])
-            if "organism"in assay :
-                record .setdefault ("assay_organism",assay ["organism"])
-            if "tax_id"in assay :
-                record .setdefault ("assay_tax_id",assay ["tax_id"])
-        if "molecule"in record and isinstance (record ["molecule"],Mapping ):
-            molecule =cast (Mapping [str ,Any ],record ["molecule"])
-            if "pref_name"in molecule :
-                record .setdefault ("molecule_pref_name",molecule ["pref_name"])
-        if "curated_by"in record :
-            curated_by =record .get ("curated_by")
-            if curated_by is not None and (not pd .isna (curated_by )):
-                record .setdefault ("curated",True )
-            else :
-                record .setdefault ("curated",False )
-        elif "curated"not in record :
-            record .setdefault ("curated",None )
-        return record
-    def _extract_activity_properties_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract TRUV fields, standard_* fields, and comments from activity_properties array as fallback.\n\n        \u041f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0438\u0437 activity_properties \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442\n        \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043e\u0442\u0432\u0435\u0442\u0435 API (\u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 \u043f\u0440\u044f\u043c\u044b\u0445 \u043f\u043e\u043b\u0435\u0439 \u0438\u0437 ACTIVITIES).\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0435 TRUV-\u043f\u043e\u043b\u044f: value, text_value, relation, units.\n        \u0422\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f: standard_upper_value, standard_text_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b: upper_value, lower_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0438: activity_comment, data_validity_comment.\n\n        \u0422\u0430\u043a\u0436\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 activity_properties \u0432 \u0437\u0430\u043f\u0438\u0441\u0438 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438.\n        \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044e \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract_activity_properties')
-        activity_id =record .get ("activity_id")
-        if "activity_properties"not in record :
-            log .warning ("activity_properties_missing",activity_id =activity_id ,message ="activity_properties not found in API response (possible ChEMBL < v24)")
-            record ["activity_properties"]=None
-            return record
-        properties =record ["activity_properties"]
-        if properties is None :
-            log .debug ("activity_properties_null",activity_id =activity_id ,message ="activity_properties is None (possible ChEMBL < v24)")
-            return record
-        if isinstance (properties ,str ):
-            try :
-                properties =json .loads (properties )
-            except (TypeError ,ValueError ,json .JSONDecodeError )as exc :
-                log .debug ("activity_properties_parse_failed",error =str (exc ),activity_id =record .get ("activity_id"))
-                return record
-        if not isinstance (properties ,Sequence )or isinstance (properties ,(str ,bytes )):
-            return record
-        property_iterable :Iterable [Any ]=cast (Iterable [Any ],properties )
-        property_items :list [Any ]=list (property_iterable )
-        def _set_fallback (key :str ,value :Any )->None :
-            "Set fallback value only if key is missing in record and value is not None."
-            if value is not None and record .get (key )is None :
-                record [key ]=value
-        def _is_empty (value :Any )->bool :
-            "Check if value is empty (None, empty string, or whitespace)."
-            if value is None :
-                return True
-            if isinstance (value ,str ):
-                return not value .strip ()
-            return False
-        items :list [Mapping [str ,Any ]]=[]
-        for property_item in property_items :
-            if isinstance (property_item ,Mapping )and "type"in property_item and ("value"in property_item or "text_value"in property_item ):
-                items .append (cast (Mapping [str ,Any ],property_item ))
-        def _is_measured (p :Mapping [str ,Any ])->bool :
-            rf =p .get ("result_flag")
-            return rf is True or (isinstance (rf ,int )and rf ==1 )
-        items .sort (key =lambda p :not _is_measured (p ))
-        for prop in items :
-            val =prop .get ("value")
-            txt =prop .get ("text_value")
-            rel =prop .get ("relation")
-            unt =prop .get ("units")
-            prop_type =str (prop .get ("type","")).lower ()
-            will_set_value =val is not None and record .get ("value")is None
-            will_set_text_value =txt is not None and record .get ("text_value")is None
-            _set_fallback ("value",val )
-            _set_fallback ("text_value",txt )
-            if will_set_value or will_set_text_value :
-                _set_fallback ("relation",rel )
-                _set_fallback ("units",unt )
-            if unt is not None and record .get ("units")is None :
-                _set_fallback ("units",unt )
-            if record .get ("upper_value")is None and ("upper"in prop_type or prop_type in ("upper_value","upper limit")):
-                if val is not None :
-                    _set_fallback ("upper_value",val )
-            if record .get ("lower_value")is None and ("lower"in prop_type or prop_type in ("lower_value","lower limit")):
-                if val is not None :
-                    _set_fallback ("lower_value",val )
-            if record .get ("standard_upper_value")is None and ("standard_upper"in prop_type or prop_type in ("standard upper","standard upper value")):
-                if val is not None :
-                    _set_fallback ("standard_upper_value",val )
-            if record .get ("standard_text_value")is None and "standard"in prop_type and ("text"in prop_type ):
-                if txt is not None :
-                    _set_fallback ("standard_text_value",txt )
-                elif val is not None :
-                    _set_fallback ("standard_text_value",val )
-        current_comment =record .get ("data_validity_comment")
-        if _is_empty (current_comment ):
-            data_validity_items :list [Mapping [str ,Any ]]=[prop for prop in items if ("data_validity"in str (prop .get ("type","")).lower ()or "validity"in str (prop .get ("type","")).lower ())and (prop .get ("text_value")is not None or prop .get ("value")is not None )]
-            if data_validity_items :
-                measured_items =[p for p in data_validity_items if _is_measured (p )]
-                if measured_items :
-                    prop =measured_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="measured",comment_value =comment_value )
-                else :
-                    prop =data_validity_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="first",comment_value =comment_value )
-            else :
-                log .debug ("data_validity_comment_fallback_no_items",activity_id =record .get ("activity_id"),activity_properties_count =len (items ),has_activity_properties =True )
-        else :
-            log .debug ("data_validity_comment_from_api",activity_id =record .get ("activity_id"),comment_value =current_comment )
-        normalized_properties =self ._normalize_activity_properties_items (property_items ,log )
-        if normalized_properties is not None :
-            validated_properties ,validation_stats =self ._validate_activity_properties_truv (normalized_properties ,log ,activity_id )
-            deduplicated_properties ,dedup_stats =self ._deduplicate_activity_properties (validated_properties ,log ,activity_id )
-            record ["activity_properties"]=deduplicated_properties
-            log .debug ("activity_properties_processed",activity_id =activity_id ,original_count =len (property_items ),normalized_count =len (normalized_properties ),validated_count =len (validated_properties ),deduplicated_count =len (deduplicated_properties ),invalid_count =validation_stats .get ("invalid_count",0 ),duplicates_removed =dedup_stats .get ("duplicates_removed",0 ))
-        else :
-            record ["activity_properties"]=properties
-            log .debug ("activity_properties_normalization_failed",activity_id =activity_id ,message ="activity_properties normalization failed, keeping original")
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
-    @staticmethod
-    def _extract_chembl_release (payload :Mapping [str ,Any ])->str |None :
-        for key in ("chembl_release","chembl_db_version","release","version"):
-            value =payload .get (key )
-            if isinstance (value ,str )and value .strip ():
-                return value
-            if value is not None :
-                return str (value )
-        return None
-    @staticmethod
-    def _extract_page_items (payload :Mapping [str ,Any ],items_keys :Sequence [str ]|None =None )->list [dict [str ,Any ]]:
-        preferred_keys :tuple [str ,...]=("activities",)
-        if items_keys is None :
-            combined_keys =preferred_keys +("data","items","results")
-        else :
-            combined_keys =tuple (dict .fromkeys ((*preferred_keys ,*items_keys )))
-        return ChemblPipelineBase ._extract_page_items (payload ,combined_keys )
-    @staticmethod
-    def _next_link (payload :Mapping [str ,Any ],base_url :str )->str |None :
-        page_meta :Any =payload .get ("page_meta")
-        if isinstance (page_meta ,Mapping ):
-            next_link_raw :Any =page_meta .get ("next")
-            next_link :str |None =cast (str |None ,next_link_raw )if next_link_raw is not None else None
-            if isinstance (next_link ,str )and next_link :
-                base_url_str =str (base_url )
-                base_path_parse_result =urlparse (base_url_str )
-                base_path_raw =base_path_parse_result .path
-                base_path_str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                base_path :str =base_path_str .rstrip ("/")
-                if next_link .startswith ("http://")or next_link .startswith ("https://"):
-                    parsed =urlparse (next_link )
-                    base_parsed =urlparse (base_url_str )
-                    parsed_path_raw =parsed .path
-                    base_path_raw =base_parsed .path
-                    path :str =parsed_path_raw .decode ("utf-8","ignore")if isinstance (parsed_path_raw ,(bytes ,bytearray ))else parsed_path_raw
-                    base_path_from_url :str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                    path_normalized :str =path .rstrip ("/")
-                    base_path_normalized :str =base_path_from_url .rstrip ("/")
-                    if base_path_normalized and path_normalized .startswith (base_path_normalized ):
-                        relative_path =path_normalized [len (base_path_normalized ):]
-                        if not relative_path :
-                            return None
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    elif "/api/data/"in path :
-                        parts =path .split ("/api/data/",1 )
-                        if len (parts )>1 :
-                            relative_path ="/"+parts [1 ]
-                        else :
-                            relative_path =path
-                    else :
-                        relative_path =path
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    if parsed .query :
-                        relative_path =f'{relative_path }?{parsed .query }'
-                    return relative_path
-                if base_path :
-                    normalized_base =base_path .lstrip ("/")
-                    stripped_link =next_link .lstrip ("/")
-                    if stripped_link .startswith (normalized_base +"/"):
-                        stripped_link =stripped_link [len (normalized_base ):]
-                    elif stripped_link ==normalized_base :
-                        stripped_link =""
-                    next_link =stripped_link
-                next_link =next_link .lstrip ("/")
-                if next_link :
-                    next_link =f'/{next_link }'
-                else :
-                    next_link ="/"
-                return next_link
-        return None
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Ensure canonical identifier columns are present before normalization."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_chembl_id"not in df .columns and "assay_id"in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "testitem_chembl_id"not in df .columns :
-            if "testitem_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["testitem_id"]
-                actions .append ("testitem_id->testitem_chembl_id")
-            elif "molecule_chembl_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["molecule_chembl_id"]
-                actions .append ("molecule_chembl_id->testitem_chembl_id")
-        if "molecule_chembl_id"not in df .columns and "testitem_chembl_id"in df .columns :
-            df ["molecule_chembl_id"]=df ["testitem_chembl_id"]
-            actions .append ("testitem_chembl_id->molecule_chembl_id")
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_required =[column for column in required_columns if column not in df .columns ]
-        if missing_required :
-            for column in missing_required :
-                df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            actions .append (f"created_missing:{",".join (missing_required )}")
-        alias_columns =[column for column in ("assay_id","testitem_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        boolean_columns =("potential_duplicate","curated","removed")
-        for column in boolean_columns :
-            specs [column ]={"dtype":"boolean","default":pd .NA }
-        return specs
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize ChEMBL and BAO identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"],pattern ="^CHEMBL\\d+$"),IdentifierRule (name ="bao",columns =["bao_endpoint","bao_format"],pattern ="^BAO_\\d{7}$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _finalize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align identifier columns after normalization and drop aliases."
-        df =df .copy ()
-        if {"molecule_chembl_id","testitem_chembl_id"}.issubset (df .columns ):
-            mismatch_mask =df ["molecule_chembl_id"].notna ()&df ["testitem_chembl_id"].notna ()&(df ["molecule_chembl_id"]!=df ["testitem_chembl_id"])
-            if mismatch_mask .any ():
-                mismatch_count =int (mismatch_mask .sum ())
-                samples_raw =df .loc [mismatch_mask ,["molecule_chembl_id","testitem_chembl_id"]].drop_duplicates ().head (5 ).to_dict ("records")
-                samples :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],samples_raw )
-                log .warning ("identifier_mismatch",count =mismatch_count ,samples =samples )
-                df .loc [mismatch_mask ,"testitem_chembl_id"]=df .loc [mismatch_mask ,"molecule_chembl_id"]
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_columns =[column for column in required_columns if column not in df .columns ]
-        if missing_columns :
-            for column in missing_columns :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            log .warning ("identifier_columns_missing",columns =missing_columns )
-        return df
-    def _finalize_output_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align final column order with schema and drop unexpected fields."
-        df =df .copy ()
-        expected =list (COLUMN_ORDER )
-        extras =[column for column in df .columns if column not in expected ]
-        if extras :
-            df =df .drop (columns =extras )
-            log .debug ("output_columns_dropped",columns =extras )
-        missing =[column for column in expected if column not in df .columns ]
-        if missing :
-            for column in missing :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([pd .NA ]*len (df ),dtype ="object")
-            log .warning ("output_columns_missing",columns =missing )
-        if not expected :
-            return df
-        return df [expected ]
-    def _filter_invalid_required_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Filter out rows with NULL values in required identifier fields.\n\n        Removes rows where any of the required fields (assay_chembl_id,\n        testitem_chembl_id, molecule_chembl_id) are NULL, as these cannot\n        pass schema validation.\n\n        Parameters\n        ----------\n        df:\n            DataFrame to filter.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            Filtered DataFrame with only rows having all required fields populated.\n        "
-        df =df .copy ()
-        if df .empty :
-            return df
-        required_fields =["assay_chembl_id","molecule_chembl_id"]
-        missing_fields =[field for field in required_fields if field not in df .columns ]
-        if missing_fields :
-            log .warning ("filter_skipped_missing_columns",missing_columns =missing_fields ,message ="Cannot filter: required columns are missing")
-            return df
-        valid_mask =df ["assay_chembl_id"].notna ()&df ["molecule_chembl_id"].notna ()
-        invalid_count =int ((~valid_mask ).sum ())
-        if invalid_count >0 :
-            invalid_rows =df [~valid_mask ]
-            sample_size =min (5 ,len (invalid_rows ))
-            sample_activity_ids =invalid_rows ["activity_id"].head (sample_size ).tolist ()if "activity_id"in invalid_rows .columns else []
-            log .warning ("filtered_invalid_rows",filtered_count =invalid_count ,remaining_count =int (valid_mask .sum ()),sample_activity_ids =sample_activity_ids ,message ="Rows with NULL in required identifier fields were filtered out")
-            df =df [valid_mask ].reset_index (drop =True )
-        return df
-    def _normalize_measurements (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize standard_value, standard_units, standard_relation, and standard_type."
-        df =df .copy ()
-        normalized_count =0
-        if "standard_value"in df .columns :
-            mask =df ["standard_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_value"]=numeric_series_std
-                negative_mask =mask &(df ["standard_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["standard_relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"standard_relation"]=series
-                invalid_mask =mask &~df ["standard_relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_type"in df .columns :
-            mask =df ["standard_type"].notna ()
-            if mask .any ():
-                df .loc [mask ,"standard_type"]=df .loc [mask ,"standard_type"].astype (str ).str .strip ()
-                standard_types_set :set [str ]=STANDARD_TYPES
-                invalid_mask =mask &~df ["standard_type"].isin (standard_types_set )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_type",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_type"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_units"in df .columns :
-            unit_mapping ={"nanomolar":"nM","nmol":"nM","nm":"nM","NM":"nM","\u00b5M":"\u03bcM","uM":"\u03bcM","UM":"\u03bcM","micromolar":"\u03bcM","microM":"\u03bcM","umol":"\u03bcM","millimolar":"mM","milliM":"mM","mmol":"mM","MM":"mM","percent":"%","pct":"%","ratios":"ratio"}
-            mask =df ["standard_units"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_units"].astype (str ).str .strip ()
-                for old_unit ,new_unit in unit_mapping .items ():
-                    series =series .str .replace (old_unit ,new_unit ,regex =False ,case =False )
-                df .loc [mask ,"standard_units"]=series
-                normalized_count +=int (mask .sum ())
-        if "relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"relation"]=series
-                invalid_mask =mask &~df ["relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_upper_value"in df .columns :
-            mask =df ["standard_upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_upper_value"]=numeric_series_std_upper
-                negative_mask =mask &(df ["standard_upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "upper_value"in df .columns :
-            mask =df ["upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"upper_value"]=numeric_series_upper
-                negative_mask =mask &(df ["upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "lower_value"in df .columns :
-            mask =df ["lower_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"lower_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_lower :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"lower_value"]=numeric_series_lower
-                negative_mask =mask &(df ["lower_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_lower_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"lower_value"]=None
-                normalized_count +=int (mask .sum ())
-        if normalized_count >0 :
-            log .debug ("measurements_normalized",normalized_count =normalized_count )
-        return df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize string fields: trim, empty string to null, title-case for organism."
-        working_df =df .copy ()
-        if "data_validity_description"in working_df .columns and "data_validity_comment"in working_df .columns :
-            invalid_mask =working_df ["data_validity_description"].notna ()&working_df ["data_validity_comment"].isna ()
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                log .warning ("invariant_data_validity_description_without_comment",count =invalid_count ,message ="data_validity_description is filled while data_validity_comment is NA")
-        rules :dict [str ,StringRule ]={"canonical_smiles":StringRule (),"bao_label":StringRule (max_length =128 ),"target_organism":StringRule (title_case =True ),"assay_organism":StringRule (title_case =True ),"data_validity_comment":StringRule (),"data_validity_description":StringRule (),"activity_comment":StringRule (),"standard_text_value":StringRule (),"text_value":StringRule (),"type":StringRule (),"units":StringRule (),"assay_type":StringRule (),"assay_description":StringRule (),"molecule_pref_name":StringRule (),"target_pref_name":StringRule (),"uo_units":StringRule (),"qudt_units":StringRule ()}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Serialize nested structures (ligand_efficiency, activity_properties) to JSON strings."
-        df =df .copy ()
-        nested_fields =["ligand_efficiency","activity_properties"]
-        for field in nested_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                serialized :list [Any ]=[]
-                for idx ,value in df .loc [mask ,field ].items ():
-                    if field =="activity_properties":
-                        serialized_value =self ._serialize_activity_properties (value ,log )
-                        serialized .append (serialized_value )
-                        continue
-                    if isinstance (value ,(Mapping ,list )):
-                        try :
-                            serialized .append (json .dumps (value ,ensure_ascii =False ,sort_keys =True ))
-                        except (TypeError ,ValueError )as exc :
-                            log .warning ("nested_serialization_failed",field =field ,index =idx ,error =str (exc ))
-                            serialized .append (None )
-                    elif isinstance (value ,str ):
-                        try :
-                            json .loads (value )
-                            serialized .append (value )
-                        except (TypeError ,ValueError ):
-                            serialized .append (None )
-                    else :
-                        serialized .append (None )
-                df .loc [mask ,field ]=pd .Series (serialized ,dtype ="object",index =df .loc [mask ,field ].index )
-        if "standard_value"in df .columns and "ligand_efficiency"in df .columns :
-            mask =df ["standard_value"].notna ()&df ["ligand_efficiency"].isna ()
-            if mask .any ():
-                log .warning ("ligand_efficiency_missing_with_standard_value",count =int (mask .sum ()),message ="ligand_efficiency is empty while standard_value exists")
-        return df
-    def _serialize_activity_properties (self ,value :Any ,log :BoundLogger |None =None )->str |None :
-        "Return normalized JSON for activity_properties or None if not serializable."
-        normalized_items =self ._normalize_activity_properties_items (value ,log )
-        if normalized_items is None :
-            return None
-        try :
-            return json .dumps (normalized_items ,ensure_ascii =False ,sort_keys =True )
-        except (TypeError ,ValueError )as exc :
-            if log is not None :
-                log .warning ("activity_properties_serialization_failed",error =str (exc ))
-            return None
-    def _normalize_activity_properties_items (self ,value :Any ,log :BoundLogger |None =None )->list [dict [str ,Any ]]|None :
-        "Coerce activity_properties payloads into a list of constrained dictionaries."
-        if value is None :
-            return None
-        raw_value =value
-        if isinstance (value ,str ):
-            stripped =value .strip ()
-            if not stripped :
-                return []
-            try :
-                parsed =json .loads (stripped )
-            except (TypeError ,ValueError ):
-                fallback_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                fallback_base ["text_value"]=stripped
-                return [fallback_base ]
-            else :
-                value =parsed
-        if isinstance (value ,Mapping ):
-            items :list [Any ]=[value ]
-        elif isinstance (value ,Sequence )and (not isinstance (value ,(str ,bytes ))):
-            items =list (value )
-        else :
-            if log is not None :
-                log .warning ("activity_properties_unhandled_type",value_type =type (raw_value ).__name__ )
-            return None
-        normalized :list [dict [str ,Any ]]=[]
-        for item in items :
-            if item is None :
-                continue
-            if isinstance (item ,Mapping ):
-                item_mapping =cast (Mapping [str ,Any ],item )
-                normalized_item :dict [str ,Any |None ]={key :item_mapping .get (key )for key in ACTIVITY_PROPERTY_KEYS }
-                result_flag_value =normalized_item .get ("result_flag")
-                if isinstance (result_flag_value ,int )and result_flag_value in (0 ,1 ):
-                    normalized_item ["result_flag"]=bool (result_flag_value )
-                normalized .append (normalized_item )
-            elif isinstance (item ,str ):
-                str_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                str_base ["text_value"]=item
-                normalized .append (str_base )
-            elif log is not None :
-                log .warning ("activity_properties_item_unhandled",item_type =type (item ).__name__ )
-        return normalized
-    def _validate_activity_properties_truv (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0434\u043b\u044f activity_properties.\n\n        \u0412\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u0435\u0442 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n        - value IS NOT NULL \u21d2 text_value IS NULL (\u0438 \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442)\n        - relation IN ('=', '<', '\u2264', '>', '\u2265', '~') OR NULL\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        "
-        validated :list [dict [str ,Any ]]=[]
-        invalid_count =0
-        invalid_items :list [dict [str ,Any ]]=[]
-        for prop in properties :
-            is_valid =True
-            validation_errors :list [str ]=[]
-            value =prop .get ("value")
-            text_value =prop .get ("text_value")
-            relation =prop .get ("relation")
-            if value is not None and text_value is not None :
-                is_valid =False
-                validation_errors .append ("both value and text_value are not None")
-            elif value is None and text_value is None :
-                pass
-            if relation is not None :
-                if not isinstance (relation ,str ):
-                    is_valid =False
-                    validation_errors .append (f'relation is not a string: {type (relation ).__name__ }')
-                elif relation not in RELATIONS :
-                    is_valid =False
-                    validation_errors .append (f"relation '{relation }' not in allowed values: {RELATIONS }")
-            validated .append (prop )
-            if not is_valid :
-                invalid_count +=1
-                invalid_items .append (prop )
-                log .warning ("activity_property_truv_validation_failed",activity_id =activity_id ,property =prop ,errors =validation_errors ,message ="TRUV validation failed, but property is kept")
-        stats ={"invalid_count":invalid_count ,"valid_count":len (validated )}
-        return (validated ,stats )
-    def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-        seen :set [tuple [Any ,...]]=set ()
-        deduplicated :list [dict [str ,Any ]]=[]
-        duplicates_removed =0
-        for prop in properties :
-            dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-            if dedup_key not in seen :
-                seen .add (dedup_key )
-                deduplicated .append (prop )
-            else :
-                duplicates_removed +=1
-                log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-        stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-        return (deduplicated ,stats )
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_added",value ="activity")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_filled",value ="activity")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :BoundLogger )->pd .DataFrame :
-        "Convert data types according to the Pandera schema."
-        df =df .copy ()
-        non_nullable_int_fields ={"activity_id":"int64"}
-        nullable_int_fields ={"row_index":"Int64","target_tax_id":"int64","assay_tax_id":"int64","record_id":"int64","src_id":"int64"}
-        float_fields ={"standard_value":"float64","standard_upper_value":"float64","pchembl_value":"float64","upper_value":"float64","lower_value":"float64"}
-        bool_fields =["potential_duplicate","curated","removed"]
-        binary_flag_fields =["standard_flag"]
-        for field in non_nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_int :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_int .astype ("Int64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field =="row_index":
-                    numeric_series_row :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=numeric_series_row .astype ("Int64")
-                    if df [field ].isna ().any ():
-                        df [field ]=range (len (df ))
-                else :
-                    nullable_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=nullable_numeric_series .astype ("Int64")
-                    mask_valid =df [field ].notna ()
-                    if mask_valid .any ():
-                        invalid_mask =mask_valid &(df [field ]<1 )
-                        if invalid_mask .any ():
-                            log .warning ("invalid_positive_integer",field =field ,count =int (invalid_mask .sum ()))
-                            df .loc [invalid_mask ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in float_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_float :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_float .astype ("float64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in bool_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field in ("curated","removed")and df [field ].dtype =="boolean":
-                    continue
-                if field in ("curated","removed"):
-                    df [field ]=df [field ].astype ("boolean")
-                else :
-                    bool_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=(bool_numeric_series !=0 ).astype ("boolean")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("bool_conversion_failed",field =field ,error =str (exc ))
-        for field in binary_flag_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_flag :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_flag .astype ("Int64")
-                mask_valid =df [field ].notna ()
-                if mask_valid .any ():
-                    valid_values =df .loc [mask_valid ,field ]
-                    invalid_valid_mask =~valid_values .isin ([0 ,1 ])
-                    if invalid_valid_mask .any ():
-                        invalid_index =valid_values .index [invalid_valid_mask ]
-                        log .warning ("invalid_standard_flag",field =field ,count =int (invalid_valid_mask .sum ()))
-                        df .loc [invalid_index ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        object_fields =["value","activity_properties"]
-        for field in object_fields :
-            if field in df .columns :
-                if df [field ].dtype !="object":
-                    df [field ]=df [field ].astype ("object")
-        return df
-    def _validate_foreign_keys (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Validate foreign key integrity and format of ChEMBL IDs."
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        chembl_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        warnings :list [str ]=[]
-        for field in chembl_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-                if invalid_mask .any ():
-                    warning_msg :str =f'{field }: {int (invalid_mask .sum ())} invalid format(s)'
-                    warnings .append (warning_msg )
-        if warnings :
-            log .warning ("foreign_key_validation",warnings =warnings )
-        return df
-    def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check uniqueness of activity_id before validation."
-        if "activity_id"not in df .columns :
-            log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-            return
-        duplicates =df [df ["activity_id"].duplicated (keep =False )]
-        if not duplicates .empty :
-            duplicate_count =len (duplicates )
-            duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-            log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-            msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-            raise ValueError (msg )
-        log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
-    def _validate_data_validity_comment_soft_enum (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Soft enum \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0434\u043b\u044f data_validity_comment.\n\n        \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0442\u0438\u0432 whitelist \u0438\u0437 \u043a\u043e\u043d\u0444\u0438\u0433\u0430. \u041d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n        \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a warning, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e (soft enum).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        "
-        if df .empty or "data_validity_comment"not in df .columns :
-            return
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if not whitelist :
-            return
-        series_candidate =df ["data_validity_comment"].dropna ()
-        if len (series_candidate )==0 :
-            return
-        non_null_comments_series =series_candidate .astype ("string")
-        whitelist_set :set [str ]=set (whitelist )
-        def _is_unknown (value :str )->bool :
-            return value not in whitelist_set
-        unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-        unknown_count =int (unknown_mask .sum ())
-        if unknown_count >0 :
-            unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-            log .warning ("soft_enum_unknown_data_validity_comment",unknown_count =unknown_count ,total_count =len (non_null_comments_series ),samples =unknown_values ,whitelist =whitelist ,message ="Unknown data_validity_comment values detected (soft enum: not blocking)")
-    def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-        reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        errors :list [str ]=[]
-        for field in reference_fields :
-            if field not in df .columns :
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-                continue
-            mask =df [field ].notna ()
-            if not mask .any ():
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-                continue
-            invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-                errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-                log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-        if errors :
-            log .error ("foreign_key_integrity_check_failed",errors =errors )
-            msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-            raise ValueError (msg )
-        log .debug ("foreign_key_integrity_verified")
-    def _log_detailed_validation_errors (self ,failure_cases :pd .DataFrame ,payload :pd .DataFrame ,log :BoundLogger )->None :
-        "Log individual validation errors with row index and activity_id."
-        if failure_cases .empty or payload .empty :
-            return
-        activity_id_col ="activity_id"if "activity_id"in payload .columns else None
-        index_col ="index"if "index"in failure_cases .columns else None
-        if index_col is None :
-            return
-        max_errors =20
-        errors_to_log =failure_cases .head (max_errors )
-        for _ ,error_row in errors_to_log .iterrows ():
-            row_index =error_row .get (index_col )
-            if row_index is None :
-                continue
-            error_details :dict [str ,Any ]={"row_index":int (row_index )if isinstance (row_index ,(int ,float ))else str (row_index )}
-            if activity_id_col :
-                try :
-                    idx =int (row_index )if isinstance (row_index ,(int ,float ))else row_index
-                    activity_id_value :Any =payload .at [cast (int ,idx ),activity_id_col ]
-                    activity_id =activity_id_value
-                except (KeyError ,IndexError ):
-                    activity_id =None
-                if activity_id is not None and pd .notna (activity_id ):
-                    error_details ["activity_id"]=int (activity_id )if isinstance (activity_id ,(int ,float ))else str (activity_id )
-            if "column"in error_row and pd .notna (error_row ["column"]):
-                error_details ["column"]=str (error_row ["column"])
-            if "schema_context"in error_row and pd .notna (error_row ["schema_context"]):
-                error_details ["schema_context"]=str (error_row ["schema_context"])
-            if "failure_case"in error_row and pd .notna (error_row ["failure_case"]):
-                error_details ["failure_case"]=str (error_row ["failure_case"])
-            log .error ("validation_error_detail",**error_details )
-        if len (failure_cases )>max_errors :
-            log .warning ("validation_errors_truncated",total_errors =len (failure_cases ),logged_errors =max_errors )
-    def build_quality_report (self ,df :pd .DataFrame )->pd .DataFrame |dict [str ,object ]|None :
-        "Return QC report with activity-specific metrics including distributions."
-        business_key =["activity_id"]if "activity_id"in df .columns else None
-        base_report =build_default_quality_report (df ,business_key_fields =business_key )
-        rows :list [dict [str ,Any ]]=[]
-        if not base_report .empty :
-            records_raw =base_report .to_dict ("records")
-            records :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],records_raw )
-            for record in records :
-                rows .append ({str (k ):v for k ,v in record .items ()})
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        foreign_key_fields =["assay_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"]
-        for field in foreign_key_fields :
-            if field in df .columns :
-                mask =df [field ].notna ()
-                if mask .any ():
-                    string_series =df [field ].astype (str )
-                    valid_mask =mask &string_series .str .match (chembl_id_pattern .pattern ,na =False )
-                    invalid_count =int ((mask &~valid_mask ).astype (int ).sum ())
-                    valid_count =int (valid_mask .astype (int ).sum ())
-                    total_count =int (mask .astype (int ).sum ())
-                    integrity_ratio =float (valid_count /total_count )if total_count >0 else 0.0
-                    rows .append ({"section":"foreign_key","metric":"integrity_ratio","column":field ,"value":float (integrity_ratio ),"valid_count":int (valid_count ),"invalid_count":int (invalid_count ),"total_count":int (total_count )})
-        if "standard_type"in df .columns :
-            type_counts_series :pd .Series [Any ]=df ["standard_type"].value_counts ()
-            type_dist_raw =type_counts_series .to_dict ()
-            type_dist :dict [Any ,int ]=cast (dict [Any ,int ],type_dist_raw )
-            for type_value ,count in type_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_type_count","column":"standard_type","value":str (type_value )if type_value is not None else "null","count":int (count )})
-        if "standard_units"in df .columns :
-            unit_counts_series :pd .Series [Any ]=df ["standard_units"].value_counts ()
-            unit_dist_raw =unit_counts_series .to_dict ()
-            unit_dist :dict [Any ,int ]=cast (dict [Any ,int ],unit_dist_raw )
-            for unit_value ,count in unit_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_units_count","column":"standard_units","value":str (unit_value )if unit_value is not None else "null","count":int (count )})
-        return pd .DataFrame (rows )
-    def write (self ,df :pd .DataFrame ,output_path :Path ,*,extended :bool =False ,include_correlation :bool |None =None ,include_qc_metrics :bool |None =None )->RunResult :
-        "Override write() to bind actor and ensure deterministic sorting.\n\n        Parameters\n        ----------\n        df:\n            The DataFrame to write.\n        output_path:\n            The base output path for all artifacts.\n        extended:\n            Whether to include extended QC artifacts.\n\n        Returns\n        -------\n        RunResult:\n            All artifacts generated by the write operation.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.write')
-        UnifiedLogger .bind (actor =self .actor )
-        sort_keys =["assay_chembl_id","testitem_chembl_id","activity_id"]
-        if df .empty or not all ((key in df .columns for key in sort_keys )):
-            return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-        original_sort_by =self .config .determinism .sort .by
-        if not original_sort_by or original_sort_by !=sort_keys :
-            from copy import deepcopy
-            from bioetl .config .models .determinism import DeterminismSortingConfig
-            modified_config =deepcopy (self .config )
-            modified_config .determinism .sort =DeterminismSortingConfig (by =sort_keys ,ascending =[True ,True ,True ],na_position ="last")
-            log .debug ("write_sort_config_set",sort_keys =sort_keys ,original_sort_keys =list (original_sort_by )if original_sort_by else [])
-            original_config =self .config
-            self .config =modified_config
-            try :
-                result =super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-            finally :
-                self .config =original_config
-            return result
-        return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
```

#### Горячий участок 2

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:111-115
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,5 +0,0 @@

-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
```

#### Горячий участок 3

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2901-2924
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,18 +0,0 @@

-def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_added",value ="activity")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_filled",value ="activity")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Горячий участок 4

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:343-415
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,22 +0,0 @@

-def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-    "Construct the descriptor driving the shared extraction template."
-    def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-        http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-        chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-        typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =source_config .parameters .select_fields
-        return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-    def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-    def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        df =pipeline ._ensure_comment_fields (df ,log )
-        chembl_client =cast (ChemblClient ,context .chembl_client )
-        if chembl_client is not None :
-            df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-        pipeline ._log_validity_comments_metrics (df ,log )
-        return df
-    def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        return pipeline ._materialize_activity_record (payload )
-    return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
```

#### Горячий участок 5

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1257-1267
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,7 +0,0 @@

-def _cache_directory (self ,release :str |None )->Path :
-    cache_root =Path (self .config .paths .cache_root )
-    directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-    release_component =self ._sanitize_cache_component (release or "unknown")
-    pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-    version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-    return cache_root /directory_name /pipeline_component /release_component /version_component
```

#### Горячий участок 6

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1252-1255
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,4 +0,0 @@

-def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-    directory =self ._cache_directory (release )
-    cache_key =self ._cache_key (batch_ids ,release )
-    return directory /f'{cache_key }.json'
```

#### Горячий участок 7

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1269-1277
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,4 +0,0 @@

-def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-    payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-    raw =json .dumps (payload ,sort_keys =True )
-    return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
```

#### Горячий участок 8

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:3108-3131
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,13 +0,0 @@

-def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check uniqueness of activity_id before validation."
-    if "activity_id"not in df .columns :
-        log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-        return
-    duplicates =df [df ["activity_id"].duplicated (keep =False )]
-    if not duplicates .empty :
-        duplicate_count =len (duplicates )
-        duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-        log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-        msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-        raise ValueError (msg )
-    log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
```

#### Горячий участок 9

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1176-1221
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,33 +0,0 @@

-def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-    cache_config =self .config .cache
-    if not cache_config .enabled :
-        return None
-    normalized_ids =[str (identifier )for identifier in batch_ids ]
-    cache_file =self ._cache_file_path (normalized_ids ,release )
-    if not cache_file .exists ():
-        return None
-    try :
-        stat =cache_file .stat ()
-    except OSError :
-        return None
-    ttl_seconds =int (cache_config .ttl )
-    if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    try :
-        payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-    except (OSError ,json .JSONDecodeError ):
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    if not isinstance (payload ,dict ):
-        return None
-    missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-    if missing :
-        return None
-    return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
```

#### Горячий участок 10

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:3185-3232
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,24 +0,0 @@

-def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-    reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-    chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-    errors :list [str ]=[]
-    for field in reference_fields :
-        if field not in df .columns :
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-            continue
-        mask =df [field ].notna ()
-        if not mask .any ():
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-            continue
-        invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-        if invalid_mask .any ():
-            invalid_count =int (invalid_mask .sum ())
-            invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-            errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-            log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-    if errors :
-        log .error ("foreign_key_integrity_check_failed",errors =errors )
-        msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-        raise ValueError (msg )
-    log .debug ("foreign_key_integrity_verified")
```

#### Горячий участок 11

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:432-450
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,14 +0,0 @@

-def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-    "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-    if isinstance (dataset ,pd .Series ):
-        return dataset .to_frame (name ="activity_id")
-    if isinstance (dataset ,pd .DataFrame ):
-        return dataset
-    if isinstance (dataset ,Mapping ):
-        mapping =cast (Mapping [str ,Any ],dataset )
-        return pd .DataFrame ([dict (mapping )])
-    if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-        dataset_list :list [Any ]=list (dataset )
-        return pd .DataFrame ({"activity_id":dataset_list })
-    msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-    raise TypeError (msg )
```

#### Горячий участок 12

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2065-2068
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,5 +0,0 @@

-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Горячий участок 13

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:498-637
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,83 +0,0 @@

-def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-    "Iterate over IDs using the shared iterator while preserving cache semantics."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    records :list [dict [str ,Any ]]=[]
-    success_count =0
-    fallback_count =0
-    error_count =0
-    cache_hits =0
-    api_calls =0
-    total_batches =0
-    key_order =[key for _ ,key in normalized_ids ]
-    key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-    for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-        total_batches +=1
-        batch_start =time .perf_counter ()
-        from_cache =False
-        chunk_records :dict [str ,dict [str ,Any ]]={}
-        try :
-            cached_records =self ._check_cache (chunk ,self ._chembl_release )
-            if cached_records is not None :
-                from_cache =True
-                cache_hits +=len (chunk )
-                chunk_records =cached_records
-            else :
-                api_calls +=1
-                fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                for item in fetched_items :
-                    if not isinstance (item ,Mapping ):
-                        continue
-                    activity_value =item .get ("activity_id")
-                    if activity_value is None :
-                        continue
-                    chunk_records [str (activity_value )]=dict (item )
-                self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-            success_in_batch =0
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                record =chunk_records .get (key )
-                if record and (not record .get ("error")):
-                    materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                    records .append (materialized )
-                    success_count +=1
-                    success_in_batch +=1
-                else :
-                    fallback_record =self ._create_fallback_record (numeric_id )
-                    records .append (fallback_record )
-                    fallback_count +=1
-                    error_count +=1
-            batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-            log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-        except CircuitBreakerOpenError as exc :
-            log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except RequestException as exc :
-            log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except Exception as exc :
-            log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-    total_records =len (normalized_ids )
-    success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-    summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-    return (records ,summary )
```

#### Горячий участок 14

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1284-1323
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,16 +0,0 @@

-def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-    "Create fallback record enriched with error metadata."
-    base_message ="Fallback: ChEMBL activity unavailable"
-    message =f'{base_message } ({error })'if error else base_message
-    timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-    metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-    if isinstance (error ,RequestException ):
-        response =getattr (error ,"response",None )
-        status_code =getattr (response ,"status_code",None )
-        if status_code is not None :
-            metadata ["http_status"]=status_code
-        metadata ["error_message"]=str (error )
-    elif error is not None :
-        metadata ["error_message"]=str (error )
-    fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-    return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
```

#### Горячий участок 15

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2843-2899
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,15 +0,0 @@

-def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-    "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-    seen :set [tuple [Any ,...]]=set ()
-    deduplicated :list [dict [str ,Any ]]=[]
-    duplicates_removed =0
-    for prop in properties :
-        dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-        if dedup_key not in seen :
-            seen .add (dedup_key )
-            deduplicated .append (prop )
-        else :
-            duplicates_removed +=1
-            log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-    stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-    return (deduplicated ,stats )
```

#### Горячий участок 16

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:892-935
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,28 +0,0 @@

-def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    assay_section :Any =enrich_section .get ("assay")
-                    if isinstance (assay_section ,Mapping ):
-                        assay_section =cast (Mapping [str ,Any ],assay_section )
-                        enrich_cfg =dict (assay_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_assay (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 17

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:822-867
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,28 +0,0 @@

-def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    compound_record_section :Any =enrich_section .get ("compound_record")
-                    if isinstance (compound_record_section ,Mapping ):
-                        compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                        enrich_cfg =dict (compound_record_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 18

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:960-1014
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,32 +0,0 @@

-def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    data_validity_section :Any =enrich_section .get ("data_validity")
-                    if isinstance (data_validity_section ,Mapping ):
-                        data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                        enrich_cfg =dict (data_validity_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    if "data_validity_description"in df .columns :
-        non_na_count =int (df ["data_validity_description"].notna ().sum ())
-        if non_na_count >0 :
-            log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 19

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1039-1108
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,37 +0,0 @@

-def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    molecule_section :Any =enrich_section .get ("molecule")
-                    if isinstance (molecule_section ,Mapping ):
-                        molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                        enrich_cfg =dict (molecule_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-    if "molecule_name"in df_join .columns :
-        if "molecule_pref_name"not in df .columns :
-            df ["molecule_pref_name"]=pd .NA
-        mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-        if mask .any ():
-            df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-            df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-            log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-    return df
```

#### Горячий участок 20

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1325-1358
- assay: нет в ветке

```diff
--- activity:run.py

+++ assay:run.py

@@ -1,11 +0,0 @@

-def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-    if df .empty :
-        return df
-    required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-    missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-    if missing_fields :
-        for field in missing_fields :
-            df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-        log .debug ("comment_fields_ensured",fields =missing_fields )
-    return df
```

### Модуль transform.py

Определение                    | activity сигнатура | assay сигнатура                                | Побочные эффекты                                                                                                      | Исключения                                    | Статус        
-------------------------------|--------------------|------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|---------------
__module_block_0               | —                  | —                                              | activity: {}
assay: {'logging': [], 'io': []}                                                                         | activity: []
assay: []                        | только в assay
__module_block_1               | —                  | —                                              | activity: {}
assay: {'logging': [], 'io': []}                                                                         | activity: []
assay: []                        | только в assay
__module_block_2               | —                  | —                                              | activity: {}
assay: {'logging': [], 'io': []}                                                                         | activity: []
assay: []                        | только в assay
__module_block_3               | —                  | —                                              | activity: {}
assay: {'logging': [], 'io': []}                                                                         | activity: []
assay: []                        | только в assay
__module_block_4               | —                  | —                                              | activity: {}
assay: {'logging': [], 'io': []}                                                                         | activity: []
assay: []                        | только в assay
__module_block_5               | —                  | —                                              | activity: {}
assay: {'logging': [], 'io': []}                                                                         | activity: []
assay: []                        | только в assay
__module_block_6               | —                  | —                                              | activity: {}
assay: {'logging': [], 'io': []}                                                                         | activity: []
assay: []                        | только в assay
__module_block_7               | —                  | —                                              | activity: {}
assay: {'logging': [], 'io': []}                                                                         | activity: []
assay: []                        | только в assay
__module_block_8               | —                  | —                                              | activity: {}
assay: {'logging': [], 'io': []}                                                                         | activity: []
assay: []                        | только в assay
_is_null_like                  | —                  | value: Any                                     | activity: {}
assay: {'logging': [], 'io': []}                                                                         | activity: []
assay: []                        | только в assay
validate_assay_parameters_truv | —                  | df: pd.DataFrame, column: str, fail_fast: bool | activity: {}
assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.warning'], 'io': ['json.loads']} | activity: []
assay: ['ValueError(error_msg)'] | только в assay

#### Горячий участок 1

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:1-1

```diff
--- activity:transform.py

+++ assay:transform.py

@@ -0,0 +1 @@

+"Transform utilities for ChEMBL assay pipeline array serialization."
```

#### Горячий участок 2

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:3-3

```diff
--- activity:transform.py

+++ assay:transform.py

@@ -0,0 +1 @@

+from __future__ import annotations
```

#### Горячий участок 3

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:10-10

```diff
--- activity:transform.py

+++ assay:transform.py

@@ -0,0 +1 @@

+from bioetl .core .logger import UnifiedLogger
```

#### Горячий участок 4

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:11-11

```diff
--- activity:transform.py

+++ assay:transform.py

@@ -0,0 +1 @@

+from bioetl .core .serialization import header_rows_serialize ,serialize_array_fields
```

#### Горячий участок 5

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:6-6

```diff
--- activity:transform.py

+++ assay:transform.py

@@ -0,0 +1 @@

+from typing import Any ,cast
```

#### Горячий участок 6

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:5-5

```diff
--- activity:transform.py

+++ assay:transform.py

@@ -0,0 +1 @@

+import json
```

#### Горячий участок 7

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:8-8

```diff
--- activity:transform.py

+++ assay:transform.py

@@ -0,0 +1 @@

+import pandas as pd
```

#### Горячий участок 8

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:13-17

```diff
--- activity:transform.py

+++ assay:transform.py

@@ -0,0 +1 @@

+__all__ =["header_rows_serialize","serialize_array_fields","validate_assay_parameters_truv"]
```

#### Горячий участок 9

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:19-19

```diff
--- activity:transform.py

+++ assay:transform.py

@@ -0,0 +1 @@

+AssayParam =dict [str ,Any ]
```

#### Горячий участок 10

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:22-39

```diff
--- activity:transform.py

+++ assay:transform.py

@@ -0,0 +1,13 @@

+def _is_null_like (value :Any )->bool :
+    "\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043c\u043e\u0436\u043d\u043e \u043b\u0438 \u0442\u0440\u0430\u043a\u0442\u043e\u0432\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u0430\u043a \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0435."
+    if value is None :
+        return True
+    if isinstance (value ,str ):
+        return value .strip ()==""
+    if isinstance (value ,float ):
+        return bool (pd .isna (value ))
+    try :
+        is_na_raw =cast (Any ,pd .isna (value ))
+    except TypeError :
+        return False
+    return bool (is_na_raw )if isinstance (is_na_raw ,bool )else False
```

#### Горячий участок 11

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:42-232

```diff
--- activity:transform.py

+++ assay:transform.py

@@ -0,0 +1,78 @@

+def validate_assay_parameters_truv (df :pd .DataFrame ,column :str ="assay_parameters",fail_fast :bool =True )->pd .DataFrame :
+    "\u0412\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u0442\u044c TRUV-\u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0434\u043b\u044f assay_parameters.\n\n    \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n    - value IS NOT NULL XOR text_value IS NOT NULL (\u043d\u0435 \u043e\u0431\u0430 \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u043d\u0435 NULL)\n    - standard_value IS NOT NULL XOR standard_text_value IS NOT NULL\n    - active \u2208 {0, 1, NULL}\n    - relation \u2208 {'=', '<', '\u2264', '>', '\u2265', '~', NULL} (\u0441 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u0435\u043c \u0434\u043b\u044f \u043d\u0435\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0445)\n\n    Parameters\n    ----------\n    df:\n        DataFrame \u0441 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 assay_parameters (JSON-\u0441\u0442\u0440\u043e\u043a\u0430 \u0441 \u043c\u0430\u0441\u0441\u0438\u0432\u043e\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432).\n    column:\n        \u0418\u043c\u044f \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 (\u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \"assay_parameters\").\n    fail_fast:\n        \u0415\u0441\u043b\u0438 True, \u0432\u044b\u0431\u0440\u0430\u0441\u044b\u0432\u0430\u0435\u0442 ValueError \u043f\u0440\u0438 \u043d\u0430\u0440\u0443\u0448\u0435\u043d\u0438\u0438 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432.\n        \u0415\u0441\u043b\u0438 False, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0435\u0442.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0439 DataFrame (\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u043d\u0435 \u0438\u0437\u043c\u0435\u043d\u044f\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435).\n\n    Raises\n    ------\n    ValueError:\n        \u0415\u0441\u043b\u0438 fail_fast=True \u0438 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043e \u043d\u0430\u0440\u0443\u0448\u0435\u043d\u0438\u0435 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432.\n\n    Notes\n    -----\n    - \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442\u0441\u044f \u043d\u0430 \u044d\u0442\u0430\u043f\u0435 transform \u0434\u043b\u044f fail-fast \u043f\u043e\u0434\u0445\u043e\u0434\u0430\n    - \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u044b relation: '=', '<', '\u2264', '>', '\u2265', '~'\n    - \u041d\u0435\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u044b \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435\n    "
+    log =UnifiedLogger .get (__name__ ).bind (component ="assay_transform")
+    if column not in df .columns :
+        log .debug ("truv_validation_skipped_missing_column",column =column )
+        return df
+    STANDARD_RELATIONS ={"=","<","\u2264",">","\u2265","~"}
+    errors :list [str ]=[]
+    warnings :list [str ]=[]
+    for idx ,row in df .iterrows ():
+        params_str =row .get (column )
+        if _is_null_like (params_str ):
+            continue
+        try :
+            if isinstance (params_str ,str ):
+                params_raw =json .loads (params_str )
+            else :
+                params_raw =params_str
+        except (json .JSONDecodeError ,TypeError )as exc :
+            errors .append (f'Row {idx }: Invalid JSON in {column }: {exc }')
+            continue
+        if not isinstance (params_raw ,list ):
+            errors .append (f'Row {idx }: {column } must be a JSON array, got {type (params_raw ).__name__ }')
+            continue
+        params_candidates =cast (list [object ],params_raw )
+        for param_idx ,param_raw in enumerate (params_candidates ):
+            if not isinstance (param_raw ,dict ):
+                errors .append (f'Row {idx }, param {param_idx }: Parameter must be a dict, got {type (param_raw ).__name__ }')
+                continue
+            param_dict :AssayParam =cast (AssayParam ,param_raw )
+            value :Any =param_dict .get ("value")
+            text_value :Any =param_dict .get ("text_value")
+            value_is_null =value is None or (isinstance (value ,float )and pd .isna (value ))or (isinstance (value ,str )and value .strip ()=="")
+            text_value_is_null =text_value is None or (isinstance (text_value ,float )and pd .isna (text_value ))or (isinstance (text_value ,str )and text_value .strip ()=="")
+            if not value_is_null and (not text_value_is_null ):
+                errors .append (f"Row {idx }, param {param_idx }: Both 'value' and 'text_value' are not NULL (value={value }, text_value={text_value }). TRUV invariant violation: value and text_value must be mutually exclusive.")
+            standard_value :Any =param_dict .get ("standard_value")
+            standard_text_value :Any =param_dict .get ("standard_text_value")
+            standard_value_is_null =standard_value is None or (isinstance (standard_value ,float )and pd .isna (standard_value ))or (isinstance (standard_value ,str )and standard_value .strip ()=="")
+            standard_text_value_is_null =standard_text_value is None or (isinstance (standard_text_value ,float )and pd .isna (standard_text_value ))or (isinstance (standard_text_value ,str )and standard_text_value .strip ()=="")
+            if not standard_value_is_null and (not standard_text_value_is_null ):
+                errors .append (f"Row {idx }, param {param_idx }: Both 'standard_value' and 'standard_text_value' are not NULL (standard_value={standard_value }, standard_text_value={standard_text_value }). TRUV invariant violation: standard_value and standard_text_value must be mutually exclusive.")
+            active :Any =param_dict .get ("active")
+            if active is not None :
+                if isinstance (active ,bool ):
+                    active_int =1 if active else 0
+                elif isinstance (active ,(int ,float )):
+                    active_int =int (active )
+                elif isinstance (active ,str ):
+                    try :
+                        active_int =int (active )
+                    except ValueError :
+                        errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' value: {active !r }. Must be 0, 1, or NULL.")
+                        continue
+                else :
+                    errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' type: {type (active ).__name__ }. Must be 0, 1, or NULL.")
+                    continue
+                if active_int not in {0 ,1 }:
+                    errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' value: {active_int }. Must be 0, 1, or NULL.")
+            relation :Any =param_dict .get ("relation")
+            if relation is not None and (not (isinstance (relation ,float )and pd .isna (relation ))):
+                relation_str =str (relation ).strip ()
+                if relation_str and relation_str not in STANDARD_RELATIONS :
+                    warnings .append (f"Row {idx }, param {param_idx }: Non-standard 'relation' value: {relation_str !r }. Standard operators: {", ".join (sorted (STANDARD_RELATIONS ))}.")
+    if warnings :
+        for warning in warnings :
+            log .warning ("truv_validation_warning",message =warning )
+    if errors :
+        error_msg =f'TRUV validation failed for {column }:\n'+"\n".join (errors )
+        if fail_fast :
+            log .error ("truv_validation_failed",error_count =len (errors ))
+            raise ValueError (error_msg )
+        else :
+            for error in errors :
+                log .warning ("truv_validation_error",message =error )
+    if not errors and (not warnings ):
+        log .debug ("truv_validation_passed",rows_checked =len (df ))
+    return df
```

### Модуль normalize.py

Определение                       | activity сигнатура                                                           | assay сигнатура                                                      | Побочные эффекты                                                                                                     | Исключения             | Статус           
----------------------------------|------------------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|------------------------|------------------
__module_block_0                  | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | отличается       
__module_block_1                  | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | совпадает        
__module_block_10                 | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | отличается       
__module_block_11                 | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | отличается       
__module_block_12                 | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | отличается       
__module_block_13                 | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | отличается       
__module_block_14                 | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {}                                                                        | activity: []
assay: [] | только в activity
__module_block_15                 | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {}                                                                        | activity: []
assay: [] | только в activity
__module_block_16                 | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {}                                                                        | activity: []
assay: [] | только в activity
__module_block_2                  | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | совпадает        
__module_block_3                  | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | совпадает        
__module_block_4                  | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | совпадает        
__module_block_5                  | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | отличается       
__module_block_6                  | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | отличается       
__module_block_7                  | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | отличается       
__module_block_8                  | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | отличается       
__module_block_9                  | —                                                                            | —                                                                    | activity: {'logging': [], 'io': []}
assay: {'logging': [], 'io': []}                                                 | activity: []
assay: [] | отличается       
_enrich_by_pairs                  | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any], log: Any | —                                                                    | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
assay: {}                                  | activity: []
assay: [] | только в activity
_enrich_by_record_id              | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any], log: Any | —                                                                    | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
assay: {}                                  | activity: []
assay: [] | только в activity
_extract_first_present            | record: Mapping[str, Any], keys: Iterable[str]                               | —                                                                    | activity: {'logging': [], 'io': []}
assay: {}                                                                        | activity: []
assay: [] | только в activity
_should_nullify_string_value      | —                                                                            | value: Any                                                           | activity: {}
assay: {'logging': [], 'io': []}                                                                        | activity: []
assay: [] | только в assay   
enrich_with_assay                 | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
assay: {}             | activity: []
assay: [] | только в activity
enrich_with_assay_classifications | —                                                                            | df_assay: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | activity: {}
assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']} | activity: []
assay: [] | только в assay   
enrich_with_assay_parameters      | —                                                                            | df_assay: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | activity: {}
assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']} | activity: []
assay: [] | только в assay   
enrich_with_compound_record       | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
assay: {}             | activity: []
assay: [] | только в activity
enrich_with_data_validity         | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
assay: {}             | activity: []
assay: [] | только в activity

_Показаны первые 20 горячих участков из 22._

#### Горячий участок 1

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:1-1
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:1-1

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +1 @@

-"Enrichment functions for Activity pipeline."
+"Enrichment functions for Assay pipeline."
```

#### Горячий участок 2

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:9-9
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:19-22

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +1 @@

-import pandas as pd
+__all__ =["enrich_with_assay_classifications","enrich_with_assay_parameters"]
```

#### Горячий участок 3

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:21-21
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:25-25

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +1 @@

-__all__ =["enrich_with_assay","enrich_with_compound_record","enrich_with_data_validity"]
+_ensure_columns =ensure_columns
```

#### Горячий участок 4

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:24-24
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:28-31

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +1 @@

-_ensure_columns =ensure_columns
+_CLASSIFICATION_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_classifications","string"),("assay_class_id","string"))
```

#### Горячий участок 5

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:27-35
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:33-33

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +1 @@

-_COMPOUND_FIELD_ALIASES :dict [str ,tuple [str ,...]]={"compound_name":("compound_name","pref_name","PREF_NAME"),"compound_key":("compound_key","standard_inchi_key","STANDARD_INCHI_KEY"),"curated":("curated","CURATED")}
+_PARAMETERS_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_parameters","string"),)
```

#### Горячий участок 6

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:37-40
- assay: нет в ветке

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +0,0 @@

-_ASSAY_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_organism","string"),("assay_tax_id","Int64"))
```

#### Горячий участок 7

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:42-47
- assay: нет в ветке

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +0,0 @@

-_COMPOUND_COLUMNS :tuple [tuple [str ,str ],...]=(("compound_name","string"),("compound_key","string"),("curated","boolean"),("removed","boolean"))
```

#### Горячий участок 8

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:49-49
- assay: нет в ветке

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +0,0 @@

-_DATA_VALIDITY_COLUMNS :tuple [tuple [str ,str ],...]=(("data_validity_description","string"),)
```

#### Горячий участок 9

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:15-19
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:14-17

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +1 @@

-from bioetl .schemas .activity import ASSAY_ENRICHMENT_SCHEMA ,COMPOUND_RECORD_ENRICHMENT_SCHEMA ,DATA_VALIDITY_ENRICHMENT_SCHEMA
+from bioetl .schemas .assay import ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA ,ASSAY_PARAMETERS_ENRICHMENT_SCHEMA
```

#### Горячий участок 10

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:5-5
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:6-6

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +1 @@

-from collections .abc import Iterable ,Mapping
+from collections .abc import Mapping
```

#### Горячий участок 11

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:10-10
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:7-7

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +1 @@

-from pandas import Series
+from typing import Any
```

#### Горячий участок 12

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:6-6
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:5-5

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +1 @@

-from typing import Any
+import json
```

#### Горячий участок 13

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:8-8
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:9-9

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1 +1 @@

-import numpy as np
+import pandas as pd
```

#### Горячий участок 14

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:439-615
- assay: нет в ветке

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1,65 +0,0 @@

-def _enrich_by_pairs (df_act :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ],log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame activity \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u044b (molecule_chembl_id, document_chembl_id)."
-    pairs_df =df_act [["molecule_chembl_id","document_chembl_id"]].astype ("string").copy ()
-    for column in pairs_df .columns :
-        pairs_df [column ]=pairs_df [column ].str .strip ().str .upper ()
-    pairs_df =pairs_df .dropna ()
-    pairs_df =pairs_df .drop_duplicates ()
-    pairs :set [tuple [str ,str ]]=set (map (tuple ,pairs_df .to_numpy ()))
-    if not pairs :
-        log .debug ("enrichment_by_pairs_skipped_no_valid_pairs")
-        return df_act
-    fields =cfg .get ("fields",["molecule_chembl_id","document_chembl_id","compound_name","compound_key","curated"])
-    page_limit =cfg .get ("page_limit",1000 )
-    log .info ("enrichment_fetching_compound_records_by_pairs",pairs_count =len (pairs ))
-    compound_records_dict :dict [tuple [str ,str ],dict [str ,Any ]]={}
-    try :
-        compound_records_dict =client .fetch_compound_records_by_pairs (pairs =pairs ,fields =list (fields ),page_limit =page_limit )or {}
-    except Exception as exc :
-        log .warning ("enrichment_fetch_error_by_pairs",pairs_count =len (pairs ),error =str (exc ),exc_info =True )
-        return df_act
-    enrichment_data :list [dict [str ,Any ]]=[]
-    pairs_found =0
-    pairs_not_found =0
-    for pair in pairs :
-        compound_record :dict [str ,Any ]|None =compound_records_dict .get (pair )
-        if compound_record :
-            record_mapping :Mapping [str ,Any ]=compound_record
-            compound_name_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("compound_name",("compound_name",)))
-            compound_key_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("compound_key",("compound_key",)))
-            curated_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("curated",("curated",)))
-            compound_name =None
-            if compound_name_raw is not None :
-                name_str =str (compound_name_raw ).strip ()
-                compound_name =name_str if name_str else None
-            compound_key =None
-            if compound_key_raw is not None :
-                key_str =str (compound_key_raw ).strip ()
-                compound_key =key_str if key_str else None
-            curated =curated_raw if curated_raw is not None else None
-            pairs_found +=1
-            enrichment_data .append ({"molecule_chembl_id":pair [0 ],"document_chembl_id":pair [1 ],"compound_name":compound_name ,"compound_key":compound_key ,"curated":curated ,"removed":None })
-        else :
-            pairs_not_found +=1
-            enrichment_data .append ({"molecule_chembl_id":pair [0 ],"document_chembl_id":pair [1 ],"compound_name":None ,"compound_key":None ,"curated":None ,"removed":None })
-    log .info ("enrichment_by_pairs_complete",pairs_requested =len (pairs ),pairs_found =pairs_found ,pairs_not_found =pairs_not_found ,records_returned =len (compound_records_dict ))
-    if pairs_not_found >0 :
-        log .warning ("enrichment_by_pairs_some_pairs_not_found",pairs_not_found =pairs_not_found ,pairs_total =len (pairs ),hint ="\u041f\u0440\u043e\u0432\u0435\u0440\u044c\u0442\u0435, \u0447\u0442\u043e \u043f\u0430\u0440\u044b (molecule_chembl_id, document_chembl_id) \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 ChEMBL API")
-    if not enrichment_data :
-        log .debug ("enrichment_by_pairs_no_records_found")
-        return df_act
-    df_enrich =pd .DataFrame (enrichment_data )
-    df_enrich ["molecule_chembl_id"]=df_enrich ["molecule_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_enrich ["document_chembl_id"]=df_enrich ["document_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_act ["molecule_chembl_id_normalized"]=df_act ["molecule_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_act ["document_chembl_id_normalized"]=df_act ["document_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_result =df_act .merge (df_enrich ,left_on =["molecule_chembl_id_normalized","document_chembl_id_normalized"],right_on =["molecule_chembl_id","document_chembl_id"],how ="left",suffixes =("","_enrich"))
-    df_result =df_result .drop (columns =["molecule_chembl_id_normalized","document_chembl_id_normalized"])
-    for col in ["compound_name","compound_key","curated"]:
-        if f'{col }_enrich'in df_result .columns :
-            if col not in df_result .columns :
-                df_result [col ]=df_result [f'{col }_enrich']
-            else :
-                df_result [col ]=df_result [col ].where (df_result [col ].notna (),df_result [f'{col }_enrich'])
-            df_result =df_result .drop (columns =[f'{col }_enrich'])
-    return df_result
```

#### Горячий участок 15

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:618-755
- assay: нет в ветке

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1,67 +0,0 @@

-def _enrich_by_record_id (df_act :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ],log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame activity \u0447\u0435\u0440\u0435\u0437 record_id (fallback \u0434\u043b\u044f \u0441\u0442\u0440\u043e\u043a \u0431\u0435\u0437 document_chembl_id)."
-    record_ids :set [str ]=set ()
-    for _ ,row in df_act .iterrows ():
-        rec =row .get ("record_id")
-        if rec is not None and (not pd .isna (rec )):
-            rec_s =str (rec ).strip ()
-            if rec_s :
-                record_ids .add (rec_s )
-    if not record_ids :
-        log .debug ("enrichment_by_record_id_skipped_no_valid_ids")
-        return df_act
-    fields =["record_id","compound_name","compound_key"]
-    page_limit =cfg .get ("page_limit",1000 )
-    batch_size =int (cfg .get ("batch_size",100 ))or 100
-    log .info ("enrichment_fetching_compound_records_by_record_id",record_ids_count =len (record_ids ))
-    compound_records_dict :dict [str ,dict [str ,Any ]]={}
-    try :
-        unique_ids =list (record_ids )
-        all_records :list [dict [str ,Any ]]=[]
-        for i in range (0 ,len (unique_ids ),batch_size ):
-            chunk =unique_ids [i :i +batch_size ]
-            params :dict [str ,Any ]={"record_id__in":",".join (chunk ),"limit":page_limit ,"only":",".join (fields ),"order_by":"record_id"}
-            try :
-                for record in client .paginate ("/compound_record.json",params =params ,page_size =page_limit ,items_key ="compound_records"):
-                    all_records .append (dict (record ))
-            except Exception as exc :
-                log .warning ("enrichment_fetch_error_by_record_id",chunk_size =len (chunk ),error =str (exc ),exc_info =True )
-        for record in all_records :
-            rid_raw =record .get ("record_id")
-            if rid_raw is None :
-                continue
-            rid_str =str (rid_raw ).strip ()
-            if rid_str and rid_str not in compound_records_dict :
-                compound_records_dict [rid_str ]={"record_id":rid_str ,"compound_key":record .get ("compound_key"),"compound_name":record .get ("compound_name")}
-    except Exception as exc :
-        log .warning ("enrichment_fetch_error_by_record_id",record_ids_count =len (record_ids ),error =str (exc ),exc_info =True )
-        return df_act
-    if not compound_records_dict :
-        log .debug ("enrichment_by_record_id_no_records_found")
-        return df_act
-    compound_data :list [dict [str ,Any ]]=[]
-    for record_id ,record in compound_records_dict .items ():
-        compound_data .append ({"record_id":record_id ,"compound_key":record .get ("compound_key"),"compound_name":record .get ("compound_name")})
-    df_compound =pd .DataFrame (compound_data )if compound_data else pd .DataFrame (columns =["record_id","compound_key","compound_name"])
-    df_act_normalized =df_act .copy ()
-    if "record_id"in df_act_normalized .columns :
-        mask_na =df_act_normalized ["record_id"].isna ()
-        df_act_normalized ["record_id"]=df_act_normalized ["record_id"].astype (str )
-        df_act_normalized .loc [df_act_normalized ["record_id"]=="nan","record_id"]=pd .NA
-        df_act_normalized .loc [mask_na ,"record_id"]=pd .NA
-        if "record_id"in df_compound .columns and (not df_compound .empty ):
-            df_compound ["record_id"]=df_compound ["record_id"].astype (str )
-            df_compound .loc [df_compound ["record_id"]=="nan","record_id"]=pd .NA
-    df_result =df_act_normalized .merge (df_compound ,on =["record_id"],how ="left",suffixes =("","_compound"))
-    for col in ["compound_name","compound_key"]:
-        if f'{col }_compound'in df_result .columns :
-            if col not in df_result .columns :
-                df_result [col ]=df_result [f'{col }_compound']
-            else :
-                df_result [col ]=df_result [col ].where (df_result [col ].notna (),df_result [f'{col }_compound'])
-            df_result =df_result .drop (columns =[f'{col }_compound'])
-    if "curated"not in df_result .columns :
-        df_result ["curated"]=pd .NA
-    if "removed"not in df_result .columns :
-        df_result ["removed"]=pd .NA
-    return df_result
```

#### Горячий участок 16

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:52-64
- assay: нет в ветке

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1,11 +0,0 @@

-def _extract_first_present (record :Mapping [str ,Any ],keys :Iterable [str ])->Any :
-    "\u0412\u043e\u0437\u0432\u0440\u0430\u0442\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u043e \u043f\u0435\u0440\u0432\u043e\u043c\u0443 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u043c\u0443 \u0430\u043b\u0438\u0430\u0441\u0443."
-    for key in keys :
-        if key in record :
-            return record [key ]
-    lowered_map ={str (k ).lower ():v for k ,v in record .items ()}
-    for key in keys :
-        candidate =str (key ).lower ()
-        if candidate in lowered_map :
-            return lowered_map [candidate ]
-    return None
```

#### Горячий участок 17

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:36-44

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -0,0 +1,9 @@

+def _should_nullify_string_value (value :Any )->bool :
+    "\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043d\u0443\u0436\u043d\u043e \u043b\u0438 \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 NA \u0432 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0435."
+    if value is None :
+        return False
+    if value is pd .NA :
+        return False
+    if isinstance (value ,float )and pd .isna (value ):
+        return False
+    return not isinstance (value ,str )
```

#### Горячий участок 18

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:67-184
- assay: нет в ветке

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -1,44 +0,0 @@

-def enrich_with_assay (df_act :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame activity \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 /assay (ChEMBL v2).\n\n    \u0422\u0440\u0435\u0431\u0443\u0435\u043c\u044b\u0435 \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438: assay_chembl_id.\n    \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442:\n      - assay_organism : pandas.StringDtype (nullable)\n      - assay_tax_id   : pandas.Int64Dtype  (nullable)\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="activity_enrichment")
-    df_act =_ensure_columns (df_act ,_ASSAY_COLUMNS )
-    if df_act .empty :
-        log .debug ("enrichment_skipped_empty_dataframe")
-        return ASSAY_ENRICHMENT_SCHEMA .validate (df_act ,lazy =True )
-    if "assay_chembl_id"not in df_act .columns :
-        log .warning ("enrichment_skipped_missing_columns",missing_columns =["assay_chembl_id"])
-        return ASSAY_ENRICHMENT_SCHEMA .validate (df_act ,lazy =True )
-    assay_ids =df_act ["assay_chembl_id"].dropna ().astype (str ).str .strip ()
-    assay_ids =assay_ids [assay_ids .ne ("")].unique ().tolist ()
-    if not assay_ids :
-        log .debug ("enrichment_skipped_no_valid_ids")
-        return ASSAY_ENRICHMENT_SCHEMA .validate (df_act ,lazy =True )
-    fields_cfg =cfg .get ("fields",["assay_chembl_id","assay_organism","assay_tax_id"])
-    required_fields ={"assay_chembl_id","assay_organism","assay_tax_id"}
-    fields =list (dict .fromkeys (list (fields_cfg )+list (required_fields )))
-    page_limit =int (cfg .get ("page_limit",1000 ))
-    log .info ("enrichment_fetching_assays",ids_count =len (assay_ids ))
-    records_by_id :dict [str ,dict [str ,Any ]]=client .fetch_assays_by_ids (ids =assay_ids ,fields =fields ,page_limit =page_limit )or {}
-    if not records_by_id :
-        log .debug ("enrichment_no_records_found")
-        return ASSAY_ENRICHMENT_SCHEMA .validate (df_act ,lazy =True )
-    enrichment_rows :list [dict [str ,Any ]]=[]
-    for assay_id ,rec in records_by_id .items ():
-        enrichment_rows .append ({"assay_chembl_id":assay_id ,"assay_organism":rec .get ("assay_organism"),"assay_tax_id":rec .get ("assay_tax_id")})
-    df_enrich =pd .DataFrame (enrichment_rows )
-    original_index =df_act .index
-    df_merged =df_act .merge (df_enrich ,on ="assay_chembl_id",how ="left",sort =False ,suffixes =("","_enrich")).reindex (original_index )
-    if "assay_organism_enrich"in df_merged .columns :
-        df_merged ["assay_organism"]=df_merged ["assay_organism_enrich"].combine_first (df_merged ["assay_organism"])
-        df_merged =df_merged .drop (columns =["assay_organism_enrich"])
-    if "assay_tax_id_enrich"in df_merged .columns :
-        df_merged ["assay_tax_id"]=df_merged ["assay_tax_id_enrich"].combine_first (df_merged ["assay_tax_id"])
-        df_merged =df_merged .drop (columns =["assay_tax_id_enrich"])
-    if "assay_organism"not in df_merged .columns :
-        df_merged ["assay_organism"]=pd .NA
-    if "assay_tax_id"not in df_merged .columns :
-        df_merged ["assay_tax_id"]=pd .NA
-    df_merged ["assay_organism"]=df_merged ["assay_organism"].astype ("string")
-    df_merged ["assay_tax_id"]=pd .to_numeric (df_merged ["assay_tax_id"],errors ="coerce").astype ("Int64")
-    log .info ("enrichment_completed",rows_enriched =int (df_merged .shape [0 ]),records_matched =int (len (records_by_id )))
-    return ASSAY_ENRICHMENT_SCHEMA .validate (df_merged ,lazy =True )
```

#### Горячий участок 19

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:47-219

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -0,0 +1,86 @@

+def enrich_with_assay_classifications (df_assay :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
+    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame assay \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 ASSAY_CLASS_MAP \u0438 ASSAY_CLASSIFICATION.\n\n    Parameters\n    ----------\n    df_assay:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.assay.enrich.classifications.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438/\u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438:\n        - assay_classifications (string, nullable) - \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u043c\u0430\u0441\u0441\u0438\u0432 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439\n        - assay_class_id (string, nullable) - \u0441\u043f\u0438\u0441\u043e\u043a assay_class_id \u0447\u0435\u0440\u0435\u0437 \";\"\n    "
+    log =UnifiedLogger .get (__name__ ).bind (component ="assay_enrichment")
+    df_assay =_ensure_columns (df_assay ,_CLASSIFICATION_COLUMNS )
+    if "assay_classifications"in df_assay .columns :
+        invalid_mask =df_assay ["assay_classifications"].map (_should_nullify_string_value )
+        if bool (invalid_mask .any ()):
+            log .warning ("assay_classifications_reset_non_string",rows =int (invalid_mask .sum ()))
+            df_assay .loc [invalid_mask ,"assay_classifications"]=pd .NA
+    if df_assay .empty :
+        log .debug ("enrichment_skipped_empty_dataframe")
+        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
+    required_cols =["assay_chembl_id"]
+    missing_cols =[col for col in required_cols if col not in df_assay .columns ]
+    if missing_cols :
+        log .warning ("enrichment_skipped_missing_columns",missing_columns =missing_cols )
+        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
+    assay_ids :list [str ]=[]
+    for _ ,row in df_assay .iterrows ():
+        assay_id =row .get ("assay_chembl_id")
+        if pd .isna (assay_id )or assay_id is None :
+            continue
+        assay_id_str =str (assay_id ).strip ()
+        if assay_id_str :
+            assay_ids .append (assay_id_str )
+    if not assay_ids :
+        log .debug ("enrichment_skipped_no_valid_ids")
+        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
+    class_map_fields =cfg .get ("class_map_fields",["assay_chembl_id","assay_class_id"])
+    classification_fields =cfg .get ("classification_fields",["assay_class_id","l1","l2","l3","pref_name"])
+    page_limit =cfg .get ("page_limit",1000 )
+    log .info ("enrichment_fetching_assay_class_map",ids_count =len (set (assay_ids )))
+    class_map_dict =client .fetch_assay_class_map_by_assay_ids (assay_ids ,list (class_map_fields ),page_limit )
+    all_class_ids :set [str ]=set ()
+    for mappings in class_map_dict .values ():
+        for mapping in mappings :
+            class_id =mapping .get ("assay_class_id")
+            if class_id and (not (isinstance (class_id ,float )and pd .isna (class_id ))):
+                all_class_ids .add (str (class_id ).strip ())
+    classification_dict :dict [str ,dict [str ,Any ]]={}
+    if all_class_ids :
+        log .info ("enrichment_fetching_assay_classifications",class_ids_count =len (all_class_ids ))
+        classification_dict =client .fetch_assay_classifications_by_class_ids (list (all_class_ids ),list (classification_fields ),page_limit )
+    df_assay =df_assay .copy ()
+    if "assay_classifications"not in df_assay .columns :
+        df_assay ["assay_classifications"]=pd .NA
+    if "assay_class_id"not in df_assay .columns :
+        df_assay ["assay_class_id"]=pd .NA
+    for idx ,row in df_assay .iterrows ():
+        row_key :Any =idx
+        assay_id =row .get ("assay_chembl_id")
+        if pd .isna (assay_id )or assay_id is None :
+            continue
+        assay_id_str =str (assay_id ).strip ()
+        mappings =class_map_dict .get (assay_id_str ,[])
+        if not mappings :
+            df_assay .at [row_key ,"assay_classifications"]=pd .NA
+            df_assay .at [row_key ,"assay_class_id"]=pd .NA
+            continue
+        classifications :list [dict [str ,Any ]]=[]
+        class_ids :list [str ]=[]
+        for mapping in mappings :
+            class_id =mapping .get ("assay_class_id")
+            if not class_id or (isinstance (class_id ,float )and pd .isna (class_id )):
+                continue
+            class_id_str =str (class_id ).strip ()
+            if not class_id_str :
+                continue
+            classification_data =classification_dict .get (class_id_str )
+            if classification_data :
+                class_record :dict [str ,Any ]={"assay_class_id":class_id_str }
+                for field in classification_fields :
+                    if field !="assay_class_id":
+                        class_record [field ]=classification_data .get (field )
+                classifications .append (class_record )
+            else :
+                class_record ={"assay_class_id":class_id_str }
+                classifications .append (class_record )
+            class_ids .append (class_id_str )
+        if classifications :
+            serialized =json .dumps (classifications ,ensure_ascii =False )
+            class_id_joined =";".join (class_ids )
+            df_assay .at [row_key ,"assay_classifications"]=serialized
+            df_assay .at [row_key ,"assay_class_id"]=class_id_joined
+    log .info ("enrichment_classifications_complete",assays_with_classifications =len (df_assay [df_assay ["assay_classifications"].notna ()]))
+    return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
```

#### Горячий участок 20

- activity: нет в ветке
- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:222-378

```diff
--- activity:normalize.py

+++ assay:normalize.py

@@ -0,0 +1,57 @@

+def enrich_with_assay_parameters (df_assay :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
+    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame assay \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 ASSAY_PARAMETERS.\n\n    \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442 \u043f\u043e\u043b\u043d\u044b\u0439 TRUV-\u043d\u0430\u0431\u043e\u0440 \u043f\u043e\u043b\u0435\u0439 (TYPE, RELATION, VALUE, UNITS, TEXT_VALUE),\n    \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (standard_*), \u0441\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (active) \u0438 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435\n    \u043f\u043e\u043b\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 (type_normalized, type_fixed).\n\n    Parameters\n    ----------\n    df_assay:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.assay.enrich.parameters.\n        \u0414\u043e\u043b\u0436\u043d\u0430 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c fields (\u0441\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439 \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f), page_limit \u0438 active_only.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0439/\u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439:\n        - assay_parameters (string, nullable) - \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 JSON-\u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\n          \u0441 \u043f\u043e\u043b\u044f\u043c\u0438: type, relation, value, units, text_value, standard_*,\n          active, type_normalized, type_fixed (\u0435\u0441\u043b\u0438 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 \u0434\u0430\u043c\u043f\u0435)\n\n    Notes\n    -----\n    - \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u0435\u0441\u0442\u044c, \u043d\u0435 \u043a\u043e\u043f\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0432 standard_* \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\n    - \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (type_normalized, type_fixed) \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438\n      \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 \u0434\u0430\u043c\u043f\u0435 ChEMBL\n    - \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0444\u0438\u043b\u044c\u0442\u0440\u0443\u044e\u0442\u0441\u044f \u043f\u043e active=1, \u0435\u0441\u043b\u0438 active_only=True \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438\n    "
+    log =UnifiedLogger .get (__name__ ).bind (component ="assay_enrichment")
+    df_assay =_ensure_columns (df_assay ,_PARAMETERS_COLUMNS )
+    if "assay_parameters"in df_assay .columns :
+        invalid_mask =df_assay ["assay_parameters"].map (_should_nullify_string_value )
+        if bool (invalid_mask .any ()):
+            log .warning ("assay_parameters_reset_non_string",rows =int (invalid_mask .sum ()))
+            df_assay .loc [invalid_mask ,"assay_parameters"]=pd .NA
+        df_assay ["assay_parameters"]=df_assay ["assay_parameters"].astype ("string")
+    if df_assay .empty :
+        log .debug ("enrichment_skipped_empty_dataframe")
+        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
+    required_cols =["assay_chembl_id"]
+    missing_cols =[col for col in required_cols if col not in df_assay .columns ]
+    if missing_cols :
+        log .warning ("enrichment_skipped_missing_columns",missing_columns =missing_cols )
+        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
+    assay_ids :list [str ]=[]
+    for _ ,row in df_assay .iterrows ():
+        assay_id =row .get ("assay_chembl_id")
+        if pd .isna (assay_id )or assay_id is None :
+            continue
+        assay_id_str =str (assay_id ).strip ()
+        if assay_id_str :
+            assay_ids .append (assay_id_str )
+    if not assay_ids :
+        log .debug ("enrichment_skipped_no_valid_ids")
+        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
+    fields =cfg .get ("fields",["assay_chembl_id","type","relation","value","units","text_value","standard_type","standard_relation","standard_value","standard_units","standard_text_value","active"])
+    page_limit =cfg .get ("page_limit",1000 )
+    active_only =cfg .get ("active_only",True )
+    log .info ("enrichment_fetching_assay_parameters",ids_count =len (set (assay_ids )))
+    parameters_dict =client .fetch_assay_parameters_by_assay_ids (assay_ids ,list (fields ),page_limit ,active_only )
+    df_assay =df_assay .copy ()
+    if "assay_parameters"not in df_assay .columns :
+        df_assay ["assay_parameters"]=pd .NA
+    for row_position ,(_ ,row )in enumerate (df_assay .iterrows ()):
+        assay_id =row .get ("assay_chembl_id")
+        if pd .isna (assay_id )or assay_id is None :
+            continue
+        assay_id_str =str (assay_id ).strip ()
+        parameters =parameters_dict .get (assay_id_str ,[])
+        if not parameters :
+            continue
+        params_list :list [dict [str ,Any ]]=[]
+        for param in parameters :
+            param_record :dict [str ,Any ]={}
+            for field in fields :
+                if field !="assay_chembl_id":
+                    param_record [field ]=param .get (field )
+            params_list .append (param_record )
+        if params_list :
+            index_label =df_assay .index [row_position ]
+            df_assay .loc [index_label ,"assay_parameters"]=json .dumps (params_list ,ensure_ascii =False )
+    log .info ("enrichment_parameters_complete",assays_with_parameters =len (df_assay [df_assay ["assay_parameters"].notna ()]))
+    return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
```

---

## Пара: activity ↔ document

- AST hash: 978d4152ad77000361c21b7f2051d3d1 ↔ 3774d930e14eb5befd7fdffc255cb346

- Jaccard по токенам: 0.231

### Модуль run.py

Определение                                                      | activity сигнатура                                                                                                                                                         | document сигнатура                        | Побочные эффекты                                                                                                                                                                                                                                                                                                                                                                                                | Исключения                                                   | Статус           
-----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------|------------------
ChemblActivityPipeline                                           | —                                                                                                                                                                          | —                                         | activity: {'logging': ['UnifiedLogger.bind', 'UnifiedLogger.get', 'bound_log.warning', 'errors_to_log.iterrows', 'log.debug', 'log.error', 'log.info', 'log.warning', 'pipeline._log_validity_comments_metrics', 'self._log_detailed_validation_errors', 'self._log_validity_comments_metrics'], 'io': ['cache_file.read_text', 'json.dumps', 'json.loads', 'payload.get', 'tmp_path.write_text']}
document: {} | activity: ['TypeError(msg)', 'ValueError(msg)']
document: [] | только в activity
ChemblActivityPipeline.__init__                                  | self, config: PipelineConfig, run_id: str                                                                                                                                  | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._add_row_metadata                         | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.debug'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._build_activity_descriptor                | self                                                                                                                                                                       | —                                         | activity: {'logging': ['pipeline._log_validity_comments_metrics'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                       | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._cache_directory                          | self, release: str | None                                                                                                                                                  | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._cache_file_path                          | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._cache_key                                | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                         | activity: {'logging': [], 'io': ['json.dumps']}
document: {}                                                                                                                                                                                                                                                                                                                                                    | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._check_activity_id_uniqueness             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.debug', 'log.error', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                         | activity: ['ValueError(msg)']
document: []                   | только в activity
ChemblActivityPipeline._check_cache                              | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                         | activity: {'logging': [], 'io': ['cache_file.read_text', 'json.loads']}
document: {}                                                                                                                                                                                                                                                                                                                            | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._check_foreign_key_integrity              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.debug', 'log.error', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                         | activity: ['ValueError(msg)']
document: []                   | только в activity
ChemblActivityPipeline._coerce_activity_dataset                  | self, dataset: object                                                                                                                                                      | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: ['TypeError(msg)']
document: []                    | только в activity
ChemblActivityPipeline._coerce_mapping                           | payload: Any                                                                                                                                                               | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._collect_records_by_ids                   | self, normalized_ids: Sequence[tuple[int, str]], activity_iterator: ChemblActivityClient, *, select_fields: Sequence[str] | None                                           | —                                         | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                    | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._create_fallback_record                   | self, activity_id: int, error: Exception | None                                                                                                                            | —                                         | activity: {'logging': [], 'io': ['json.dumps']}
document: {}                                                                                                                                                                                                                                                                                                                                                    | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._deduplicate_activity_properties          | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                         | activity: {'logging': ['log.debug'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._enrich_assay                             | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                              | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._enrich_compound_record                   | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                              | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._enrich_data_validity                     | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                  | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._enrich_molecule                          | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                  | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._ensure_comment_fields                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.debug'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._extract_activity_properties_fields       | self, record: dict[str, Any]                                                                                                                                               | —                                         | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.warning'], 'io': ['json.loads']}
document: {}                                                                                                                                                                                                                                                                                                     | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._extract_assay_fields                     | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                         | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                          | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._extract_chembl_release                   | payload: Mapping[str, Any]                                                                                                                                                 | —                                         | activity: {'logging': [], 'io': ['payload.get']}
document: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._extract_data_validity_descriptions       | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                         | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                          | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._extract_from_chembl                      | self, dataset: object, chembl_client: ChemblClient | Any, activity_iterator: ChemblActivityClient, *, limit: int | None = None, select_fields: Sequence[str] | None = None | —                                         | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'self._log_validity_comments_metrics'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                          | activity: ['ValueError(msg)']
document: []                   | только в activity
ChemblActivityPipeline._extract_nested_fields                    | self, record: dict[str, Any]                                                                                                                                               | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._extract_page_items                       | payload: Mapping[str, Any], items_keys: Sequence[str] | None                                                                                                               | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._filter_invalid_required_fields           | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._finalize_identifier_columns              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._finalize_output_columns                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._get_data_validity_comment_whitelist      | self                                                                                                                                                                       | —                                         | activity: {'logging': ['UnifiedLogger.get'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                             | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._harmonize_identifier_columns             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.debug'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._log_detailed_validation_errors           | self, failure_cases: pd.DataFrame, payload: pd.DataFrame, log: BoundLogger                                                                                                 | —                                         | activity: {'logging': ['errors_to_log.iterrows', 'log.error', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                            | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._log_validity_comments_metrics            | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.info', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._materialize_activity_record              | self, payload: Mapping[str, Any], *, activity_id: int | None = None                                                                                                        | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._next_link                                | payload: Mapping[str, Any], base_url: str                                                                                                                                  | —                                         | activity: {'logging': [], 'io': ['payload.get']}
document: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._normalize_activity_ids                   | self, input_frame: pd.DataFrame, *, limit: int | None, log: BoundLogger                                                                                                    | —                                         | activity: {'logging': ['log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._normalize_activity_properties_items      | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                         | activity: {'logging': ['log.warning'], 'io': ['json.loads']}
document: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._normalize_data_types                     | self, df: pd.DataFrame, schema: Any, log: BoundLogger                                                                                                                      | —                                         | activity: {'logging': ['log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._normalize_identifiers                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.debug'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._normalize_measurements                   | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._normalize_nested_structures              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.warning'], 'io': ['json.dumps', 'json.loads']}
document: {}                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._normalize_string_fields                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._prepare_activity_iteration               | self, *, client_name: str = 'chembl_activity_client'                                                                                                                       | —                                         | activity: {'logging': ['UnifiedLogger.get'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                             | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._sanitize_cache_component                 | value: str                                                                                                                                                                 | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._schema_column_specs                      | self                                                                                                                                                                       | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._serialize_activity_properties            | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                         | activity: {'logging': ['log.warning'], 'io': ['json.dumps']}
document: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._should_enrich_assay                      | self                                                                                                                                                                       | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._should_enrich_compound_record            | self                                                                                                                                                                       | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._should_enrich_data_validity              | self                                                                                                                                                                       | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._should_enrich_molecule                   | self                                                                                                                                                                       | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._store_cache                              | self, batch_ids: Sequence[str], batch_data: Mapping[str, Mapping[str, Any]], release: str | None                                                                           | —                                         | activity: {'logging': ['UnifiedLogger.get', 'log.debug'], 'io': ['json.dumps', 'tmp_path.write_text']}
document: {}                                                                                                                                                                                                                                                                                             | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._validate_activity_properties_truv        | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                         | activity: {'logging': ['log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._validate_data_validity_comment_soft_enum | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
document: []                                    | только в activity
ChemblActivityPipeline._validate_foreign_keys                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: {'logging': ['log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
document: []                                    | только в activity
ChemblActivityPipeline.build_quality_report                      | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline.chembl_release                            | self                                                                                                                                                                       | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline.extract                                   | self, *args, **kwargs                                                                                                                                                      | —                                         | activity: {'logging': ['UnifiedLogger.get', 'bound_log.warning'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                        | activity: []
document: []                                    | только в activity
ChemblActivityPipeline.extract_all                               | self                                                                                                                                                                       | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
ChemblActivityPipeline.extract_by_ids                            | self, ids: Sequence[str]                                                                                                                                                   | —                                         | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning', 'self._log_validity_comments_metrics'], 'io': []}
document: {}                                                                                                                                                                                                                                                                           | activity: []
document: []                                    | только в activity
ChemblActivityPipeline.transform                                 | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                    | activity: []
document: []                                    | только в activity
ChemblActivityPipeline.validate                                  | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.info', 'self._log_detailed_validation_errors'], 'io': []}
document: {}                                                                                                                                                                                                                                                               | activity: ['ValueError(msg)']
document: []                   | только в activity
ChemblActivityPipeline.write                                     | self, df: pd.DataFrame, output_path: Path, *, extended: bool = False, include_correlation: bool | None = None, include_qc_metrics: bool | None = None                      | —                                         | activity: {'logging': ['UnifiedLogger.bind', 'UnifiedLogger.get', 'log.debug'], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                          | activity: []
document: []                                    | только в activity
ChemblDocumentPipeline                                           | —                                                                                                                                                                          | —                                         | activity: {}
document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                     | activity: []
document: ['TypeError(msg)']                    | только в document
ChemblDocumentPipeline.__init__                                  | —                                                                                                                                                                          | self, config: PipelineConfig, run_id: str | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._add_system_fields                        | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any          | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._build_document_descriptor                | —                                                                                                                                                                          | self: SelfChemblDocumentPipeline          | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: ['TypeError(msg)']                    | только в document
ChemblDocumentPipeline._check_document_id_uniqueness             | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any          | activity: {}
document: {'logging': ['log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                                                                   | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._coerce_mapping                           | —                                                                                                                                                                          | payload: Any                              | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._enrich_document_terms                    | —                                                                                                                                                                          | self, df: pd.DataFrame                    | activity: {}
document: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                                              | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._extract_nested_fields                    | —                                                                                                                                                                          | self, record: dict[str, Any]              | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._normalize_authors                        | —                                                                                                                                                                          | authors: Any, separator: str              | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._normalize_doi                            | —                                                                                                                                                                          | doi: str | None                           | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._normalize_identifiers                    | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any          | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._normalize_journal                        | —                                                                                                                                                                          | value: Any, max_len: int                  | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._normalize_numeric_fields                 | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any          | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._normalize_string_fields                  | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any          | activity: {}
document: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._schema_column_specs                      | —                                                                                                                                                                          | self                                      | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline._should_enrich_document_terms             | —                                                                                                                                                                          | self                                      | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline.extract                                   | —                                                                                                                                                                          | self, *args, **kwargs                     | activity: {}
document: {'logging': ['UnifiedLogger.get'], 'io': []}                                                                                                                                                                                                                                                                                                                                             | activity: []
document: []                                    | только в document
ChemblDocumentPipeline.extract_all                               | —                                                                                                                                                                          | self                                      | activity: {}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в document
ChemblDocumentPipeline.extract_by_ids                            | —                                                                                                                                                                          | self, ids: Sequence[str]                  | activity: {}
document: {'logging': ['UnifiedLogger.get', 'log.info'], 'io': []}                                                                                                                                                                                                                                                                                                                                 | activity: []
document: []                                    | только в document
ChemblDocumentPipeline.transform                                 | —                                                                                                                                                                          | self, df: pd.DataFrame                    | activity: {}
document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                     | activity: []
document: []                                    | только в document
ChemblDocumentPipeline.validate                                  | —                                                                                                                                                                          | self, df: pd.DataFrame                    | activity: {}
document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                                                                                                                                                                                                                                                                                                                    | activity: []
document: []                                    | только в document
__module_block_0                                                 | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_1                                                 | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_10                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_11                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_12                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_13                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_14                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_15                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_16                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_17                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_18                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_19                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_2                                                 | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_20                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_21                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
__module_block_22                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
__module_block_23                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
__module_block_24                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
__module_block_25                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
__module_block_26                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
__module_block_27                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
__module_block_28                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
__module_block_29                                                | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
document: []                                    | только в activity
__module_block_3                                                 | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_4                                                 | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_5                                                 | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_6                                                 | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_7                                                 | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       
__module_block_8                                                 | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | совпадает        
__module_block_9                                                 | —                                                                                                                                                                          | —                                         | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
document: []                                    | отличается       

_Показаны первые 20 горячих участков из 113._

#### Горячий участок 1

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:106-3476
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,1684 +0,0 @@

-class ChemblActivityPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting activity records from the ChEMBL API."
-    actor ="activity_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-        self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch activity payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        def legacy_activity_ids (bound_log :BoundLogger )->Sequence [str ]|None :
-            payload_activity_ids =kwargs .get ("activity_ids")
-            if payload_activity_ids is None :
-                return None
-            bound_log .warning ("chembl_activity.deprecated_kwargs",message ="Using activity_ids in kwargs is deprecated. Use --input-file instead.")
-            if isinstance (payload_activity_ids ,Sequence )and (not isinstance (payload_activity_ids ,(str ,bytes ))):
-                sequence_ids :Sequence [str |int ]=cast (Sequence [str |int ],payload_activity_ids )
-                return [str (id_val )for id_val in sequence_ids ]
-            return [str (payload_activity_ids )]
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_activity.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="activity_id",legacy_id_resolver =legacy_activity_ids ,legacy_source ="deprecated_kwargs")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all activity records from ChEMBL using the shared iterator."
-        return self .run_extract_all (self ._build_activity_descriptor ())
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract activity records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of activity_id values to extract (as strings or integers).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted activity records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_config ,chembl_client ,activity_iterator ,select_fields =self ._prepare_activity_iteration ()
-        limit =self .config .cli .limit
-        invalid_ids :list [Any ]=[]
-        def normalize_activity_id (raw :Any )->tuple [str |None ,Any ]:
-            if pd .isna (raw ):
-                return (None ,None )
-            try :
-                if isinstance (raw ,str ):
-                    candidate =raw .strip ()
-                    if not candidate :
-                        return (None ,None )
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw ,(int ,float )):
-                    numeric_id =int (raw )
-                else :
-                    numeric_id =int (raw )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw )
-                return (None ,None )
-            return (str (numeric_id ),int (numeric_id ))
-        def delegated_fetch (canonical_ids :Sequence [str ],context :BatchExtractionContext )->tuple [Sequence [Mapping [str ,Any ]],Mapping [str ,Any ]]:
-            numeric_map =context .metadata
-            normalized_ids :list [tuple [int ,str ]]=[]
-            for identifier in canonical_ids :
-                numeric_value =numeric_map .get (identifier )
-                if numeric_value is None :
-                    continue
-                normalized_ids .append ((int (numeric_value ),identifier ))
-            if not normalized_ids :
-                summary ={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-                context .extra ["delegated_summary"]=summary
-                return ([],summary )
-            records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =context .select_fields or None )
-            context .extra ["delegated_summary"]=summary
-            return (records ,summary )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            dataframe =self ._ensure_comment_fields (dataframe ,log )
-            dataframe =self ._extract_data_validity_descriptions (dataframe ,chembl_client ,log )
-            dataframe =self ._extract_assay_fields (dataframe ,chembl_client ,log )
-            self ._log_validity_comments_metrics (dataframe ,log )
-            return dataframe
-        def empty_activity_frame ()->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def finalize_context (context :BatchExtractionContext )->None :
-            summary =context .extra .get ("delegated_summary")
-            if isinstance (summary ,Mapping ):
-                summary_dict =dict (summary )
-                context .extra ["stats_attribute_override"]=summary_dict
-                self ._last_batch_extract_stats =summary_dict
-            else :
-                context .extra ["stats_attribute_override"]=context .stats .as_dict ()
-                self ._last_batch_extract_stats =context .stats .as_dict ()
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="activity_id",fetcher =delegated_fetch ,select_fields =select_fields ,batch_size =source_config .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release ,id_normalizer =normalize_activity_id ,sort_key =lambda pair :int (pair [0 ]),finalize =finalize_dataframe ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats",fetch_mode ="delegated",empty_frame_factory =empty_activity_frame )
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        batch_stats =self ._last_batch_extract_stats or {}
-        log .info ("chembl_activity.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =batch_stats .get ("batches"),api_calls =batch_stats .get ("api_calls"),cache_hits =batch_stats .get ("cache_hits"))
-        return dataframe
-    def _prepare_activity_iteration (self ,*,client_name :str ="chembl_activity_client")->tuple [ActivitySourceConfig ,ChemblClient ,ChemblActivityClient ,list [str ]]:
-        "Construct reusable ChEMBL clients and iterator for activity extraction."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name =client_name )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =self ._resolve_select_fields (source_raw ,default_fields =API_ACTIVITY_FIELDS )
-        return (source_config ,chembl_client ,activity_iterator ,select_fields )
-    def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-        "Construct the descriptor driving the shared extraction template."
-        def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-            http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-            chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-            typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-            activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-            select_fields =source_config .parameters .select_fields
-            return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-        def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            df =pipeline ._ensure_comment_fields (df ,log )
-            chembl_client =cast (ChemblClient ,context .chembl_client )
-            if chembl_client is not None :
-                df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-            pipeline ._log_validity_comments_metrics (df ,log )
-            return df
-        def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            return pipeline ._materialize_activity_record (payload )
-        return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
-    def _materialize_activity_record (self ,payload :Mapping [str ,Any ],*,activity_id :int |None =None )->dict [str ,Any ]:
-        "Normalize nested fields within an activity payload."
-        record =dict (payload )
-        record =self ._extract_nested_fields (record )
-        record =self ._extract_activity_properties_fields (record )
-        if activity_id is not None :
-            record .setdefault ("activity_id",activity_id )
-        return record
-    def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-        "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-        if isinstance (dataset ,pd .Series ):
-            return dataset .to_frame (name ="activity_id")
-        if isinstance (dataset ,pd .DataFrame ):
-            return dataset
-        if isinstance (dataset ,Mapping ):
-            mapping =cast (Mapping [str ,Any ],dataset )
-            return pd .DataFrame ([dict (mapping )])
-        if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-            dataset_list :list [Any ]=list (dataset )
-            return pd .DataFrame ({"activity_id":dataset_list })
-        msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-        raise TypeError (msg )
-    def _normalize_activity_ids (self ,input_frame :pd .DataFrame ,*,limit :int |None ,log :BoundLogger )->list [tuple [int ,str ]]:
-        "Normalize raw identifier values into deduplicated integer/string pairs."
-        normalized_ids :list [tuple [int ,str ]]=[]
-        invalid_ids :list [Any ]=[]
-        seen :set [str ]=set ()
-        for raw_id in input_frame ["activity_id"].tolist ():
-            if pd .isna (raw_id ):
-                continue
-            try :
-                if isinstance (raw_id ,str ):
-                    candidate =raw_id .strip ()
-                    if not candidate :
-                        continue
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw_id ,(int ,float )):
-                    numeric_id =int (raw_id )
-                else :
-                    numeric_id =int (raw_id )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw_id )
-                continue
-            key =str (numeric_id )
-            if key not in seen :
-                seen .add (key )
-                normalized_ids .append ((numeric_id ,key ))
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        if limit is not None :
-            normalized_ids =normalized_ids [:max (int (limit ),0 )]
-        return normalized_ids
-    def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-        "Iterate over IDs using the shared iterator while preserving cache semantics."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        records :list [dict [str ,Any ]]=[]
-        success_count =0
-        fallback_count =0
-        error_count =0
-        cache_hits =0
-        api_calls =0
-        total_batches =0
-        key_order =[key for _ ,key in normalized_ids ]
-        key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-        for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-            total_batches +=1
-            batch_start =time .perf_counter ()
-            from_cache =False
-            chunk_records :dict [str ,dict [str ,Any ]]={}
-            try :
-                cached_records =self ._check_cache (chunk ,self ._chembl_release )
-                if cached_records is not None :
-                    from_cache =True
-                    cache_hits +=len (chunk )
-                    chunk_records =cached_records
-                else :
-                    api_calls +=1
-                    fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                    for item in fetched_items :
-                        if not isinstance (item ,Mapping ):
-                            continue
-                        activity_value =item .get ("activity_id")
-                        if activity_value is None :
-                            continue
-                        chunk_records [str (activity_value )]=dict (item )
-                    self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-                success_in_batch =0
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    record =chunk_records .get (key )
-                    if record and (not record .get ("error")):
-                        materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                        records .append (materialized )
-                        success_count +=1
-                        success_in_batch +=1
-                    else :
-                        fallback_record =self ._create_fallback_record (numeric_id )
-                        records .append (fallback_record )
-                        fallback_count +=1
-                        error_count +=1
-                batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-                log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-            except CircuitBreakerOpenError as exc :
-                log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except RequestException as exc :
-                log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except Exception as exc :
-                log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-        total_records =len (normalized_ids )
-        success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-        summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-        return (records ,summary )
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw activity data by normalizing measurements, identifiers, and data types."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_measurements (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,ActivitySchema ,log )
-        if "curated"in df .columns or "curated_by"in df .columns :
-            if "curated"not in df .columns :
-                df ["curated"]=pd .NA
-            if "curated_by"in df .columns :
-                mask =df ["curated"].isna ()
-                df .loc [mask ,"curated"]=df .loc [mask ,"curated_by"].notna ()
-            df ["curated"]=df ["curated"].astype ("boolean")
-        df =self ._validate_foreign_keys (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if self ._should_enrich_compound_record ():
-            df =self ._enrich_compound_record (df )
-        if self ._should_enrich_assay ():
-            df =self ._enrich_assay (df )
-        if self ._should_enrich_molecule ():
-            df =self ._enrich_molecule (df )
-        if self ._should_enrich_data_validity ():
-            df =self ._enrich_data_validity (df )
-        df =self ._finalize_identifier_columns (df ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        df =self ._finalize_output_columns (df ,log )
-        df =self ._filter_invalid_required_fields (df ,log )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against ActivitySchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        if "target_tax_id"in df .columns :
-            dtype_name :str =str (df ["target_tax_id"].dtype .name )
-            if dtype_name !="Int64":
-                numeric_series :pd .Series [Any ]=pd .to_numeric (df ["target_tax_id"],errors ="coerce")
-                df ["target_tax_id"]=numeric_series .astype ("Int64")
-        self ._check_activity_id_uniqueness (df ,log )
-        self ._check_foreign_key_integrity (df ,log )
-        self ._validate_data_validity_comment_soft_enum (df ,log )
-        original_coerce =self .config .validation .coerce
-        try :
-            self .config .validation .coerce =False
-            validated =super ().validate (df )
-            log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce )
-            return validated
-        except pandera .errors .SchemaErrors as exc :
-            failure_cases_df :pd .DataFrame |None =None
-            if hasattr (exc ,"failure_cases"):
-                failure_cases_df =cast (pd .DataFrame ,exc .failure_cases )
-            error_count =len (failure_cases_df )if failure_cases_df is not None else 0
-            error_summary =summarize_schema_errors (exc )
-            log .error ("validation_failed",error_count =error_count ,schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce ,error_summary =error_summary ,exc_info =True )
-            if failure_cases_df is not None and (not failure_cases_df .empty ):
-                failure_cases_summary =format_failure_cases (failure_cases_df )
-                log .error ("validation_failure_cases",failure_cases =failure_cases_summary )
-                self ._log_detailed_validation_errors (failure_cases_df ,df ,log )
-            msg =f'Validation failed with {error_count } error(s) against schema {self .config .validation .schema_out }: {error_summary }'
-            raise ValueError (msg )from exc
-        except Exception as exc :
-            log .error ("validation_error",error =str (exc ),schema =self .config .validation .schema_out ,exc_info =True )
-            raise
-        finally :
-            self .config .validation .coerce =original_coerce
-    def _should_enrich_compound_record (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 compound_record \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            compound_record_section :Any =enrich_section .get ("compound_record")
-            if not isinstance (compound_record_section ,Mapping ):
-                return False
-            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-            enabled :Any =compound_record_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        compound_record_section :Any =enrich_section .get ("compound_record")
-                        if isinstance (compound_record_section ,Mapping ):
-                            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                            enrich_cfg =dict (compound_record_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_assay (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 assay \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            assay_section :Any =enrich_section .get ("assay")
-            if not isinstance (assay_section ,Mapping ):
-                return False
-            assay_section =cast (Mapping [str ,Any ],assay_section )
-            enabled :Any =assay_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        assay_section :Any =enrich_section .get ("assay")
-                        if isinstance (assay_section ,Mapping ):
-                            assay_section =cast (Mapping [str ,Any ],assay_section )
-                            enrich_cfg =dict (assay_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_assay (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_data_validity (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 data_validity \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            data_validity_section :Any =enrich_section .get ("data_validity")
-            if not isinstance (data_validity_section ,Mapping ):
-                return False
-            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-            enabled :Any =data_validity_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        data_validity_section :Any =enrich_section .get ("data_validity")
-                        if isinstance (data_validity_section ,Mapping ):
-                            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                            enrich_cfg =dict (data_validity_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        if "data_validity_description"in df .columns :
-            non_na_count =int (df ["data_validity_description"].notna ().sum ())
-            if non_na_count >0 :
-                log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_molecule (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 molecule \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            molecule_section :Any =enrich_section .get ("molecule")
-            if not isinstance (molecule_section ,Mapping ):
-                return False
-            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-            enabled :Any =molecule_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        molecule_section :Any =enrich_section .get ("molecule")
-                        if isinstance (molecule_section ,Mapping ):
-                            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                            enrich_cfg =dict (molecule_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-        if "molecule_name"in df_join .columns :
-            if "molecule_pref_name"not in df .columns :
-                df ["molecule_pref_name"]=pd .NA
-            mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-            if mask .any ():
-                df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-                df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-                log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-        return df
-    def _extract_from_chembl (self ,dataset :object ,chembl_client :ChemblClient |Any ,activity_iterator :ChemblActivityClient ,*,limit :int |None =None ,select_fields :Sequence [str ]|None =None )->pd .DataFrame :
-        "Extract activity records by delegating batching to ``ChemblActivityClient``."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        method_start =time .perf_counter ()
-        self ._last_batch_extract_stats =None
-        input_frame =self ._coerce_activity_dataset (dataset )
-        if "activity_id"not in input_frame .columns :
-            msg ="Input dataset must contain an 'activity_id' column"
-            raise ValueError (msg )
-        normalized_ids =self ._normalize_activity_ids (input_frame ,limit =limit ,log =log )
-        if not normalized_ids :
-            summary :dict [str ,Any ]={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-            self ._last_batch_extract_stats =summary
-            log .info ("chembl_activity.batch_summary",**summary )
-            empty_frame =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-            return self ._ensure_comment_fields (empty_frame ,log )
-        records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =select_fields )
-        duration_ms =(time .perf_counter ()-method_start )*1000.0
-        summary ["duration_ms"]=duration_ms
-        self ._last_batch_extract_stats =summary
-        log .info ("chembl_activity.batch_summary",**summary )
-        result_df :pd .DataFrame =pd .DataFrame .from_records (records )
-        if result_df .empty :
-            result_df =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        elif "activity_id"in result_df .columns :
-            result_df =result_df .sort_values ("activity_id").reset_index (drop =True )
-        result_df =self ._ensure_comment_fields (result_df ,log )
-        result_df =self ._extract_data_validity_descriptions (result_df ,chembl_client ,log )
-        result_df =self ._extract_assay_fields (result_df ,chembl_client ,log )
-        self ._log_validity_comments_metrics (result_df ,log )
-        return result_df
-    def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-        cache_config =self .config .cache
-        if not cache_config .enabled :
-            return None
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        if not cache_file .exists ():
-            return None
-        try :
-            stat =cache_file .stat ()
-        except OSError :
-            return None
-        ttl_seconds =int (cache_config .ttl )
-        if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        try :
-            payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-        except (OSError ,json .JSONDecodeError ):
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        if not isinstance (payload ,dict ):
-            return None
-        missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-        if missing :
-            return None
-        return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
-    def _store_cache (self ,batch_ids :Sequence [str ],batch_data :Mapping [str ,Mapping [str ,Any ]],release :str |None )->None :
-        cache_config =self .config .cache
-        if not cache_config .enabled or not batch_ids or (not batch_data ):
-            return
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        normalized_set =set (normalized_ids )
-        data_to_store ={key :batch_data [key ]for key in normalized_set if key in batch_data }
-        if not data_to_store :
-            return
-        try :
-            cache_file .parent .mkdir (parents =True ,exist_ok =True )
-            tmp_path =cache_file .with_suffix (cache_file .suffix +".tmp")
-            tmp_path .write_text (json .dumps (data_to_store ,sort_keys =True ,default =str ),encoding ="utf-8")
-            tmp_path .replace (cache_file )
-        except Exception as exc :
-            log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-            log .debug ("chembl_activity.cache_store_failed",error =str (exc ))
-    def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-        directory =self ._cache_directory (release )
-        cache_key =self ._cache_key (batch_ids ,release )
-        return directory /f'{cache_key }.json'
-    def _cache_directory (self ,release :str |None )->Path :
-        cache_root =Path (self .config .paths .cache_root )
-        directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-        release_component =self ._sanitize_cache_component (release or "unknown")
-        pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-        version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-        return cache_root /directory_name /pipeline_component /release_component /version_component
-    def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-        payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-        raw =json .dumps (payload ,sort_keys =True )
-        return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
-    @staticmethod
-    def _sanitize_cache_component (value :str )->str :
-        sanitized =re .sub ("[^0-9A-Za-z_.-]","_",value )
-        return sanitized or "default"
-    def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-        "Create fallback record enriched with error metadata."
-        base_message ="Fallback: ChEMBL activity unavailable"
-        message =f'{base_message } ({error })'if error else base_message
-        timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-        metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-        if isinstance (error ,RequestException ):
-            response =getattr (error ,"response",None )
-            status_code =getattr (response ,"status_code",None )
-            if status_code is not None :
-                metadata ["http_status"]=status_code
-            metadata ["error_message"]=str (error )
-        elif error is not None :
-            metadata ["error_message"]=str (error )
-        fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-        return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
-    def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-        if df .empty :
-            return df
-        required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-        if missing_fields :
-            for field in missing_fields :
-                df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            log .debug ("comment_fields_ensured",fields =missing_fields )
-        return df
-    def _extract_data_validity_descriptions (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c data_validity_description \u0438\u0437 DATA_VALIDITY_LOOKUP \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f data_validity_comment \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_data_validity_lookup() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c data_validity_comment.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 data_validity_description.\n        "
-        if df .empty :
-            return df
-        if "data_validity_comment"not in df .columns :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="data_validity_comment_column_missing")
-            return df
-        validity_comments :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            comment =row .get ("data_validity_comment")
-            if pd .isna (comment )or comment is None :
-                continue
-            comment_str =str (comment ).strip ()
-            if comment_str :
-                validity_comments .append (comment_str )
-        if not validity_comments :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="no_valid_comments")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        unique_comments =list (set (validity_comments ))
-        log .info ("extract_data_validity_descriptions_fetching",comments_count =len (unique_comments ))
-        try :
-            records_dict =client .fetch_data_validity_lookup (comments =unique_comments ,fields =["data_validity_comment","description"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_data_validity_descriptions_fetch_error",error =str (exc ),exc_info =True )
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for comment in unique_comments :
-            record =records_dict .get (comment )
-            if record :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":record .get ("description")})
-            else :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":None })
-        if not enrichment_data :
-            log .debug ("extract_data_validity_descriptions_no_records")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        original_index =df .index .copy ()
-        df_result =df .merge (df_enrich ,on =["data_validity_comment"],how ="left",suffixes =("","_enrich"))
-        if "data_validity_description"not in df_result .columns :
-            df_result ["data_validity_description"]=pd .Series ([pd .NA ]*len (df_result ),dtype ="string")
-        else :
-            df_result ["data_validity_description"]=df_result ["data_validity_description"].astype ("string")
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_data_validity_descriptions_complete",comments_requested =len (unique_comments ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _extract_assay_fields (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c assay_organism \u0438 assay_tax_id \u0438\u0437 ASSAYS \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f assay_chembl_id \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_assays_by_ids() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438 assay_organism \u0438 assay_tax_id.\n        "
-        if df .empty :
-            return df
-        if "assay_chembl_id"not in df .columns :
-            log .debug ("extract_assay_fields_skipped",reason ="assay_chembl_id_column_missing")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        assay_ids :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            assay_id =row .get ("assay_chembl_id")
-            if pd .isna (assay_id )or assay_id is None :
-                continue
-            assay_id_str =str (assay_id ).strip ().upper ()
-            if assay_id_str :
-                assay_ids .append (assay_id_str )
-        if not assay_ids :
-            log .debug ("extract_assay_fields_skipped",reason ="no_valid_assay_ids")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        unique_assay_ids =list (set (assay_ids ))
-        log .info ("extract_assay_fields_fetching",assay_ids_count =len (unique_assay_ids ))
-        try :
-            records_dict =client .fetch_assays_by_ids (ids =unique_assay_ids ,fields =["assay_chembl_id","assay_organism","assay_tax_id"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_assay_fields_fetch_error",error =str (exc ),exc_info =True )
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for assay_id in unique_assay_ids :
-            record =records_dict .get (assay_id )if records_dict else None
-            if record :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":record .get ("assay_organism"),"assay_tax_id":record .get ("assay_tax_id")})
-            else :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":None ,"assay_tax_id":None })
-        if not enrichment_data :
-            log .debug ("extract_assay_fields_no_records")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        df_enrich ["assay_chembl_id"]=df_enrich ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        original_index =df .index .copy ()
-        df_normalized =df .copy ()
-        df_normalized ["assay_chembl_id_normalized"]=df_normalized ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        df_result =df_normalized .merge (df_enrich ,left_on ="assay_chembl_id_normalized",right_on ="assay_chembl_id",how ="left",suffixes =("","_enrich"))
-        df_result =df_result .drop (columns =["assay_chembl_id_normalized"])
-        for col in ["assay_organism","assay_tax_id"]:
-            if f'{col }_enrich'in df_result .columns :
-                if col not in df_result .columns :
-                    df_result [col ]=df_result [f'{col }_enrich']
-                else :
-                    base_series :pd .Series [Any ]=df_result [col ]
-                    enrich_series :pd .Series [Any ]=df_result [f'{col }_enrich']
-                    missing_mask =base_series .isna ()
-                    if bool (missing_mask .any ()):
-                        df_result .loc [missing_mask ,col ]=enrich_series .loc [missing_mask ]
-                df_result =df_result .drop (columns =[f'{col }_enrich'])
-        for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-            if col not in df_result .columns :
-                df_result [col ]=pd .Series ([pd .NA ]*len (df_result ),dtype =dtype )
-        df_result ["assay_organism"]=df_result ["assay_organism"].astype ("string")
-        df_result ["assay_tax_id"]=pd .to_numeric (df_result ["assay_tax_id"],errors ="coerce").astype ("Int64")
-        mask_valid =df_result ["assay_tax_id"].notna ()
-        if mask_valid .any ():
-            invalid_mask =mask_valid &(df_result ["assay_tax_id"]<1 )
-            if invalid_mask .any ():
-                log .warning ("invalid_assay_tax_id_range",count =int (invalid_mask .sum ()))
-                df_result .loc [invalid_mask ,"assay_tax_id"]=pd .NA
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_assay_fields_complete",assay_ids_requested =len (unique_assay_ids ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _log_validity_comments_metrics (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "\u041b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442:\n        - \u0414\u043e\u043b\u044e NA \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0442\u0440\u0435\u0445 \u043f\u043e\u043b\u0435\u0439\n        - \u0422\u043e\u043f-10 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment\n        - \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment (\u043d\u0435 \u0432 whitelist)\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity.\n        log:\n            Logger instance.\n        "
-        if df .empty :
-            return
-        metrics :dict [str ,Any ]={}
-        comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        for field in comment_fields :
-            if field in df .columns :
-                na_count =int (df [field ].isna ().sum ())
-                total_count =len (df )
-                na_rate =float (na_count )/float (total_count )if total_count >0 else 0.0
-                metrics [f'{field }_na_rate']=na_rate
-                metrics [f'{field }_na_count']=na_count
-                metrics [f'{field }_total_count']=total_count
-        non_null_comments_series :pd .Series [str ]|None =None
-        if "data_validity_comment"in df .columns :
-            series_candidate =df ["data_validity_comment"].dropna ()
-            if len (series_candidate )>0 :
-                typed_series :pd .Series [str ]=series_candidate .astype ("string")
-                non_null_comments_series =typed_series
-                value_counts =typed_series .value_counts ().head (10 )
-                top_10 ={str (key ):int (value )for key ,value in value_counts .items ()}
-                metrics ["top_10_data_validity_comments"]=top_10
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if whitelist and non_null_comments_series is not None :
-            whitelist_set :set [str ]=set (whitelist )
-            def _is_unknown (value :str )->bool :
-                return value not in whitelist_set
-            unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-            unknown_count =int (unknown_mask .sum ())
-            if unknown_count >0 :
-                unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-                metrics ["unknown_data_validity_comments_count"]=unknown_count
-                metrics ["unknown_data_validity_comments_samples"]=unknown_values
-                log .warning ("unknown_data_validity_comments_detected",unknown_count =unknown_count ,samples =unknown_values ,whitelist =whitelist )
-        if metrics :
-            log .info ("validity_comments_metrics",**metrics )
-    def _get_data_validity_comment_whitelist (self )->list [str ]:
-        "\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c whitelist \u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0434\u043b\u044f data_validity_comment \u0438\u0437 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\n\n        Raises\n        ------\n        RuntimeError\n            \u0415\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d \u0438\u043b\u0438 \u043f\u0443\u0441\u0442.\n        "
-        try :
-            values =sorted (self ._required_vocab_ids ("data_validity_comment"))
-        except RuntimeError as exc :
-            UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validation',run_id =self .run_id ).error ("data_validity_comment_whitelist_unavailable",error =str (exc ))
-            raise
-        return values
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested assay and molecule objects."
-        if "assay"in record and isinstance (record ["assay"],Mapping ):
-            assay =cast (Mapping [str ,Any ],record ["assay"])
-            if "organism"in assay :
-                record .setdefault ("assay_organism",assay ["organism"])
-            if "tax_id"in assay :
-                record .setdefault ("assay_tax_id",assay ["tax_id"])
-        if "molecule"in record and isinstance (record ["molecule"],Mapping ):
-            molecule =cast (Mapping [str ,Any ],record ["molecule"])
-            if "pref_name"in molecule :
-                record .setdefault ("molecule_pref_name",molecule ["pref_name"])
-        if "curated_by"in record :
-            curated_by =record .get ("curated_by")
-            if curated_by is not None and (not pd .isna (curated_by )):
-                record .setdefault ("curated",True )
-            else :
-                record .setdefault ("curated",False )
-        elif "curated"not in record :
-            record .setdefault ("curated",None )
-        return record
-    def _extract_activity_properties_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract TRUV fields, standard_* fields, and comments from activity_properties array as fallback.\n\n        \u041f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0438\u0437 activity_properties \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442\n        \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043e\u0442\u0432\u0435\u0442\u0435 API (\u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 \u043f\u0440\u044f\u043c\u044b\u0445 \u043f\u043e\u043b\u0435\u0439 \u0438\u0437 ACTIVITIES).\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0435 TRUV-\u043f\u043e\u043b\u044f: value, text_value, relation, units.\n        \u0422\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f: standard_upper_value, standard_text_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b: upper_value, lower_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0438: activity_comment, data_validity_comment.\n\n        \u0422\u0430\u043a\u0436\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 activity_properties \u0432 \u0437\u0430\u043f\u0438\u0441\u0438 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438.\n        \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044e \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract_activity_properties')
-        activity_id =record .get ("activity_id")
-        if "activity_properties"not in record :
-            log .warning ("activity_properties_missing",activity_id =activity_id ,message ="activity_properties not found in API response (possible ChEMBL < v24)")
-            record ["activity_properties"]=None
-            return record
-        properties =record ["activity_properties"]
-        if properties is None :
-            log .debug ("activity_properties_null",activity_id =activity_id ,message ="activity_properties is None (possible ChEMBL < v24)")
-            return record
-        if isinstance (properties ,str ):
-            try :
-                properties =json .loads (properties )
-            except (TypeError ,ValueError ,json .JSONDecodeError )as exc :
-                log .debug ("activity_properties_parse_failed",error =str (exc ),activity_id =record .get ("activity_id"))
-                return record
-        if not isinstance (properties ,Sequence )or isinstance (properties ,(str ,bytes )):
-            return record
-        property_iterable :Iterable [Any ]=cast (Iterable [Any ],properties )
-        property_items :list [Any ]=list (property_iterable )
-        def _set_fallback (key :str ,value :Any )->None :
-            "Set fallback value only if key is missing in record and value is not None."
-            if value is not None and record .get (key )is None :
-                record [key ]=value
-        def _is_empty (value :Any )->bool :
-            "Check if value is empty (None, empty string, or whitespace)."
-            if value is None :
-                return True
-            if isinstance (value ,str ):
-                return not value .strip ()
-            return False
-        items :list [Mapping [str ,Any ]]=[]
-        for property_item in property_items :
-            if isinstance (property_item ,Mapping )and "type"in property_item and ("value"in property_item or "text_value"in property_item ):
-                items .append (cast (Mapping [str ,Any ],property_item ))
-        def _is_measured (p :Mapping [str ,Any ])->bool :
-            rf =p .get ("result_flag")
-            return rf is True or (isinstance (rf ,int )and rf ==1 )
-        items .sort (key =lambda p :not _is_measured (p ))
-        for prop in items :
-            val =prop .get ("value")
-            txt =prop .get ("text_value")
-            rel =prop .get ("relation")
-            unt =prop .get ("units")
-            prop_type =str (prop .get ("type","")).lower ()
-            will_set_value =val is not None and record .get ("value")is None
-            will_set_text_value =txt is not None and record .get ("text_value")is None
-            _set_fallback ("value",val )
-            _set_fallback ("text_value",txt )
-            if will_set_value or will_set_text_value :
-                _set_fallback ("relation",rel )
-                _set_fallback ("units",unt )
-            if unt is not None and record .get ("units")is None :
-                _set_fallback ("units",unt )
-            if record .get ("upper_value")is None and ("upper"in prop_type or prop_type in ("upper_value","upper limit")):
-                if val is not None :
-                    _set_fallback ("upper_value",val )
-            if record .get ("lower_value")is None and ("lower"in prop_type or prop_type in ("lower_value","lower limit")):
-                if val is not None :
-                    _set_fallback ("lower_value",val )
-            if record .get ("standard_upper_value")is None and ("standard_upper"in prop_type or prop_type in ("standard upper","standard upper value")):
-                if val is not None :
-                    _set_fallback ("standard_upper_value",val )
-            if record .get ("standard_text_value")is None and "standard"in prop_type and ("text"in prop_type ):
-                if txt is not None :
-                    _set_fallback ("standard_text_value",txt )
-                elif val is not None :
-                    _set_fallback ("standard_text_value",val )
-        current_comment =record .get ("data_validity_comment")
-        if _is_empty (current_comment ):
-            data_validity_items :list [Mapping [str ,Any ]]=[prop for prop in items if ("data_validity"in str (prop .get ("type","")).lower ()or "validity"in str (prop .get ("type","")).lower ())and (prop .get ("text_value")is not None or prop .get ("value")is not None )]
-            if data_validity_items :
-                measured_items =[p for p in data_validity_items if _is_measured (p )]
-                if measured_items :
-                    prop =measured_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="measured",comment_value =comment_value )
-                else :
-                    prop =data_validity_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="first",comment_value =comment_value )
-            else :
-                log .debug ("data_validity_comment_fallback_no_items",activity_id =record .get ("activity_id"),activity_properties_count =len (items ),has_activity_properties =True )
-        else :
-            log .debug ("data_validity_comment_from_api",activity_id =record .get ("activity_id"),comment_value =current_comment )
-        normalized_properties =self ._normalize_activity_properties_items (property_items ,log )
-        if normalized_properties is not None :
-            validated_properties ,validation_stats =self ._validate_activity_properties_truv (normalized_properties ,log ,activity_id )
-            deduplicated_properties ,dedup_stats =self ._deduplicate_activity_properties (validated_properties ,log ,activity_id )
-            record ["activity_properties"]=deduplicated_properties
-            log .debug ("activity_properties_processed",activity_id =activity_id ,original_count =len (property_items ),normalized_count =len (normalized_properties ),validated_count =len (validated_properties ),deduplicated_count =len (deduplicated_properties ),invalid_count =validation_stats .get ("invalid_count",0 ),duplicates_removed =dedup_stats .get ("duplicates_removed",0 ))
-        else :
-            record ["activity_properties"]=properties
-            log .debug ("activity_properties_normalization_failed",activity_id =activity_id ,message ="activity_properties normalization failed, keeping original")
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
-    @staticmethod
-    def _extract_chembl_release (payload :Mapping [str ,Any ])->str |None :
-        for key in ("chembl_release","chembl_db_version","release","version"):
-            value =payload .get (key )
-            if isinstance (value ,str )and value .strip ():
-                return value
-            if value is not None :
-                return str (value )
-        return None
-    @staticmethod
-    def _extract_page_items (payload :Mapping [str ,Any ],items_keys :Sequence [str ]|None =None )->list [dict [str ,Any ]]:
-        preferred_keys :tuple [str ,...]=("activities",)
-        if items_keys is None :
-            combined_keys =preferred_keys +("data","items","results")
-        else :
-            combined_keys =tuple (dict .fromkeys ((*preferred_keys ,*items_keys )))
-        return ChemblPipelineBase ._extract_page_items (payload ,combined_keys )
-    @staticmethod
-    def _next_link (payload :Mapping [str ,Any ],base_url :str )->str |None :
-        page_meta :Any =payload .get ("page_meta")
-        if isinstance (page_meta ,Mapping ):
-            next_link_raw :Any =page_meta .get ("next")
-            next_link :str |None =cast (str |None ,next_link_raw )if next_link_raw is not None else None
-            if isinstance (next_link ,str )and next_link :
-                base_url_str =str (base_url )
-                base_path_parse_result =urlparse (base_url_str )
-                base_path_raw =base_path_parse_result .path
-                base_path_str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                base_path :str =base_path_str .rstrip ("/")
-                if next_link .startswith ("http://")or next_link .startswith ("https://"):
-                    parsed =urlparse (next_link )
-                    base_parsed =urlparse (base_url_str )
-                    parsed_path_raw =parsed .path
-                    base_path_raw =base_parsed .path
-                    path :str =parsed_path_raw .decode ("utf-8","ignore")if isinstance (parsed_path_raw ,(bytes ,bytearray ))else parsed_path_raw
-                    base_path_from_url :str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                    path_normalized :str =path .rstrip ("/")
-                    base_path_normalized :str =base_path_from_url .rstrip ("/")
-                    if base_path_normalized and path_normalized .startswith (base_path_normalized ):
-                        relative_path =path_normalized [len (base_path_normalized ):]
-                        if not relative_path :
-                            return None
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    elif "/api/data/"in path :
-                        parts =path .split ("/api/data/",1 )
-                        if len (parts )>1 :
-                            relative_path ="/"+parts [1 ]
-                        else :
-                            relative_path =path
-                    else :
-                        relative_path =path
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    if parsed .query :
-                        relative_path =f'{relative_path }?{parsed .query }'
-                    return relative_path
-                if base_path :
-                    normalized_base =base_path .lstrip ("/")
-                    stripped_link =next_link .lstrip ("/")
-                    if stripped_link .startswith (normalized_base +"/"):
-                        stripped_link =stripped_link [len (normalized_base ):]
-                    elif stripped_link ==normalized_base :
-                        stripped_link =""
-                    next_link =stripped_link
-                next_link =next_link .lstrip ("/")
-                if next_link :
-                    next_link =f'/{next_link }'
-                else :
-                    next_link ="/"
-                return next_link
-        return None
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Ensure canonical identifier columns are present before normalization."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_chembl_id"not in df .columns and "assay_id"in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "testitem_chembl_id"not in df .columns :
-            if "testitem_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["testitem_id"]
-                actions .append ("testitem_id->testitem_chembl_id")
-            elif "molecule_chembl_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["molecule_chembl_id"]
-                actions .append ("molecule_chembl_id->testitem_chembl_id")
-        if "molecule_chembl_id"not in df .columns and "testitem_chembl_id"in df .columns :
-            df ["molecule_chembl_id"]=df ["testitem_chembl_id"]
-            actions .append ("testitem_chembl_id->molecule_chembl_id")
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_required =[column for column in required_columns if column not in df .columns ]
-        if missing_required :
-            for column in missing_required :
-                df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            actions .append (f"created_missing:{",".join (missing_required )}")
-        alias_columns =[column for column in ("assay_id","testitem_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        boolean_columns =("potential_duplicate","curated","removed")
-        for column in boolean_columns :
-            specs [column ]={"dtype":"boolean","default":pd .NA }
-        return specs
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize ChEMBL and BAO identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"],pattern ="^CHEMBL\\d+$"),IdentifierRule (name ="bao",columns =["bao_endpoint","bao_format"],pattern ="^BAO_\\d{7}$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _finalize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align identifier columns after normalization and drop aliases."
-        df =df .copy ()
-        if {"molecule_chembl_id","testitem_chembl_id"}.issubset (df .columns ):
-            mismatch_mask =df ["molecule_chembl_id"].notna ()&df ["testitem_chembl_id"].notna ()&(df ["molecule_chembl_id"]!=df ["testitem_chembl_id"])
-            if mismatch_mask .any ():
-                mismatch_count =int (mismatch_mask .sum ())
-                samples_raw =df .loc [mismatch_mask ,["molecule_chembl_id","testitem_chembl_id"]].drop_duplicates ().head (5 ).to_dict ("records")
-                samples :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],samples_raw )
-                log .warning ("identifier_mismatch",count =mismatch_count ,samples =samples )
-                df .loc [mismatch_mask ,"testitem_chembl_id"]=df .loc [mismatch_mask ,"molecule_chembl_id"]
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_columns =[column for column in required_columns if column not in df .columns ]
-        if missing_columns :
-            for column in missing_columns :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            log .warning ("identifier_columns_missing",columns =missing_columns )
-        return df
-    def _finalize_output_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align final column order with schema and drop unexpected fields."
-        df =df .copy ()
-        expected =list (COLUMN_ORDER )
-        extras =[column for column in df .columns if column not in expected ]
-        if extras :
-            df =df .drop (columns =extras )
-            log .debug ("output_columns_dropped",columns =extras )
-        missing =[column for column in expected if column not in df .columns ]
-        if missing :
-            for column in missing :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([pd .NA ]*len (df ),dtype ="object")
-            log .warning ("output_columns_missing",columns =missing )
-        if not expected :
-            return df
-        return df [expected ]
-    def _filter_invalid_required_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Filter out rows with NULL values in required identifier fields.\n\n        Removes rows where any of the required fields (assay_chembl_id,\n        testitem_chembl_id, molecule_chembl_id) are NULL, as these cannot\n        pass schema validation.\n\n        Parameters\n        ----------\n        df:\n            DataFrame to filter.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            Filtered DataFrame with only rows having all required fields populated.\n        "
-        df =df .copy ()
-        if df .empty :
-            return df
-        required_fields =["assay_chembl_id","molecule_chembl_id"]
-        missing_fields =[field for field in required_fields if field not in df .columns ]
-        if missing_fields :
-            log .warning ("filter_skipped_missing_columns",missing_columns =missing_fields ,message ="Cannot filter: required columns are missing")
-            return df
-        valid_mask =df ["assay_chembl_id"].notna ()&df ["molecule_chembl_id"].notna ()
-        invalid_count =int ((~valid_mask ).sum ())
-        if invalid_count >0 :
-            invalid_rows =df [~valid_mask ]
-            sample_size =min (5 ,len (invalid_rows ))
-            sample_activity_ids =invalid_rows ["activity_id"].head (sample_size ).tolist ()if "activity_id"in invalid_rows .columns else []
-            log .warning ("filtered_invalid_rows",filtered_count =invalid_count ,remaining_count =int (valid_mask .sum ()),sample_activity_ids =sample_activity_ids ,message ="Rows with NULL in required identifier fields were filtered out")
-            df =df [valid_mask ].reset_index (drop =True )
-        return df
-    def _normalize_measurements (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize standard_value, standard_units, standard_relation, and standard_type."
-        df =df .copy ()
-        normalized_count =0
-        if "standard_value"in df .columns :
-            mask =df ["standard_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_value"]=numeric_series_std
-                negative_mask =mask &(df ["standard_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["standard_relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"standard_relation"]=series
-                invalid_mask =mask &~df ["standard_relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_type"in df .columns :
-            mask =df ["standard_type"].notna ()
-            if mask .any ():
-                df .loc [mask ,"standard_type"]=df .loc [mask ,"standard_type"].astype (str ).str .strip ()
-                standard_types_set :set [str ]=STANDARD_TYPES
-                invalid_mask =mask &~df ["standard_type"].isin (standard_types_set )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_type",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_type"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_units"in df .columns :
-            unit_mapping ={"nanomolar":"nM","nmol":"nM","nm":"nM","NM":"nM","\u00b5M":"\u03bcM","uM":"\u03bcM","UM":"\u03bcM","micromolar":"\u03bcM","microM":"\u03bcM","umol":"\u03bcM","millimolar":"mM","milliM":"mM","mmol":"mM","MM":"mM","percent":"%","pct":"%","ratios":"ratio"}
-            mask =df ["standard_units"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_units"].astype (str ).str .strip ()
-                for old_unit ,new_unit in unit_mapping .items ():
-                    series =series .str .replace (old_unit ,new_unit ,regex =False ,case =False )
-                df .loc [mask ,"standard_units"]=series
-                normalized_count +=int (mask .sum ())
-        if "relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"relation"]=series
-                invalid_mask =mask &~df ["relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_upper_value"in df .columns :
-            mask =df ["standard_upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_upper_value"]=numeric_series_std_upper
-                negative_mask =mask &(df ["standard_upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "upper_value"in df .columns :
-            mask =df ["upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"upper_value"]=numeric_series_upper
-                negative_mask =mask &(df ["upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "lower_value"in df .columns :
-            mask =df ["lower_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"lower_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_lower :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"lower_value"]=numeric_series_lower
-                negative_mask =mask &(df ["lower_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_lower_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"lower_value"]=None
-                normalized_count +=int (mask .sum ())
-        if normalized_count >0 :
-            log .debug ("measurements_normalized",normalized_count =normalized_count )
-        return df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize string fields: trim, empty string to null, title-case for organism."
-        working_df =df .copy ()
-        if "data_validity_description"in working_df .columns and "data_validity_comment"in working_df .columns :
-            invalid_mask =working_df ["data_validity_description"].notna ()&working_df ["data_validity_comment"].isna ()
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                log .warning ("invariant_data_validity_description_without_comment",count =invalid_count ,message ="data_validity_description is filled while data_validity_comment is NA")
-        rules :dict [str ,StringRule ]={"canonical_smiles":StringRule (),"bao_label":StringRule (max_length =128 ),"target_organism":StringRule (title_case =True ),"assay_organism":StringRule (title_case =True ),"data_validity_comment":StringRule (),"data_validity_description":StringRule (),"activity_comment":StringRule (),"standard_text_value":StringRule (),"text_value":StringRule (),"type":StringRule (),"units":StringRule (),"assay_type":StringRule (),"assay_description":StringRule (),"molecule_pref_name":StringRule (),"target_pref_name":StringRule (),"uo_units":StringRule (),"qudt_units":StringRule ()}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Serialize nested structures (ligand_efficiency, activity_properties) to JSON strings."
-        df =df .copy ()
-        nested_fields =["ligand_efficiency","activity_properties"]
-        for field in nested_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                serialized :list [Any ]=[]
-                for idx ,value in df .loc [mask ,field ].items ():
-                    if field =="activity_properties":
-                        serialized_value =self ._serialize_activity_properties (value ,log )
-                        serialized .append (serialized_value )
-                        continue
-                    if isinstance (value ,(Mapping ,list )):
-                        try :
-                            serialized .append (json .dumps (value ,ensure_ascii =False ,sort_keys =True ))
-                        except (TypeError ,ValueError )as exc :
-                            log .warning ("nested_serialization_failed",field =field ,index =idx ,error =str (exc ))
-                            serialized .append (None )
-                    elif isinstance (value ,str ):
-                        try :
-                            json .loads (value )
-                            serialized .append (value )
-                        except (TypeError ,ValueError ):
-                            serialized .append (None )
-                    else :
-                        serialized .append (None )
-                df .loc [mask ,field ]=pd .Series (serialized ,dtype ="object",index =df .loc [mask ,field ].index )
-        if "standard_value"in df .columns and "ligand_efficiency"in df .columns :
-            mask =df ["standard_value"].notna ()&df ["ligand_efficiency"].isna ()
-            if mask .any ():
-                log .warning ("ligand_efficiency_missing_with_standard_value",count =int (mask .sum ()),message ="ligand_efficiency is empty while standard_value exists")
-        return df
-    def _serialize_activity_properties (self ,value :Any ,log :BoundLogger |None =None )->str |None :
-        "Return normalized JSON for activity_properties or None if not serializable."
-        normalized_items =self ._normalize_activity_properties_items (value ,log )
-        if normalized_items is None :
-            return None
-        try :
-            return json .dumps (normalized_items ,ensure_ascii =False ,sort_keys =True )
-        except (TypeError ,ValueError )as exc :
-            if log is not None :
-                log .warning ("activity_properties_serialization_failed",error =str (exc ))
-            return None
-    def _normalize_activity_properties_items (self ,value :Any ,log :BoundLogger |None =None )->list [dict [str ,Any ]]|None :
-        "Coerce activity_properties payloads into a list of constrained dictionaries."
-        if value is None :
-            return None
-        raw_value =value
-        if isinstance (value ,str ):
-            stripped =value .strip ()
-            if not stripped :
-                return []
-            try :
-                parsed =json .loads (stripped )
-            except (TypeError ,ValueError ):
-                fallback_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                fallback_base ["text_value"]=stripped
-                return [fallback_base ]
-            else :
-                value =parsed
-        if isinstance (value ,Mapping ):
-            items :list [Any ]=[value ]
-        elif isinstance (value ,Sequence )and (not isinstance (value ,(str ,bytes ))):
-            items =list (value )
-        else :
-            if log is not None :
-                log .warning ("activity_properties_unhandled_type",value_type =type (raw_value ).__name__ )
-            return None
-        normalized :list [dict [str ,Any ]]=[]
-        for item in items :
-            if item is None :
-                continue
-            if isinstance (item ,Mapping ):
-                item_mapping =cast (Mapping [str ,Any ],item )
-                normalized_item :dict [str ,Any |None ]={key :item_mapping .get (key )for key in ACTIVITY_PROPERTY_KEYS }
-                result_flag_value =normalized_item .get ("result_flag")
-                if isinstance (result_flag_value ,int )and result_flag_value in (0 ,1 ):
-                    normalized_item ["result_flag"]=bool (result_flag_value )
-                normalized .append (normalized_item )
-            elif isinstance (item ,str ):
-                str_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                str_base ["text_value"]=item
-                normalized .append (str_base )
-            elif log is not None :
-                log .warning ("activity_properties_item_unhandled",item_type =type (item ).__name__ )
-        return normalized
-    def _validate_activity_properties_truv (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0434\u043b\u044f activity_properties.\n\n        \u0412\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u0435\u0442 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n        - value IS NOT NULL \u21d2 text_value IS NULL (\u0438 \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442)\n        - relation IN ('=', '<', '\u2264', '>', '\u2265', '~') OR NULL\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        "
-        validated :list [dict [str ,Any ]]=[]
-        invalid_count =0
-        invalid_items :list [dict [str ,Any ]]=[]
-        for prop in properties :
-            is_valid =True
-            validation_errors :list [str ]=[]
-            value =prop .get ("value")
-            text_value =prop .get ("text_value")
-            relation =prop .get ("relation")
-            if value is not None and text_value is not None :
-                is_valid =False
-                validation_errors .append ("both value and text_value are not None")
-            elif value is None and text_value is None :
-                pass
-            if relation is not None :
-                if not isinstance (relation ,str ):
-                    is_valid =False
-                    validation_errors .append (f'relation is not a string: {type (relation ).__name__ }')
-                elif relation not in RELATIONS :
-                    is_valid =False
-                    validation_errors .append (f"relation '{relation }' not in allowed values: {RELATIONS }")
-            validated .append (prop )
-            if not is_valid :
-                invalid_count +=1
-                invalid_items .append (prop )
-                log .warning ("activity_property_truv_validation_failed",activity_id =activity_id ,property =prop ,errors =validation_errors ,message ="TRUV validation failed, but property is kept")
-        stats ={"invalid_count":invalid_count ,"valid_count":len (validated )}
-        return (validated ,stats )
-    def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-        seen :set [tuple [Any ,...]]=set ()
-        deduplicated :list [dict [str ,Any ]]=[]
-        duplicates_removed =0
-        for prop in properties :
-            dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-            if dedup_key not in seen :
-                seen .add (dedup_key )
-                deduplicated .append (prop )
-            else :
-                duplicates_removed +=1
-                log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-        stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-        return (deduplicated ,stats )
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_added",value ="activity")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_filled",value ="activity")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :BoundLogger )->pd .DataFrame :
-        "Convert data types according to the Pandera schema."
-        df =df .copy ()
-        non_nullable_int_fields ={"activity_id":"int64"}
-        nullable_int_fields ={"row_index":"Int64","target_tax_id":"int64","assay_tax_id":"int64","record_id":"int64","src_id":"int64"}
-        float_fields ={"standard_value":"float64","standard_upper_value":"float64","pchembl_value":"float64","upper_value":"float64","lower_value":"float64"}
-        bool_fields =["potential_duplicate","curated","removed"]
-        binary_flag_fields =["standard_flag"]
-        for field in non_nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_int :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_int .astype ("Int64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field =="row_index":
-                    numeric_series_row :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=numeric_series_row .astype ("Int64")
-                    if df [field ].isna ().any ():
-                        df [field ]=range (len (df ))
-                else :
-                    nullable_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=nullable_numeric_series .astype ("Int64")
-                    mask_valid =df [field ].notna ()
-                    if mask_valid .any ():
-                        invalid_mask =mask_valid &(df [field ]<1 )
-                        if invalid_mask .any ():
-                            log .warning ("invalid_positive_integer",field =field ,count =int (invalid_mask .sum ()))
-                            df .loc [invalid_mask ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in float_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_float :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_float .astype ("float64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in bool_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field in ("curated","removed")and df [field ].dtype =="boolean":
-                    continue
-                if field in ("curated","removed"):
-                    df [field ]=df [field ].astype ("boolean")
-                else :
-                    bool_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=(bool_numeric_series !=0 ).astype ("boolean")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("bool_conversion_failed",field =field ,error =str (exc ))
-        for field in binary_flag_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_flag :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_flag .astype ("Int64")
-                mask_valid =df [field ].notna ()
-                if mask_valid .any ():
-                    valid_values =df .loc [mask_valid ,field ]
-                    invalid_valid_mask =~valid_values .isin ([0 ,1 ])
-                    if invalid_valid_mask .any ():
-                        invalid_index =valid_values .index [invalid_valid_mask ]
-                        log .warning ("invalid_standard_flag",field =field ,count =int (invalid_valid_mask .sum ()))
-                        df .loc [invalid_index ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        object_fields =["value","activity_properties"]
-        for field in object_fields :
-            if field in df .columns :
-                if df [field ].dtype !="object":
-                    df [field ]=df [field ].astype ("object")
-        return df
-    def _validate_foreign_keys (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Validate foreign key integrity and format of ChEMBL IDs."
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        chembl_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        warnings :list [str ]=[]
-        for field in chembl_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-                if invalid_mask .any ():
-                    warning_msg :str =f'{field }: {int (invalid_mask .sum ())} invalid format(s)'
-                    warnings .append (warning_msg )
-        if warnings :
-            log .warning ("foreign_key_validation",warnings =warnings )
-        return df
-    def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check uniqueness of activity_id before validation."
-        if "activity_id"not in df .columns :
-            log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-            return
-        duplicates =df [df ["activity_id"].duplicated (keep =False )]
-        if not duplicates .empty :
-            duplicate_count =len (duplicates )
-            duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-            log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-            msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-            raise ValueError (msg )
-        log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
-    def _validate_data_validity_comment_soft_enum (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Soft enum \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0434\u043b\u044f data_validity_comment.\n\n        \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0442\u0438\u0432 whitelist \u0438\u0437 \u043a\u043e\u043d\u0444\u0438\u0433\u0430. \u041d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n        \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a warning, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e (soft enum).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        "
-        if df .empty or "data_validity_comment"not in df .columns :
-            return
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if not whitelist :
-            return
-        series_candidate =df ["data_validity_comment"].dropna ()
-        if len (series_candidate )==0 :
-            return
-        non_null_comments_series =series_candidate .astype ("string")
-        whitelist_set :set [str ]=set (whitelist )
-        def _is_unknown (value :str )->bool :
-            return value not in whitelist_set
-        unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-        unknown_count =int (unknown_mask .sum ())
-        if unknown_count >0 :
-            unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-            log .warning ("soft_enum_unknown_data_validity_comment",unknown_count =unknown_count ,total_count =len (non_null_comments_series ),samples =unknown_values ,whitelist =whitelist ,message ="Unknown data_validity_comment values detected (soft enum: not blocking)")
-    def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-        reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        errors :list [str ]=[]
-        for field in reference_fields :
-            if field not in df .columns :
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-                continue
-            mask =df [field ].notna ()
-            if not mask .any ():
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-                continue
-            invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-                errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-                log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-        if errors :
-            log .error ("foreign_key_integrity_check_failed",errors =errors )
-            msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-            raise ValueError (msg )
-        log .debug ("foreign_key_integrity_verified")
-    def _log_detailed_validation_errors (self ,failure_cases :pd .DataFrame ,payload :pd .DataFrame ,log :BoundLogger )->None :
-        "Log individual validation errors with row index and activity_id."
-        if failure_cases .empty or payload .empty :
-            return
-        activity_id_col ="activity_id"if "activity_id"in payload .columns else None
-        index_col ="index"if "index"in failure_cases .columns else None
-        if index_col is None :
-            return
-        max_errors =20
-        errors_to_log =failure_cases .head (max_errors )
-        for _ ,error_row in errors_to_log .iterrows ():
-            row_index =error_row .get (index_col )
-            if row_index is None :
-                continue
-            error_details :dict [str ,Any ]={"row_index":int (row_index )if isinstance (row_index ,(int ,float ))else str (row_index )}
-            if activity_id_col :
-                try :
-                    idx =int (row_index )if isinstance (row_index ,(int ,float ))else row_index
-                    activity_id_value :Any =payload .at [cast (int ,idx ),activity_id_col ]
-                    activity_id =activity_id_value
-                except (KeyError ,IndexError ):
-                    activity_id =None
-                if activity_id is not None and pd .notna (activity_id ):
-                    error_details ["activity_id"]=int (activity_id )if isinstance (activity_id ,(int ,float ))else str (activity_id )
-            if "column"in error_row and pd .notna (error_row ["column"]):
-                error_details ["column"]=str (error_row ["column"])
-            if "schema_context"in error_row and pd .notna (error_row ["schema_context"]):
-                error_details ["schema_context"]=str (error_row ["schema_context"])
-            if "failure_case"in error_row and pd .notna (error_row ["failure_case"]):
-                error_details ["failure_case"]=str (error_row ["failure_case"])
-            log .error ("validation_error_detail",**error_details )
-        if len (failure_cases )>max_errors :
-            log .warning ("validation_errors_truncated",total_errors =len (failure_cases ),logged_errors =max_errors )
-    def build_quality_report (self ,df :pd .DataFrame )->pd .DataFrame |dict [str ,object ]|None :
-        "Return QC report with activity-specific metrics including distributions."
-        business_key =["activity_id"]if "activity_id"in df .columns else None
-        base_report =build_default_quality_report (df ,business_key_fields =business_key )
-        rows :list [dict [str ,Any ]]=[]
-        if not base_report .empty :
-            records_raw =base_report .to_dict ("records")
-            records :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],records_raw )
-            for record in records :
-                rows .append ({str (k ):v for k ,v in record .items ()})
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        foreign_key_fields =["assay_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"]
-        for field in foreign_key_fields :
-            if field in df .columns :
-                mask =df [field ].notna ()
-                if mask .any ():
-                    string_series =df [field ].astype (str )
-                    valid_mask =mask &string_series .str .match (chembl_id_pattern .pattern ,na =False )
-                    invalid_count =int ((mask &~valid_mask ).astype (int ).sum ())
-                    valid_count =int (valid_mask .astype (int ).sum ())
-                    total_count =int (mask .astype (int ).sum ())
-                    integrity_ratio =float (valid_count /total_count )if total_count >0 else 0.0
-                    rows .append ({"section":"foreign_key","metric":"integrity_ratio","column":field ,"value":float (integrity_ratio ),"valid_count":int (valid_count ),"invalid_count":int (invalid_count ),"total_count":int (total_count )})
-        if "standard_type"in df .columns :
-            type_counts_series :pd .Series [Any ]=df ["standard_type"].value_counts ()
-            type_dist_raw =type_counts_series .to_dict ()
-            type_dist :dict [Any ,int ]=cast (dict [Any ,int ],type_dist_raw )
-            for type_value ,count in type_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_type_count","column":"standard_type","value":str (type_value )if type_value is not None else "null","count":int (count )})
-        if "standard_units"in df .columns :
-            unit_counts_series :pd .Series [Any ]=df ["standard_units"].value_counts ()
-            unit_dist_raw =unit_counts_series .to_dict ()
-            unit_dist :dict [Any ,int ]=cast (dict [Any ,int ],unit_dist_raw )
-            for unit_value ,count in unit_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_units_count","column":"standard_units","value":str (unit_value )if unit_value is not None else "null","count":int (count )})
-        return pd .DataFrame (rows )
-    def write (self ,df :pd .DataFrame ,output_path :Path ,*,extended :bool =False ,include_correlation :bool |None =None ,include_qc_metrics :bool |None =None )->RunResult :
-        "Override write() to bind actor and ensure deterministic sorting.\n\n        Parameters\n        ----------\n        df:\n            The DataFrame to write.\n        output_path:\n            The base output path for all artifacts.\n        extended:\n            Whether to include extended QC artifacts.\n\n        Returns\n        -------\n        RunResult:\n            All artifacts generated by the write operation.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.write')
-        UnifiedLogger .bind (actor =self .actor )
-        sort_keys =["assay_chembl_id","testitem_chembl_id","activity_id"]
-        if df .empty or not all ((key in df .columns for key in sort_keys )):
-            return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-        original_sort_by =self .config .determinism .sort .by
-        if not original_sort_by or original_sort_by !=sort_keys :
-            from copy import deepcopy
-            from bioetl .config .models .determinism import DeterminismSortingConfig
-            modified_config =deepcopy (self .config )
-            modified_config .determinism .sort =DeterminismSortingConfig (by =sort_keys ,ascending =[True ,True ,True ],na_position ="last")
-            log .debug ("write_sort_config_set",sort_keys =sort_keys ,original_sort_keys =list (original_sort_by )if original_sort_by else [])
-            original_config =self .config
-            self .config =modified_config
-            try :
-                result =super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-            finally :
-                self .config =original_config
-            return result
-        return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
```

#### Горячий участок 2

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:111-115
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,5 +0,0 @@

-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
```

#### Горячий участок 3

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2901-2924
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,18 +0,0 @@

-def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_added",value ="activity")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_filled",value ="activity")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Горячий участок 4

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:343-415
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,22 +0,0 @@

-def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-    "Construct the descriptor driving the shared extraction template."
-    def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-        http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-        chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-        typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =source_config .parameters .select_fields
-        return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-    def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-    def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        df =pipeline ._ensure_comment_fields (df ,log )
-        chembl_client =cast (ChemblClient ,context .chembl_client )
-        if chembl_client is not None :
-            df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-        pipeline ._log_validity_comments_metrics (df ,log )
-        return df
-    def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        return pipeline ._materialize_activity_record (payload )
-    return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
```

#### Горячий участок 5

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1257-1267
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,7 +0,0 @@

-def _cache_directory (self ,release :str |None )->Path :
-    cache_root =Path (self .config .paths .cache_root )
-    directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-    release_component =self ._sanitize_cache_component (release or "unknown")
-    pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-    version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-    return cache_root /directory_name /pipeline_component /release_component /version_component
```

#### Горячий участок 6

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1252-1255
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,4 +0,0 @@

-def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-    directory =self ._cache_directory (release )
-    cache_key =self ._cache_key (batch_ids ,release )
-    return directory /f'{cache_key }.json'
```

#### Горячий участок 7

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1269-1277
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,4 +0,0 @@

-def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-    payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-    raw =json .dumps (payload ,sort_keys =True )
-    return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
```

#### Горячий участок 8

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:3108-3131
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,13 +0,0 @@

-def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check uniqueness of activity_id before validation."
-    if "activity_id"not in df .columns :
-        log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-        return
-    duplicates =df [df ["activity_id"].duplicated (keep =False )]
-    if not duplicates .empty :
-        duplicate_count =len (duplicates )
-        duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-        log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-        msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-        raise ValueError (msg )
-    log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
```

#### Горячий участок 9

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1176-1221
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,33 +0,0 @@

-def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-    cache_config =self .config .cache
-    if not cache_config .enabled :
-        return None
-    normalized_ids =[str (identifier )for identifier in batch_ids ]
-    cache_file =self ._cache_file_path (normalized_ids ,release )
-    if not cache_file .exists ():
-        return None
-    try :
-        stat =cache_file .stat ()
-    except OSError :
-        return None
-    ttl_seconds =int (cache_config .ttl )
-    if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    try :
-        payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-    except (OSError ,json .JSONDecodeError ):
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    if not isinstance (payload ,dict ):
-        return None
-    missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-    if missing :
-        return None
-    return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
```

#### Горячий участок 10

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:3185-3232
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,24 +0,0 @@

-def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-    reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-    chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-    errors :list [str ]=[]
-    for field in reference_fields :
-        if field not in df .columns :
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-            continue
-        mask =df [field ].notna ()
-        if not mask .any ():
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-            continue
-        invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-        if invalid_mask .any ():
-            invalid_count =int (invalid_mask .sum ())
-            invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-            errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-            log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-    if errors :
-        log .error ("foreign_key_integrity_check_failed",errors =errors )
-        msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-        raise ValueError (msg )
-    log .debug ("foreign_key_integrity_verified")
```

#### Горячий участок 11

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:432-450
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,14 +0,0 @@

-def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-    "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-    if isinstance (dataset ,pd .Series ):
-        return dataset .to_frame (name ="activity_id")
-    if isinstance (dataset ,pd .DataFrame ):
-        return dataset
-    if isinstance (dataset ,Mapping ):
-        mapping =cast (Mapping [str ,Any ],dataset )
-        return pd .DataFrame ([dict (mapping )])
-    if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-        dataset_list :list [Any ]=list (dataset )
-        return pd .DataFrame ({"activity_id":dataset_list })
-    msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-    raise TypeError (msg )
```

#### Горячий участок 12

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2065-2068
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,5 +0,0 @@

-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Горячий участок 13

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:498-637
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,83 +0,0 @@

-def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-    "Iterate over IDs using the shared iterator while preserving cache semantics."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    records :list [dict [str ,Any ]]=[]
-    success_count =0
-    fallback_count =0
-    error_count =0
-    cache_hits =0
-    api_calls =0
-    total_batches =0
-    key_order =[key for _ ,key in normalized_ids ]
-    key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-    for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-        total_batches +=1
-        batch_start =time .perf_counter ()
-        from_cache =False
-        chunk_records :dict [str ,dict [str ,Any ]]={}
-        try :
-            cached_records =self ._check_cache (chunk ,self ._chembl_release )
-            if cached_records is not None :
-                from_cache =True
-                cache_hits +=len (chunk )
-                chunk_records =cached_records
-            else :
-                api_calls +=1
-                fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                for item in fetched_items :
-                    if not isinstance (item ,Mapping ):
-                        continue
-                    activity_value =item .get ("activity_id")
-                    if activity_value is None :
-                        continue
-                    chunk_records [str (activity_value )]=dict (item )
-                self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-            success_in_batch =0
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                record =chunk_records .get (key )
-                if record and (not record .get ("error")):
-                    materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                    records .append (materialized )
-                    success_count +=1
-                    success_in_batch +=1
-                else :
-                    fallback_record =self ._create_fallback_record (numeric_id )
-                    records .append (fallback_record )
-                    fallback_count +=1
-                    error_count +=1
-            batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-            log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-        except CircuitBreakerOpenError as exc :
-            log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except RequestException as exc :
-            log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except Exception as exc :
-            log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-    total_records =len (normalized_ids )
-    success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-    summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-    return (records ,summary )
```

#### Горячий участок 14

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1284-1323
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,16 +0,0 @@

-def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-    "Create fallback record enriched with error metadata."
-    base_message ="Fallback: ChEMBL activity unavailable"
-    message =f'{base_message } ({error })'if error else base_message
-    timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-    metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-    if isinstance (error ,RequestException ):
-        response =getattr (error ,"response",None )
-        status_code =getattr (response ,"status_code",None )
-        if status_code is not None :
-            metadata ["http_status"]=status_code
-        metadata ["error_message"]=str (error )
-    elif error is not None :
-        metadata ["error_message"]=str (error )
-    fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-    return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
```

#### Горячий участок 15

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2843-2899
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,15 +0,0 @@

-def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-    "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-    seen :set [tuple [Any ,...]]=set ()
-    deduplicated :list [dict [str ,Any ]]=[]
-    duplicates_removed =0
-    for prop in properties :
-        dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-        if dedup_key not in seen :
-            seen .add (dedup_key )
-            deduplicated .append (prop )
-        else :
-            duplicates_removed +=1
-            log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-    stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-    return (deduplicated ,stats )
```

#### Горячий участок 16

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:892-935
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,28 +0,0 @@

-def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    assay_section :Any =enrich_section .get ("assay")
-                    if isinstance (assay_section ,Mapping ):
-                        assay_section =cast (Mapping [str ,Any ],assay_section )
-                        enrich_cfg =dict (assay_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_assay (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 17

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:822-867
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,28 +0,0 @@

-def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    compound_record_section :Any =enrich_section .get ("compound_record")
-                    if isinstance (compound_record_section ,Mapping ):
-                        compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                        enrich_cfg =dict (compound_record_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 18

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:960-1014
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,32 +0,0 @@

-def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    data_validity_section :Any =enrich_section .get ("data_validity")
-                    if isinstance (data_validity_section ,Mapping ):
-                        data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                        enrich_cfg =dict (data_validity_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    if "data_validity_description"in df .columns :
-        non_na_count =int (df ["data_validity_description"].notna ().sum ())
-        if non_na_count >0 :
-            log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 19

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1039-1108
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,37 +0,0 @@

-def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    molecule_section :Any =enrich_section .get ("molecule")
-                    if isinstance (molecule_section ,Mapping ):
-                        molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                        enrich_cfg =dict (molecule_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-    if "molecule_name"in df_join .columns :
-        if "molecule_pref_name"not in df .columns :
-            df ["molecule_pref_name"]=pd .NA
-        mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-        if mask .any ():
-            df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-            df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-            log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-    return df
```

#### Горячий участок 20

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1325-1358
- document: нет в ветке

```diff
--- activity:run.py

+++ document:run.py

@@ -1,11 +0,0 @@

-def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-    if df .empty :
-        return df
-    required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-    missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-    if missing_fields :
-        for field in missing_fields :
-            df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-        log .debug ("comment_fields_ensured",fields =missing_fields )
-    return df
```

### Модуль normalize.py

Определение                 | activity сигнатура                                                           | document сигнатура                                                  | Побочные эффекты                                                                                            | Исключения                | Статус           
----------------------------|------------------------------------------------------------------------------|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|---------------------------|------------------
__module_block_0            | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | отличается       
__module_block_1            | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | совпадает        
__module_block_10           | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | отличается       
__module_block_11           | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | отличается       
__module_block_12           | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {}                                                            | activity: []
document: [] | только в activity
__module_block_13           | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | отличается       
__module_block_14           | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {}                                                            | activity: []
document: [] | только в activity
__module_block_15           | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {}                                                            | activity: []
document: [] | только в activity
__module_block_16           | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {}                                                            | activity: []
document: [] | только в activity
__module_block_2            | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | совпадает        
__module_block_3            | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | совпадает        
__module_block_4            | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | совпадает        
__module_block_5            | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | отличается       
__module_block_6            | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | отличается       
__module_block_7            | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | отличается       
__module_block_8            | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | совпадает        
__module_block_9            | —                                                                            | —                                                                   | activity: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | activity: []
document: [] | отличается       
_enrich_by_pairs            | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any], log: Any | —                                                                   | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
document: {}                      | activity: []
document: [] | только в activity
_enrich_by_record_id        | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any], log: Any | —                                                                   | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
document: {}                      | activity: []
document: [] | только в activity
_escape_pipe                | —                                                                            | value: str | Any                                                    | activity: {}
document: {'logging': [], 'io': []}                                                            | activity: []
document: [] | только в document
_extract_first_present      | record: Mapping[str, Any], keys: Iterable[str]                               | —                                                                   | activity: {'logging': [], 'io': []}
document: {}                                                            | activity: []
document: [] | только в activity
_is_numeric                 | —                                                                            | value: Any                                                          | activity: {}
document: {'logging': [], 'io': []}                                                            | activity: []
document: [] | только в document
aggregate_terms             | —                                                                            | rows: Iterable[dict[str, Any]], sort: str                           | activity: {}
document: {'logging': [], 'io': []}                                                            | activity: []
document: [] | только в document
enrich_with_assay           | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                                                                   | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
document: {} | activity: []
document: [] | только в activity
enrich_with_compound_record | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                                                                   | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
document: {} | activity: []
document: [] | только в activity
enrich_with_data_validity   | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                                                                   | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
document: {} | activity: []
document: [] | только в activity
enrich_with_document_terms  | —                                                                            | df_docs: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | activity: {}
document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []} | activity: []
document: [] | только в document

_Показаны первые 20 горячих участков из 22._

#### Горячий участок 1

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:1-1
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:1-1

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-"Enrichment functions for Activity pipeline."
+"Enrichment functions for Document pipeline."
```

#### Горячий участок 2

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:9-9
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:16-16

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-import pandas as pd
+__all__ =["enrich_with_document_terms","aggregate_terms","_escape_pipe"]
```

#### Горячий участок 3

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:21-21
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:19-19

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-__all__ =["enrich_with_assay","enrich_with_compound_record","enrich_with_data_validity"]
+_ensure_columns =ensure_columns
```

#### Горячий участок 4

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:24-24
- document: нет в ветке

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +0,0 @@

-_ensure_columns =ensure_columns
```

#### Горячий участок 5

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:27-35
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:46-49

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-_COMPOUND_FIELD_ALIASES :dict [str ,tuple [str ,...]]={"compound_name":("compound_name","pref_name","PREF_NAME"),"compound_key":("compound_key","standard_inchi_key","STANDARD_INCHI_KEY"),"curated":("curated","CURATED")}
+_DOCUMENT_TERM_COLUMNS :tuple [tuple [str ,str ],...]=(("term","string"),("weight","string"))
```

#### Горячий участок 6

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:37-40
- document: нет в ветке

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +0,0 @@

-_ASSAY_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_organism","string"),("assay_tax_id","Int64"))
```

#### Горячий участок 7

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:42-47
- document: нет в ветке

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +0,0 @@

-_COMPOUND_COLUMNS :tuple [tuple [str ,str ],...]=(("compound_name","string"),("compound_key","string"),("curated","boolean"),("removed","boolean"))
```

#### Горячий участок 8

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:49-49
- document: нет в ветке

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +0,0 @@

-_DATA_VALIDITY_COLUMNS :tuple [tuple [str ,str ],...]=(("data_validity_description","string"),)
```

#### Горячий участок 9

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:15-19
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:14-14

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-from bioetl .schemas .activity import ASSAY_ENRICHMENT_SCHEMA ,COMPOUND_RECORD_ENRICHMENT_SCHEMA ,DATA_VALIDITY_ENRICHMENT_SCHEMA
+from bioetl .schemas .document import DOCUMENT_TERMS_ENRICHMENT_SCHEMA
```

#### Горячий участок 10

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:5-5
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:5-5

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-from collections .abc import Iterable ,Mapping
+from collections import defaultdict
```

#### Горячий участок 11

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:10-10
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:6-6

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-from pandas import Series
+from collections .abc import Iterable ,Mapping
```

#### Горячий участок 12

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:8-8
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:9-9

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-import numpy as np
+import pandas as pd
```

#### Горячий участок 13

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:439-615
- document: нет в ветке

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1,65 +0,0 @@

-def _enrich_by_pairs (df_act :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ],log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame activity \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u044b (molecule_chembl_id, document_chembl_id)."
-    pairs_df =df_act [["molecule_chembl_id","document_chembl_id"]].astype ("string").copy ()
-    for column in pairs_df .columns :
-        pairs_df [column ]=pairs_df [column ].str .strip ().str .upper ()
-    pairs_df =pairs_df .dropna ()
-    pairs_df =pairs_df .drop_duplicates ()
-    pairs :set [tuple [str ,str ]]=set (map (tuple ,pairs_df .to_numpy ()))
-    if not pairs :
-        log .debug ("enrichment_by_pairs_skipped_no_valid_pairs")
-        return df_act
-    fields =cfg .get ("fields",["molecule_chembl_id","document_chembl_id","compound_name","compound_key","curated"])
-    page_limit =cfg .get ("page_limit",1000 )
-    log .info ("enrichment_fetching_compound_records_by_pairs",pairs_count =len (pairs ))
-    compound_records_dict :dict [tuple [str ,str ],dict [str ,Any ]]={}
-    try :
-        compound_records_dict =client .fetch_compound_records_by_pairs (pairs =pairs ,fields =list (fields ),page_limit =page_limit )or {}
-    except Exception as exc :
-        log .warning ("enrichment_fetch_error_by_pairs",pairs_count =len (pairs ),error =str (exc ),exc_info =True )
-        return df_act
-    enrichment_data :list [dict [str ,Any ]]=[]
-    pairs_found =0
-    pairs_not_found =0
-    for pair in pairs :
-        compound_record :dict [str ,Any ]|None =compound_records_dict .get (pair )
-        if compound_record :
-            record_mapping :Mapping [str ,Any ]=compound_record
-            compound_name_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("compound_name",("compound_name",)))
-            compound_key_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("compound_key",("compound_key",)))
-            curated_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("curated",("curated",)))
-            compound_name =None
-            if compound_name_raw is not None :
-                name_str =str (compound_name_raw ).strip ()
-                compound_name =name_str if name_str else None
-            compound_key =None
-            if compound_key_raw is not None :
-                key_str =str (compound_key_raw ).strip ()
-                compound_key =key_str if key_str else None
-            curated =curated_raw if curated_raw is not None else None
-            pairs_found +=1
-            enrichment_data .append ({"molecule_chembl_id":pair [0 ],"document_chembl_id":pair [1 ],"compound_name":compound_name ,"compound_key":compound_key ,"curated":curated ,"removed":None })
-        else :
-            pairs_not_found +=1
-            enrichment_data .append ({"molecule_chembl_id":pair [0 ],"document_chembl_id":pair [1 ],"compound_name":None ,"compound_key":None ,"curated":None ,"removed":None })
-    log .info ("enrichment_by_pairs_complete",pairs_requested =len (pairs ),pairs_found =pairs_found ,pairs_not_found =pairs_not_found ,records_returned =len (compound_records_dict ))
-    if pairs_not_found >0 :
-        log .warning ("enrichment_by_pairs_some_pairs_not_found",pairs_not_found =pairs_not_found ,pairs_total =len (pairs ),hint ="\u041f\u0440\u043e\u0432\u0435\u0440\u044c\u0442\u0435, \u0447\u0442\u043e \u043f\u0430\u0440\u044b (molecule_chembl_id, document_chembl_id) \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 ChEMBL API")
-    if not enrichment_data :
-        log .debug ("enrichment_by_pairs_no_records_found")
-        return df_act
-    df_enrich =pd .DataFrame (enrichment_data )
-    df_enrich ["molecule_chembl_id"]=df_enrich ["molecule_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_enrich ["document_chembl_id"]=df_enrich ["document_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_act ["molecule_chembl_id_normalized"]=df_act ["molecule_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_act ["document_chembl_id_normalized"]=df_act ["document_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_result =df_act .merge (df_enrich ,left_on =["molecule_chembl_id_normalized","document_chembl_id_normalized"],right_on =["molecule_chembl_id","document_chembl_id"],how ="left",suffixes =("","_enrich"))
-    df_result =df_result .drop (columns =["molecule_chembl_id_normalized","document_chembl_id_normalized"])
-    for col in ["compound_name","compound_key","curated"]:
-        if f'{col }_enrich'in df_result .columns :
-            if col not in df_result .columns :
-                df_result [col ]=df_result [f'{col }_enrich']
-            else :
-                df_result [col ]=df_result [col ].where (df_result [col ].notna (),df_result [f'{col }_enrich'])
-            df_result =df_result .drop (columns =[f'{col }_enrich'])
-    return df_result
```

#### Горячий участок 14

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:618-755
- document: нет в ветке

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1,67 +0,0 @@

-def _enrich_by_record_id (df_act :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ],log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame activity \u0447\u0435\u0440\u0435\u0437 record_id (fallback \u0434\u043b\u044f \u0441\u0442\u0440\u043e\u043a \u0431\u0435\u0437 document_chembl_id)."
-    record_ids :set [str ]=set ()
-    for _ ,row in df_act .iterrows ():
-        rec =row .get ("record_id")
-        if rec is not None and (not pd .isna (rec )):
-            rec_s =str (rec ).strip ()
-            if rec_s :
-                record_ids .add (rec_s )
-    if not record_ids :
-        log .debug ("enrichment_by_record_id_skipped_no_valid_ids")
-        return df_act
-    fields =["record_id","compound_name","compound_key"]
-    page_limit =cfg .get ("page_limit",1000 )
-    batch_size =int (cfg .get ("batch_size",100 ))or 100
-    log .info ("enrichment_fetching_compound_records_by_record_id",record_ids_count =len (record_ids ))
-    compound_records_dict :dict [str ,dict [str ,Any ]]={}
-    try :
-        unique_ids =list (record_ids )
-        all_records :list [dict [str ,Any ]]=[]
-        for i in range (0 ,len (unique_ids ),batch_size ):
-            chunk =unique_ids [i :i +batch_size ]
-            params :dict [str ,Any ]={"record_id__in":",".join (chunk ),"limit":page_limit ,"only":",".join (fields ),"order_by":"record_id"}
-            try :
-                for record in client .paginate ("/compound_record.json",params =params ,page_size =page_limit ,items_key ="compound_records"):
-                    all_records .append (dict (record ))
-            except Exception as exc :
-                log .warning ("enrichment_fetch_error_by_record_id",chunk_size =len (chunk ),error =str (exc ),exc_info =True )
-        for record in all_records :
-            rid_raw =record .get ("record_id")
-            if rid_raw is None :
-                continue
-            rid_str =str (rid_raw ).strip ()
-            if rid_str and rid_str not in compound_records_dict :
-                compound_records_dict [rid_str ]={"record_id":rid_str ,"compound_key":record .get ("compound_key"),"compound_name":record .get ("compound_name")}
-    except Exception as exc :
-        log .warning ("enrichment_fetch_error_by_record_id",record_ids_count =len (record_ids ),error =str (exc ),exc_info =True )
-        return df_act
-    if not compound_records_dict :
-        log .debug ("enrichment_by_record_id_no_records_found")
-        return df_act
-    compound_data :list [dict [str ,Any ]]=[]
-    for record_id ,record in compound_records_dict .items ():
-        compound_data .append ({"record_id":record_id ,"compound_key":record .get ("compound_key"),"compound_name":record .get ("compound_name")})
-    df_compound =pd .DataFrame (compound_data )if compound_data else pd .DataFrame (columns =["record_id","compound_key","compound_name"])
-    df_act_normalized =df_act .copy ()
-    if "record_id"in df_act_normalized .columns :
-        mask_na =df_act_normalized ["record_id"].isna ()
-        df_act_normalized ["record_id"]=df_act_normalized ["record_id"].astype (str )
-        df_act_normalized .loc [df_act_normalized ["record_id"]=="nan","record_id"]=pd .NA
-        df_act_normalized .loc [mask_na ,"record_id"]=pd .NA
-        if "record_id"in df_compound .columns and (not df_compound .empty ):
-            df_compound ["record_id"]=df_compound ["record_id"].astype (str )
-            df_compound .loc [df_compound ["record_id"]=="nan","record_id"]=pd .NA
-    df_result =df_act_normalized .merge (df_compound ,on =["record_id"],how ="left",suffixes =("","_compound"))
-    for col in ["compound_name","compound_key"]:
-        if f'{col }_compound'in df_result .columns :
-            if col not in df_result .columns :
-                df_result [col ]=df_result [f'{col }_compound']
-            else :
-                df_result [col ]=df_result [col ].where (df_result [col ].notna (),df_result [f'{col }_compound'])
-            df_result =df_result .drop (columns =[f'{col }_compound'])
-    if "curated"not in df_result .columns :
-        df_result ["curated"]=pd .NA
-    if "removed"not in df_result .columns :
-        df_result ["removed"]=pd .NA
-    return df_result
```

#### Горячий участок 15

- activity: нет в ветке
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:22-43

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -0,0 +1,8 @@

+def _escape_pipe (value :str |Any )->str :
+    "Escape pipe and backslash delimiters in string values.\n\n    Parameters\n    ----------\n    value:\n        Input value to escape. ``None`` \u0438 NA \u043f\u0440\u0438\u0432\u043e\u0434\u044f\u0442\u0441\u044f \u043a \u043f\u0443\u0441\u0442\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435.\n\n    Returns\n    -------\n    str:\n        \u0421\u0442\u0440\u043e\u043a\u0430 \u0441 \u044d\u043a\u0440\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u0435\u043b\u044f\u043c\u0438: ``|`` \u2192 ``\\|``, ``\\`` \u2192 ``\\\\``.\n    "
+    if value is None or pd .isna (value ):
+        return ""
+    text =str (value )
+    if not text :
+        return ""
+    return text .replace ("\\","\\\\").replace ("|","\\|")
```

#### Горячий участок 16

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:52-64
- document: нет в ветке

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1,11 +0,0 @@

-def _extract_first_present (record :Mapping [str ,Any ],keys :Iterable [str ])->Any :
-    "\u0412\u043e\u0437\u0432\u0440\u0430\u0442\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u043e \u043f\u0435\u0440\u0432\u043e\u043c\u0443 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u043c\u0443 \u0430\u043b\u0438\u0430\u0441\u0443."
-    for key in keys :
-        if key in record :
-            return record [key ]
-    lowered_map ={str (k ).lower ():v for k ,v in record .items ()}
-    for key in keys :
-        candidate =str (key ).lower ()
-        if candidate in lowered_map :
-            return lowered_map [candidate ]
-    return None
```

#### Горячий участок 17

- activity: нет в ветке
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:122-128

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -0,0 +1,7 @@

+def _is_numeric (value :Any )->bool :
+    "Check if value can be converted to float."
+    try :
+        float (value )
+        return True
+    except (ValueError ,TypeError ):
+        return False
```

#### Горячий участок 18

- activity: нет в ветке
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:52-119

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -0,0 +1,26 @@

+def aggregate_terms (rows :Iterable [dict [str ,Any ]],sort :str ="weight_desc")->dict [str ,dict [str ,str ]]:
+    "Aggregate document terms by document_chembl_id.\n\n    Parameters\n    ----------\n    rows:\n        Iterable of document_term records, each with 'document_chembl_id', 'term', 'weight'.\n    sort:\n        Sort order: 'weight_desc' (default) sorts by weight descending, None preserves order.\n\n    Returns\n    -------\n    dict[str, dict[str, str]]:\n        Dictionary keyed by document_chembl_id -> {'term': 't1|t2|...', 'weight': 'w1|w2|...'}.\n        Terms and weights are serialized with \"|\" separator, order is synchronized.\n    "
+    bucket :dict [str ,list [tuple [str ,Any ]]]=defaultdict (list )
+    for r in rows :
+        did =r .get ("document_chembl_id")
+        if not did :
+            continue
+        term_value =r .get ("term")
+        weight_value =r .get ("weight")
+        term_str =str (term_value )if term_value is not None else ""
+        bucket [did ].append ((term_str ,weight_value ))
+    result :dict [str ,dict [str ,str ]]={}
+    for did ,items in bucket .items ():
+        if sort =="weight_desc":
+            items .sort (key =lambda x :float (x [1 ])if x [1 ]not in (None ,"")and _is_numeric (x [1 ])else float ("-inf"),reverse =True )
+        terms_list :list [str ]=[]
+        weights_list :list [str ]=[]
+        for term ,weight in items :
+            escaped_term =_escape_pipe (term or "")
+            terms_list .append (escaped_term )
+            if weight in (None ,""):
+                weights_list .append ("")
+            else :
+                weights_list .append (str (weight ))
+        result [did ]={"term":"|".join (terms_list ),"weight":"|".join (weights_list )}
+    return result
```

#### Горячий участок 19

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:67-184
- document: нет в ветке

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1,44 +0,0 @@

-def enrich_with_assay (df_act :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame activity \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 /assay (ChEMBL v2).\n\n    \u0422\u0440\u0435\u0431\u0443\u0435\u043c\u044b\u0435 \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438: assay_chembl_id.\n    \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442:\n      - assay_organism : pandas.StringDtype (nullable)\n      - assay_tax_id   : pandas.Int64Dtype  (nullable)\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="activity_enrichment")
-    df_act =_ensure_columns (df_act ,_ASSAY_COLUMNS )
-    if df_act .empty :
-        log .debug ("enrichment_skipped_empty_dataframe")
-        return ASSAY_ENRICHMENT_SCHEMA .validate (df_act ,lazy =True )
-    if "assay_chembl_id"not in df_act .columns :
-        log .warning ("enrichment_skipped_missing_columns",missing_columns =["assay_chembl_id"])
-        return ASSAY_ENRICHMENT_SCHEMA .validate (df_act ,lazy =True )
-    assay_ids =df_act ["assay_chembl_id"].dropna ().astype (str ).str .strip ()
-    assay_ids =assay_ids [assay_ids .ne ("")].unique ().tolist ()
-    if not assay_ids :
-        log .debug ("enrichment_skipped_no_valid_ids")
-        return ASSAY_ENRICHMENT_SCHEMA .validate (df_act ,lazy =True )
-    fields_cfg =cfg .get ("fields",["assay_chembl_id","assay_organism","assay_tax_id"])
-    required_fields ={"assay_chembl_id","assay_organism","assay_tax_id"}
-    fields =list (dict .fromkeys (list (fields_cfg )+list (required_fields )))
-    page_limit =int (cfg .get ("page_limit",1000 ))
-    log .info ("enrichment_fetching_assays",ids_count =len (assay_ids ))
-    records_by_id :dict [str ,dict [str ,Any ]]=client .fetch_assays_by_ids (ids =assay_ids ,fields =fields ,page_limit =page_limit )or {}
-    if not records_by_id :
-        log .debug ("enrichment_no_records_found")
-        return ASSAY_ENRICHMENT_SCHEMA .validate (df_act ,lazy =True )
-    enrichment_rows :list [dict [str ,Any ]]=[]
-    for assay_id ,rec in records_by_id .items ():
-        enrichment_rows .append ({"assay_chembl_id":assay_id ,"assay_organism":rec .get ("assay_organism"),"assay_tax_id":rec .get ("assay_tax_id")})
-    df_enrich =pd .DataFrame (enrichment_rows )
-    original_index =df_act .index
-    df_merged =df_act .merge (df_enrich ,on ="assay_chembl_id",how ="left",sort =False ,suffixes =("","_enrich")).reindex (original_index )
-    if "assay_organism_enrich"in df_merged .columns :
-        df_merged ["assay_organism"]=df_merged ["assay_organism_enrich"].combine_first (df_merged ["assay_organism"])
-        df_merged =df_merged .drop (columns =["assay_organism_enrich"])
-    if "assay_tax_id_enrich"in df_merged .columns :
-        df_merged ["assay_tax_id"]=df_merged ["assay_tax_id_enrich"].combine_first (df_merged ["assay_tax_id"])
-        df_merged =df_merged .drop (columns =["assay_tax_id_enrich"])
-    if "assay_organism"not in df_merged .columns :
-        df_merged ["assay_organism"]=pd .NA
-    if "assay_tax_id"not in df_merged .columns :
-        df_merged ["assay_tax_id"]=pd .NA
-    df_merged ["assay_organism"]=df_merged ["assay_organism"].astype ("string")
-    df_merged ["assay_tax_id"]=pd .to_numeric (df_merged ["assay_tax_id"],errors ="coerce").astype ("Int64")
-    log .info ("enrichment_completed",rows_enriched =int (df_merged .shape [0 ]),records_matched =int (len (records_by_id )))
-    return ASSAY_ENRICHMENT_SCHEMA .validate (df_merged ,lazy =True )
```

#### Горячий участок 20

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:187-436
- document: нет в ветке

```diff
--- activity:normalize.py

+++ document:normalize.py

@@ -1,125 +0,0 @@

-def enrich_with_compound_record (df_act :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame activity \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record.\n\n    Parameters\n    ----------\n    df_act:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c molecule_chembl_id.\n        \u0414\u043b\u044f \u0441\u0442\u0440\u043e\u043a \u0441 document_chembl_id \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u043f\u0443\u0442\u044c \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u044b (molecule_chembl_id, document_chembl_id).\n        \u0414\u043b\u044f \u0441\u0442\u0440\u043e\u043a \u0431\u0435\u0437 document_chembl_id \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f fallback \u0447\u0435\u0440\u0435\u0437 record_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.activity.enrich.compound_record.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438:\n        - compound_name (nullable string) - \u0438\u0437 compound_record.compound_name\n        - compound_key (nullable string) - \u0438\u0437 compound_record.compound_key\n        - curated (nullable bool) - \u0438\u0437 compound_record.curated\n        - removed (nullable bool) - \u0432\u0441\u0435\u0433\u0434\u0430 NULL (\u043d\u0435 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u0438\u0437 ChEMBL)\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="activity_enrichment")
-    df_act =_ensure_columns (df_act ,_COMPOUND_COLUMNS )
-    if df_act .empty :
-        log .debug ("enrichment_skipped_empty_dataframe")
-        return COMPOUND_RECORD_ENRICHMENT_SCHEMA .validate (df_act ,lazy =True )
-    if "molecule_chembl_id"not in df_act .columns :
-        log .warning ("enrichment_skipped_missing_columns",missing_columns =["molecule_chembl_id"])
-        return COMPOUND_RECORD_ENRICHMENT_SCHEMA .validate (df_act ,lazy =True )
-    df_act =df_act .copy ()
-    df_act ["_row_id"]=np .arange (len (df_act ))
-    has_doc_id ="document_chembl_id"in df_act .columns
-    if has_doc_id :
-        mask_with_doc =df_act ["document_chembl_id"].notna ()&(df_act ["document_chembl_id"].astype ("string").str .strip ()!="")
-        df_with_doc =df_act [mask_with_doc ].copy ()
-        df_without_doc =df_act [~mask_with_doc ].copy ()
-    else :
-        df_with_doc =pd .DataFrame ()
-        df_without_doc =df_act .copy ()
-    enrichment_by_pairs :pd .DataFrame |None =None
-    if not df_with_doc .empty :
-        enrichment_by_pairs =_enrich_by_pairs (df_with_doc ,client ,cfg ,log )
-    df_need_fallback =df_without_doc .copy ()
-    if enrichment_by_pairs is not None and (not enrichment_by_pairs .empty ):
-        if {"compound_name","compound_key"}.issubset (enrichment_by_pairs .columns ):
-            mask_empty =(enrichment_by_pairs ["compound_name"].isna ()|(enrichment_by_pairs ["compound_name"].astype ("string").str .strip ()==""))&(enrichment_by_pairs ["compound_key"].isna ()|(enrichment_by_pairs ["compound_key"].astype ("string").str .strip ()==""))
-        else :
-            mask_empty =pd .Series (False ,index =enrichment_by_pairs .index )
-        rows_need_fallback =enrichment_by_pairs [mask_empty ].copy ()
-        if not rows_need_fallback .empty and "record_id"in rows_need_fallback .columns :
-            df_need_fallback =pd .concat ([df_need_fallback ,rows_need_fallback ],ignore_index =True )
-    enrichment_by_record_id :pd .DataFrame |None =None
-    if not df_need_fallback .empty and "record_id"in df_need_fallback .columns :
-        df_need_fallback =df_need_fallback .drop_duplicates (subset =["_row_id"],keep ="first")
-        enrichment_by_record_id =_enrich_by_record_id (df_need_fallback ,client ,cfg ,log )
-    df_result =df_act .copy ()
-    if enrichment_by_pairs is not None and (not enrichment_by_pairs .empty ):
-        pairs_dict :dict [int ,dict [str ,Any ]]={}
-        for _ ,row in enrichment_by_pairs .iterrows ():
-            row_id_raw =row .get ("_row_id")
-            if not pd .isna (row_id_raw ):
-                try :
-                    if isinstance (row_id_raw ,(int ,float )):
-                        row_id =int (row_id_raw )
-                    else :
-                        row_id =int (str (row_id_raw ))
-                    pairs_dict [row_id ]={"compound_name":row .get ("compound_name"),"compound_key":row .get ("compound_key"),"curated":row .get ("curated")}
-                except (ValueError ,TypeError ):
-                    continue
-        if "_row_id"in df_result .columns :
-            for idx in df_result .index :
-                row_id_raw =df_result .loc [idx ,"_row_id"]
-                if not pd .isna (row_id_raw ):
-                    try :
-                        if isinstance (row_id_raw ,(int ,float )):
-                            row_id =int (row_id_raw )
-                        else :
-                            row_id =int (str (row_id_raw ))
-                        if row_id in pairs_dict :
-                            pairs_data =pairs_dict [row_id ]
-                            if pairs_data .get ("compound_name")is not None :
-                                df_result .loc [idx ,"compound_name"]=pairs_data ["compound_name"]
-                            if pairs_data .get ("compound_key")is not None :
-                                df_result .loc [idx ,"compound_key"]=pairs_data ["compound_key"]
-                            if pairs_data .get ("curated")is not None :
-                                df_result .loc [idx ,"curated"]=pairs_data ["curated"]
-                    except (ValueError ,TypeError ):
-                        continue
-    if "_row_id"in df_result .columns :
-        df_result =df_result .sort_values ("_row_id").reset_index (drop =True )
-    if enrichment_by_record_id is not None and (not enrichment_by_record_id .empty ):
-        fallback_dict :dict [int ,dict [str ,Any ]]={}
-        for _ ,row in enrichment_by_record_id .iterrows ():
-            row_id_raw =row .get ("_row_id")
-            if not pd .isna (row_id_raw ):
-                try :
-                    if isinstance (row_id_raw ,(int ,float )):
-                        row_id =int (row_id_raw )
-                    else :
-                        row_id =int (str (row_id_raw ))
-                    fallback_dict [row_id ]={"compound_name":row .get ("compound_name"),"compound_key":row .get ("compound_key")}
-                except (ValueError ,TypeError ):
-                    continue
-        if "_row_id"in df_result .columns :
-            for idx in df_result .index :
-                row_id_raw =df_result .loc [idx ,"_row_id"]
-                if not pd .isna (row_id_raw ):
-                    try :
-                        if isinstance (row_id_raw ,(int ,float )):
-                            row_id =int (row_id_raw )
-                        else :
-                            row_id =int (str (row_id_raw ))
-                        if row_id in fallback_dict :
-                            compound_name =df_result .loc [idx ,"compound_name"]if "compound_name"in df_result .columns else pd .NA
-                            compound_key =df_result .loc [idx ,"compound_key"]if "compound_key"in df_result .columns else pd .NA
-                            name_empty =pd .isna (compound_name )or str (compound_name ).strip ()==""
-                            key_empty =pd .isna (compound_key )or str (compound_key ).strip ()==""
-                            if name_empty or key_empty :
-                                fallback_data =fallback_dict [row_id ]
-                                if name_empty and fallback_data .get ("compound_name")is not None :
-                                    df_result .loc [idx ,"compound_name"]=fallback_data ["compound_name"]
-                                if key_empty and fallback_data .get ("compound_key")is not None :
-                                    df_result .loc [idx ,"compound_key"]=fallback_data ["compound_key"]
-                    except (ValueError ,TypeError ):
-                        continue
-    for col in ["compound_name","compound_key","curated","removed"]:
-        if col not in df_result .columns :
-            df_result [col ]=pd .NA
-    df_result ["removed"]=pd .NA
-    if "_row_id"in df_result .columns :
-        df_result =df_result .drop (columns =["_row_id"])
-    if "curated"in df_result .columns :
-        curated_mapping :Mapping [object ,bool ]={1 :True ,0 :False ,"1":True ,"0":False ,"true":True ,"false":False ,"True":True ,"False":False }
-        curated_series =df_result ["curated"]
-        normalized_curated =curated_series .map (curated_mapping )
-        mapped_mask =normalized_curated .notna ()
-        if mapped_mask .any ():
-            df_result .loc [mapped_mask ,"curated"]=normalized_curated [mapped_mask ]
-        df_result ["curated"]=df_result ["curated"].astype ("boolean")
-    df_result ["compound_name"]=df_result ["compound_name"].astype ("string")
-    df_result ["compound_key"]=df_result ["compound_key"].astype ("string")
-    df_result ["removed"]=df_result ["removed"].astype ("boolean")
-    log .info ("enrichment_completed",rows_enriched =df_result .shape [0 ],rows_with_doc_id =len (df_with_doc )if not df_with_doc .empty else 0 ,rows_without_doc_id =len (df_without_doc )if not df_without_doc .empty else 0 )
-    return COMPOUND_RECORD_ENRICHMENT_SCHEMA .validate (df_result ,lazy =True )
```

---

## Пара: activity ↔ target

- AST hash: 978d4152ad77000361c21b7f2051d3d1 ↔ 107171553e4f1c509bba122a3d0ccb96

- Jaccard по токенам: 0.176

### Модуль run.py

Определение                                                      | activity сигнатура                                                                                                                                                         | target сигнатура                                     | Побочные эффекты                                                                                                                                                                                                                                                                                                                                                                                              | Исключения                                                 | Статус           
-----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------|------------------
ChemblActivityPipeline                                           | —                                                                                                                                                                          | —                                                    | activity: {'logging': ['UnifiedLogger.bind', 'UnifiedLogger.get', 'bound_log.warning', 'errors_to_log.iterrows', 'log.debug', 'log.error', 'log.info', 'log.warning', 'pipeline._log_validity_comments_metrics', 'self._log_detailed_validation_errors', 'self._log_validity_comments_metrics'], 'io': ['cache_file.read_text', 'json.dumps', 'json.loads', 'payload.get', 'tmp_path.write_text']}
target: {} | activity: ['TypeError(msg)', 'ValueError(msg)']
target: [] | только в activity
ChemblActivityPipeline.__init__                                  | self, config: PipelineConfig, run_id: str                                                                                                                                  | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._add_row_metadata                         | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.debug'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._build_activity_descriptor                | self                                                                                                                                                                       | —                                                    | activity: {'logging': ['pipeline._log_validity_comments_metrics'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                       | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._cache_directory                          | self, release: str | None                                                                                                                                                  | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._cache_file_path                          | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._cache_key                                | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                    | activity: {'logging': [], 'io': ['json.dumps']}
target: {}                                                                                                                                                                                                                                                                                                                                                    | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._check_activity_id_uniqueness             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.debug', 'log.error', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                         | activity: ['ValueError(msg)']
target: []                   | только в activity
ChemblActivityPipeline._check_cache                              | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                    | activity: {'logging': [], 'io': ['cache_file.read_text', 'json.loads']}
target: {}                                                                                                                                                                                                                                                                                                                            | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._check_foreign_key_integrity              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.debug', 'log.error', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                         | activity: ['ValueError(msg)']
target: []                   | только в activity
ChemblActivityPipeline._coerce_activity_dataset                  | self, dataset: object                                                                                                                                                      | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: ['TypeError(msg)']
target: []                    | только в activity
ChemblActivityPipeline._coerce_mapping                           | payload: Any                                                                                                                                                               | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._collect_records_by_ids                   | self, normalized_ids: Sequence[tuple[int, str]], activity_iterator: ChemblActivityClient, *, select_fields: Sequence[str] | None                                           | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                    | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._create_fallback_record                   | self, activity_id: int, error: Exception | None                                                                                                                            | —                                                    | activity: {'logging': [], 'io': ['json.dumps']}
target: {}                                                                                                                                                                                                                                                                                                                                                    | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._deduplicate_activity_properties          | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                    | activity: {'logging': ['log.debug'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._enrich_assay                             | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                              | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._enrich_compound_record                   | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                              | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._enrich_data_validity                     | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                  | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._enrich_molecule                          | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                  | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._ensure_comment_fields                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.debug'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._extract_activity_properties_fields       | self, record: dict[str, Any]                                                                                                                                               | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.warning'], 'io': ['json.loads']}
target: {}                                                                                                                                                                                                                                                                                                     | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._extract_assay_fields                     | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                    | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                          | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._extract_chembl_release                   | payload: Mapping[str, Any]                                                                                                                                                 | —                                                    | activity: {'logging': [], 'io': ['payload.get']}
target: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._extract_data_validity_descriptions       | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                    | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                          | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._extract_from_chembl                      | self, dataset: object, chembl_client: ChemblClient | Any, activity_iterator: ChemblActivityClient, *, limit: int | None = None, select_fields: Sequence[str] | None = None | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'self._log_validity_comments_metrics'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                          | activity: ['ValueError(msg)']
target: []                   | только в activity
ChemblActivityPipeline._extract_nested_fields                    | self, record: dict[str, Any]                                                                                                                                               | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._extract_page_items                       | payload: Mapping[str, Any], items_keys: Sequence[str] | None                                                                                                               | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._filter_invalid_required_fields           | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._finalize_identifier_columns              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._finalize_output_columns                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._get_data_validity_comment_whitelist      | self                                                                                                                                                                       | —                                                    | activity: {'logging': ['UnifiedLogger.get'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                             | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._harmonize_identifier_columns             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.debug'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._log_detailed_validation_errors           | self, failure_cases: pd.DataFrame, payload: pd.DataFrame, log: BoundLogger                                                                                                 | —                                                    | activity: {'logging': ['errors_to_log.iterrows', 'log.error', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                            | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._log_validity_comments_metrics            | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.info', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._materialize_activity_record              | self, payload: Mapping[str, Any], *, activity_id: int | None = None                                                                                                        | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._next_link                                | payload: Mapping[str, Any], base_url: str                                                                                                                                  | —                                                    | activity: {'logging': [], 'io': ['payload.get']}
target: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._normalize_activity_ids                   | self, input_frame: pd.DataFrame, *, limit: int | None, log: BoundLogger                                                                                                    | —                                                    | activity: {'logging': ['log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._normalize_activity_properties_items      | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                    | activity: {'logging': ['log.warning'], 'io': ['json.loads']}
target: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._normalize_data_types                     | self, df: pd.DataFrame, schema: Any, log: BoundLogger                                                                                                                      | —                                                    | activity: {'logging': ['log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._normalize_identifiers                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.debug'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._normalize_measurements                   | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._normalize_nested_structures              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.warning'], 'io': ['json.dumps', 'json.loads']}
target: {}                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._normalize_string_fields                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._prepare_activity_iteration               | self, *, client_name: str = 'chembl_activity_client'                                                                                                                       | —                                                    | activity: {'logging': ['UnifiedLogger.get'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                             | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._sanitize_cache_component                 | value: str                                                                                                                                                                 | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._schema_column_specs                      | self                                                                                                                                                                       | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._serialize_activity_properties            | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                    | activity: {'logging': ['log.warning'], 'io': ['json.dumps']}
target: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._should_enrich_assay                      | self                                                                                                                                                                       | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._should_enrich_compound_record            | self                                                                                                                                                                       | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._should_enrich_data_validity              | self                                                                                                                                                                       | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._should_enrich_molecule                   | self                                                                                                                                                                       | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._store_cache                              | self, batch_ids: Sequence[str], batch_data: Mapping[str, Mapping[str, Any]], release: str | None                                                                           | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.debug'], 'io': ['json.dumps', 'tmp_path.write_text']}
target: {}                                                                                                                                                                                                                                                                                             | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._validate_activity_properties_truv        | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                    | activity: {'logging': ['log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._validate_data_validity_comment_soft_enum | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
target: []                                    | только в activity
ChemblActivityPipeline._validate_foreign_keys                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: {'logging': ['log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
target: []                                    | только в activity
ChemblActivityPipeline.build_quality_report                      | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline.chembl_release                            | self                                                                                                                                                                       | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline.extract                                   | self, *args, **kwargs                                                                                                                                                      | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'bound_log.warning'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                        | activity: []
target: []                                    | только в activity
ChemblActivityPipeline.extract_all                               | self                                                                                                                                                                       | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
ChemblActivityPipeline.extract_by_ids                            | self, ids: Sequence[str]                                                                                                                                                   | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning', 'self._log_validity_comments_metrics'], 'io': []}
target: {}                                                                                                                                                                                                                                                                           | activity: []
target: []                                    | только в activity
ChemblActivityPipeline.transform                                 | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                    | activity: []
target: []                                    | только в activity
ChemblActivityPipeline.validate                                  | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.info', 'self._log_detailed_validation_errors'], 'io': []}
target: {}                                                                                                                                                                                                                                                               | activity: ['ValueError(msg)']
target: []                   | только в activity
ChemblActivityPipeline.write                                     | self, df: pd.DataFrame, output_path: Path, *, extended: bool = False, include_correlation: bool | None = None, include_qc_metrics: bool | None = None                      | —                                                    | activity: {'logging': ['UnifiedLogger.bind', 'UnifiedLogger.get', 'log.debug'], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                          | activity: []
target: []                                    | только в activity
ChemblTargetPipeline                                             | —                                                                                                                                                                          | —                                                    | activity: {}
target: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | только в target  
ChemblTargetPipeline.__init__                                    | —                                                                                                                                                                          | self, config: PipelineConfig, run_id: str            | activity: {}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в target  
ChemblTargetPipeline._build_target_descriptor                    | —                                                                                                                                                                          | self                                                 | activity: {}
target: {'logging': ['log.info'], 'io': []}                                                                                                                                                                                                                                                                                                                                                      | activity: []
target: []                                    | только в target  
ChemblTargetPipeline._enrich_protein_classifications             | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                     | activity: {}
target: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}                                                                                                                                                                                                                                                                                                              | activity: []
target: []                                    | только в target  
ChemblTargetPipeline._enrich_target_components                   | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                     | activity: {}
target: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}                                                                                                                                                                                                                                                                                                              | activity: []
target: []                                    | только в target  
ChemblTargetPipeline._harmonize_identifier_columns               | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                     | activity: {}
target: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
target: []                                    | только в target  
ChemblTargetPipeline._normalize_data_types                       | —                                                                                                                                                                          | self, df: pd.DataFrame, schema: Any | None, log: Any | activity: {}
target: {'logging': ['log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                                                                   | activity: []
target: []                                    | только в target  
ChemblTargetPipeline._normalize_identifiers                      | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                     | activity: {}
target: {'logging': ['log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                                                                   | activity: []
target: []                                    | только в target  
ChemblTargetPipeline._normalize_string_fields                    | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                     | activity: {}
target: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
target: []                                    | только в target  
ChemblTargetPipeline.extract                                     | —                                                                                                                                                                          | self, *args, **kwargs                                | activity: {}
target: {'logging': ['UnifiedLogger.get'], 'io': []}                                                                                                                                                                                                                                                                                                                                             | activity: []
target: []                                    | только в target  
ChemblTargetPipeline.extract_all                                 | —                                                                                                                                                                          | self                                                 | activity: {}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в target  
ChemblTargetPipeline.extract_by_ids                              | —                                                                                                                                                                          | self, ids: Sequence[str]                             | activity: {}
target: {'logging': ['UnifiedLogger.get', 'log.info'], 'io': []}                                                                                                                                                                                                                                                                                                                                 | activity: []
target: []                                    | только в target  
ChemblTargetPipeline.transform                                   | —                                                                                                                                                                          | self, df: pd.DataFrame                               | activity: {}
target: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                                                                                                                                                                                                                                                                                                                    | activity: []
target: []                                    | только в target  
__module_block_0                                                 | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_1                                                 | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_10                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_11                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_12                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_13                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_14                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_15                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_16                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_17                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_18                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_19                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_2                                                 | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_20                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
__module_block_21                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
__module_block_22                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
__module_block_23                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
__module_block_24                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
__module_block_25                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
__module_block_26                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
__module_block_27                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
__module_block_28                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
__module_block_29                                                | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
target: []                                    | только в activity
__module_block_3                                                 | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_4                                                 | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_5                                                 | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_6                                                 | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_7                                                 | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_8                                                 | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       
__module_block_9                                                 | —                                                                                                                                                                          | —                                                    | activity: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
target: []                                    | отличается       

_Показаны первые 20 горячих участков из 106._

#### Горячий участок 1

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:106-3476
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,1684 +0,0 @@

-class ChemblActivityPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting activity records from the ChEMBL API."
-    actor ="activity_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-        self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch activity payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        def legacy_activity_ids (bound_log :BoundLogger )->Sequence [str ]|None :
-            payload_activity_ids =kwargs .get ("activity_ids")
-            if payload_activity_ids is None :
-                return None
-            bound_log .warning ("chembl_activity.deprecated_kwargs",message ="Using activity_ids in kwargs is deprecated. Use --input-file instead.")
-            if isinstance (payload_activity_ids ,Sequence )and (not isinstance (payload_activity_ids ,(str ,bytes ))):
-                sequence_ids :Sequence [str |int ]=cast (Sequence [str |int ],payload_activity_ids )
-                return [str (id_val )for id_val in sequence_ids ]
-            return [str (payload_activity_ids )]
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_activity.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="activity_id",legacy_id_resolver =legacy_activity_ids ,legacy_source ="deprecated_kwargs")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all activity records from ChEMBL using the shared iterator."
-        return self .run_extract_all (self ._build_activity_descriptor ())
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract activity records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of activity_id values to extract (as strings or integers).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted activity records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_config ,chembl_client ,activity_iterator ,select_fields =self ._prepare_activity_iteration ()
-        limit =self .config .cli .limit
-        invalid_ids :list [Any ]=[]
-        def normalize_activity_id (raw :Any )->tuple [str |None ,Any ]:
-            if pd .isna (raw ):
-                return (None ,None )
-            try :
-                if isinstance (raw ,str ):
-                    candidate =raw .strip ()
-                    if not candidate :
-                        return (None ,None )
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw ,(int ,float )):
-                    numeric_id =int (raw )
-                else :
-                    numeric_id =int (raw )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw )
-                return (None ,None )
-            return (str (numeric_id ),int (numeric_id ))
-        def delegated_fetch (canonical_ids :Sequence [str ],context :BatchExtractionContext )->tuple [Sequence [Mapping [str ,Any ]],Mapping [str ,Any ]]:
-            numeric_map =context .metadata
-            normalized_ids :list [tuple [int ,str ]]=[]
-            for identifier in canonical_ids :
-                numeric_value =numeric_map .get (identifier )
-                if numeric_value is None :
-                    continue
-                normalized_ids .append ((int (numeric_value ),identifier ))
-            if not normalized_ids :
-                summary ={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-                context .extra ["delegated_summary"]=summary
-                return ([],summary )
-            records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =context .select_fields or None )
-            context .extra ["delegated_summary"]=summary
-            return (records ,summary )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            dataframe =self ._ensure_comment_fields (dataframe ,log )
-            dataframe =self ._extract_data_validity_descriptions (dataframe ,chembl_client ,log )
-            dataframe =self ._extract_assay_fields (dataframe ,chembl_client ,log )
-            self ._log_validity_comments_metrics (dataframe ,log )
-            return dataframe
-        def empty_activity_frame ()->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def finalize_context (context :BatchExtractionContext )->None :
-            summary =context .extra .get ("delegated_summary")
-            if isinstance (summary ,Mapping ):
-                summary_dict =dict (summary )
-                context .extra ["stats_attribute_override"]=summary_dict
-                self ._last_batch_extract_stats =summary_dict
-            else :
-                context .extra ["stats_attribute_override"]=context .stats .as_dict ()
-                self ._last_batch_extract_stats =context .stats .as_dict ()
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="activity_id",fetcher =delegated_fetch ,select_fields =select_fields ,batch_size =source_config .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release ,id_normalizer =normalize_activity_id ,sort_key =lambda pair :int (pair [0 ]),finalize =finalize_dataframe ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats",fetch_mode ="delegated",empty_frame_factory =empty_activity_frame )
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        batch_stats =self ._last_batch_extract_stats or {}
-        log .info ("chembl_activity.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =batch_stats .get ("batches"),api_calls =batch_stats .get ("api_calls"),cache_hits =batch_stats .get ("cache_hits"))
-        return dataframe
-    def _prepare_activity_iteration (self ,*,client_name :str ="chembl_activity_client")->tuple [ActivitySourceConfig ,ChemblClient ,ChemblActivityClient ,list [str ]]:
-        "Construct reusable ChEMBL clients and iterator for activity extraction."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name =client_name )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =self ._resolve_select_fields (source_raw ,default_fields =API_ACTIVITY_FIELDS )
-        return (source_config ,chembl_client ,activity_iterator ,select_fields )
-    def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-        "Construct the descriptor driving the shared extraction template."
-        def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-            http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-            chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-            typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-            activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-            select_fields =source_config .parameters .select_fields
-            return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-        def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            df =pipeline ._ensure_comment_fields (df ,log )
-            chembl_client =cast (ChemblClient ,context .chembl_client )
-            if chembl_client is not None :
-                df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-            pipeline ._log_validity_comments_metrics (df ,log )
-            return df
-        def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            return pipeline ._materialize_activity_record (payload )
-        return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
-    def _materialize_activity_record (self ,payload :Mapping [str ,Any ],*,activity_id :int |None =None )->dict [str ,Any ]:
-        "Normalize nested fields within an activity payload."
-        record =dict (payload )
-        record =self ._extract_nested_fields (record )
-        record =self ._extract_activity_properties_fields (record )
-        if activity_id is not None :
-            record .setdefault ("activity_id",activity_id )
-        return record
-    def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-        "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-        if isinstance (dataset ,pd .Series ):
-            return dataset .to_frame (name ="activity_id")
-        if isinstance (dataset ,pd .DataFrame ):
-            return dataset
-        if isinstance (dataset ,Mapping ):
-            mapping =cast (Mapping [str ,Any ],dataset )
-            return pd .DataFrame ([dict (mapping )])
-        if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-            dataset_list :list [Any ]=list (dataset )
-            return pd .DataFrame ({"activity_id":dataset_list })
-        msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-        raise TypeError (msg )
-    def _normalize_activity_ids (self ,input_frame :pd .DataFrame ,*,limit :int |None ,log :BoundLogger )->list [tuple [int ,str ]]:
-        "Normalize raw identifier values into deduplicated integer/string pairs."
-        normalized_ids :list [tuple [int ,str ]]=[]
-        invalid_ids :list [Any ]=[]
-        seen :set [str ]=set ()
-        for raw_id in input_frame ["activity_id"].tolist ():
-            if pd .isna (raw_id ):
-                continue
-            try :
-                if isinstance (raw_id ,str ):
-                    candidate =raw_id .strip ()
-                    if not candidate :
-                        continue
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw_id ,(int ,float )):
-                    numeric_id =int (raw_id )
-                else :
-                    numeric_id =int (raw_id )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw_id )
-                continue
-            key =str (numeric_id )
-            if key not in seen :
-                seen .add (key )
-                normalized_ids .append ((numeric_id ,key ))
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        if limit is not None :
-            normalized_ids =normalized_ids [:max (int (limit ),0 )]
-        return normalized_ids
-    def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-        "Iterate over IDs using the shared iterator while preserving cache semantics."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        records :list [dict [str ,Any ]]=[]
-        success_count =0
-        fallback_count =0
-        error_count =0
-        cache_hits =0
-        api_calls =0
-        total_batches =0
-        key_order =[key for _ ,key in normalized_ids ]
-        key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-        for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-            total_batches +=1
-            batch_start =time .perf_counter ()
-            from_cache =False
-            chunk_records :dict [str ,dict [str ,Any ]]={}
-            try :
-                cached_records =self ._check_cache (chunk ,self ._chembl_release )
-                if cached_records is not None :
-                    from_cache =True
-                    cache_hits +=len (chunk )
-                    chunk_records =cached_records
-                else :
-                    api_calls +=1
-                    fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                    for item in fetched_items :
-                        if not isinstance (item ,Mapping ):
-                            continue
-                        activity_value =item .get ("activity_id")
-                        if activity_value is None :
-                            continue
-                        chunk_records [str (activity_value )]=dict (item )
-                    self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-                success_in_batch =0
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    record =chunk_records .get (key )
-                    if record and (not record .get ("error")):
-                        materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                        records .append (materialized )
-                        success_count +=1
-                        success_in_batch +=1
-                    else :
-                        fallback_record =self ._create_fallback_record (numeric_id )
-                        records .append (fallback_record )
-                        fallback_count +=1
-                        error_count +=1
-                batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-                log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-            except CircuitBreakerOpenError as exc :
-                log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except RequestException as exc :
-                log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except Exception as exc :
-                log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-        total_records =len (normalized_ids )
-        success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-        summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-        return (records ,summary )
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw activity data by normalizing measurements, identifiers, and data types."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_measurements (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,ActivitySchema ,log )
-        if "curated"in df .columns or "curated_by"in df .columns :
-            if "curated"not in df .columns :
-                df ["curated"]=pd .NA
-            if "curated_by"in df .columns :
-                mask =df ["curated"].isna ()
-                df .loc [mask ,"curated"]=df .loc [mask ,"curated_by"].notna ()
-            df ["curated"]=df ["curated"].astype ("boolean")
-        df =self ._validate_foreign_keys (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if self ._should_enrich_compound_record ():
-            df =self ._enrich_compound_record (df )
-        if self ._should_enrich_assay ():
-            df =self ._enrich_assay (df )
-        if self ._should_enrich_molecule ():
-            df =self ._enrich_molecule (df )
-        if self ._should_enrich_data_validity ():
-            df =self ._enrich_data_validity (df )
-        df =self ._finalize_identifier_columns (df ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        df =self ._finalize_output_columns (df ,log )
-        df =self ._filter_invalid_required_fields (df ,log )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against ActivitySchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        if "target_tax_id"in df .columns :
-            dtype_name :str =str (df ["target_tax_id"].dtype .name )
-            if dtype_name !="Int64":
-                numeric_series :pd .Series [Any ]=pd .to_numeric (df ["target_tax_id"],errors ="coerce")
-                df ["target_tax_id"]=numeric_series .astype ("Int64")
-        self ._check_activity_id_uniqueness (df ,log )
-        self ._check_foreign_key_integrity (df ,log )
-        self ._validate_data_validity_comment_soft_enum (df ,log )
-        original_coerce =self .config .validation .coerce
-        try :
-            self .config .validation .coerce =False
-            validated =super ().validate (df )
-            log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce )
-            return validated
-        except pandera .errors .SchemaErrors as exc :
-            failure_cases_df :pd .DataFrame |None =None
-            if hasattr (exc ,"failure_cases"):
-                failure_cases_df =cast (pd .DataFrame ,exc .failure_cases )
-            error_count =len (failure_cases_df )if failure_cases_df is not None else 0
-            error_summary =summarize_schema_errors (exc )
-            log .error ("validation_failed",error_count =error_count ,schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce ,error_summary =error_summary ,exc_info =True )
-            if failure_cases_df is not None and (not failure_cases_df .empty ):
-                failure_cases_summary =format_failure_cases (failure_cases_df )
-                log .error ("validation_failure_cases",failure_cases =failure_cases_summary )
-                self ._log_detailed_validation_errors (failure_cases_df ,df ,log )
-            msg =f'Validation failed with {error_count } error(s) against schema {self .config .validation .schema_out }: {error_summary }'
-            raise ValueError (msg )from exc
-        except Exception as exc :
-            log .error ("validation_error",error =str (exc ),schema =self .config .validation .schema_out ,exc_info =True )
-            raise
-        finally :
-            self .config .validation .coerce =original_coerce
-    def _should_enrich_compound_record (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 compound_record \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            compound_record_section :Any =enrich_section .get ("compound_record")
-            if not isinstance (compound_record_section ,Mapping ):
-                return False
-            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-            enabled :Any =compound_record_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        compound_record_section :Any =enrich_section .get ("compound_record")
-                        if isinstance (compound_record_section ,Mapping ):
-                            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                            enrich_cfg =dict (compound_record_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_assay (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 assay \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            assay_section :Any =enrich_section .get ("assay")
-            if not isinstance (assay_section ,Mapping ):
-                return False
-            assay_section =cast (Mapping [str ,Any ],assay_section )
-            enabled :Any =assay_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        assay_section :Any =enrich_section .get ("assay")
-                        if isinstance (assay_section ,Mapping ):
-                            assay_section =cast (Mapping [str ,Any ],assay_section )
-                            enrich_cfg =dict (assay_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_assay (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_data_validity (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 data_validity \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            data_validity_section :Any =enrich_section .get ("data_validity")
-            if not isinstance (data_validity_section ,Mapping ):
-                return False
-            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-            enabled :Any =data_validity_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        data_validity_section :Any =enrich_section .get ("data_validity")
-                        if isinstance (data_validity_section ,Mapping ):
-                            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                            enrich_cfg =dict (data_validity_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        if "data_validity_description"in df .columns :
-            non_na_count =int (df ["data_validity_description"].notna ().sum ())
-            if non_na_count >0 :
-                log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_molecule (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 molecule \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            molecule_section :Any =enrich_section .get ("molecule")
-            if not isinstance (molecule_section ,Mapping ):
-                return False
-            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-            enabled :Any =molecule_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        molecule_section :Any =enrich_section .get ("molecule")
-                        if isinstance (molecule_section ,Mapping ):
-                            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                            enrich_cfg =dict (molecule_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-        if "molecule_name"in df_join .columns :
-            if "molecule_pref_name"not in df .columns :
-                df ["molecule_pref_name"]=pd .NA
-            mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-            if mask .any ():
-                df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-                df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-                log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-        return df
-    def _extract_from_chembl (self ,dataset :object ,chembl_client :ChemblClient |Any ,activity_iterator :ChemblActivityClient ,*,limit :int |None =None ,select_fields :Sequence [str ]|None =None )->pd .DataFrame :
-        "Extract activity records by delegating batching to ``ChemblActivityClient``."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        method_start =time .perf_counter ()
-        self ._last_batch_extract_stats =None
-        input_frame =self ._coerce_activity_dataset (dataset )
-        if "activity_id"not in input_frame .columns :
-            msg ="Input dataset must contain an 'activity_id' column"
-            raise ValueError (msg )
-        normalized_ids =self ._normalize_activity_ids (input_frame ,limit =limit ,log =log )
-        if not normalized_ids :
-            summary :dict [str ,Any ]={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-            self ._last_batch_extract_stats =summary
-            log .info ("chembl_activity.batch_summary",**summary )
-            empty_frame =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-            return self ._ensure_comment_fields (empty_frame ,log )
-        records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =select_fields )
-        duration_ms =(time .perf_counter ()-method_start )*1000.0
-        summary ["duration_ms"]=duration_ms
-        self ._last_batch_extract_stats =summary
-        log .info ("chembl_activity.batch_summary",**summary )
-        result_df :pd .DataFrame =pd .DataFrame .from_records (records )
-        if result_df .empty :
-            result_df =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        elif "activity_id"in result_df .columns :
-            result_df =result_df .sort_values ("activity_id").reset_index (drop =True )
-        result_df =self ._ensure_comment_fields (result_df ,log )
-        result_df =self ._extract_data_validity_descriptions (result_df ,chembl_client ,log )
-        result_df =self ._extract_assay_fields (result_df ,chembl_client ,log )
-        self ._log_validity_comments_metrics (result_df ,log )
-        return result_df
-    def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-        cache_config =self .config .cache
-        if not cache_config .enabled :
-            return None
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        if not cache_file .exists ():
-            return None
-        try :
-            stat =cache_file .stat ()
-        except OSError :
-            return None
-        ttl_seconds =int (cache_config .ttl )
-        if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        try :
-            payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-        except (OSError ,json .JSONDecodeError ):
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        if not isinstance (payload ,dict ):
-            return None
-        missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-        if missing :
-            return None
-        return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
-    def _store_cache (self ,batch_ids :Sequence [str ],batch_data :Mapping [str ,Mapping [str ,Any ]],release :str |None )->None :
-        cache_config =self .config .cache
-        if not cache_config .enabled or not batch_ids or (not batch_data ):
-            return
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        normalized_set =set (normalized_ids )
-        data_to_store ={key :batch_data [key ]for key in normalized_set if key in batch_data }
-        if not data_to_store :
-            return
-        try :
-            cache_file .parent .mkdir (parents =True ,exist_ok =True )
-            tmp_path =cache_file .with_suffix (cache_file .suffix +".tmp")
-            tmp_path .write_text (json .dumps (data_to_store ,sort_keys =True ,default =str ),encoding ="utf-8")
-            tmp_path .replace (cache_file )
-        except Exception as exc :
-            log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-            log .debug ("chembl_activity.cache_store_failed",error =str (exc ))
-    def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-        directory =self ._cache_directory (release )
-        cache_key =self ._cache_key (batch_ids ,release )
-        return directory /f'{cache_key }.json'
-    def _cache_directory (self ,release :str |None )->Path :
-        cache_root =Path (self .config .paths .cache_root )
-        directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-        release_component =self ._sanitize_cache_component (release or "unknown")
-        pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-        version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-        return cache_root /directory_name /pipeline_component /release_component /version_component
-    def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-        payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-        raw =json .dumps (payload ,sort_keys =True )
-        return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
-    @staticmethod
-    def _sanitize_cache_component (value :str )->str :
-        sanitized =re .sub ("[^0-9A-Za-z_.-]","_",value )
-        return sanitized or "default"
-    def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-        "Create fallback record enriched with error metadata."
-        base_message ="Fallback: ChEMBL activity unavailable"
-        message =f'{base_message } ({error })'if error else base_message
-        timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-        metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-        if isinstance (error ,RequestException ):
-            response =getattr (error ,"response",None )
-            status_code =getattr (response ,"status_code",None )
-            if status_code is not None :
-                metadata ["http_status"]=status_code
-            metadata ["error_message"]=str (error )
-        elif error is not None :
-            metadata ["error_message"]=str (error )
-        fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-        return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
-    def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-        if df .empty :
-            return df
-        required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-        if missing_fields :
-            for field in missing_fields :
-                df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            log .debug ("comment_fields_ensured",fields =missing_fields )
-        return df
-    def _extract_data_validity_descriptions (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c data_validity_description \u0438\u0437 DATA_VALIDITY_LOOKUP \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f data_validity_comment \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_data_validity_lookup() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c data_validity_comment.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 data_validity_description.\n        "
-        if df .empty :
-            return df
-        if "data_validity_comment"not in df .columns :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="data_validity_comment_column_missing")
-            return df
-        validity_comments :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            comment =row .get ("data_validity_comment")
-            if pd .isna (comment )or comment is None :
-                continue
-            comment_str =str (comment ).strip ()
-            if comment_str :
-                validity_comments .append (comment_str )
-        if not validity_comments :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="no_valid_comments")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        unique_comments =list (set (validity_comments ))
-        log .info ("extract_data_validity_descriptions_fetching",comments_count =len (unique_comments ))
-        try :
-            records_dict =client .fetch_data_validity_lookup (comments =unique_comments ,fields =["data_validity_comment","description"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_data_validity_descriptions_fetch_error",error =str (exc ),exc_info =True )
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for comment in unique_comments :
-            record =records_dict .get (comment )
-            if record :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":record .get ("description")})
-            else :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":None })
-        if not enrichment_data :
-            log .debug ("extract_data_validity_descriptions_no_records")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        original_index =df .index .copy ()
-        df_result =df .merge (df_enrich ,on =["data_validity_comment"],how ="left",suffixes =("","_enrich"))
-        if "data_validity_description"not in df_result .columns :
-            df_result ["data_validity_description"]=pd .Series ([pd .NA ]*len (df_result ),dtype ="string")
-        else :
-            df_result ["data_validity_description"]=df_result ["data_validity_description"].astype ("string")
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_data_validity_descriptions_complete",comments_requested =len (unique_comments ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _extract_assay_fields (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c assay_organism \u0438 assay_tax_id \u0438\u0437 ASSAYS \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f assay_chembl_id \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_assays_by_ids() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438 assay_organism \u0438 assay_tax_id.\n        "
-        if df .empty :
-            return df
-        if "assay_chembl_id"not in df .columns :
-            log .debug ("extract_assay_fields_skipped",reason ="assay_chembl_id_column_missing")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        assay_ids :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            assay_id =row .get ("assay_chembl_id")
-            if pd .isna (assay_id )or assay_id is None :
-                continue
-            assay_id_str =str (assay_id ).strip ().upper ()
-            if assay_id_str :
-                assay_ids .append (assay_id_str )
-        if not assay_ids :
-            log .debug ("extract_assay_fields_skipped",reason ="no_valid_assay_ids")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        unique_assay_ids =list (set (assay_ids ))
-        log .info ("extract_assay_fields_fetching",assay_ids_count =len (unique_assay_ids ))
-        try :
-            records_dict =client .fetch_assays_by_ids (ids =unique_assay_ids ,fields =["assay_chembl_id","assay_organism","assay_tax_id"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_assay_fields_fetch_error",error =str (exc ),exc_info =True )
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for assay_id in unique_assay_ids :
-            record =records_dict .get (assay_id )if records_dict else None
-            if record :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":record .get ("assay_organism"),"assay_tax_id":record .get ("assay_tax_id")})
-            else :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":None ,"assay_tax_id":None })
-        if not enrichment_data :
-            log .debug ("extract_assay_fields_no_records")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        df_enrich ["assay_chembl_id"]=df_enrich ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        original_index =df .index .copy ()
-        df_normalized =df .copy ()
-        df_normalized ["assay_chembl_id_normalized"]=df_normalized ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        df_result =df_normalized .merge (df_enrich ,left_on ="assay_chembl_id_normalized",right_on ="assay_chembl_id",how ="left",suffixes =("","_enrich"))
-        df_result =df_result .drop (columns =["assay_chembl_id_normalized"])
-        for col in ["assay_organism","assay_tax_id"]:
-            if f'{col }_enrich'in df_result .columns :
-                if col not in df_result .columns :
-                    df_result [col ]=df_result [f'{col }_enrich']
-                else :
-                    base_series :pd .Series [Any ]=df_result [col ]
-                    enrich_series :pd .Series [Any ]=df_result [f'{col }_enrich']
-                    missing_mask =base_series .isna ()
-                    if bool (missing_mask .any ()):
-                        df_result .loc [missing_mask ,col ]=enrich_series .loc [missing_mask ]
-                df_result =df_result .drop (columns =[f'{col }_enrich'])
-        for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-            if col not in df_result .columns :
-                df_result [col ]=pd .Series ([pd .NA ]*len (df_result ),dtype =dtype )
-        df_result ["assay_organism"]=df_result ["assay_organism"].astype ("string")
-        df_result ["assay_tax_id"]=pd .to_numeric (df_result ["assay_tax_id"],errors ="coerce").astype ("Int64")
-        mask_valid =df_result ["assay_tax_id"].notna ()
-        if mask_valid .any ():
-            invalid_mask =mask_valid &(df_result ["assay_tax_id"]<1 )
-            if invalid_mask .any ():
-                log .warning ("invalid_assay_tax_id_range",count =int (invalid_mask .sum ()))
-                df_result .loc [invalid_mask ,"assay_tax_id"]=pd .NA
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_assay_fields_complete",assay_ids_requested =len (unique_assay_ids ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _log_validity_comments_metrics (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "\u041b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442:\n        - \u0414\u043e\u043b\u044e NA \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0442\u0440\u0435\u0445 \u043f\u043e\u043b\u0435\u0439\n        - \u0422\u043e\u043f-10 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment\n        - \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment (\u043d\u0435 \u0432 whitelist)\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity.\n        log:\n            Logger instance.\n        "
-        if df .empty :
-            return
-        metrics :dict [str ,Any ]={}
-        comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        for field in comment_fields :
-            if field in df .columns :
-                na_count =int (df [field ].isna ().sum ())
-                total_count =len (df )
-                na_rate =float (na_count )/float (total_count )if total_count >0 else 0.0
-                metrics [f'{field }_na_rate']=na_rate
-                metrics [f'{field }_na_count']=na_count
-                metrics [f'{field }_total_count']=total_count
-        non_null_comments_series :pd .Series [str ]|None =None
-        if "data_validity_comment"in df .columns :
-            series_candidate =df ["data_validity_comment"].dropna ()
-            if len (series_candidate )>0 :
-                typed_series :pd .Series [str ]=series_candidate .astype ("string")
-                non_null_comments_series =typed_series
-                value_counts =typed_series .value_counts ().head (10 )
-                top_10 ={str (key ):int (value )for key ,value in value_counts .items ()}
-                metrics ["top_10_data_validity_comments"]=top_10
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if whitelist and non_null_comments_series is not None :
-            whitelist_set :set [str ]=set (whitelist )
-            def _is_unknown (value :str )->bool :
-                return value not in whitelist_set
-            unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-            unknown_count =int (unknown_mask .sum ())
-            if unknown_count >0 :
-                unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-                metrics ["unknown_data_validity_comments_count"]=unknown_count
-                metrics ["unknown_data_validity_comments_samples"]=unknown_values
-                log .warning ("unknown_data_validity_comments_detected",unknown_count =unknown_count ,samples =unknown_values ,whitelist =whitelist )
-        if metrics :
-            log .info ("validity_comments_metrics",**metrics )
-    def _get_data_validity_comment_whitelist (self )->list [str ]:
-        "\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c whitelist \u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0434\u043b\u044f data_validity_comment \u0438\u0437 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\n\n        Raises\n        ------\n        RuntimeError\n            \u0415\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d \u0438\u043b\u0438 \u043f\u0443\u0441\u0442.\n        "
-        try :
-            values =sorted (self ._required_vocab_ids ("data_validity_comment"))
-        except RuntimeError as exc :
-            UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validation',run_id =self .run_id ).error ("data_validity_comment_whitelist_unavailable",error =str (exc ))
-            raise
-        return values
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested assay and molecule objects."
-        if "assay"in record and isinstance (record ["assay"],Mapping ):
-            assay =cast (Mapping [str ,Any ],record ["assay"])
-            if "organism"in assay :
-                record .setdefault ("assay_organism",assay ["organism"])
-            if "tax_id"in assay :
-                record .setdefault ("assay_tax_id",assay ["tax_id"])
-        if "molecule"in record and isinstance (record ["molecule"],Mapping ):
-            molecule =cast (Mapping [str ,Any ],record ["molecule"])
-            if "pref_name"in molecule :
-                record .setdefault ("molecule_pref_name",molecule ["pref_name"])
-        if "curated_by"in record :
-            curated_by =record .get ("curated_by")
-            if curated_by is not None and (not pd .isna (curated_by )):
-                record .setdefault ("curated",True )
-            else :
-                record .setdefault ("curated",False )
-        elif "curated"not in record :
-            record .setdefault ("curated",None )
-        return record
-    def _extract_activity_properties_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract TRUV fields, standard_* fields, and comments from activity_properties array as fallback.\n\n        \u041f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0438\u0437 activity_properties \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442\n        \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043e\u0442\u0432\u0435\u0442\u0435 API (\u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 \u043f\u0440\u044f\u043c\u044b\u0445 \u043f\u043e\u043b\u0435\u0439 \u0438\u0437 ACTIVITIES).\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0435 TRUV-\u043f\u043e\u043b\u044f: value, text_value, relation, units.\n        \u0422\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f: standard_upper_value, standard_text_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b: upper_value, lower_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0438: activity_comment, data_validity_comment.\n\n        \u0422\u0430\u043a\u0436\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 activity_properties \u0432 \u0437\u0430\u043f\u0438\u0441\u0438 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438.\n        \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044e \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract_activity_properties')
-        activity_id =record .get ("activity_id")
-        if "activity_properties"not in record :
-            log .warning ("activity_properties_missing",activity_id =activity_id ,message ="activity_properties not found in API response (possible ChEMBL < v24)")
-            record ["activity_properties"]=None
-            return record
-        properties =record ["activity_properties"]
-        if properties is None :
-            log .debug ("activity_properties_null",activity_id =activity_id ,message ="activity_properties is None (possible ChEMBL < v24)")
-            return record
-        if isinstance (properties ,str ):
-            try :
-                properties =json .loads (properties )
-            except (TypeError ,ValueError ,json .JSONDecodeError )as exc :
-                log .debug ("activity_properties_parse_failed",error =str (exc ),activity_id =record .get ("activity_id"))
-                return record
-        if not isinstance (properties ,Sequence )or isinstance (properties ,(str ,bytes )):
-            return record
-        property_iterable :Iterable [Any ]=cast (Iterable [Any ],properties )
-        property_items :list [Any ]=list (property_iterable )
-        def _set_fallback (key :str ,value :Any )->None :
-            "Set fallback value only if key is missing in record and value is not None."
-            if value is not None and record .get (key )is None :
-                record [key ]=value
-        def _is_empty (value :Any )->bool :
-            "Check if value is empty (None, empty string, or whitespace)."
-            if value is None :
-                return True
-            if isinstance (value ,str ):
-                return not value .strip ()
-            return False
-        items :list [Mapping [str ,Any ]]=[]
-        for property_item in property_items :
-            if isinstance (property_item ,Mapping )and "type"in property_item and ("value"in property_item or "text_value"in property_item ):
-                items .append (cast (Mapping [str ,Any ],property_item ))
-        def _is_measured (p :Mapping [str ,Any ])->bool :
-            rf =p .get ("result_flag")
-            return rf is True or (isinstance (rf ,int )and rf ==1 )
-        items .sort (key =lambda p :not _is_measured (p ))
-        for prop in items :
-            val =prop .get ("value")
-            txt =prop .get ("text_value")
-            rel =prop .get ("relation")
-            unt =prop .get ("units")
-            prop_type =str (prop .get ("type","")).lower ()
-            will_set_value =val is not None and record .get ("value")is None
-            will_set_text_value =txt is not None and record .get ("text_value")is None
-            _set_fallback ("value",val )
-            _set_fallback ("text_value",txt )
-            if will_set_value or will_set_text_value :
-                _set_fallback ("relation",rel )
-                _set_fallback ("units",unt )
-            if unt is not None and record .get ("units")is None :
-                _set_fallback ("units",unt )
-            if record .get ("upper_value")is None and ("upper"in prop_type or prop_type in ("upper_value","upper limit")):
-                if val is not None :
-                    _set_fallback ("upper_value",val )
-            if record .get ("lower_value")is None and ("lower"in prop_type or prop_type in ("lower_value","lower limit")):
-                if val is not None :
-                    _set_fallback ("lower_value",val )
-            if record .get ("standard_upper_value")is None and ("standard_upper"in prop_type or prop_type in ("standard upper","standard upper value")):
-                if val is not None :
-                    _set_fallback ("standard_upper_value",val )
-            if record .get ("standard_text_value")is None and "standard"in prop_type and ("text"in prop_type ):
-                if txt is not None :
-                    _set_fallback ("standard_text_value",txt )
-                elif val is not None :
-                    _set_fallback ("standard_text_value",val )
-        current_comment =record .get ("data_validity_comment")
-        if _is_empty (current_comment ):
-            data_validity_items :list [Mapping [str ,Any ]]=[prop for prop in items if ("data_validity"in str (prop .get ("type","")).lower ()or "validity"in str (prop .get ("type","")).lower ())and (prop .get ("text_value")is not None or prop .get ("value")is not None )]
-            if data_validity_items :
-                measured_items =[p for p in data_validity_items if _is_measured (p )]
-                if measured_items :
-                    prop =measured_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="measured",comment_value =comment_value )
-                else :
-                    prop =data_validity_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="first",comment_value =comment_value )
-            else :
-                log .debug ("data_validity_comment_fallback_no_items",activity_id =record .get ("activity_id"),activity_properties_count =len (items ),has_activity_properties =True )
-        else :
-            log .debug ("data_validity_comment_from_api",activity_id =record .get ("activity_id"),comment_value =current_comment )
-        normalized_properties =self ._normalize_activity_properties_items (property_items ,log )
-        if normalized_properties is not None :
-            validated_properties ,validation_stats =self ._validate_activity_properties_truv (normalized_properties ,log ,activity_id )
-            deduplicated_properties ,dedup_stats =self ._deduplicate_activity_properties (validated_properties ,log ,activity_id )
-            record ["activity_properties"]=deduplicated_properties
-            log .debug ("activity_properties_processed",activity_id =activity_id ,original_count =len (property_items ),normalized_count =len (normalized_properties ),validated_count =len (validated_properties ),deduplicated_count =len (deduplicated_properties ),invalid_count =validation_stats .get ("invalid_count",0 ),duplicates_removed =dedup_stats .get ("duplicates_removed",0 ))
-        else :
-            record ["activity_properties"]=properties
-            log .debug ("activity_properties_normalization_failed",activity_id =activity_id ,message ="activity_properties normalization failed, keeping original")
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
-    @staticmethod
-    def _extract_chembl_release (payload :Mapping [str ,Any ])->str |None :
-        for key in ("chembl_release","chembl_db_version","release","version"):
-            value =payload .get (key )
-            if isinstance (value ,str )and value .strip ():
-                return value
-            if value is not None :
-                return str (value )
-        return None
-    @staticmethod
-    def _extract_page_items (payload :Mapping [str ,Any ],items_keys :Sequence [str ]|None =None )->list [dict [str ,Any ]]:
-        preferred_keys :tuple [str ,...]=("activities",)
-        if items_keys is None :
-            combined_keys =preferred_keys +("data","items","results")
-        else :
-            combined_keys =tuple (dict .fromkeys ((*preferred_keys ,*items_keys )))
-        return ChemblPipelineBase ._extract_page_items (payload ,combined_keys )
-    @staticmethod
-    def _next_link (payload :Mapping [str ,Any ],base_url :str )->str |None :
-        page_meta :Any =payload .get ("page_meta")
-        if isinstance (page_meta ,Mapping ):
-            next_link_raw :Any =page_meta .get ("next")
-            next_link :str |None =cast (str |None ,next_link_raw )if next_link_raw is not None else None
-            if isinstance (next_link ,str )and next_link :
-                base_url_str =str (base_url )
-                base_path_parse_result =urlparse (base_url_str )
-                base_path_raw =base_path_parse_result .path
-                base_path_str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                base_path :str =base_path_str .rstrip ("/")
-                if next_link .startswith ("http://")or next_link .startswith ("https://"):
-                    parsed =urlparse (next_link )
-                    base_parsed =urlparse (base_url_str )
-                    parsed_path_raw =parsed .path
-                    base_path_raw =base_parsed .path
-                    path :str =parsed_path_raw .decode ("utf-8","ignore")if isinstance (parsed_path_raw ,(bytes ,bytearray ))else parsed_path_raw
-                    base_path_from_url :str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                    path_normalized :str =path .rstrip ("/")
-                    base_path_normalized :str =base_path_from_url .rstrip ("/")
-                    if base_path_normalized and path_normalized .startswith (base_path_normalized ):
-                        relative_path =path_normalized [len (base_path_normalized ):]
-                        if not relative_path :
-                            return None
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    elif "/api/data/"in path :
-                        parts =path .split ("/api/data/",1 )
-                        if len (parts )>1 :
-                            relative_path ="/"+parts [1 ]
-                        else :
-                            relative_path =path
-                    else :
-                        relative_path =path
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    if parsed .query :
-                        relative_path =f'{relative_path }?{parsed .query }'
-                    return relative_path
-                if base_path :
-                    normalized_base =base_path .lstrip ("/")
-                    stripped_link =next_link .lstrip ("/")
-                    if stripped_link .startswith (normalized_base +"/"):
-                        stripped_link =stripped_link [len (normalized_base ):]
-                    elif stripped_link ==normalized_base :
-                        stripped_link =""
-                    next_link =stripped_link
-                next_link =next_link .lstrip ("/")
-                if next_link :
-                    next_link =f'/{next_link }'
-                else :
-                    next_link ="/"
-                return next_link
-        return None
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Ensure canonical identifier columns are present before normalization."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_chembl_id"not in df .columns and "assay_id"in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "testitem_chembl_id"not in df .columns :
-            if "testitem_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["testitem_id"]
-                actions .append ("testitem_id->testitem_chembl_id")
-            elif "molecule_chembl_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["molecule_chembl_id"]
-                actions .append ("molecule_chembl_id->testitem_chembl_id")
-        if "molecule_chembl_id"not in df .columns and "testitem_chembl_id"in df .columns :
-            df ["molecule_chembl_id"]=df ["testitem_chembl_id"]
-            actions .append ("testitem_chembl_id->molecule_chembl_id")
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_required =[column for column in required_columns if column not in df .columns ]
-        if missing_required :
-            for column in missing_required :
-                df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            actions .append (f"created_missing:{",".join (missing_required )}")
-        alias_columns =[column for column in ("assay_id","testitem_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        boolean_columns =("potential_duplicate","curated","removed")
-        for column in boolean_columns :
-            specs [column ]={"dtype":"boolean","default":pd .NA }
-        return specs
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize ChEMBL and BAO identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"],pattern ="^CHEMBL\\d+$"),IdentifierRule (name ="bao",columns =["bao_endpoint","bao_format"],pattern ="^BAO_\\d{7}$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _finalize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align identifier columns after normalization and drop aliases."
-        df =df .copy ()
-        if {"molecule_chembl_id","testitem_chembl_id"}.issubset (df .columns ):
-            mismatch_mask =df ["molecule_chembl_id"].notna ()&df ["testitem_chembl_id"].notna ()&(df ["molecule_chembl_id"]!=df ["testitem_chembl_id"])
-            if mismatch_mask .any ():
-                mismatch_count =int (mismatch_mask .sum ())
-                samples_raw =df .loc [mismatch_mask ,["molecule_chembl_id","testitem_chembl_id"]].drop_duplicates ().head (5 ).to_dict ("records")
-                samples :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],samples_raw )
-                log .warning ("identifier_mismatch",count =mismatch_count ,samples =samples )
-                df .loc [mismatch_mask ,"testitem_chembl_id"]=df .loc [mismatch_mask ,"molecule_chembl_id"]
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_columns =[column for column in required_columns if column not in df .columns ]
-        if missing_columns :
-            for column in missing_columns :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            log .warning ("identifier_columns_missing",columns =missing_columns )
-        return df
-    def _finalize_output_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align final column order with schema and drop unexpected fields."
-        df =df .copy ()
-        expected =list (COLUMN_ORDER )
-        extras =[column for column in df .columns if column not in expected ]
-        if extras :
-            df =df .drop (columns =extras )
-            log .debug ("output_columns_dropped",columns =extras )
-        missing =[column for column in expected if column not in df .columns ]
-        if missing :
-            for column in missing :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([pd .NA ]*len (df ),dtype ="object")
-            log .warning ("output_columns_missing",columns =missing )
-        if not expected :
-            return df
-        return df [expected ]
-    def _filter_invalid_required_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Filter out rows with NULL values in required identifier fields.\n\n        Removes rows where any of the required fields (assay_chembl_id,\n        testitem_chembl_id, molecule_chembl_id) are NULL, as these cannot\n        pass schema validation.\n\n        Parameters\n        ----------\n        df:\n            DataFrame to filter.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            Filtered DataFrame with only rows having all required fields populated.\n        "
-        df =df .copy ()
-        if df .empty :
-            return df
-        required_fields =["assay_chembl_id","molecule_chembl_id"]
-        missing_fields =[field for field in required_fields if field not in df .columns ]
-        if missing_fields :
-            log .warning ("filter_skipped_missing_columns",missing_columns =missing_fields ,message ="Cannot filter: required columns are missing")
-            return df
-        valid_mask =df ["assay_chembl_id"].notna ()&df ["molecule_chembl_id"].notna ()
-        invalid_count =int ((~valid_mask ).sum ())
-        if invalid_count >0 :
-            invalid_rows =df [~valid_mask ]
-            sample_size =min (5 ,len (invalid_rows ))
-            sample_activity_ids =invalid_rows ["activity_id"].head (sample_size ).tolist ()if "activity_id"in invalid_rows .columns else []
-            log .warning ("filtered_invalid_rows",filtered_count =invalid_count ,remaining_count =int (valid_mask .sum ()),sample_activity_ids =sample_activity_ids ,message ="Rows with NULL in required identifier fields were filtered out")
-            df =df [valid_mask ].reset_index (drop =True )
-        return df
-    def _normalize_measurements (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize standard_value, standard_units, standard_relation, and standard_type."
-        df =df .copy ()
-        normalized_count =0
-        if "standard_value"in df .columns :
-            mask =df ["standard_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_value"]=numeric_series_std
-                negative_mask =mask &(df ["standard_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["standard_relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"standard_relation"]=series
-                invalid_mask =mask &~df ["standard_relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_type"in df .columns :
-            mask =df ["standard_type"].notna ()
-            if mask .any ():
-                df .loc [mask ,"standard_type"]=df .loc [mask ,"standard_type"].astype (str ).str .strip ()
-                standard_types_set :set [str ]=STANDARD_TYPES
-                invalid_mask =mask &~df ["standard_type"].isin (standard_types_set )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_type",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_type"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_units"in df .columns :
-            unit_mapping ={"nanomolar":"nM","nmol":"nM","nm":"nM","NM":"nM","\u00b5M":"\u03bcM","uM":"\u03bcM","UM":"\u03bcM","micromolar":"\u03bcM","microM":"\u03bcM","umol":"\u03bcM","millimolar":"mM","milliM":"mM","mmol":"mM","MM":"mM","percent":"%","pct":"%","ratios":"ratio"}
-            mask =df ["standard_units"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_units"].astype (str ).str .strip ()
-                for old_unit ,new_unit in unit_mapping .items ():
-                    series =series .str .replace (old_unit ,new_unit ,regex =False ,case =False )
-                df .loc [mask ,"standard_units"]=series
-                normalized_count +=int (mask .sum ())
-        if "relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"relation"]=series
-                invalid_mask =mask &~df ["relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_upper_value"in df .columns :
-            mask =df ["standard_upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_upper_value"]=numeric_series_std_upper
-                negative_mask =mask &(df ["standard_upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "upper_value"in df .columns :
-            mask =df ["upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"upper_value"]=numeric_series_upper
-                negative_mask =mask &(df ["upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "lower_value"in df .columns :
-            mask =df ["lower_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"lower_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_lower :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"lower_value"]=numeric_series_lower
-                negative_mask =mask &(df ["lower_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_lower_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"lower_value"]=None
-                normalized_count +=int (mask .sum ())
-        if normalized_count >0 :
-            log .debug ("measurements_normalized",normalized_count =normalized_count )
-        return df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize string fields: trim, empty string to null, title-case for organism."
-        working_df =df .copy ()
-        if "data_validity_description"in working_df .columns and "data_validity_comment"in working_df .columns :
-            invalid_mask =working_df ["data_validity_description"].notna ()&working_df ["data_validity_comment"].isna ()
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                log .warning ("invariant_data_validity_description_without_comment",count =invalid_count ,message ="data_validity_description is filled while data_validity_comment is NA")
-        rules :dict [str ,StringRule ]={"canonical_smiles":StringRule (),"bao_label":StringRule (max_length =128 ),"target_organism":StringRule (title_case =True ),"assay_organism":StringRule (title_case =True ),"data_validity_comment":StringRule (),"data_validity_description":StringRule (),"activity_comment":StringRule (),"standard_text_value":StringRule (),"text_value":StringRule (),"type":StringRule (),"units":StringRule (),"assay_type":StringRule (),"assay_description":StringRule (),"molecule_pref_name":StringRule (),"target_pref_name":StringRule (),"uo_units":StringRule (),"qudt_units":StringRule ()}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Serialize nested structures (ligand_efficiency, activity_properties) to JSON strings."
-        df =df .copy ()
-        nested_fields =["ligand_efficiency","activity_properties"]
-        for field in nested_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                serialized :list [Any ]=[]
-                for idx ,value in df .loc [mask ,field ].items ():
-                    if field =="activity_properties":
-                        serialized_value =self ._serialize_activity_properties (value ,log )
-                        serialized .append (serialized_value )
-                        continue
-                    if isinstance (value ,(Mapping ,list )):
-                        try :
-                            serialized .append (json .dumps (value ,ensure_ascii =False ,sort_keys =True ))
-                        except (TypeError ,ValueError )as exc :
-                            log .warning ("nested_serialization_failed",field =field ,index =idx ,error =str (exc ))
-                            serialized .append (None )
-                    elif isinstance (value ,str ):
-                        try :
-                            json .loads (value )
-                            serialized .append (value )
-                        except (TypeError ,ValueError ):
-                            serialized .append (None )
-                    else :
-                        serialized .append (None )
-                df .loc [mask ,field ]=pd .Series (serialized ,dtype ="object",index =df .loc [mask ,field ].index )
-        if "standard_value"in df .columns and "ligand_efficiency"in df .columns :
-            mask =df ["standard_value"].notna ()&df ["ligand_efficiency"].isna ()
-            if mask .any ():
-                log .warning ("ligand_efficiency_missing_with_standard_value",count =int (mask .sum ()),message ="ligand_efficiency is empty while standard_value exists")
-        return df
-    def _serialize_activity_properties (self ,value :Any ,log :BoundLogger |None =None )->str |None :
-        "Return normalized JSON for activity_properties or None if not serializable."
-        normalized_items =self ._normalize_activity_properties_items (value ,log )
-        if normalized_items is None :
-            return None
-        try :
-            return json .dumps (normalized_items ,ensure_ascii =False ,sort_keys =True )
-        except (TypeError ,ValueError )as exc :
-            if log is not None :
-                log .warning ("activity_properties_serialization_failed",error =str (exc ))
-            return None
-    def _normalize_activity_properties_items (self ,value :Any ,log :BoundLogger |None =None )->list [dict [str ,Any ]]|None :
-        "Coerce activity_properties payloads into a list of constrained dictionaries."
-        if value is None :
-            return None
-        raw_value =value
-        if isinstance (value ,str ):
-            stripped =value .strip ()
-            if not stripped :
-                return []
-            try :
-                parsed =json .loads (stripped )
-            except (TypeError ,ValueError ):
-                fallback_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                fallback_base ["text_value"]=stripped
-                return [fallback_base ]
-            else :
-                value =parsed
-        if isinstance (value ,Mapping ):
-            items :list [Any ]=[value ]
-        elif isinstance (value ,Sequence )and (not isinstance (value ,(str ,bytes ))):
-            items =list (value )
-        else :
-            if log is not None :
-                log .warning ("activity_properties_unhandled_type",value_type =type (raw_value ).__name__ )
-            return None
-        normalized :list [dict [str ,Any ]]=[]
-        for item in items :
-            if item is None :
-                continue
-            if isinstance (item ,Mapping ):
-                item_mapping =cast (Mapping [str ,Any ],item )
-                normalized_item :dict [str ,Any |None ]={key :item_mapping .get (key )for key in ACTIVITY_PROPERTY_KEYS }
-                result_flag_value =normalized_item .get ("result_flag")
-                if isinstance (result_flag_value ,int )and result_flag_value in (0 ,1 ):
-                    normalized_item ["result_flag"]=bool (result_flag_value )
-                normalized .append (normalized_item )
-            elif isinstance (item ,str ):
-                str_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                str_base ["text_value"]=item
-                normalized .append (str_base )
-            elif log is not None :
-                log .warning ("activity_properties_item_unhandled",item_type =type (item ).__name__ )
-        return normalized
-    def _validate_activity_properties_truv (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0434\u043b\u044f activity_properties.\n\n        \u0412\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u0435\u0442 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n        - value IS NOT NULL \u21d2 text_value IS NULL (\u0438 \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442)\n        - relation IN ('=', '<', '\u2264', '>', '\u2265', '~') OR NULL\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        "
-        validated :list [dict [str ,Any ]]=[]
-        invalid_count =0
-        invalid_items :list [dict [str ,Any ]]=[]
-        for prop in properties :
-            is_valid =True
-            validation_errors :list [str ]=[]
-            value =prop .get ("value")
-            text_value =prop .get ("text_value")
-            relation =prop .get ("relation")
-            if value is not None and text_value is not None :
-                is_valid =False
-                validation_errors .append ("both value and text_value are not None")
-            elif value is None and text_value is None :
-                pass
-            if relation is not None :
-                if not isinstance (relation ,str ):
-                    is_valid =False
-                    validation_errors .append (f'relation is not a string: {type (relation ).__name__ }')
-                elif relation not in RELATIONS :
-                    is_valid =False
-                    validation_errors .append (f"relation '{relation }' not in allowed values: {RELATIONS }")
-            validated .append (prop )
-            if not is_valid :
-                invalid_count +=1
-                invalid_items .append (prop )
-                log .warning ("activity_property_truv_validation_failed",activity_id =activity_id ,property =prop ,errors =validation_errors ,message ="TRUV validation failed, but property is kept")
-        stats ={"invalid_count":invalid_count ,"valid_count":len (validated )}
-        return (validated ,stats )
-    def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-        seen :set [tuple [Any ,...]]=set ()
-        deduplicated :list [dict [str ,Any ]]=[]
-        duplicates_removed =0
-        for prop in properties :
-            dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-            if dedup_key not in seen :
-                seen .add (dedup_key )
-                deduplicated .append (prop )
-            else :
-                duplicates_removed +=1
-                log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-        stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-        return (deduplicated ,stats )
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_added",value ="activity")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_filled",value ="activity")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :BoundLogger )->pd .DataFrame :
-        "Convert data types according to the Pandera schema."
-        df =df .copy ()
-        non_nullable_int_fields ={"activity_id":"int64"}
-        nullable_int_fields ={"row_index":"Int64","target_tax_id":"int64","assay_tax_id":"int64","record_id":"int64","src_id":"int64"}
-        float_fields ={"standard_value":"float64","standard_upper_value":"float64","pchembl_value":"float64","upper_value":"float64","lower_value":"float64"}
-        bool_fields =["potential_duplicate","curated","removed"]
-        binary_flag_fields =["standard_flag"]
-        for field in non_nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_int :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_int .astype ("Int64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field =="row_index":
-                    numeric_series_row :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=numeric_series_row .astype ("Int64")
-                    if df [field ].isna ().any ():
-                        df [field ]=range (len (df ))
-                else :
-                    nullable_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=nullable_numeric_series .astype ("Int64")
-                    mask_valid =df [field ].notna ()
-                    if mask_valid .any ():
-                        invalid_mask =mask_valid &(df [field ]<1 )
-                        if invalid_mask .any ():
-                            log .warning ("invalid_positive_integer",field =field ,count =int (invalid_mask .sum ()))
-                            df .loc [invalid_mask ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in float_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_float :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_float .astype ("float64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in bool_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field in ("curated","removed")and df [field ].dtype =="boolean":
-                    continue
-                if field in ("curated","removed"):
-                    df [field ]=df [field ].astype ("boolean")
-                else :
-                    bool_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=(bool_numeric_series !=0 ).astype ("boolean")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("bool_conversion_failed",field =field ,error =str (exc ))
-        for field in binary_flag_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_flag :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_flag .astype ("Int64")
-                mask_valid =df [field ].notna ()
-                if mask_valid .any ():
-                    valid_values =df .loc [mask_valid ,field ]
-                    invalid_valid_mask =~valid_values .isin ([0 ,1 ])
-                    if invalid_valid_mask .any ():
-                        invalid_index =valid_values .index [invalid_valid_mask ]
-                        log .warning ("invalid_standard_flag",field =field ,count =int (invalid_valid_mask .sum ()))
-                        df .loc [invalid_index ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        object_fields =["value","activity_properties"]
-        for field in object_fields :
-            if field in df .columns :
-                if df [field ].dtype !="object":
-                    df [field ]=df [field ].astype ("object")
-        return df
-    def _validate_foreign_keys (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Validate foreign key integrity and format of ChEMBL IDs."
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        chembl_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        warnings :list [str ]=[]
-        for field in chembl_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-                if invalid_mask .any ():
-                    warning_msg :str =f'{field }: {int (invalid_mask .sum ())} invalid format(s)'
-                    warnings .append (warning_msg )
-        if warnings :
-            log .warning ("foreign_key_validation",warnings =warnings )
-        return df
-    def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check uniqueness of activity_id before validation."
-        if "activity_id"not in df .columns :
-            log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-            return
-        duplicates =df [df ["activity_id"].duplicated (keep =False )]
-        if not duplicates .empty :
-            duplicate_count =len (duplicates )
-            duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-            log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-            msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-            raise ValueError (msg )
-        log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
-    def _validate_data_validity_comment_soft_enum (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Soft enum \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0434\u043b\u044f data_validity_comment.\n\n        \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0442\u0438\u0432 whitelist \u0438\u0437 \u043a\u043e\u043d\u0444\u0438\u0433\u0430. \u041d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n        \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a warning, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e (soft enum).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        "
-        if df .empty or "data_validity_comment"not in df .columns :
-            return
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if not whitelist :
-            return
-        series_candidate =df ["data_validity_comment"].dropna ()
-        if len (series_candidate )==0 :
-            return
-        non_null_comments_series =series_candidate .astype ("string")
-        whitelist_set :set [str ]=set (whitelist )
-        def _is_unknown (value :str )->bool :
-            return value not in whitelist_set
-        unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-        unknown_count =int (unknown_mask .sum ())
-        if unknown_count >0 :
-            unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-            log .warning ("soft_enum_unknown_data_validity_comment",unknown_count =unknown_count ,total_count =len (non_null_comments_series ),samples =unknown_values ,whitelist =whitelist ,message ="Unknown data_validity_comment values detected (soft enum: not blocking)")
-    def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-        reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        errors :list [str ]=[]
-        for field in reference_fields :
-            if field not in df .columns :
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-                continue
-            mask =df [field ].notna ()
-            if not mask .any ():
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-                continue
-            invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-                errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-                log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-        if errors :
-            log .error ("foreign_key_integrity_check_failed",errors =errors )
-            msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-            raise ValueError (msg )
-        log .debug ("foreign_key_integrity_verified")
-    def _log_detailed_validation_errors (self ,failure_cases :pd .DataFrame ,payload :pd .DataFrame ,log :BoundLogger )->None :
-        "Log individual validation errors with row index and activity_id."
-        if failure_cases .empty or payload .empty :
-            return
-        activity_id_col ="activity_id"if "activity_id"in payload .columns else None
-        index_col ="index"if "index"in failure_cases .columns else None
-        if index_col is None :
-            return
-        max_errors =20
-        errors_to_log =failure_cases .head (max_errors )
-        for _ ,error_row in errors_to_log .iterrows ():
-            row_index =error_row .get (index_col )
-            if row_index is None :
-                continue
-            error_details :dict [str ,Any ]={"row_index":int (row_index )if isinstance (row_index ,(int ,float ))else str (row_index )}
-            if activity_id_col :
-                try :
-                    idx =int (row_index )if isinstance (row_index ,(int ,float ))else row_index
-                    activity_id_value :Any =payload .at [cast (int ,idx ),activity_id_col ]
-                    activity_id =activity_id_value
-                except (KeyError ,IndexError ):
-                    activity_id =None
-                if activity_id is not None and pd .notna (activity_id ):
-                    error_details ["activity_id"]=int (activity_id )if isinstance (activity_id ,(int ,float ))else str (activity_id )
-            if "column"in error_row and pd .notna (error_row ["column"]):
-                error_details ["column"]=str (error_row ["column"])
-            if "schema_context"in error_row and pd .notna (error_row ["schema_context"]):
-                error_details ["schema_context"]=str (error_row ["schema_context"])
-            if "failure_case"in error_row and pd .notna (error_row ["failure_case"]):
-                error_details ["failure_case"]=str (error_row ["failure_case"])
-            log .error ("validation_error_detail",**error_details )
-        if len (failure_cases )>max_errors :
-            log .warning ("validation_errors_truncated",total_errors =len (failure_cases ),logged_errors =max_errors )
-    def build_quality_report (self ,df :pd .DataFrame )->pd .DataFrame |dict [str ,object ]|None :
-        "Return QC report with activity-specific metrics including distributions."
-        business_key =["activity_id"]if "activity_id"in df .columns else None
-        base_report =build_default_quality_report (df ,business_key_fields =business_key )
-        rows :list [dict [str ,Any ]]=[]
-        if not base_report .empty :
-            records_raw =base_report .to_dict ("records")
-            records :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],records_raw )
-            for record in records :
-                rows .append ({str (k ):v for k ,v in record .items ()})
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        foreign_key_fields =["assay_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"]
-        for field in foreign_key_fields :
-            if field in df .columns :
-                mask =df [field ].notna ()
-                if mask .any ():
-                    string_series =df [field ].astype (str )
-                    valid_mask =mask &string_series .str .match (chembl_id_pattern .pattern ,na =False )
-                    invalid_count =int ((mask &~valid_mask ).astype (int ).sum ())
-                    valid_count =int (valid_mask .astype (int ).sum ())
-                    total_count =int (mask .astype (int ).sum ())
-                    integrity_ratio =float (valid_count /total_count )if total_count >0 else 0.0
-                    rows .append ({"section":"foreign_key","metric":"integrity_ratio","column":field ,"value":float (integrity_ratio ),"valid_count":int (valid_count ),"invalid_count":int (invalid_count ),"total_count":int (total_count )})
-        if "standard_type"in df .columns :
-            type_counts_series :pd .Series [Any ]=df ["standard_type"].value_counts ()
-            type_dist_raw =type_counts_series .to_dict ()
-            type_dist :dict [Any ,int ]=cast (dict [Any ,int ],type_dist_raw )
-            for type_value ,count in type_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_type_count","column":"standard_type","value":str (type_value )if type_value is not None else "null","count":int (count )})
-        if "standard_units"in df .columns :
-            unit_counts_series :pd .Series [Any ]=df ["standard_units"].value_counts ()
-            unit_dist_raw =unit_counts_series .to_dict ()
-            unit_dist :dict [Any ,int ]=cast (dict [Any ,int ],unit_dist_raw )
-            for unit_value ,count in unit_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_units_count","column":"standard_units","value":str (unit_value )if unit_value is not None else "null","count":int (count )})
-        return pd .DataFrame (rows )
-    def write (self ,df :pd .DataFrame ,output_path :Path ,*,extended :bool =False ,include_correlation :bool |None =None ,include_qc_metrics :bool |None =None )->RunResult :
-        "Override write() to bind actor and ensure deterministic sorting.\n\n        Parameters\n        ----------\n        df:\n            The DataFrame to write.\n        output_path:\n            The base output path for all artifacts.\n        extended:\n            Whether to include extended QC artifacts.\n\n        Returns\n        -------\n        RunResult:\n            All artifacts generated by the write operation.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.write')
-        UnifiedLogger .bind (actor =self .actor )
-        sort_keys =["assay_chembl_id","testitem_chembl_id","activity_id"]
-        if df .empty or not all ((key in df .columns for key in sort_keys )):
-            return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-        original_sort_by =self .config .determinism .sort .by
-        if not original_sort_by or original_sort_by !=sort_keys :
-            from copy import deepcopy
-            from bioetl .config .models .determinism import DeterminismSortingConfig
-            modified_config =deepcopy (self .config )
-            modified_config .determinism .sort =DeterminismSortingConfig (by =sort_keys ,ascending =[True ,True ,True ],na_position ="last")
-            log .debug ("write_sort_config_set",sort_keys =sort_keys ,original_sort_keys =list (original_sort_by )if original_sort_by else [])
-            original_config =self .config
-            self .config =modified_config
-            try :
-                result =super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-            finally :
-                self .config =original_config
-            return result
-        return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
```

#### Горячий участок 2

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:111-115
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,5 +0,0 @@

-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
```

#### Горячий участок 3

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2901-2924
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,18 +0,0 @@

-def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_added",value ="activity")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_filled",value ="activity")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Горячий участок 4

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:343-415
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,22 +0,0 @@

-def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-    "Construct the descriptor driving the shared extraction template."
-    def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-        http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-        chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-        typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =source_config .parameters .select_fields
-        return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-    def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-    def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        df =pipeline ._ensure_comment_fields (df ,log )
-        chembl_client =cast (ChemblClient ,context .chembl_client )
-        if chembl_client is not None :
-            df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-        pipeline ._log_validity_comments_metrics (df ,log )
-        return df
-    def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        return pipeline ._materialize_activity_record (payload )
-    return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
```

#### Горячий участок 5

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1257-1267
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,7 +0,0 @@

-def _cache_directory (self ,release :str |None )->Path :
-    cache_root =Path (self .config .paths .cache_root )
-    directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-    release_component =self ._sanitize_cache_component (release or "unknown")
-    pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-    version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-    return cache_root /directory_name /pipeline_component /release_component /version_component
```

#### Горячий участок 6

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1252-1255
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,4 +0,0 @@

-def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-    directory =self ._cache_directory (release )
-    cache_key =self ._cache_key (batch_ids ,release )
-    return directory /f'{cache_key }.json'
```

#### Горячий участок 7

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1269-1277
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,4 +0,0 @@

-def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-    payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-    raw =json .dumps (payload ,sort_keys =True )
-    return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
```

#### Горячий участок 8

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:3108-3131
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,13 +0,0 @@

-def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check uniqueness of activity_id before validation."
-    if "activity_id"not in df .columns :
-        log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-        return
-    duplicates =df [df ["activity_id"].duplicated (keep =False )]
-    if not duplicates .empty :
-        duplicate_count =len (duplicates )
-        duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-        log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-        msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-        raise ValueError (msg )
-    log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
```

#### Горячий участок 9

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1176-1221
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,33 +0,0 @@

-def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-    cache_config =self .config .cache
-    if not cache_config .enabled :
-        return None
-    normalized_ids =[str (identifier )for identifier in batch_ids ]
-    cache_file =self ._cache_file_path (normalized_ids ,release )
-    if not cache_file .exists ():
-        return None
-    try :
-        stat =cache_file .stat ()
-    except OSError :
-        return None
-    ttl_seconds =int (cache_config .ttl )
-    if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    try :
-        payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-    except (OSError ,json .JSONDecodeError ):
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    if not isinstance (payload ,dict ):
-        return None
-    missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-    if missing :
-        return None
-    return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
```

#### Горячий участок 10

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:3185-3232
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,24 +0,0 @@

-def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-    reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-    chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-    errors :list [str ]=[]
-    for field in reference_fields :
-        if field not in df .columns :
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-            continue
-        mask =df [field ].notna ()
-        if not mask .any ():
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-            continue
-        invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-        if invalid_mask .any ():
-            invalid_count =int (invalid_mask .sum ())
-            invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-            errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-            log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-    if errors :
-        log .error ("foreign_key_integrity_check_failed",errors =errors )
-        msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-        raise ValueError (msg )
-    log .debug ("foreign_key_integrity_verified")
```

#### Горячий участок 11

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:432-450
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,14 +0,0 @@

-def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-    "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-    if isinstance (dataset ,pd .Series ):
-        return dataset .to_frame (name ="activity_id")
-    if isinstance (dataset ,pd .DataFrame ):
-        return dataset
-    if isinstance (dataset ,Mapping ):
-        mapping =cast (Mapping [str ,Any ],dataset )
-        return pd .DataFrame ([dict (mapping )])
-    if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-        dataset_list :list [Any ]=list (dataset )
-        return pd .DataFrame ({"activity_id":dataset_list })
-    msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-    raise TypeError (msg )
```

#### Горячий участок 12

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2065-2068
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,5 +0,0 @@

-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Горячий участок 13

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:498-637
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,83 +0,0 @@

-def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-    "Iterate over IDs using the shared iterator while preserving cache semantics."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    records :list [dict [str ,Any ]]=[]
-    success_count =0
-    fallback_count =0
-    error_count =0
-    cache_hits =0
-    api_calls =0
-    total_batches =0
-    key_order =[key for _ ,key in normalized_ids ]
-    key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-    for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-        total_batches +=1
-        batch_start =time .perf_counter ()
-        from_cache =False
-        chunk_records :dict [str ,dict [str ,Any ]]={}
-        try :
-            cached_records =self ._check_cache (chunk ,self ._chembl_release )
-            if cached_records is not None :
-                from_cache =True
-                cache_hits +=len (chunk )
-                chunk_records =cached_records
-            else :
-                api_calls +=1
-                fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                for item in fetched_items :
-                    if not isinstance (item ,Mapping ):
-                        continue
-                    activity_value =item .get ("activity_id")
-                    if activity_value is None :
-                        continue
-                    chunk_records [str (activity_value )]=dict (item )
-                self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-            success_in_batch =0
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                record =chunk_records .get (key )
-                if record and (not record .get ("error")):
-                    materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                    records .append (materialized )
-                    success_count +=1
-                    success_in_batch +=1
-                else :
-                    fallback_record =self ._create_fallback_record (numeric_id )
-                    records .append (fallback_record )
-                    fallback_count +=1
-                    error_count +=1
-            batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-            log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-        except CircuitBreakerOpenError as exc :
-            log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except RequestException as exc :
-            log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except Exception as exc :
-            log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-    total_records =len (normalized_ids )
-    success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-    summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-    return (records ,summary )
```

#### Горячий участок 14

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1284-1323
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,16 +0,0 @@

-def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-    "Create fallback record enriched with error metadata."
-    base_message ="Fallback: ChEMBL activity unavailable"
-    message =f'{base_message } ({error })'if error else base_message
-    timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-    metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-    if isinstance (error ,RequestException ):
-        response =getattr (error ,"response",None )
-        status_code =getattr (response ,"status_code",None )
-        if status_code is not None :
-            metadata ["http_status"]=status_code
-        metadata ["error_message"]=str (error )
-    elif error is not None :
-        metadata ["error_message"]=str (error )
-    fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-    return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
```

#### Горячий участок 15

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2843-2899
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,15 +0,0 @@

-def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-    "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-    seen :set [tuple [Any ,...]]=set ()
-    deduplicated :list [dict [str ,Any ]]=[]
-    duplicates_removed =0
-    for prop in properties :
-        dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-        if dedup_key not in seen :
-            seen .add (dedup_key )
-            deduplicated .append (prop )
-        else :
-            duplicates_removed +=1
-            log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-    stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-    return (deduplicated ,stats )
```

#### Горячий участок 16

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:892-935
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,28 +0,0 @@

-def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    assay_section :Any =enrich_section .get ("assay")
-                    if isinstance (assay_section ,Mapping ):
-                        assay_section =cast (Mapping [str ,Any ],assay_section )
-                        enrich_cfg =dict (assay_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_assay (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 17

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:822-867
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,28 +0,0 @@

-def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    compound_record_section :Any =enrich_section .get ("compound_record")
-                    if isinstance (compound_record_section ,Mapping ):
-                        compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                        enrich_cfg =dict (compound_record_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 18

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:960-1014
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,32 +0,0 @@

-def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    data_validity_section :Any =enrich_section .get ("data_validity")
-                    if isinstance (data_validity_section ,Mapping ):
-                        data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                        enrich_cfg =dict (data_validity_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    if "data_validity_description"in df .columns :
-        non_na_count =int (df ["data_validity_description"].notna ().sum ())
-        if non_na_count >0 :
-            log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 19

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1039-1108
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,37 +0,0 @@

-def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    molecule_section :Any =enrich_section .get ("molecule")
-                    if isinstance (molecule_section ,Mapping ):
-                        molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                        enrich_cfg =dict (molecule_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-    if "molecule_name"in df_join .columns :
-        if "molecule_pref_name"not in df .columns :
-            df ["molecule_pref_name"]=pd .NA
-        mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-        if mask .any ():
-            df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-            df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-            log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-    return df
```

#### Горячий участок 20

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1325-1358
- target: нет в ветке

```diff
--- activity:run.py

+++ target:run.py

@@ -1,11 +0,0 @@

-def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-    if df .empty :
-        return df
-    required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-    missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-    if missing_fields :
-        for field in missing_fields :
-            df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-        log .debug ("comment_fields_ensured",fields =missing_fields )
-    return df
```

### Модуль transform.py

Определение                              | activity сигнатура | target сигнатура              | Побочные эффекты                                           | Исключения              | Статус         
-----------------------------------------|--------------------|-------------------------------|------------------------------------------------------------|-------------------------|----------------
__module_block_0                         | —                  | —                             | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
__module_block_1                         | —                  | —                             | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
__module_block_10                        | —                  | —                             | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
__module_block_2                         | —                  | —                             | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
__module_block_3                         | —                  | —                             | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
__module_block_4                         | —                  | —                             | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
__module_block_5                         | —                  | —                             | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
__module_block_6                         | —                  | —                             | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
__module_block_7                         | —                  | —                             | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
__module_block_8                         | —                  | —                             | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
__module_block_9                         | —                  | —                             | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
_collect_dicts                           | —                  | source: Any                   | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
_is_iterable_of_objects                  | —                  | value: Any                    | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
_is_json_dict                            | —                  | value: Any                    | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
extract_and_serialize_component_synonyms | —                  | target_components: Any        | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target
flatten_target_components                | —                  | rec: dict[str, Any]           | activity: {}
target: {'logging': [], 'io': ['json.dumps']} | activity: []
target: [] | только в target
serialize_target_arrays                  | —                  | df: pd.DataFrame, config: Any | activity: {}
target: {'logging': [], 'io': []}             | activity: []
target: [] | только в target

#### Горячий участок 1

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:1-1

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+"Transform utilities for ChEMBL target pipeline array serialization."
```

#### Горячий участок 2

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:3-3

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+from __future__ import annotations
```

#### Горячий участок 3

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:22-22

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+JsonDict =dict [str ,Any ]
```

#### Горячий участок 4

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:13-13

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+from bioetl .core .serialization import header_rows_serialize
```

#### Горячий участок 5

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:6-6

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+from collections .abc import Iterable
```

#### Горячий участок 6

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:7-7

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+from typing import Any ,TypeGuard ,cast
```

#### Горячий участок 7

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:5-5

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+import json
```

#### Горячий участок 8

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:9-9

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+import numpy as np
```

#### Горячий участок 9

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:10-10

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+import numpy .typing as npt
```

#### Горячий участок 10

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:11-11

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+import pandas as pd
```

#### Горячий участок 11

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:15-19

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+__all__ =["serialize_target_arrays","extract_and_serialize_component_synonyms","flatten_target_components"]
```

#### Горячий участок 12

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:33-47

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1,12 @@

+def _collect_dicts (source :Any )->list [JsonDict ]:
+    "Collect dictionary entries from arbitrary source keeping order."
+    result :list [JsonDict ]=[]
+    if _is_json_dict (source ):
+        result .append (source )
+        return result
+    if _is_iterable_of_objects (source ):
+        for element in source :
+            element_any :Any =element
+            if _is_json_dict (element_any ):
+                result .append (element_any )
+    return result
```

#### Горячий участок 13

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:29-30

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1,2 @@

+def _is_iterable_of_objects (value :Any )->TypeGuard [Iterable [Any ]]:
+    return isinstance (value ,Iterable )and (not isinstance (value ,(str ,bytes )))
```

#### Горячий участок 14

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:25-26

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1,2 @@

+def _is_json_dict (value :Any )->TypeGuard [JsonDict ]:
+    return isinstance (value ,dict )
```

#### Горячий участок 15

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:133-162

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1,15 @@

+def extract_and_serialize_component_synonyms (target_components :Any )->str :
+    "Extract target_component_synonyms from target_components and serialize.\n\n    Parameters\n    ----------\n    target_components:\n        List of target component dicts, None, or empty list.\n\n    Returns\n    -------\n    str:\n        Serialized string in header+rows format, or empty string for None/empty.\n    "
+    if target_components is None :
+        return ""
+    components :list [dict [str ,Any ]]=_collect_dicts (target_components )
+    if not components :
+        return ""
+    all_synonyms :list [dict [str ,Any ]]=[]
+    for component in components :
+        syns_item :Any =component .get ("target_component_synonyms")
+        if syns_item :
+            all_synonyms .extend (_collect_dicts (syns_item ))
+    if not all_synonyms :
+        return ""
+    return header_rows_serialize (all_synonyms )
```

#### Горячий участок 16

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:50-130

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1,34 @@

+def flatten_target_components (rec :dict [str ,Any ])->dict [str ,Any ]:
+    "Flatten nested target_components data into flat columns.\n\n    Extracts:\n    - uniprot_accessions from target_components[*].accession\n    - target_component_synonyms__flat from target_components[*].target_component_synonyms[*].component_synonym\n    - target_components__flat (serialized container)\n    - cross_references__flat (serialized from top-level)\n    - component_count (counted from accessions or from top-level)\n\n    Parameters\n    ----------\n    rec:\n        Target record dict from ChEMBL API.\n\n    Returns\n    -------\n    dict[str, Any]:\n        Dictionary with flattened fields:\n        - uniprot_accessions: sorted list of unique UniProt accessions (as JSON string)\n        - target_component_synonyms__flat: serialized synonyms\n        - target_components__flat: serialized components\n        - cross_references__flat: serialized cross-references\n        - component_count: count of unique accessions\n    "
+    result :dict [str ,Any ]={"uniprot_accessions":"","target_component_synonyms__flat":"","target_components__flat":"","cross_references__flat":"","component_count":None }
+    comps_raw :Any =rec .get ("target_components")or []
+    comps :list [dict [str ,Any ]]=_collect_dicts (comps_raw )
+    accessions :list [str ]=[]
+    all_synonyms :list [dict [str ,Any ]]=[]
+    for component in comps :
+        accession :Any =component .get ("accession")
+        if isinstance (accession ,str )and accession .strip ():
+            accessions .append (accession .strip ())
+        syns :Any =component .get ("target_component_synonyms")
+        if syns :
+            all_synonyms .extend (_collect_dicts (syns ))
+    unique_accessions =sorted (set (accessions ))
+    if unique_accessions :
+        result ["uniprot_accessions"]=json .dumps (unique_accessions ,ensure_ascii =False )
+        result ["component_count"]=len (unique_accessions )
+    else :
+        top_level_count =rec .get ("component_count")
+        if top_level_count is not None :
+            try :
+                result ["component_count"]=int (top_level_count )
+            except (ValueError ,TypeError ):
+                result ["component_count"]=None
+    if all_synonyms :
+        result ["target_component_synonyms__flat"]=header_rows_serialize (all_synonyms )
+    if comps :
+        result ["target_components__flat"]=header_rows_serialize (comps )
+    xrefs_raw :Any =rec .get ("cross_references")or []
+    xrefs :list [dict [str ,Any ]]=_collect_dicts (xrefs_raw )
+    if xrefs :
+        result ["cross_references__flat"]=header_rows_serialize (xrefs )
+    return result
```

#### Горячий участок 17

- activity: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:165-251

```diff
--- activity:transform.py

+++ target:transform.py

@@ -0,0 +1,46 @@

+def serialize_target_arrays (df :pd .DataFrame ,config :Any )->pd .DataFrame :
+    "Serialize array fields for target pipeline.\n\n    Uses flatten_target_components() to extract and serialize nested data\n    from target_components and cross_references.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    config:\n        Pipeline config with transform.arrays_to_header_rows.\n\n    Returns\n    -------\n    pd.DataFrame:\n        DataFrame with serialized array fields.\n    "
+    df =df .copy ()
+    arrays_to_serialize :list [str ]=[]
+    try :
+        if hasattr (config ,"transform")and config .transform is not None :
+            if hasattr (config .transform ,"arrays_to_header_rows"):
+                arrays_to_serialize =list (config .transform .arrays_to_header_rows )
+    except (AttributeError ,TypeError ):
+        pass
+    if not df .empty :
+        flattened_data :list [dict [str ,Any ]]=[]
+        for _ ,row in df .iterrows ():
+            row_dict :dict [str ,Any ]=row .to_dict ()
+            for key ,value in row_dict .items ():
+                if isinstance (value ,np .ndarray ):
+                    array_value =cast (npt .NDArray [Any ],value )
+                    if array_value .size ==0 :
+                        row_dict [key ]=None
+                    else :
+                        try :
+                            nan_mask =np .asarray (pd .isna (array_value ),dtype =bool )
+                            if bool (np .all (nan_mask )):
+                                row_dict [key ]=None
+                        except (TypeError ,ValueError ):
+                            pass
+                elif pd .api .types .is_scalar (value ):
+                    try :
+                        if pd .isna (value ):
+                            row_dict [key ]=None
+                    except (TypeError ,ValueError ):
+                        pass
+            flattened =flatten_target_components (row_dict )
+            row_dict .update (flattened )
+            flattened_data .append (row_dict )
+        df =pd .DataFrame (flattened_data )
+    else :
+        df ["cross_references__flat"]=""
+        df ["target_components__flat"]=""
+        df ["target_component_synonyms__flat"]=""
+        df ["uniprot_accessions"]=""
+        df ["component_count"]=None
+    for col in arrays_to_serialize :
+        if col in df .columns and f'{col }__flat'in df .columns :
+            df =df .drop (columns =[col ])
+    return df
```

### Модуль normalize.py

Определение                 | activity сигнатура                                                           | target сигнатура | Побочные эффекты                                                                                          | Исключения              | Статус           
----------------------------|------------------------------------------------------------------------------|------------------|-----------------------------------------------------------------------------------------------------------|-------------------------|------------------
__module_block_0            | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_1            | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_10           | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_11           | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_12           | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_13           | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_14           | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_15           | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_16           | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_2            | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_3            | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_4            | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_5            | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_6            | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_7            | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_8            | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
__module_block_9            | —                                                                            | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
_enrich_by_pairs            | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any], log: Any | —                | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
target: {}                      | activity: []
target: [] | только в activity
_enrich_by_record_id        | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any], log: Any | —                | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
target: {}                      | activity: []
target: [] | только в activity
_extract_first_present      | record: Mapping[str, Any], keys: Iterable[str]                               | —                | activity: {'logging': [], 'io': []}
target: {}                                                            | activity: []
target: [] | только в activity
enrich_with_assay           | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
target: {} | activity: []
target: [] | только в activity
enrich_with_compound_record | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
target: {} | activity: []
target: [] | только в activity
enrich_with_data_validity   | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
target: {} | activity: []
target: [] | только в activity

_Показаны первые 20 горячих участков из 23._

#### Горячий участок 1

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:1-1
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-"Enrichment functions for Activity pipeline."
```

#### Горячий участок 2

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:3-3
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from __future__ import annotations
```

#### Горячий участок 3

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:9-9
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-import pandas as pd
```

#### Горячий участок 4

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:21-21
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-__all__ =["enrich_with_assay","enrich_with_compound_record","enrich_with_data_validity"]
```

#### Горячий участок 5

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:24-24
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-_ensure_columns =ensure_columns
```

#### Горячий участок 6

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:27-35
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-_COMPOUND_FIELD_ALIASES :dict [str ,tuple [str ,...]]={"compound_name":("compound_name","pref_name","PREF_NAME"),"compound_key":("compound_key","standard_inchi_key","STANDARD_INCHI_KEY"),"curated":("curated","CURATED")}
```

#### Горячий участок 7

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:37-40
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-_ASSAY_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_organism","string"),("assay_tax_id","Int64"))
```

#### Горячий участок 8

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:42-47
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-_COMPOUND_COLUMNS :tuple [tuple [str ,str ],...]=(("compound_name","string"),("compound_key","string"),("curated","boolean"),("removed","boolean"))
```

#### Горячий участок 9

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:49-49
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-_DATA_VALIDITY_COLUMNS :tuple [tuple [str ,str ],...]=(("data_validity_description","string"),)
```

#### Горячий участок 10

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:12-12
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .clients .chembl import ChemblClient
```

#### Горячий участок 11

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:13-13
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .frame import ensure_columns
```

#### Горячий участок 12

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:14-14
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .logger import UnifiedLogger
```

#### Горячий участок 13

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:15-19
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .schemas .activity import ASSAY_ENRICHMENT_SCHEMA ,COMPOUND_RECORD_ENRICHMENT_SCHEMA ,DATA_VALIDITY_ENRICHMENT_SCHEMA
```

#### Горячий участок 14

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:5-5
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from collections .abc import Iterable ,Mapping
```

#### Горячий участок 15

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:10-10
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from pandas import Series
```

#### Горячий участок 16

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:6-6
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from typing import Any
```

#### Горячий участок 17

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:8-8
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-import numpy as np
```

#### Горячий участок 18

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:439-615
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1,65 +0,0 @@

-def _enrich_by_pairs (df_act :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ],log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame activity \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u044b (molecule_chembl_id, document_chembl_id)."
-    pairs_df =df_act [["molecule_chembl_id","document_chembl_id"]].astype ("string").copy ()
-    for column in pairs_df .columns :
-        pairs_df [column ]=pairs_df [column ].str .strip ().str .upper ()
-    pairs_df =pairs_df .dropna ()
-    pairs_df =pairs_df .drop_duplicates ()
-    pairs :set [tuple [str ,str ]]=set (map (tuple ,pairs_df .to_numpy ()))
-    if not pairs :
-        log .debug ("enrichment_by_pairs_skipped_no_valid_pairs")
-        return df_act
-    fields =cfg .get ("fields",["molecule_chembl_id","document_chembl_id","compound_name","compound_key","curated"])
-    page_limit =cfg .get ("page_limit",1000 )
-    log .info ("enrichment_fetching_compound_records_by_pairs",pairs_count =len (pairs ))
-    compound_records_dict :dict [tuple [str ,str ],dict [str ,Any ]]={}
-    try :
-        compound_records_dict =client .fetch_compound_records_by_pairs (pairs =pairs ,fields =list (fields ),page_limit =page_limit )or {}
-    except Exception as exc :
-        log .warning ("enrichment_fetch_error_by_pairs",pairs_count =len (pairs ),error =str (exc ),exc_info =True )
-        return df_act
-    enrichment_data :list [dict [str ,Any ]]=[]
-    pairs_found =0
-    pairs_not_found =0
-    for pair in pairs :
-        compound_record :dict [str ,Any ]|None =compound_records_dict .get (pair )
-        if compound_record :
-            record_mapping :Mapping [str ,Any ]=compound_record
-            compound_name_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("compound_name",("compound_name",)))
-            compound_key_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("compound_key",("compound_key",)))
-            curated_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("curated",("curated",)))
-            compound_name =None
-            if compound_name_raw is not None :
-                name_str =str (compound_name_raw ).strip ()
-                compound_name =name_str if name_str else None
-            compound_key =None
-            if compound_key_raw is not None :
-                key_str =str (compound_key_raw ).strip ()
-                compound_key =key_str if key_str else None
-            curated =curated_raw if curated_raw is not None else None
-            pairs_found +=1
-            enrichment_data .append ({"molecule_chembl_id":pair [0 ],"document_chembl_id":pair [1 ],"compound_name":compound_name ,"compound_key":compound_key ,"curated":curated ,"removed":None })
-        else :
-            pairs_not_found +=1
-            enrichment_data .append ({"molecule_chembl_id":pair [0 ],"document_chembl_id":pair [1 ],"compound_name":None ,"compound_key":None ,"curated":None ,"removed":None })
-    log .info ("enrichment_by_pairs_complete",pairs_requested =len (pairs ),pairs_found =pairs_found ,pairs_not_found =pairs_not_found ,records_returned =len (compound_records_dict ))
-    if pairs_not_found >0 :
-        log .warning ("enrichment_by_pairs_some_pairs_not_found",pairs_not_found =pairs_not_found ,pairs_total =len (pairs ),hint ="\u041f\u0440\u043e\u0432\u0435\u0440\u044c\u0442\u0435, \u0447\u0442\u043e \u043f\u0430\u0440\u044b (molecule_chembl_id, document_chembl_id) \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 ChEMBL API")
-    if not enrichment_data :
-        log .debug ("enrichment_by_pairs_no_records_found")
-        return df_act
-    df_enrich =pd .DataFrame (enrichment_data )
-    df_enrich ["molecule_chembl_id"]=df_enrich ["molecule_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_enrich ["document_chembl_id"]=df_enrich ["document_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_act ["molecule_chembl_id_normalized"]=df_act ["molecule_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_act ["document_chembl_id_normalized"]=df_act ["document_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_result =df_act .merge (df_enrich ,left_on =["molecule_chembl_id_normalized","document_chembl_id_normalized"],right_on =["molecule_chembl_id","document_chembl_id"],how ="left",suffixes =("","_enrich"))
-    df_result =df_result .drop (columns =["molecule_chembl_id_normalized","document_chembl_id_normalized"])
-    for col in ["compound_name","compound_key","curated"]:
-        if f'{col }_enrich'in df_result .columns :
-            if col not in df_result .columns :
-                df_result [col ]=df_result [f'{col }_enrich']
-            else :
-                df_result [col ]=df_result [col ].where (df_result [col ].notna (),df_result [f'{col }_enrich'])
-            df_result =df_result .drop (columns =[f'{col }_enrich'])
-    return df_result
```

#### Горячий участок 19

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:618-755
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1,67 +0,0 @@

-def _enrich_by_record_id (df_act :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ],log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame activity \u0447\u0435\u0440\u0435\u0437 record_id (fallback \u0434\u043b\u044f \u0441\u0442\u0440\u043e\u043a \u0431\u0435\u0437 document_chembl_id)."
-    record_ids :set [str ]=set ()
-    for _ ,row in df_act .iterrows ():
-        rec =row .get ("record_id")
-        if rec is not None and (not pd .isna (rec )):
-            rec_s =str (rec ).strip ()
-            if rec_s :
-                record_ids .add (rec_s )
-    if not record_ids :
-        log .debug ("enrichment_by_record_id_skipped_no_valid_ids")
-        return df_act
-    fields =["record_id","compound_name","compound_key"]
-    page_limit =cfg .get ("page_limit",1000 )
-    batch_size =int (cfg .get ("batch_size",100 ))or 100
-    log .info ("enrichment_fetching_compound_records_by_record_id",record_ids_count =len (record_ids ))
-    compound_records_dict :dict [str ,dict [str ,Any ]]={}
-    try :
-        unique_ids =list (record_ids )
-        all_records :list [dict [str ,Any ]]=[]
-        for i in range (0 ,len (unique_ids ),batch_size ):
-            chunk =unique_ids [i :i +batch_size ]
-            params :dict [str ,Any ]={"record_id__in":",".join (chunk ),"limit":page_limit ,"only":",".join (fields ),"order_by":"record_id"}
-            try :
-                for record in client .paginate ("/compound_record.json",params =params ,page_size =page_limit ,items_key ="compound_records"):
-                    all_records .append (dict (record ))
-            except Exception as exc :
-                log .warning ("enrichment_fetch_error_by_record_id",chunk_size =len (chunk ),error =str (exc ),exc_info =True )
-        for record in all_records :
-            rid_raw =record .get ("record_id")
-            if rid_raw is None :
-                continue
-            rid_str =str (rid_raw ).strip ()
-            if rid_str and rid_str not in compound_records_dict :
-                compound_records_dict [rid_str ]={"record_id":rid_str ,"compound_key":record .get ("compound_key"),"compound_name":record .get ("compound_name")}
-    except Exception as exc :
-        log .warning ("enrichment_fetch_error_by_record_id",record_ids_count =len (record_ids ),error =str (exc ),exc_info =True )
-        return df_act
-    if not compound_records_dict :
-        log .debug ("enrichment_by_record_id_no_records_found")
-        return df_act
-    compound_data :list [dict [str ,Any ]]=[]
-    for record_id ,record in compound_records_dict .items ():
-        compound_data .append ({"record_id":record_id ,"compound_key":record .get ("compound_key"),"compound_name":record .get ("compound_name")})
-    df_compound =pd .DataFrame (compound_data )if compound_data else pd .DataFrame (columns =["record_id","compound_key","compound_name"])
-    df_act_normalized =df_act .copy ()
-    if "record_id"in df_act_normalized .columns :
-        mask_na =df_act_normalized ["record_id"].isna ()
-        df_act_normalized ["record_id"]=df_act_normalized ["record_id"].astype (str )
-        df_act_normalized .loc [df_act_normalized ["record_id"]=="nan","record_id"]=pd .NA
-        df_act_normalized .loc [mask_na ,"record_id"]=pd .NA
-        if "record_id"in df_compound .columns and (not df_compound .empty ):
-            df_compound ["record_id"]=df_compound ["record_id"].astype (str )
-            df_compound .loc [df_compound ["record_id"]=="nan","record_id"]=pd .NA
-    df_result =df_act_normalized .merge (df_compound ,on =["record_id"],how ="left",suffixes =("","_compound"))
-    for col in ["compound_name","compound_key"]:
-        if f'{col }_compound'in df_result .columns :
-            if col not in df_result .columns :
-                df_result [col ]=df_result [f'{col }_compound']
-            else :
-                df_result [col ]=df_result [col ].where (df_result [col ].notna (),df_result [f'{col }_compound'])
-            df_result =df_result .drop (columns =[f'{col }_compound'])
-    if "curated"not in df_result .columns :
-        df_result ["curated"]=pd .NA
-    if "removed"not in df_result .columns :
-        df_result ["removed"]=pd .NA
-    return df_result
```

#### Горячий участок 20

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:52-64
- target: нет в ветке

```diff
--- activity:normalize.py

+++ target:normalize.py

@@ -1,11 +0,0 @@

-def _extract_first_present (record :Mapping [str ,Any ],keys :Iterable [str ])->Any :
-    "\u0412\u043e\u0437\u0432\u0440\u0430\u0442\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u043e \u043f\u0435\u0440\u0432\u043e\u043c\u0443 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u043c\u0443 \u0430\u043b\u0438\u0430\u0441\u0443."
-    for key in keys :
-        if key in record :
-            return record [key ]
-    lowered_map ={str (k ).lower ():v for k ,v in record .items ()}
-    for key in keys :
-        candidate =str (key ).lower ()
-        if candidate in lowered_map :
-            return lowered_map [candidate ]
-    return None
```

---

## Пара: activity ↔ testitem

- AST hash: 978d4152ad77000361c21b7f2051d3d1 ↔ 729263c98934cfcb08473bcacfb20e9e

- Jaccard по токенам: 0.171

### Модуль run.py

Определение                                                      | activity сигнатура                                                                                                                                                         | testitem сигнатура                                                           | Побочные эффекты                                                                                                                                                                                                                                                                                                                                                                                                | Исключения                                                   | Статус           
-----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------|------------------
ChemblActivityPipeline                                           | —                                                                                                                                                                          | —                                                                            | activity: {'logging': ['UnifiedLogger.bind', 'UnifiedLogger.get', 'bound_log.warning', 'errors_to_log.iterrows', 'log.debug', 'log.error', 'log.info', 'log.warning', 'pipeline._log_validity_comments_metrics', 'self._log_detailed_validation_errors', 'self._log_validity_comments_metrics'], 'io': ['cache_file.read_text', 'json.dumps', 'json.loads', 'payload.get', 'tmp_path.write_text']}
testitem: {} | activity: ['TypeError(msg)', 'ValueError(msg)']
testitem: [] | только в activity
ChemblActivityPipeline.__init__                                  | self, config: PipelineConfig, run_id: str                                                                                                                                  | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._add_row_metadata                         | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._build_activity_descriptor                | self                                                                                                                                                                       | —                                                                            | activity: {'logging': ['pipeline._log_validity_comments_metrics'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                       | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._cache_directory                          | self, release: str | None                                                                                                                                                  | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._cache_file_path                          | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._cache_key                                | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                            | activity: {'logging': [], 'io': ['json.dumps']}
testitem: {}                                                                                                                                                                                                                                                                                                                                                    | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._check_activity_id_uniqueness             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.debug', 'log.error', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                         | activity: ['ValueError(msg)']
testitem: []                   | только в activity
ChemblActivityPipeline._check_cache                              | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                            | activity: {'logging': [], 'io': ['cache_file.read_text', 'json.loads']}
testitem: {}                                                                                                                                                                                                                                                                                                                            | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._check_foreign_key_integrity              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.debug', 'log.error', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                         | activity: ['ValueError(msg)']
testitem: []                   | только в activity
ChemblActivityPipeline._coerce_activity_dataset                  | self, dataset: object                                                                                                                                                      | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: ['TypeError(msg)']
testitem: []                    | только в activity
ChemblActivityPipeline._coerce_mapping                           | payload: Any                                                                                                                                                               | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._collect_records_by_ids                   | self, normalized_ids: Sequence[tuple[int, str]], activity_iterator: ChemblActivityClient, *, select_fields: Sequence[str] | None                                           | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                    | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._create_fallback_record                   | self, activity_id: int, error: Exception | None                                                                                                                            | —                                                                            | activity: {'logging': [], 'io': ['json.dumps']}
testitem: {}                                                                                                                                                                                                                                                                                                                                                    | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._deduplicate_activity_properties          | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                                            | activity: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._enrich_assay                             | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                              | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._enrich_compound_record                   | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                              | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._enrich_data_validity                     | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                  | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._enrich_molecule                          | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                  | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._ensure_comment_fields                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._extract_activity_properties_fields       | self, record: dict[str, Any]                                                                                                                                               | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.warning'], 'io': ['json.loads']}
testitem: {}                                                                                                                                                                                                                                                                                                     | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._extract_assay_fields                     | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                                            | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                          | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._extract_chembl_release                   | payload: Mapping[str, Any]                                                                                                                                                 | —                                                                            | activity: {'logging': [], 'io': ['payload.get']}
testitem: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._extract_data_validity_descriptions       | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                                            | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                          | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._extract_from_chembl                      | self, dataset: object, chembl_client: ChemblClient | Any, activity_iterator: ChemblActivityClient, *, limit: int | None = None, select_fields: Sequence[str] | None = None | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'self._log_validity_comments_metrics'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                          | activity: ['ValueError(msg)']
testitem: []                   | только в activity
ChemblActivityPipeline._extract_nested_fields                    | self, record: dict[str, Any]                                                                                                                                               | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._extract_page_items                       | payload: Mapping[str, Any], items_keys: Sequence[str] | None                                                                                                               | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._filter_invalid_required_fields           | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._finalize_identifier_columns              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._finalize_output_columns                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._get_data_validity_comment_whitelist      | self                                                                                                                                                                       | —                                                                            | activity: {'logging': ['UnifiedLogger.get'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                             | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._harmonize_identifier_columns             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._log_detailed_validation_errors           | self, failure_cases: pd.DataFrame, payload: pd.DataFrame, log: BoundLogger                                                                                                 | —                                                                            | activity: {'logging': ['errors_to_log.iterrows', 'log.error', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                            | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._log_validity_comments_metrics            | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.info', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._materialize_activity_record              | self, payload: Mapping[str, Any], *, activity_id: int | None = None                                                                                                        | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._next_link                                | payload: Mapping[str, Any], base_url: str                                                                                                                                  | —                                                                            | activity: {'logging': [], 'io': ['payload.get']}
testitem: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._normalize_activity_ids                   | self, input_frame: pd.DataFrame, *, limit: int | None, log: BoundLogger                                                                                                    | —                                                                            | activity: {'logging': ['log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._normalize_activity_properties_items      | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                                            | activity: {'logging': ['log.warning'], 'io': ['json.loads']}
testitem: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._normalize_data_types                     | self, df: pd.DataFrame, schema: Any, log: BoundLogger                                                                                                                      | —                                                                            | activity: {'logging': ['log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._normalize_identifiers                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                     | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._normalize_measurements                   | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._normalize_nested_structures              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.warning'], 'io': ['json.dumps', 'json.loads']}
testitem: {}                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._normalize_string_fields                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.debug', 'log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                      | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._prepare_activity_iteration               | self, *, client_name: str = 'chembl_activity_client'                                                                                                                       | —                                                                            | activity: {'logging': ['UnifiedLogger.get'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                             | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._sanitize_cache_component                 | value: str                                                                                                                                                                 | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._schema_column_specs                      | self                                                                                                                                                                       | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._serialize_activity_properties            | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                                            | activity: {'logging': ['log.warning'], 'io': ['json.dumps']}
testitem: {}                                                                                                                                                                                                                                                                                                                                       | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._should_enrich_assay                      | self                                                                                                                                                                       | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._should_enrich_compound_record            | self                                                                                                                                                                       | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._should_enrich_data_validity              | self                                                                                                                                                                       | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._should_enrich_molecule                   | self                                                                                                                                                                       | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._store_cache                              | self, batch_ids: Sequence[str], batch_data: Mapping[str, Mapping[str, Any]], release: str | None                                                                           | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'log.debug'], 'io': ['json.dumps', 'tmp_path.write_text']}
testitem: {}                                                                                                                                                                                                                                                                                             | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._validate_activity_properties_truv        | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                                            | activity: {'logging': ['log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._validate_data_validity_comment_soft_enum | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline._validate_foreign_keys                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: {'logging': ['log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                   | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline.build_quality_report                      | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline.chembl_release                            | self                                                                                                                                                                       | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline.extract                                   | self, *args, **kwargs                                                                                                                                                      | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'bound_log.warning'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                        | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline.extract_all                               | self                                                                                                                                                                       | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline.extract_by_ids                            | self, ids: Sequence[str]                                                                                                                                                   | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'log.info', 'log.warning', 'self._log_validity_comments_metrics'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                           | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline.transform                                 | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                    | activity: []
testitem: []                                    | только в activity
ChemblActivityPipeline.validate                                  | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.info', 'self._log_detailed_validation_errors'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                               | activity: ['ValueError(msg)']
testitem: []                   | только в activity
ChemblActivityPipeline.write                                     | self, df: pd.DataFrame, output_path: Path, *, extended: bool = False, include_correlation: bool | None = None, include_qc_metrics: bool | None = None                      | —                                                                            | activity: {'logging': ['UnifiedLogger.bind', 'UnifiedLogger.get', 'log.debug'], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                          | activity: []
testitem: []                                    | только в activity
TestItemChemblPipeline                                           | —                                                                                                                                                                          | —                                                                            | activity: {}
testitem: {'logging': ['UnifiedLogger.get', 'bound_log.info', 'bound_log.warning', 'log.debug', 'log.info', 'log.warning'], 'io': ['status_payload.get']}                                                                                                                                                                                                                                          | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline.__init__                                  | —                                                                                                                                                                          | self, config: PipelineConfig, run_id: str                                    | activity: {}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline._build_testitem_descriptor                | —                                                                                                                                                                          | self: SelfTestitemChemblPipeline                                             | activity: {}
testitem: {'logging': ['log.debug', 'log.info'], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline._check_empty_columns                      | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: {}
testitem: {'logging': ['log.warning'], 'io': []}                                                                                                                                                                                                                                                                                                                                                   | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline._deduplicate_molecules                    | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: {}
testitem: {'logging': ['log.info'], 'io': []}                                                                                                                                                                                                                                                                                                                                                      | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline._fetch_chembl_release                     | —                                                                                                                                                                          | self, client: UnifiedAPIClient | ChemblClient | Any, log: BoundLogger | None | activity: {}
testitem: {'logging': ['UnifiedLogger.get', 'bound_log.info', 'bound_log.warning'], 'io': ['status_payload.get']}                                                                                                                                                                                                                                                                                  | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline._flatten_nested_structures                | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline._normalize_identifiers                    | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline._normalize_numeric_fields                 | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline._normalize_string_fields                  | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline._remove_extra_columns                     | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                                                                                                                                                                                                                                                                     | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline._schema_column_specs                      | —                                                                                                                                                                          | self                                                                         | activity: {}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline.api_version                               | —                                                                                                                                                                          | self                                                                         | activity: {}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline.augment_metadata                          | —                                                                                                                                                                          | self, metadata: Mapping[str, object], df: pd.DataFrame                       | activity: {}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline.chembl_db_version                         | —                                                                                                                                                                          | self                                                                         | activity: {}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline.extract                                   | —                                                                                                                                                                          | self, *args, **kwargs                                                        | activity: {}
testitem: {'logging': ['UnifiedLogger.get'], 'io': []}                                                                                                                                                                                                                                                                                                                                             | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline.extract_all                               | —                                                                                                                                                                          | self                                                                         | activity: {}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline.extract_by_ids                            | —                                                                                                                                                                          | self, ids: Sequence[str]                                                     | activity: {}
testitem: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                                                                                                                                                                                                                                                                                                                    | activity: []
testitem: []                                    | только в testitem
TestItemChemblPipeline.transform                                 | —                                                                                                                                                                          | self, df: pd.DataFrame                                                       | activity: {}
testitem: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                                                                                                                                                                                                                                                                                                                    | activity: []
testitem: []                                    | только в testitem
__module_block_0                                                 | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_1                                                 | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_10                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_11                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_12                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_13                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_14                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_15                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_16                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_17                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_18                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_19                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
__module_block_2                                                 | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_20                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
__module_block_21                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
__module_block_22                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
__module_block_23                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
__module_block_24                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
__module_block_25                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
__module_block_26                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
__module_block_27                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
__module_block_28                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
__module_block_29                                                | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {}                                                                                                                                                                                                                                                                                                                                                                | activity: []
testitem: []                                    | только в activity
__module_block_3                                                 | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_4                                                 | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_5                                                 | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_6                                                 | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_7                                                 | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_8                                                 | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       
__module_block_9                                                 | —                                                                                                                                                                          | —                                                                            | activity: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                                                                                                                                                                                                                                                         | activity: []
testitem: []                                    | отличается       

_Показаны первые 20 горячих участков из 112._

#### Горячий участок 1

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:106-3476
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,1684 +0,0 @@

-class ChemblActivityPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting activity records from the ChEMBL API."
-    actor ="activity_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-        self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch activity payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        def legacy_activity_ids (bound_log :BoundLogger )->Sequence [str ]|None :
-            payload_activity_ids =kwargs .get ("activity_ids")
-            if payload_activity_ids is None :
-                return None
-            bound_log .warning ("chembl_activity.deprecated_kwargs",message ="Using activity_ids in kwargs is deprecated. Use --input-file instead.")
-            if isinstance (payload_activity_ids ,Sequence )and (not isinstance (payload_activity_ids ,(str ,bytes ))):
-                sequence_ids :Sequence [str |int ]=cast (Sequence [str |int ],payload_activity_ids )
-                return [str (id_val )for id_val in sequence_ids ]
-            return [str (payload_activity_ids )]
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_activity.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="activity_id",legacy_id_resolver =legacy_activity_ids ,legacy_source ="deprecated_kwargs")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all activity records from ChEMBL using the shared iterator."
-        return self .run_extract_all (self ._build_activity_descriptor ())
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract activity records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of activity_id values to extract (as strings or integers).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted activity records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_config ,chembl_client ,activity_iterator ,select_fields =self ._prepare_activity_iteration ()
-        limit =self .config .cli .limit
-        invalid_ids :list [Any ]=[]
-        def normalize_activity_id (raw :Any )->tuple [str |None ,Any ]:
-            if pd .isna (raw ):
-                return (None ,None )
-            try :
-                if isinstance (raw ,str ):
-                    candidate =raw .strip ()
-                    if not candidate :
-                        return (None ,None )
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw ,(int ,float )):
-                    numeric_id =int (raw )
-                else :
-                    numeric_id =int (raw )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw )
-                return (None ,None )
-            return (str (numeric_id ),int (numeric_id ))
-        def delegated_fetch (canonical_ids :Sequence [str ],context :BatchExtractionContext )->tuple [Sequence [Mapping [str ,Any ]],Mapping [str ,Any ]]:
-            numeric_map =context .metadata
-            normalized_ids :list [tuple [int ,str ]]=[]
-            for identifier in canonical_ids :
-                numeric_value =numeric_map .get (identifier )
-                if numeric_value is None :
-                    continue
-                normalized_ids .append ((int (numeric_value ),identifier ))
-            if not normalized_ids :
-                summary ={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-                context .extra ["delegated_summary"]=summary
-                return ([],summary )
-            records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =context .select_fields or None )
-            context .extra ["delegated_summary"]=summary
-            return (records ,summary )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            dataframe =self ._ensure_comment_fields (dataframe ,log )
-            dataframe =self ._extract_data_validity_descriptions (dataframe ,chembl_client ,log )
-            dataframe =self ._extract_assay_fields (dataframe ,chembl_client ,log )
-            self ._log_validity_comments_metrics (dataframe ,log )
-            return dataframe
-        def empty_activity_frame ()->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def finalize_context (context :BatchExtractionContext )->None :
-            summary =context .extra .get ("delegated_summary")
-            if isinstance (summary ,Mapping ):
-                summary_dict =dict (summary )
-                context .extra ["stats_attribute_override"]=summary_dict
-                self ._last_batch_extract_stats =summary_dict
-            else :
-                context .extra ["stats_attribute_override"]=context .stats .as_dict ()
-                self ._last_batch_extract_stats =context .stats .as_dict ()
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="activity_id",fetcher =delegated_fetch ,select_fields =select_fields ,batch_size =source_config .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release ,id_normalizer =normalize_activity_id ,sort_key =lambda pair :int (pair [0 ]),finalize =finalize_dataframe ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats",fetch_mode ="delegated",empty_frame_factory =empty_activity_frame )
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        batch_stats =self ._last_batch_extract_stats or {}
-        log .info ("chembl_activity.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =batch_stats .get ("batches"),api_calls =batch_stats .get ("api_calls"),cache_hits =batch_stats .get ("cache_hits"))
-        return dataframe
-    def _prepare_activity_iteration (self ,*,client_name :str ="chembl_activity_client")->tuple [ActivitySourceConfig ,ChemblClient ,ChemblActivityClient ,list [str ]]:
-        "Construct reusable ChEMBL clients and iterator for activity extraction."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name =client_name )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =self ._resolve_select_fields (source_raw ,default_fields =API_ACTIVITY_FIELDS )
-        return (source_config ,chembl_client ,activity_iterator ,select_fields )
-    def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-        "Construct the descriptor driving the shared extraction template."
-        def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-            http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-            chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-            typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-            activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-            select_fields =source_config .parameters .select_fields
-            return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-        def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            df =pipeline ._ensure_comment_fields (df ,log )
-            chembl_client =cast (ChemblClient ,context .chembl_client )
-            if chembl_client is not None :
-                df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-            pipeline ._log_validity_comments_metrics (df ,log )
-            return df
-        def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            return pipeline ._materialize_activity_record (payload )
-        return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
-    def _materialize_activity_record (self ,payload :Mapping [str ,Any ],*,activity_id :int |None =None )->dict [str ,Any ]:
-        "Normalize nested fields within an activity payload."
-        record =dict (payload )
-        record =self ._extract_nested_fields (record )
-        record =self ._extract_activity_properties_fields (record )
-        if activity_id is not None :
-            record .setdefault ("activity_id",activity_id )
-        return record
-    def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-        "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-        if isinstance (dataset ,pd .Series ):
-            return dataset .to_frame (name ="activity_id")
-        if isinstance (dataset ,pd .DataFrame ):
-            return dataset
-        if isinstance (dataset ,Mapping ):
-            mapping =cast (Mapping [str ,Any ],dataset )
-            return pd .DataFrame ([dict (mapping )])
-        if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-            dataset_list :list [Any ]=list (dataset )
-            return pd .DataFrame ({"activity_id":dataset_list })
-        msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-        raise TypeError (msg )
-    def _normalize_activity_ids (self ,input_frame :pd .DataFrame ,*,limit :int |None ,log :BoundLogger )->list [tuple [int ,str ]]:
-        "Normalize raw identifier values into deduplicated integer/string pairs."
-        normalized_ids :list [tuple [int ,str ]]=[]
-        invalid_ids :list [Any ]=[]
-        seen :set [str ]=set ()
-        for raw_id in input_frame ["activity_id"].tolist ():
-            if pd .isna (raw_id ):
-                continue
-            try :
-                if isinstance (raw_id ,str ):
-                    candidate =raw_id .strip ()
-                    if not candidate :
-                        continue
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw_id ,(int ,float )):
-                    numeric_id =int (raw_id )
-                else :
-                    numeric_id =int (raw_id )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw_id )
-                continue
-            key =str (numeric_id )
-            if key not in seen :
-                seen .add (key )
-                normalized_ids .append ((numeric_id ,key ))
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        if limit is not None :
-            normalized_ids =normalized_ids [:max (int (limit ),0 )]
-        return normalized_ids
-    def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-        "Iterate over IDs using the shared iterator while preserving cache semantics."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        records :list [dict [str ,Any ]]=[]
-        success_count =0
-        fallback_count =0
-        error_count =0
-        cache_hits =0
-        api_calls =0
-        total_batches =0
-        key_order =[key for _ ,key in normalized_ids ]
-        key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-        for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-            total_batches +=1
-            batch_start =time .perf_counter ()
-            from_cache =False
-            chunk_records :dict [str ,dict [str ,Any ]]={}
-            try :
-                cached_records =self ._check_cache (chunk ,self ._chembl_release )
-                if cached_records is not None :
-                    from_cache =True
-                    cache_hits +=len (chunk )
-                    chunk_records =cached_records
-                else :
-                    api_calls +=1
-                    fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                    for item in fetched_items :
-                        if not isinstance (item ,Mapping ):
-                            continue
-                        activity_value =item .get ("activity_id")
-                        if activity_value is None :
-                            continue
-                        chunk_records [str (activity_value )]=dict (item )
-                    self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-                success_in_batch =0
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    record =chunk_records .get (key )
-                    if record and (not record .get ("error")):
-                        materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                        records .append (materialized )
-                        success_count +=1
-                        success_in_batch +=1
-                    else :
-                        fallback_record =self ._create_fallback_record (numeric_id )
-                        records .append (fallback_record )
-                        fallback_count +=1
-                        error_count +=1
-                batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-                log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-            except CircuitBreakerOpenError as exc :
-                log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except RequestException as exc :
-                log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except Exception as exc :
-                log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-        total_records =len (normalized_ids )
-        success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-        summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-        return (records ,summary )
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw activity data by normalizing measurements, identifiers, and data types."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_measurements (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,ActivitySchema ,log )
-        if "curated"in df .columns or "curated_by"in df .columns :
-            if "curated"not in df .columns :
-                df ["curated"]=pd .NA
-            if "curated_by"in df .columns :
-                mask =df ["curated"].isna ()
-                df .loc [mask ,"curated"]=df .loc [mask ,"curated_by"].notna ()
-            df ["curated"]=df ["curated"].astype ("boolean")
-        df =self ._validate_foreign_keys (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if self ._should_enrich_compound_record ():
-            df =self ._enrich_compound_record (df )
-        if self ._should_enrich_assay ():
-            df =self ._enrich_assay (df )
-        if self ._should_enrich_molecule ():
-            df =self ._enrich_molecule (df )
-        if self ._should_enrich_data_validity ():
-            df =self ._enrich_data_validity (df )
-        df =self ._finalize_identifier_columns (df ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        df =self ._finalize_output_columns (df ,log )
-        df =self ._filter_invalid_required_fields (df ,log )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against ActivitySchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        if "target_tax_id"in df .columns :
-            dtype_name :str =str (df ["target_tax_id"].dtype .name )
-            if dtype_name !="Int64":
-                numeric_series :pd .Series [Any ]=pd .to_numeric (df ["target_tax_id"],errors ="coerce")
-                df ["target_tax_id"]=numeric_series .astype ("Int64")
-        self ._check_activity_id_uniqueness (df ,log )
-        self ._check_foreign_key_integrity (df ,log )
-        self ._validate_data_validity_comment_soft_enum (df ,log )
-        original_coerce =self .config .validation .coerce
-        try :
-            self .config .validation .coerce =False
-            validated =super ().validate (df )
-            log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce )
-            return validated
-        except pandera .errors .SchemaErrors as exc :
-            failure_cases_df :pd .DataFrame |None =None
-            if hasattr (exc ,"failure_cases"):
-                failure_cases_df =cast (pd .DataFrame ,exc .failure_cases )
-            error_count =len (failure_cases_df )if failure_cases_df is not None else 0
-            error_summary =summarize_schema_errors (exc )
-            log .error ("validation_failed",error_count =error_count ,schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce ,error_summary =error_summary ,exc_info =True )
-            if failure_cases_df is not None and (not failure_cases_df .empty ):
-                failure_cases_summary =format_failure_cases (failure_cases_df )
-                log .error ("validation_failure_cases",failure_cases =failure_cases_summary )
-                self ._log_detailed_validation_errors (failure_cases_df ,df ,log )
-            msg =f'Validation failed with {error_count } error(s) against schema {self .config .validation .schema_out }: {error_summary }'
-            raise ValueError (msg )from exc
-        except Exception as exc :
-            log .error ("validation_error",error =str (exc ),schema =self .config .validation .schema_out ,exc_info =True )
-            raise
-        finally :
-            self .config .validation .coerce =original_coerce
-    def _should_enrich_compound_record (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 compound_record \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            compound_record_section :Any =enrich_section .get ("compound_record")
-            if not isinstance (compound_record_section ,Mapping ):
-                return False
-            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-            enabled :Any =compound_record_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        compound_record_section :Any =enrich_section .get ("compound_record")
-                        if isinstance (compound_record_section ,Mapping ):
-                            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                            enrich_cfg =dict (compound_record_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_assay (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 assay \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            assay_section :Any =enrich_section .get ("assay")
-            if not isinstance (assay_section ,Mapping ):
-                return False
-            assay_section =cast (Mapping [str ,Any ],assay_section )
-            enabled :Any =assay_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        assay_section :Any =enrich_section .get ("assay")
-                        if isinstance (assay_section ,Mapping ):
-                            assay_section =cast (Mapping [str ,Any ],assay_section )
-                            enrich_cfg =dict (assay_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_assay (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_data_validity (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 data_validity \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            data_validity_section :Any =enrich_section .get ("data_validity")
-            if not isinstance (data_validity_section ,Mapping ):
-                return False
-            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-            enabled :Any =data_validity_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        data_validity_section :Any =enrich_section .get ("data_validity")
-                        if isinstance (data_validity_section ,Mapping ):
-                            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                            enrich_cfg =dict (data_validity_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        if "data_validity_description"in df .columns :
-            non_na_count =int (df ["data_validity_description"].notna ().sum ())
-            if non_na_count >0 :
-                log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_molecule (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 molecule \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            molecule_section :Any =enrich_section .get ("molecule")
-            if not isinstance (molecule_section ,Mapping ):
-                return False
-            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-            enabled :Any =molecule_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        molecule_section :Any =enrich_section .get ("molecule")
-                        if isinstance (molecule_section ,Mapping ):
-                            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                            enrich_cfg =dict (molecule_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-        if "molecule_name"in df_join .columns :
-            if "molecule_pref_name"not in df .columns :
-                df ["molecule_pref_name"]=pd .NA
-            mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-            if mask .any ():
-                df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-                df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-                log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-        return df
-    def _extract_from_chembl (self ,dataset :object ,chembl_client :ChemblClient |Any ,activity_iterator :ChemblActivityClient ,*,limit :int |None =None ,select_fields :Sequence [str ]|None =None )->pd .DataFrame :
-        "Extract activity records by delegating batching to ``ChemblActivityClient``."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        method_start =time .perf_counter ()
-        self ._last_batch_extract_stats =None
-        input_frame =self ._coerce_activity_dataset (dataset )
-        if "activity_id"not in input_frame .columns :
-            msg ="Input dataset must contain an 'activity_id' column"
-            raise ValueError (msg )
-        normalized_ids =self ._normalize_activity_ids (input_frame ,limit =limit ,log =log )
-        if not normalized_ids :
-            summary :dict [str ,Any ]={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-            self ._last_batch_extract_stats =summary
-            log .info ("chembl_activity.batch_summary",**summary )
-            empty_frame =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-            return self ._ensure_comment_fields (empty_frame ,log )
-        records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =select_fields )
-        duration_ms =(time .perf_counter ()-method_start )*1000.0
-        summary ["duration_ms"]=duration_ms
-        self ._last_batch_extract_stats =summary
-        log .info ("chembl_activity.batch_summary",**summary )
-        result_df :pd .DataFrame =pd .DataFrame .from_records (records )
-        if result_df .empty :
-            result_df =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        elif "activity_id"in result_df .columns :
-            result_df =result_df .sort_values ("activity_id").reset_index (drop =True )
-        result_df =self ._ensure_comment_fields (result_df ,log )
-        result_df =self ._extract_data_validity_descriptions (result_df ,chembl_client ,log )
-        result_df =self ._extract_assay_fields (result_df ,chembl_client ,log )
-        self ._log_validity_comments_metrics (result_df ,log )
-        return result_df
-    def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-        cache_config =self .config .cache
-        if not cache_config .enabled :
-            return None
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        if not cache_file .exists ():
-            return None
-        try :
-            stat =cache_file .stat ()
-        except OSError :
-            return None
-        ttl_seconds =int (cache_config .ttl )
-        if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        try :
-            payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-        except (OSError ,json .JSONDecodeError ):
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        if not isinstance (payload ,dict ):
-            return None
-        missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-        if missing :
-            return None
-        return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
-    def _store_cache (self ,batch_ids :Sequence [str ],batch_data :Mapping [str ,Mapping [str ,Any ]],release :str |None )->None :
-        cache_config =self .config .cache
-        if not cache_config .enabled or not batch_ids or (not batch_data ):
-            return
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        normalized_set =set (normalized_ids )
-        data_to_store ={key :batch_data [key ]for key in normalized_set if key in batch_data }
-        if not data_to_store :
-            return
-        try :
-            cache_file .parent .mkdir (parents =True ,exist_ok =True )
-            tmp_path =cache_file .with_suffix (cache_file .suffix +".tmp")
-            tmp_path .write_text (json .dumps (data_to_store ,sort_keys =True ,default =str ),encoding ="utf-8")
-            tmp_path .replace (cache_file )
-        except Exception as exc :
-            log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-            log .debug ("chembl_activity.cache_store_failed",error =str (exc ))
-    def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-        directory =self ._cache_directory (release )
-        cache_key =self ._cache_key (batch_ids ,release )
-        return directory /f'{cache_key }.json'
-    def _cache_directory (self ,release :str |None )->Path :
-        cache_root =Path (self .config .paths .cache_root )
-        directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-        release_component =self ._sanitize_cache_component (release or "unknown")
-        pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-        version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-        return cache_root /directory_name /pipeline_component /release_component /version_component
-    def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-        payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-        raw =json .dumps (payload ,sort_keys =True )
-        return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
-    @staticmethod
-    def _sanitize_cache_component (value :str )->str :
-        sanitized =re .sub ("[^0-9A-Za-z_.-]","_",value )
-        return sanitized or "default"
-    def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-        "Create fallback record enriched with error metadata."
-        base_message ="Fallback: ChEMBL activity unavailable"
-        message =f'{base_message } ({error })'if error else base_message
-        timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-        metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-        if isinstance (error ,RequestException ):
-            response =getattr (error ,"response",None )
-            status_code =getattr (response ,"status_code",None )
-            if status_code is not None :
-                metadata ["http_status"]=status_code
-            metadata ["error_message"]=str (error )
-        elif error is not None :
-            metadata ["error_message"]=str (error )
-        fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-        return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
-    def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-        if df .empty :
-            return df
-        required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-        if missing_fields :
-            for field in missing_fields :
-                df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            log .debug ("comment_fields_ensured",fields =missing_fields )
-        return df
-    def _extract_data_validity_descriptions (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c data_validity_description \u0438\u0437 DATA_VALIDITY_LOOKUP \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f data_validity_comment \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_data_validity_lookup() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c data_validity_comment.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 data_validity_description.\n        "
-        if df .empty :
-            return df
-        if "data_validity_comment"not in df .columns :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="data_validity_comment_column_missing")
-            return df
-        validity_comments :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            comment =row .get ("data_validity_comment")
-            if pd .isna (comment )or comment is None :
-                continue
-            comment_str =str (comment ).strip ()
-            if comment_str :
-                validity_comments .append (comment_str )
-        if not validity_comments :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="no_valid_comments")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        unique_comments =list (set (validity_comments ))
-        log .info ("extract_data_validity_descriptions_fetching",comments_count =len (unique_comments ))
-        try :
-            records_dict =client .fetch_data_validity_lookup (comments =unique_comments ,fields =["data_validity_comment","description"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_data_validity_descriptions_fetch_error",error =str (exc ),exc_info =True )
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for comment in unique_comments :
-            record =records_dict .get (comment )
-            if record :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":record .get ("description")})
-            else :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":None })
-        if not enrichment_data :
-            log .debug ("extract_data_validity_descriptions_no_records")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        original_index =df .index .copy ()
-        df_result =df .merge (df_enrich ,on =["data_validity_comment"],how ="left",suffixes =("","_enrich"))
-        if "data_validity_description"not in df_result .columns :
-            df_result ["data_validity_description"]=pd .Series ([pd .NA ]*len (df_result ),dtype ="string")
-        else :
-            df_result ["data_validity_description"]=df_result ["data_validity_description"].astype ("string")
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_data_validity_descriptions_complete",comments_requested =len (unique_comments ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _extract_assay_fields (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c assay_organism \u0438 assay_tax_id \u0438\u0437 ASSAYS \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f assay_chembl_id \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_assays_by_ids() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438 assay_organism \u0438 assay_tax_id.\n        "
-        if df .empty :
-            return df
-        if "assay_chembl_id"not in df .columns :
-            log .debug ("extract_assay_fields_skipped",reason ="assay_chembl_id_column_missing")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        assay_ids :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            assay_id =row .get ("assay_chembl_id")
-            if pd .isna (assay_id )or assay_id is None :
-                continue
-            assay_id_str =str (assay_id ).strip ().upper ()
-            if assay_id_str :
-                assay_ids .append (assay_id_str )
-        if not assay_ids :
-            log .debug ("extract_assay_fields_skipped",reason ="no_valid_assay_ids")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        unique_assay_ids =list (set (assay_ids ))
-        log .info ("extract_assay_fields_fetching",assay_ids_count =len (unique_assay_ids ))
-        try :
-            records_dict =client .fetch_assays_by_ids (ids =unique_assay_ids ,fields =["assay_chembl_id","assay_organism","assay_tax_id"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_assay_fields_fetch_error",error =str (exc ),exc_info =True )
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for assay_id in unique_assay_ids :
-            record =records_dict .get (assay_id )if records_dict else None
-            if record :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":record .get ("assay_organism"),"assay_tax_id":record .get ("assay_tax_id")})
-            else :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":None ,"assay_tax_id":None })
-        if not enrichment_data :
-            log .debug ("extract_assay_fields_no_records")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        df_enrich ["assay_chembl_id"]=df_enrich ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        original_index =df .index .copy ()
-        df_normalized =df .copy ()
-        df_normalized ["assay_chembl_id_normalized"]=df_normalized ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        df_result =df_normalized .merge (df_enrich ,left_on ="assay_chembl_id_normalized",right_on ="assay_chembl_id",how ="left",suffixes =("","_enrich"))
-        df_result =df_result .drop (columns =["assay_chembl_id_normalized"])
-        for col in ["assay_organism","assay_tax_id"]:
-            if f'{col }_enrich'in df_result .columns :
-                if col not in df_result .columns :
-                    df_result [col ]=df_result [f'{col }_enrich']
-                else :
-                    base_series :pd .Series [Any ]=df_result [col ]
-                    enrich_series :pd .Series [Any ]=df_result [f'{col }_enrich']
-                    missing_mask =base_series .isna ()
-                    if bool (missing_mask .any ()):
-                        df_result .loc [missing_mask ,col ]=enrich_series .loc [missing_mask ]
-                df_result =df_result .drop (columns =[f'{col }_enrich'])
-        for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-            if col not in df_result .columns :
-                df_result [col ]=pd .Series ([pd .NA ]*len (df_result ),dtype =dtype )
-        df_result ["assay_organism"]=df_result ["assay_organism"].astype ("string")
-        df_result ["assay_tax_id"]=pd .to_numeric (df_result ["assay_tax_id"],errors ="coerce").astype ("Int64")
-        mask_valid =df_result ["assay_tax_id"].notna ()
-        if mask_valid .any ():
-            invalid_mask =mask_valid &(df_result ["assay_tax_id"]<1 )
-            if invalid_mask .any ():
-                log .warning ("invalid_assay_tax_id_range",count =int (invalid_mask .sum ()))
-                df_result .loc [invalid_mask ,"assay_tax_id"]=pd .NA
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_assay_fields_complete",assay_ids_requested =len (unique_assay_ids ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _log_validity_comments_metrics (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "\u041b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442:\n        - \u0414\u043e\u043b\u044e NA \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0442\u0440\u0435\u0445 \u043f\u043e\u043b\u0435\u0439\n        - \u0422\u043e\u043f-10 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment\n        - \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment (\u043d\u0435 \u0432 whitelist)\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity.\n        log:\n            Logger instance.\n        "
-        if df .empty :
-            return
-        metrics :dict [str ,Any ]={}
-        comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        for field in comment_fields :
-            if field in df .columns :
-                na_count =int (df [field ].isna ().sum ())
-                total_count =len (df )
-                na_rate =float (na_count )/float (total_count )if total_count >0 else 0.0
-                metrics [f'{field }_na_rate']=na_rate
-                metrics [f'{field }_na_count']=na_count
-                metrics [f'{field }_total_count']=total_count
-        non_null_comments_series :pd .Series [str ]|None =None
-        if "data_validity_comment"in df .columns :
-            series_candidate =df ["data_validity_comment"].dropna ()
-            if len (series_candidate )>0 :
-                typed_series :pd .Series [str ]=series_candidate .astype ("string")
-                non_null_comments_series =typed_series
-                value_counts =typed_series .value_counts ().head (10 )
-                top_10 ={str (key ):int (value )for key ,value in value_counts .items ()}
-                metrics ["top_10_data_validity_comments"]=top_10
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if whitelist and non_null_comments_series is not None :
-            whitelist_set :set [str ]=set (whitelist )
-            def _is_unknown (value :str )->bool :
-                return value not in whitelist_set
-            unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-            unknown_count =int (unknown_mask .sum ())
-            if unknown_count >0 :
-                unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-                metrics ["unknown_data_validity_comments_count"]=unknown_count
-                metrics ["unknown_data_validity_comments_samples"]=unknown_values
-                log .warning ("unknown_data_validity_comments_detected",unknown_count =unknown_count ,samples =unknown_values ,whitelist =whitelist )
-        if metrics :
-            log .info ("validity_comments_metrics",**metrics )
-    def _get_data_validity_comment_whitelist (self )->list [str ]:
-        "\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c whitelist \u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0434\u043b\u044f data_validity_comment \u0438\u0437 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\n\n        Raises\n        ------\n        RuntimeError\n            \u0415\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d \u0438\u043b\u0438 \u043f\u0443\u0441\u0442.\n        "
-        try :
-            values =sorted (self ._required_vocab_ids ("data_validity_comment"))
-        except RuntimeError as exc :
-            UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validation',run_id =self .run_id ).error ("data_validity_comment_whitelist_unavailable",error =str (exc ))
-            raise
-        return values
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested assay and molecule objects."
-        if "assay"in record and isinstance (record ["assay"],Mapping ):
-            assay =cast (Mapping [str ,Any ],record ["assay"])
-            if "organism"in assay :
-                record .setdefault ("assay_organism",assay ["organism"])
-            if "tax_id"in assay :
-                record .setdefault ("assay_tax_id",assay ["tax_id"])
-        if "molecule"in record and isinstance (record ["molecule"],Mapping ):
-            molecule =cast (Mapping [str ,Any ],record ["molecule"])
-            if "pref_name"in molecule :
-                record .setdefault ("molecule_pref_name",molecule ["pref_name"])
-        if "curated_by"in record :
-            curated_by =record .get ("curated_by")
-            if curated_by is not None and (not pd .isna (curated_by )):
-                record .setdefault ("curated",True )
-            else :
-                record .setdefault ("curated",False )
-        elif "curated"not in record :
-            record .setdefault ("curated",None )
-        return record
-    def _extract_activity_properties_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract TRUV fields, standard_* fields, and comments from activity_properties array as fallback.\n\n        \u041f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0438\u0437 activity_properties \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442\n        \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043e\u0442\u0432\u0435\u0442\u0435 API (\u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 \u043f\u0440\u044f\u043c\u044b\u0445 \u043f\u043e\u043b\u0435\u0439 \u0438\u0437 ACTIVITIES).\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0435 TRUV-\u043f\u043e\u043b\u044f: value, text_value, relation, units.\n        \u0422\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f: standard_upper_value, standard_text_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b: upper_value, lower_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0438: activity_comment, data_validity_comment.\n\n        \u0422\u0430\u043a\u0436\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 activity_properties \u0432 \u0437\u0430\u043f\u0438\u0441\u0438 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438.\n        \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044e \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract_activity_properties')
-        activity_id =record .get ("activity_id")
-        if "activity_properties"not in record :
-            log .warning ("activity_properties_missing",activity_id =activity_id ,message ="activity_properties not found in API response (possible ChEMBL < v24)")
-            record ["activity_properties"]=None
-            return record
-        properties =record ["activity_properties"]
-        if properties is None :
-            log .debug ("activity_properties_null",activity_id =activity_id ,message ="activity_properties is None (possible ChEMBL < v24)")
-            return record
-        if isinstance (properties ,str ):
-            try :
-                properties =json .loads (properties )
-            except (TypeError ,ValueError ,json .JSONDecodeError )as exc :
-                log .debug ("activity_properties_parse_failed",error =str (exc ),activity_id =record .get ("activity_id"))
-                return record
-        if not isinstance (properties ,Sequence )or isinstance (properties ,(str ,bytes )):
-            return record
-        property_iterable :Iterable [Any ]=cast (Iterable [Any ],properties )
-        property_items :list [Any ]=list (property_iterable )
-        def _set_fallback (key :str ,value :Any )->None :
-            "Set fallback value only if key is missing in record and value is not None."
-            if value is not None and record .get (key )is None :
-                record [key ]=value
-        def _is_empty (value :Any )->bool :
-            "Check if value is empty (None, empty string, or whitespace)."
-            if value is None :
-                return True
-            if isinstance (value ,str ):
-                return not value .strip ()
-            return False
-        items :list [Mapping [str ,Any ]]=[]
-        for property_item in property_items :
-            if isinstance (property_item ,Mapping )and "type"in property_item and ("value"in property_item or "text_value"in property_item ):
-                items .append (cast (Mapping [str ,Any ],property_item ))
-        def _is_measured (p :Mapping [str ,Any ])->bool :
-            rf =p .get ("result_flag")
-            return rf is True or (isinstance (rf ,int )and rf ==1 )
-        items .sort (key =lambda p :not _is_measured (p ))
-        for prop in items :
-            val =prop .get ("value")
-            txt =prop .get ("text_value")
-            rel =prop .get ("relation")
-            unt =prop .get ("units")
-            prop_type =str (prop .get ("type","")).lower ()
-            will_set_value =val is not None and record .get ("value")is None
-            will_set_text_value =txt is not None and record .get ("text_value")is None
-            _set_fallback ("value",val )
-            _set_fallback ("text_value",txt )
-            if will_set_value or will_set_text_value :
-                _set_fallback ("relation",rel )
-                _set_fallback ("units",unt )
-            if unt is not None and record .get ("units")is None :
-                _set_fallback ("units",unt )
-            if record .get ("upper_value")is None and ("upper"in prop_type or prop_type in ("upper_value","upper limit")):
-                if val is not None :
-                    _set_fallback ("upper_value",val )
-            if record .get ("lower_value")is None and ("lower"in prop_type or prop_type in ("lower_value","lower limit")):
-                if val is not None :
-                    _set_fallback ("lower_value",val )
-            if record .get ("standard_upper_value")is None and ("standard_upper"in prop_type or prop_type in ("standard upper","standard upper value")):
-                if val is not None :
-                    _set_fallback ("standard_upper_value",val )
-            if record .get ("standard_text_value")is None and "standard"in prop_type and ("text"in prop_type ):
-                if txt is not None :
-                    _set_fallback ("standard_text_value",txt )
-                elif val is not None :
-                    _set_fallback ("standard_text_value",val )
-        current_comment =record .get ("data_validity_comment")
-        if _is_empty (current_comment ):
-            data_validity_items :list [Mapping [str ,Any ]]=[prop for prop in items if ("data_validity"in str (prop .get ("type","")).lower ()or "validity"in str (prop .get ("type","")).lower ())and (prop .get ("text_value")is not None or prop .get ("value")is not None )]
-            if data_validity_items :
-                measured_items =[p for p in data_validity_items if _is_measured (p )]
-                if measured_items :
-                    prop =measured_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="measured",comment_value =comment_value )
-                else :
-                    prop =data_validity_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="first",comment_value =comment_value )
-            else :
-                log .debug ("data_validity_comment_fallback_no_items",activity_id =record .get ("activity_id"),activity_properties_count =len (items ),has_activity_properties =True )
-        else :
-            log .debug ("data_validity_comment_from_api",activity_id =record .get ("activity_id"),comment_value =current_comment )
-        normalized_properties =self ._normalize_activity_properties_items (property_items ,log )
-        if normalized_properties is not None :
-            validated_properties ,validation_stats =self ._validate_activity_properties_truv (normalized_properties ,log ,activity_id )
-            deduplicated_properties ,dedup_stats =self ._deduplicate_activity_properties (validated_properties ,log ,activity_id )
-            record ["activity_properties"]=deduplicated_properties
-            log .debug ("activity_properties_processed",activity_id =activity_id ,original_count =len (property_items ),normalized_count =len (normalized_properties ),validated_count =len (validated_properties ),deduplicated_count =len (deduplicated_properties ),invalid_count =validation_stats .get ("invalid_count",0 ),duplicates_removed =dedup_stats .get ("duplicates_removed",0 ))
-        else :
-            record ["activity_properties"]=properties
-            log .debug ("activity_properties_normalization_failed",activity_id =activity_id ,message ="activity_properties normalization failed, keeping original")
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
-    @staticmethod
-    def _extract_chembl_release (payload :Mapping [str ,Any ])->str |None :
-        for key in ("chembl_release","chembl_db_version","release","version"):
-            value =payload .get (key )
-            if isinstance (value ,str )and value .strip ():
-                return value
-            if value is not None :
-                return str (value )
-        return None
-    @staticmethod
-    def _extract_page_items (payload :Mapping [str ,Any ],items_keys :Sequence [str ]|None =None )->list [dict [str ,Any ]]:
-        preferred_keys :tuple [str ,...]=("activities",)
-        if items_keys is None :
-            combined_keys =preferred_keys +("data","items","results")
-        else :
-            combined_keys =tuple (dict .fromkeys ((*preferred_keys ,*items_keys )))
-        return ChemblPipelineBase ._extract_page_items (payload ,combined_keys )
-    @staticmethod
-    def _next_link (payload :Mapping [str ,Any ],base_url :str )->str |None :
-        page_meta :Any =payload .get ("page_meta")
-        if isinstance (page_meta ,Mapping ):
-            next_link_raw :Any =page_meta .get ("next")
-            next_link :str |None =cast (str |None ,next_link_raw )if next_link_raw is not None else None
-            if isinstance (next_link ,str )and next_link :
-                base_url_str =str (base_url )
-                base_path_parse_result =urlparse (base_url_str )
-                base_path_raw =base_path_parse_result .path
-                base_path_str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                base_path :str =base_path_str .rstrip ("/")
-                if next_link .startswith ("http://")or next_link .startswith ("https://"):
-                    parsed =urlparse (next_link )
-                    base_parsed =urlparse (base_url_str )
-                    parsed_path_raw =parsed .path
-                    base_path_raw =base_parsed .path
-                    path :str =parsed_path_raw .decode ("utf-8","ignore")if isinstance (parsed_path_raw ,(bytes ,bytearray ))else parsed_path_raw
-                    base_path_from_url :str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                    path_normalized :str =path .rstrip ("/")
-                    base_path_normalized :str =base_path_from_url .rstrip ("/")
-                    if base_path_normalized and path_normalized .startswith (base_path_normalized ):
-                        relative_path =path_normalized [len (base_path_normalized ):]
-                        if not relative_path :
-                            return None
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    elif "/api/data/"in path :
-                        parts =path .split ("/api/data/",1 )
-                        if len (parts )>1 :
-                            relative_path ="/"+parts [1 ]
-                        else :
-                            relative_path =path
-                    else :
-                        relative_path =path
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    if parsed .query :
-                        relative_path =f'{relative_path }?{parsed .query }'
-                    return relative_path
-                if base_path :
-                    normalized_base =base_path .lstrip ("/")
-                    stripped_link =next_link .lstrip ("/")
-                    if stripped_link .startswith (normalized_base +"/"):
-                        stripped_link =stripped_link [len (normalized_base ):]
-                    elif stripped_link ==normalized_base :
-                        stripped_link =""
-                    next_link =stripped_link
-                next_link =next_link .lstrip ("/")
-                if next_link :
-                    next_link =f'/{next_link }'
-                else :
-                    next_link ="/"
-                return next_link
-        return None
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Ensure canonical identifier columns are present before normalization."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_chembl_id"not in df .columns and "assay_id"in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "testitem_chembl_id"not in df .columns :
-            if "testitem_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["testitem_id"]
-                actions .append ("testitem_id->testitem_chembl_id")
-            elif "molecule_chembl_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["molecule_chembl_id"]
-                actions .append ("molecule_chembl_id->testitem_chembl_id")
-        if "molecule_chembl_id"not in df .columns and "testitem_chembl_id"in df .columns :
-            df ["molecule_chembl_id"]=df ["testitem_chembl_id"]
-            actions .append ("testitem_chembl_id->molecule_chembl_id")
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_required =[column for column in required_columns if column not in df .columns ]
-        if missing_required :
-            for column in missing_required :
-                df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            actions .append (f"created_missing:{",".join (missing_required )}")
-        alias_columns =[column for column in ("assay_id","testitem_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        boolean_columns =("potential_duplicate","curated","removed")
-        for column in boolean_columns :
-            specs [column ]={"dtype":"boolean","default":pd .NA }
-        return specs
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize ChEMBL and BAO identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"],pattern ="^CHEMBL\\d+$"),IdentifierRule (name ="bao",columns =["bao_endpoint","bao_format"],pattern ="^BAO_\\d{7}$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _finalize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align identifier columns after normalization and drop aliases."
-        df =df .copy ()
-        if {"molecule_chembl_id","testitem_chembl_id"}.issubset (df .columns ):
-            mismatch_mask =df ["molecule_chembl_id"].notna ()&df ["testitem_chembl_id"].notna ()&(df ["molecule_chembl_id"]!=df ["testitem_chembl_id"])
-            if mismatch_mask .any ():
-                mismatch_count =int (mismatch_mask .sum ())
-                samples_raw =df .loc [mismatch_mask ,["molecule_chembl_id","testitem_chembl_id"]].drop_duplicates ().head (5 ).to_dict ("records")
-                samples :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],samples_raw )
-                log .warning ("identifier_mismatch",count =mismatch_count ,samples =samples )
-                df .loc [mismatch_mask ,"testitem_chembl_id"]=df .loc [mismatch_mask ,"molecule_chembl_id"]
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_columns =[column for column in required_columns if column not in df .columns ]
-        if missing_columns :
-            for column in missing_columns :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            log .warning ("identifier_columns_missing",columns =missing_columns )
-        return df
-    def _finalize_output_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align final column order with schema and drop unexpected fields."
-        df =df .copy ()
-        expected =list (COLUMN_ORDER )
-        extras =[column for column in df .columns if column not in expected ]
-        if extras :
-            df =df .drop (columns =extras )
-            log .debug ("output_columns_dropped",columns =extras )
-        missing =[column for column in expected if column not in df .columns ]
-        if missing :
-            for column in missing :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([pd .NA ]*len (df ),dtype ="object")
-            log .warning ("output_columns_missing",columns =missing )
-        if not expected :
-            return df
-        return df [expected ]
-    def _filter_invalid_required_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Filter out rows with NULL values in required identifier fields.\n\n        Removes rows where any of the required fields (assay_chembl_id,\n        testitem_chembl_id, molecule_chembl_id) are NULL, as these cannot\n        pass schema validation.\n\n        Parameters\n        ----------\n        df:\n            DataFrame to filter.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            Filtered DataFrame with only rows having all required fields populated.\n        "
-        df =df .copy ()
-        if df .empty :
-            return df
-        required_fields =["assay_chembl_id","molecule_chembl_id"]
-        missing_fields =[field for field in required_fields if field not in df .columns ]
-        if missing_fields :
-            log .warning ("filter_skipped_missing_columns",missing_columns =missing_fields ,message ="Cannot filter: required columns are missing")
-            return df
-        valid_mask =df ["assay_chembl_id"].notna ()&df ["molecule_chembl_id"].notna ()
-        invalid_count =int ((~valid_mask ).sum ())
-        if invalid_count >0 :
-            invalid_rows =df [~valid_mask ]
-            sample_size =min (5 ,len (invalid_rows ))
-            sample_activity_ids =invalid_rows ["activity_id"].head (sample_size ).tolist ()if "activity_id"in invalid_rows .columns else []
-            log .warning ("filtered_invalid_rows",filtered_count =invalid_count ,remaining_count =int (valid_mask .sum ()),sample_activity_ids =sample_activity_ids ,message ="Rows with NULL in required identifier fields were filtered out")
-            df =df [valid_mask ].reset_index (drop =True )
-        return df
-    def _normalize_measurements (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize standard_value, standard_units, standard_relation, and standard_type."
-        df =df .copy ()
-        normalized_count =0
-        if "standard_value"in df .columns :
-            mask =df ["standard_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_value"]=numeric_series_std
-                negative_mask =mask &(df ["standard_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["standard_relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"standard_relation"]=series
-                invalid_mask =mask &~df ["standard_relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_type"in df .columns :
-            mask =df ["standard_type"].notna ()
-            if mask .any ():
-                df .loc [mask ,"standard_type"]=df .loc [mask ,"standard_type"].astype (str ).str .strip ()
-                standard_types_set :set [str ]=STANDARD_TYPES
-                invalid_mask =mask &~df ["standard_type"].isin (standard_types_set )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_type",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_type"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_units"in df .columns :
-            unit_mapping ={"nanomolar":"nM","nmol":"nM","nm":"nM","NM":"nM","\u00b5M":"\u03bcM","uM":"\u03bcM","UM":"\u03bcM","micromolar":"\u03bcM","microM":"\u03bcM","umol":"\u03bcM","millimolar":"mM","milliM":"mM","mmol":"mM","MM":"mM","percent":"%","pct":"%","ratios":"ratio"}
-            mask =df ["standard_units"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_units"].astype (str ).str .strip ()
-                for old_unit ,new_unit in unit_mapping .items ():
-                    series =series .str .replace (old_unit ,new_unit ,regex =False ,case =False )
-                df .loc [mask ,"standard_units"]=series
-                normalized_count +=int (mask .sum ())
-        if "relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"relation"]=series
-                invalid_mask =mask &~df ["relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_upper_value"in df .columns :
-            mask =df ["standard_upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_upper_value"]=numeric_series_std_upper
-                negative_mask =mask &(df ["standard_upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "upper_value"in df .columns :
-            mask =df ["upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"upper_value"]=numeric_series_upper
-                negative_mask =mask &(df ["upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "lower_value"in df .columns :
-            mask =df ["lower_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"lower_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_lower :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"lower_value"]=numeric_series_lower
-                negative_mask =mask &(df ["lower_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_lower_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"lower_value"]=None
-                normalized_count +=int (mask .sum ())
-        if normalized_count >0 :
-            log .debug ("measurements_normalized",normalized_count =normalized_count )
-        return df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize string fields: trim, empty string to null, title-case for organism."
-        working_df =df .copy ()
-        if "data_validity_description"in working_df .columns and "data_validity_comment"in working_df .columns :
-            invalid_mask =working_df ["data_validity_description"].notna ()&working_df ["data_validity_comment"].isna ()
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                log .warning ("invariant_data_validity_description_without_comment",count =invalid_count ,message ="data_validity_description is filled while data_validity_comment is NA")
-        rules :dict [str ,StringRule ]={"canonical_smiles":StringRule (),"bao_label":StringRule (max_length =128 ),"target_organism":StringRule (title_case =True ),"assay_organism":StringRule (title_case =True ),"data_validity_comment":StringRule (),"data_validity_description":StringRule (),"activity_comment":StringRule (),"standard_text_value":StringRule (),"text_value":StringRule (),"type":StringRule (),"units":StringRule (),"assay_type":StringRule (),"assay_description":StringRule (),"molecule_pref_name":StringRule (),"target_pref_name":StringRule (),"uo_units":StringRule (),"qudt_units":StringRule ()}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Serialize nested structures (ligand_efficiency, activity_properties) to JSON strings."
-        df =df .copy ()
-        nested_fields =["ligand_efficiency","activity_properties"]
-        for field in nested_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                serialized :list [Any ]=[]
-                for idx ,value in df .loc [mask ,field ].items ():
-                    if field =="activity_properties":
-                        serialized_value =self ._serialize_activity_properties (value ,log )
-                        serialized .append (serialized_value )
-                        continue
-                    if isinstance (value ,(Mapping ,list )):
-                        try :
-                            serialized .append (json .dumps (value ,ensure_ascii =False ,sort_keys =True ))
-                        except (TypeError ,ValueError )as exc :
-                            log .warning ("nested_serialization_failed",field =field ,index =idx ,error =str (exc ))
-                            serialized .append (None )
-                    elif isinstance (value ,str ):
-                        try :
-                            json .loads (value )
-                            serialized .append (value )
-                        except (TypeError ,ValueError ):
-                            serialized .append (None )
-                    else :
-                        serialized .append (None )
-                df .loc [mask ,field ]=pd .Series (serialized ,dtype ="object",index =df .loc [mask ,field ].index )
-        if "standard_value"in df .columns and "ligand_efficiency"in df .columns :
-            mask =df ["standard_value"].notna ()&df ["ligand_efficiency"].isna ()
-            if mask .any ():
-                log .warning ("ligand_efficiency_missing_with_standard_value",count =int (mask .sum ()),message ="ligand_efficiency is empty while standard_value exists")
-        return df
-    def _serialize_activity_properties (self ,value :Any ,log :BoundLogger |None =None )->str |None :
-        "Return normalized JSON for activity_properties or None if not serializable."
-        normalized_items =self ._normalize_activity_properties_items (value ,log )
-        if normalized_items is None :
-            return None
-        try :
-            return json .dumps (normalized_items ,ensure_ascii =False ,sort_keys =True )
-        except (TypeError ,ValueError )as exc :
-            if log is not None :
-                log .warning ("activity_properties_serialization_failed",error =str (exc ))
-            return None
-    def _normalize_activity_properties_items (self ,value :Any ,log :BoundLogger |None =None )->list [dict [str ,Any ]]|None :
-        "Coerce activity_properties payloads into a list of constrained dictionaries."
-        if value is None :
-            return None
-        raw_value =value
-        if isinstance (value ,str ):
-            stripped =value .strip ()
-            if not stripped :
-                return []
-            try :
-                parsed =json .loads (stripped )
-            except (TypeError ,ValueError ):
-                fallback_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                fallback_base ["text_value"]=stripped
-                return [fallback_base ]
-            else :
-                value =parsed
-        if isinstance (value ,Mapping ):
-            items :list [Any ]=[value ]
-        elif isinstance (value ,Sequence )and (not isinstance (value ,(str ,bytes ))):
-            items =list (value )
-        else :
-            if log is not None :
-                log .warning ("activity_properties_unhandled_type",value_type =type (raw_value ).__name__ )
-            return None
-        normalized :list [dict [str ,Any ]]=[]
-        for item in items :
-            if item is None :
-                continue
-            if isinstance (item ,Mapping ):
-                item_mapping =cast (Mapping [str ,Any ],item )
-                normalized_item :dict [str ,Any |None ]={key :item_mapping .get (key )for key in ACTIVITY_PROPERTY_KEYS }
-                result_flag_value =normalized_item .get ("result_flag")
-                if isinstance (result_flag_value ,int )and result_flag_value in (0 ,1 ):
-                    normalized_item ["result_flag"]=bool (result_flag_value )
-                normalized .append (normalized_item )
-            elif isinstance (item ,str ):
-                str_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                str_base ["text_value"]=item
-                normalized .append (str_base )
-            elif log is not None :
-                log .warning ("activity_properties_item_unhandled",item_type =type (item ).__name__ )
-        return normalized
-    def _validate_activity_properties_truv (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0434\u043b\u044f activity_properties.\n\n        \u0412\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u0435\u0442 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n        - value IS NOT NULL \u21d2 text_value IS NULL (\u0438 \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442)\n        - relation IN ('=', '<', '\u2264', '>', '\u2265', '~') OR NULL\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        "
-        validated :list [dict [str ,Any ]]=[]
-        invalid_count =0
-        invalid_items :list [dict [str ,Any ]]=[]
-        for prop in properties :
-            is_valid =True
-            validation_errors :list [str ]=[]
-            value =prop .get ("value")
-            text_value =prop .get ("text_value")
-            relation =prop .get ("relation")
-            if value is not None and text_value is not None :
-                is_valid =False
-                validation_errors .append ("both value and text_value are not None")
-            elif value is None and text_value is None :
-                pass
-            if relation is not None :
-                if not isinstance (relation ,str ):
-                    is_valid =False
-                    validation_errors .append (f'relation is not a string: {type (relation ).__name__ }')
-                elif relation not in RELATIONS :
-                    is_valid =False
-                    validation_errors .append (f"relation '{relation }' not in allowed values: {RELATIONS }")
-            validated .append (prop )
-            if not is_valid :
-                invalid_count +=1
-                invalid_items .append (prop )
-                log .warning ("activity_property_truv_validation_failed",activity_id =activity_id ,property =prop ,errors =validation_errors ,message ="TRUV validation failed, but property is kept")
-        stats ={"invalid_count":invalid_count ,"valid_count":len (validated )}
-        return (validated ,stats )
-    def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-        seen :set [tuple [Any ,...]]=set ()
-        deduplicated :list [dict [str ,Any ]]=[]
-        duplicates_removed =0
-        for prop in properties :
-            dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-            if dedup_key not in seen :
-                seen .add (dedup_key )
-                deduplicated .append (prop )
-            else :
-                duplicates_removed +=1
-                log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-        stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-        return (deduplicated ,stats )
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_added",value ="activity")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_filled",value ="activity")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :BoundLogger )->pd .DataFrame :
-        "Convert data types according to the Pandera schema."
-        df =df .copy ()
-        non_nullable_int_fields ={"activity_id":"int64"}
-        nullable_int_fields ={"row_index":"Int64","target_tax_id":"int64","assay_tax_id":"int64","record_id":"int64","src_id":"int64"}
-        float_fields ={"standard_value":"float64","standard_upper_value":"float64","pchembl_value":"float64","upper_value":"float64","lower_value":"float64"}
-        bool_fields =["potential_duplicate","curated","removed"]
-        binary_flag_fields =["standard_flag"]
-        for field in non_nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_int :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_int .astype ("Int64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field =="row_index":
-                    numeric_series_row :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=numeric_series_row .astype ("Int64")
-                    if df [field ].isna ().any ():
-                        df [field ]=range (len (df ))
-                else :
-                    nullable_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=nullable_numeric_series .astype ("Int64")
-                    mask_valid =df [field ].notna ()
-                    if mask_valid .any ():
-                        invalid_mask =mask_valid &(df [field ]<1 )
-                        if invalid_mask .any ():
-                            log .warning ("invalid_positive_integer",field =field ,count =int (invalid_mask .sum ()))
-                            df .loc [invalid_mask ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in float_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_float :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_float .astype ("float64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in bool_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field in ("curated","removed")and df [field ].dtype =="boolean":
-                    continue
-                if field in ("curated","removed"):
-                    df [field ]=df [field ].astype ("boolean")
-                else :
-                    bool_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=(bool_numeric_series !=0 ).astype ("boolean")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("bool_conversion_failed",field =field ,error =str (exc ))
-        for field in binary_flag_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_flag :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_flag .astype ("Int64")
-                mask_valid =df [field ].notna ()
-                if mask_valid .any ():
-                    valid_values =df .loc [mask_valid ,field ]
-                    invalid_valid_mask =~valid_values .isin ([0 ,1 ])
-                    if invalid_valid_mask .any ():
-                        invalid_index =valid_values .index [invalid_valid_mask ]
-                        log .warning ("invalid_standard_flag",field =field ,count =int (invalid_valid_mask .sum ()))
-                        df .loc [invalid_index ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        object_fields =["value","activity_properties"]
-        for field in object_fields :
-            if field in df .columns :
-                if df [field ].dtype !="object":
-                    df [field ]=df [field ].astype ("object")
-        return df
-    def _validate_foreign_keys (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Validate foreign key integrity and format of ChEMBL IDs."
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        chembl_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        warnings :list [str ]=[]
-        for field in chembl_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-                if invalid_mask .any ():
-                    warning_msg :str =f'{field }: {int (invalid_mask .sum ())} invalid format(s)'
-                    warnings .append (warning_msg )
-        if warnings :
-            log .warning ("foreign_key_validation",warnings =warnings )
-        return df
-    def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check uniqueness of activity_id before validation."
-        if "activity_id"not in df .columns :
-            log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-            return
-        duplicates =df [df ["activity_id"].duplicated (keep =False )]
-        if not duplicates .empty :
-            duplicate_count =len (duplicates )
-            duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-            log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-            msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-            raise ValueError (msg )
-        log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
-    def _validate_data_validity_comment_soft_enum (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Soft enum \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0434\u043b\u044f data_validity_comment.\n\n        \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0442\u0438\u0432 whitelist \u0438\u0437 \u043a\u043e\u043d\u0444\u0438\u0433\u0430. \u041d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n        \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a warning, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e (soft enum).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        "
-        if df .empty or "data_validity_comment"not in df .columns :
-            return
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if not whitelist :
-            return
-        series_candidate =df ["data_validity_comment"].dropna ()
-        if len (series_candidate )==0 :
-            return
-        non_null_comments_series =series_candidate .astype ("string")
-        whitelist_set :set [str ]=set (whitelist )
-        def _is_unknown (value :str )->bool :
-            return value not in whitelist_set
-        unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-        unknown_count =int (unknown_mask .sum ())
-        if unknown_count >0 :
-            unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-            log .warning ("soft_enum_unknown_data_validity_comment",unknown_count =unknown_count ,total_count =len (non_null_comments_series ),samples =unknown_values ,whitelist =whitelist ,message ="Unknown data_validity_comment values detected (soft enum: not blocking)")
-    def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-        reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        errors :list [str ]=[]
-        for field in reference_fields :
-            if field not in df .columns :
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-                continue
-            mask =df [field ].notna ()
-            if not mask .any ():
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-                continue
-            invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-                errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-                log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-        if errors :
-            log .error ("foreign_key_integrity_check_failed",errors =errors )
-            msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-            raise ValueError (msg )
-        log .debug ("foreign_key_integrity_verified")
-    def _log_detailed_validation_errors (self ,failure_cases :pd .DataFrame ,payload :pd .DataFrame ,log :BoundLogger )->None :
-        "Log individual validation errors with row index and activity_id."
-        if failure_cases .empty or payload .empty :
-            return
-        activity_id_col ="activity_id"if "activity_id"in payload .columns else None
-        index_col ="index"if "index"in failure_cases .columns else None
-        if index_col is None :
-            return
-        max_errors =20
-        errors_to_log =failure_cases .head (max_errors )
-        for _ ,error_row in errors_to_log .iterrows ():
-            row_index =error_row .get (index_col )
-            if row_index is None :
-                continue
-            error_details :dict [str ,Any ]={"row_index":int (row_index )if isinstance (row_index ,(int ,float ))else str (row_index )}
-            if activity_id_col :
-                try :
-                    idx =int (row_index )if isinstance (row_index ,(int ,float ))else row_index
-                    activity_id_value :Any =payload .at [cast (int ,idx ),activity_id_col ]
-                    activity_id =activity_id_value
-                except (KeyError ,IndexError ):
-                    activity_id =None
-                if activity_id is not None and pd .notna (activity_id ):
-                    error_details ["activity_id"]=int (activity_id )if isinstance (activity_id ,(int ,float ))else str (activity_id )
-            if "column"in error_row and pd .notna (error_row ["column"]):
-                error_details ["column"]=str (error_row ["column"])
-            if "schema_context"in error_row and pd .notna (error_row ["schema_context"]):
-                error_details ["schema_context"]=str (error_row ["schema_context"])
-            if "failure_case"in error_row and pd .notna (error_row ["failure_case"]):
-                error_details ["failure_case"]=str (error_row ["failure_case"])
-            log .error ("validation_error_detail",**error_details )
-        if len (failure_cases )>max_errors :
-            log .warning ("validation_errors_truncated",total_errors =len (failure_cases ),logged_errors =max_errors )
-    def build_quality_report (self ,df :pd .DataFrame )->pd .DataFrame |dict [str ,object ]|None :
-        "Return QC report with activity-specific metrics including distributions."
-        business_key =["activity_id"]if "activity_id"in df .columns else None
-        base_report =build_default_quality_report (df ,business_key_fields =business_key )
-        rows :list [dict [str ,Any ]]=[]
-        if not base_report .empty :
-            records_raw =base_report .to_dict ("records")
-            records :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],records_raw )
-            for record in records :
-                rows .append ({str (k ):v for k ,v in record .items ()})
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        foreign_key_fields =["assay_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"]
-        for field in foreign_key_fields :
-            if field in df .columns :
-                mask =df [field ].notna ()
-                if mask .any ():
-                    string_series =df [field ].astype (str )
-                    valid_mask =mask &string_series .str .match (chembl_id_pattern .pattern ,na =False )
-                    invalid_count =int ((mask &~valid_mask ).astype (int ).sum ())
-                    valid_count =int (valid_mask .astype (int ).sum ())
-                    total_count =int (mask .astype (int ).sum ())
-                    integrity_ratio =float (valid_count /total_count )if total_count >0 else 0.0
-                    rows .append ({"section":"foreign_key","metric":"integrity_ratio","column":field ,"value":float (integrity_ratio ),"valid_count":int (valid_count ),"invalid_count":int (invalid_count ),"total_count":int (total_count )})
-        if "standard_type"in df .columns :
-            type_counts_series :pd .Series [Any ]=df ["standard_type"].value_counts ()
-            type_dist_raw =type_counts_series .to_dict ()
-            type_dist :dict [Any ,int ]=cast (dict [Any ,int ],type_dist_raw )
-            for type_value ,count in type_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_type_count","column":"standard_type","value":str (type_value )if type_value is not None else "null","count":int (count )})
-        if "standard_units"in df .columns :
-            unit_counts_series :pd .Series [Any ]=df ["standard_units"].value_counts ()
-            unit_dist_raw =unit_counts_series .to_dict ()
-            unit_dist :dict [Any ,int ]=cast (dict [Any ,int ],unit_dist_raw )
-            for unit_value ,count in unit_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_units_count","column":"standard_units","value":str (unit_value )if unit_value is not None else "null","count":int (count )})
-        return pd .DataFrame (rows )
-    def write (self ,df :pd .DataFrame ,output_path :Path ,*,extended :bool =False ,include_correlation :bool |None =None ,include_qc_metrics :bool |None =None )->RunResult :
-        "Override write() to bind actor and ensure deterministic sorting.\n\n        Parameters\n        ----------\n        df:\n            The DataFrame to write.\n        output_path:\n            The base output path for all artifacts.\n        extended:\n            Whether to include extended QC artifacts.\n\n        Returns\n        -------\n        RunResult:\n            All artifacts generated by the write operation.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.write')
-        UnifiedLogger .bind (actor =self .actor )
-        sort_keys =["assay_chembl_id","testitem_chembl_id","activity_id"]
-        if df .empty or not all ((key in df .columns for key in sort_keys )):
-            return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-        original_sort_by =self .config .determinism .sort .by
-        if not original_sort_by or original_sort_by !=sort_keys :
-            from copy import deepcopy
-            from bioetl .config .models .determinism import DeterminismSortingConfig
-            modified_config =deepcopy (self .config )
-            modified_config .determinism .sort =DeterminismSortingConfig (by =sort_keys ,ascending =[True ,True ,True ],na_position ="last")
-            log .debug ("write_sort_config_set",sort_keys =sort_keys ,original_sort_keys =list (original_sort_by )if original_sort_by else [])
-            original_config =self .config
-            self .config =modified_config
-            try :
-                result =super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-            finally :
-                self .config =original_config
-            return result
-        return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
```

#### Горячий участок 2

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:111-115
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,5 +0,0 @@

-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
```

#### Горячий участок 3

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2901-2924
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,18 +0,0 @@

-def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_added",value ="activity")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_filled",value ="activity")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Горячий участок 4

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:343-415
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,22 +0,0 @@

-def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-    "Construct the descriptor driving the shared extraction template."
-    def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-        http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-        chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-        typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =source_config .parameters .select_fields
-        return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-    def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-    def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        df =pipeline ._ensure_comment_fields (df ,log )
-        chembl_client =cast (ChemblClient ,context .chembl_client )
-        if chembl_client is not None :
-            df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-        pipeline ._log_validity_comments_metrics (df ,log )
-        return df
-    def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        return pipeline ._materialize_activity_record (payload )
-    return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
```

#### Горячий участок 5

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1257-1267
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,7 +0,0 @@

-def _cache_directory (self ,release :str |None )->Path :
-    cache_root =Path (self .config .paths .cache_root )
-    directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-    release_component =self ._sanitize_cache_component (release or "unknown")
-    pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-    version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-    return cache_root /directory_name /pipeline_component /release_component /version_component
```

#### Горячий участок 6

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1252-1255
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,4 +0,0 @@

-def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-    directory =self ._cache_directory (release )
-    cache_key =self ._cache_key (batch_ids ,release )
-    return directory /f'{cache_key }.json'
```

#### Горячий участок 7

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1269-1277
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,4 +0,0 @@

-def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-    payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-    raw =json .dumps (payload ,sort_keys =True )
-    return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
```

#### Горячий участок 8

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:3108-3131
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,13 +0,0 @@

-def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check uniqueness of activity_id before validation."
-    if "activity_id"not in df .columns :
-        log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-        return
-    duplicates =df [df ["activity_id"].duplicated (keep =False )]
-    if not duplicates .empty :
-        duplicate_count =len (duplicates )
-        duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-        log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-        msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-        raise ValueError (msg )
-    log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
```

#### Горячий участок 9

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1176-1221
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,33 +0,0 @@

-def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-    cache_config =self .config .cache
-    if not cache_config .enabled :
-        return None
-    normalized_ids =[str (identifier )for identifier in batch_ids ]
-    cache_file =self ._cache_file_path (normalized_ids ,release )
-    if not cache_file .exists ():
-        return None
-    try :
-        stat =cache_file .stat ()
-    except OSError :
-        return None
-    ttl_seconds =int (cache_config .ttl )
-    if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    try :
-        payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-    except (OSError ,json .JSONDecodeError ):
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    if not isinstance (payload ,dict ):
-        return None
-    missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-    if missing :
-        return None
-    return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
```

#### Горячий участок 10

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:3185-3232
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,24 +0,0 @@

-def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-    reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-    chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-    errors :list [str ]=[]
-    for field in reference_fields :
-        if field not in df .columns :
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-            continue
-        mask =df [field ].notna ()
-        if not mask .any ():
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-            continue
-        invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-        if invalid_mask .any ():
-            invalid_count =int (invalid_mask .sum ())
-            invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-            errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-            log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-    if errors :
-        log .error ("foreign_key_integrity_check_failed",errors =errors )
-        msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-        raise ValueError (msg )
-    log .debug ("foreign_key_integrity_verified")
```

#### Горячий участок 11

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:432-450
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,14 +0,0 @@

-def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-    "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-    if isinstance (dataset ,pd .Series ):
-        return dataset .to_frame (name ="activity_id")
-    if isinstance (dataset ,pd .DataFrame ):
-        return dataset
-    if isinstance (dataset ,Mapping ):
-        mapping =cast (Mapping [str ,Any ],dataset )
-        return pd .DataFrame ([dict (mapping )])
-    if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-        dataset_list :list [Any ]=list (dataset )
-        return pd .DataFrame ({"activity_id":dataset_list })
-    msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-    raise TypeError (msg )
```

#### Горячий участок 12

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2065-2068
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,5 +0,0 @@

-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Горячий участок 13

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:498-637
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,83 +0,0 @@

-def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-    "Iterate over IDs using the shared iterator while preserving cache semantics."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    records :list [dict [str ,Any ]]=[]
-    success_count =0
-    fallback_count =0
-    error_count =0
-    cache_hits =0
-    api_calls =0
-    total_batches =0
-    key_order =[key for _ ,key in normalized_ids ]
-    key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-    for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-        total_batches +=1
-        batch_start =time .perf_counter ()
-        from_cache =False
-        chunk_records :dict [str ,dict [str ,Any ]]={}
-        try :
-            cached_records =self ._check_cache (chunk ,self ._chembl_release )
-            if cached_records is not None :
-                from_cache =True
-                cache_hits +=len (chunk )
-                chunk_records =cached_records
-            else :
-                api_calls +=1
-                fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                for item in fetched_items :
-                    if not isinstance (item ,Mapping ):
-                        continue
-                    activity_value =item .get ("activity_id")
-                    if activity_value is None :
-                        continue
-                    chunk_records [str (activity_value )]=dict (item )
-                self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-            success_in_batch =0
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                record =chunk_records .get (key )
-                if record and (not record .get ("error")):
-                    materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                    records .append (materialized )
-                    success_count +=1
-                    success_in_batch +=1
-                else :
-                    fallback_record =self ._create_fallback_record (numeric_id )
-                    records .append (fallback_record )
-                    fallback_count +=1
-                    error_count +=1
-            batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-            log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-        except CircuitBreakerOpenError as exc :
-            log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except RequestException as exc :
-            log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except Exception as exc :
-            log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-    total_records =len (normalized_ids )
-    success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-    summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-    return (records ,summary )
```

#### Горячий участок 14

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1284-1323
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,16 +0,0 @@

-def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-    "Create fallback record enriched with error metadata."
-    base_message ="Fallback: ChEMBL activity unavailable"
-    message =f'{base_message } ({error })'if error else base_message
-    timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-    metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-    if isinstance (error ,RequestException ):
-        response =getattr (error ,"response",None )
-        status_code =getattr (response ,"status_code",None )
-        if status_code is not None :
-            metadata ["http_status"]=status_code
-        metadata ["error_message"]=str (error )
-    elif error is not None :
-        metadata ["error_message"]=str (error )
-    fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-    return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
```

#### Горячий участок 15

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:2843-2899
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,15 +0,0 @@

-def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-    "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-    seen :set [tuple [Any ,...]]=set ()
-    deduplicated :list [dict [str ,Any ]]=[]
-    duplicates_removed =0
-    for prop in properties :
-        dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-        if dedup_key not in seen :
-            seen .add (dedup_key )
-            deduplicated .append (prop )
-        else :
-            duplicates_removed +=1
-            log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-    stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-    return (deduplicated ,stats )
```

#### Горячий участок 16

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:892-935
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,28 +0,0 @@

-def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    assay_section :Any =enrich_section .get ("assay")
-                    if isinstance (assay_section ,Mapping ):
-                        assay_section =cast (Mapping [str ,Any ],assay_section )
-                        enrich_cfg =dict (assay_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_assay (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 17

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:822-867
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,28 +0,0 @@

-def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    compound_record_section :Any =enrich_section .get ("compound_record")
-                    if isinstance (compound_record_section ,Mapping ):
-                        compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                        enrich_cfg =dict (compound_record_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 18

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:960-1014
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,32 +0,0 @@

-def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    data_validity_section :Any =enrich_section .get ("data_validity")
-                    if isinstance (data_validity_section ,Mapping ):
-                        data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                        enrich_cfg =dict (data_validity_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    if "data_validity_description"in df .columns :
-        non_na_count =int (df ["data_validity_description"].notna ().sum ())
-        if non_na_count >0 :
-            log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 19

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1039-1108
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,37 +0,0 @@

-def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    molecule_section :Any =enrich_section .get ("molecule")
-                    if isinstance (molecule_section ,Mapping ):
-                        molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                        enrich_cfg =dict (molecule_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-    if "molecule_name"in df_join .columns :
-        if "molecule_pref_name"not in df .columns :
-            df ["molecule_pref_name"]=pd .NA
-        mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-        if mask .any ():
-            df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-            df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-            log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-    return df
```

#### Горячий участок 20

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\run.py:1325-1358
- testitem: нет в ветке

```diff
--- activity:run.py

+++ testitem:run.py

@@ -1,11 +0,0 @@

-def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-    if df .empty :
-        return df
-    required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-    missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-    if missing_fields :
-        for field in missing_fields :
-            df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-        log .debug ("comment_fields_ensured",fields =missing_fields )
-    return df
```

### Модуль transform.py

Определение        | activity сигнатура | testitem сигнатура                                             | Побочные эффекты                                 | Исключения                | Статус           
-------------------|--------------------|----------------------------------------------------------------|--------------------------------------------------|---------------------------|------------------
__module_block_0   | —                  | —                                                              | activity: {}
testitem: {'logging': [], 'io': []} | activity: []
testitem: [] | только в testitem
__module_block_1   | —                  | —                                                              | activity: {}
testitem: {'logging': [], 'io': []} | activity: []
testitem: [] | только в testitem
__module_block_2   | —                  | —                                                              | activity: {}
testitem: {'logging': [], 'io': []} | activity: []
testitem: [] | только в testitem
__module_block_3   | —                  | —                                                              | activity: {}
testitem: {'logging': [], 'io': []} | activity: []
testitem: [] | только в testitem
__module_block_4   | —                  | —                                                              | activity: {}
testitem: {'logging': [], 'io': []} | activity: []
testitem: [] | только в testitem
__module_block_5   | —                  | —                                                              | activity: {}
testitem: {'logging': [], 'io': []} | activity: []
testitem: [] | только в testitem
__module_block_6   | —                  | —                                                              | activity: {}
testitem: {'logging': [], 'io': []} | activity: []
testitem: [] | только в testitem
flatten_object_col | —                  | df: pd.DataFrame, col: str, fields: Sequence[str], prefix: str | activity: {}
testitem: {'logging': [], 'io': []} | activity: []
testitem: [] | только в testitem
transform          | —                  | df: pd.DataFrame, cfg: Any                                     | activity: {}
testitem: {'logging': [], 'io': []} | activity: []
testitem: [] | только в testitem

#### Горячий участок 1

- activity: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:1-1

```diff
--- activity:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+"Transform utilities for ChEMBL testitem pipeline array serialization and flattening."
```

#### Горячий участок 2

- activity: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:3-3

```diff
--- activity:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+from __future__ import annotations
```

#### Горячий участок 3

- activity: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:10-10

```diff
--- activity:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+from bioetl .core .serialization import serialize_objects ,serialize_simple_list
```

#### Горячий участок 4

- activity: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:5-5

```diff
--- activity:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+from collections .abc import Sequence
```

#### Горячий участок 5

- activity: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:6-6

```diff
--- activity:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+from typing import Any
```

#### Горячий участок 6

- activity: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:8-8

```diff
--- activity:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+import pandas as pd
```

#### Горячий участок 7

- activity: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:12-17

```diff
--- activity:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+__all__ =["serialize_simple_list","serialize_objects","flatten_object_col","transform"]
```

#### Горячий участок 8

- activity: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:20-75

```diff
--- activity:transform.py

+++ testitem:transform.py

@@ -0,0 +1,16 @@

+def flatten_object_col (df :pd .DataFrame ,col :str ,fields :Sequence [str ],prefix :str )->pd .DataFrame :
+    "Flatten nested object column into flat columns with prefix.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    col:\n        Column name containing nested objects.\n    fields:\n        List of field names to extract from nested objects.\n    prefix:\n        Prefix to add to flattened column names (e.g., \"molecule_hierarchy__\").\n\n    Returns\n    -------\n    pd.DataFrame:\n        DataFrame with flattened columns added and original column removed.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame({\n    ...     \"molecule_chembl_id\": [\"CHEMBL1\"],\n    ...     \"molecule_hierarchy\": [{\"molecule_chembl_id\": \"CHEMBL1\", \"parent_chembl_id\": \"CHEMBL2\"}],\n    ... })\n    >>> result = flatten_object_col(df, \"molecule_hierarchy\", [\"molecule_chembl_id\", \"parent_chembl_id\"], \"molecule_hierarchy__\")\n    >>> \"molecule_hierarchy__molecule_chembl_id\" in result.columns\n    True\n    >>> \"molecule_hierarchy__parent_chembl_id\" in result.columns\n    True\n    >>> \"molecule_hierarchy\" not in result.columns\n    True\n    "
+    df =df .copy ()
+    if col not in df .columns :
+        for f in fields :
+            df [f'{prefix }{f }']=None
+        return df
+    def row_to_dict (obj :Any )->dict [str ,Any ]:
+        "Extract fields from nested object."
+        if not isinstance (obj ,dict ):
+            return dict .fromkeys (fields )
+        return {f :obj .get (f )for f in fields }
+    expanded =df [col ].map (row_to_dict ).apply (pd .Series )
+    expanded .columns =[f'{prefix }{c }'for c in expanded .columns ]
+    df =df .drop (columns =[col ])
+    return pd .concat ([df ,expanded ],axis =1 )
```

#### Горячий участок 9

- activity: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:78-138

```diff
--- activity:transform.py

+++ testitem:transform.py

@@ -0,0 +1,25 @@

+def transform (df :pd .DataFrame ,cfg :Any )->pd .DataFrame :
+    "Transform testitem DataFrame by flattening nested objects and serializing arrays.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    cfg:\n        Pipeline config with transform section (enable_flatten, enable_serialization,\n        arrays_simple_to_pipe, arrays_objects_to_header_rows, flatten_objects).\n\n    Returns\n    -------\n    pd.DataFrame:\n        Transformed DataFrame with flattened objects and serialized arrays.\n    "
+    df =df .copy ()
+    enable_flatten =getattr (cfg .transform ,"enable_flatten",True )if hasattr (cfg ,"transform")else True
+    enable_serialization =getattr (cfg .transform ,"enable_serialization",True )if hasattr (cfg ,"transform")else True
+    if enable_flatten and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"flatten_objects"):
+        flatten_objects =cfg .transform .flatten_objects
+        if isinstance (flatten_objects ,dict ):
+            for obj_col ,fields in flatten_objects .items ():
+                if isinstance (fields ,Sequence )and (not isinstance (fields ,(str ,bytes ))):
+                    df =flatten_object_col (df ,obj_col ,fields ,prefix =f'{obj_col }__')
+    if enable_serialization and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"arrays_simple_to_pipe"):
+        arrays_simple =cfg .transform .arrays_simple_to_pipe
+        if isinstance (arrays_simple ,Sequence )and (not isinstance (arrays_simple ,(str ,bytes ))):
+            for col in arrays_simple :
+                if col in df .columns :
+                    df [col ]=df [col ].map (serialize_simple_list )
+    if enable_serialization and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"arrays_objects_to_header_rows"):
+        arrays_objects =cfg .transform .arrays_objects_to_header_rows
+        if isinstance (arrays_objects ,Sequence )and (not isinstance (arrays_objects ,(str ,bytes ))):
+            for col in arrays_objects :
+                if col in df .columns :
+                    df [f'{col }__flat']=df [col ].map (serialize_objects )
+                    df =df .drop (columns =[col ])
+    return df
```

### Модуль normalize.py

Определение                 | activity сигнатура                                                           | testitem сигнатура | Побочные эффекты                                                                                            | Исключения                | Статус           
----------------------------|------------------------------------------------------------------------------|--------------------|-------------------------------------------------------------------------------------------------------------|---------------------------|------------------
__module_block_0            | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_1            | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_10           | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_11           | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_12           | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_13           | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_14           | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_15           | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_16           | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_2            | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_3            | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_4            | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_5            | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_6            | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_7            | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_8            | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
__module_block_9            | —                                                                            | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
_enrich_by_pairs            | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any], log: Any | —                  | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {}                      | activity: []
testitem: [] | только в activity
_enrich_by_record_id        | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any], log: Any | —                  | activity: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {}                      | activity: []
testitem: [] | только в activity
_extract_first_present      | record: Mapping[str, Any], keys: Iterable[str]                               | —                  | activity: {'logging': [], 'io': []}
testitem: {}                                                            | activity: []
testitem: [] | только в activity
enrich_with_assay           | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                  | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {} | activity: []
testitem: [] | только в activity
enrich_with_compound_record | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                  | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {} | activity: []
testitem: [] | только в activity
enrich_with_data_validity   | df_act: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any]           | —                  | activity: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {} | activity: []
testitem: [] | только в activity

_Показаны первые 20 горячих участков из 23._

#### Горячий участок 1

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:1-1
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-"Enrichment functions for Activity pipeline."
```

#### Горячий участок 2

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:3-3
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from __future__ import annotations
```

#### Горячий участок 3

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:9-9
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-import pandas as pd
```

#### Горячий участок 4

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:21-21
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-__all__ =["enrich_with_assay","enrich_with_compound_record","enrich_with_data_validity"]
```

#### Горячий участок 5

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:24-24
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-_ensure_columns =ensure_columns
```

#### Горячий участок 6

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:27-35
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-_COMPOUND_FIELD_ALIASES :dict [str ,tuple [str ,...]]={"compound_name":("compound_name","pref_name","PREF_NAME"),"compound_key":("compound_key","standard_inchi_key","STANDARD_INCHI_KEY"),"curated":("curated","CURATED")}
```

#### Горячий участок 7

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:37-40
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-_ASSAY_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_organism","string"),("assay_tax_id","Int64"))
```

#### Горячий участок 8

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:42-47
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-_COMPOUND_COLUMNS :tuple [tuple [str ,str ],...]=(("compound_name","string"),("compound_key","string"),("curated","boolean"),("removed","boolean"))
```

#### Горячий участок 9

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:49-49
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-_DATA_VALIDITY_COLUMNS :tuple [tuple [str ,str ],...]=(("data_validity_description","string"),)
```

#### Горячий участок 10

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:12-12
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .clients .chembl import ChemblClient
```

#### Горячий участок 11

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:13-13
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .frame import ensure_columns
```

#### Горячий участок 12

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:14-14
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .logger import UnifiedLogger
```

#### Горячий участок 13

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:15-19
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .schemas .activity import ASSAY_ENRICHMENT_SCHEMA ,COMPOUND_RECORD_ENRICHMENT_SCHEMA ,DATA_VALIDITY_ENRICHMENT_SCHEMA
```

#### Горячий участок 14

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:5-5
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from collections .abc import Iterable ,Mapping
```

#### Горячий участок 15

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:10-10
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from pandas import Series
```

#### Горячий участок 16

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:6-6
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from typing import Any
```

#### Горячий участок 17

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:8-8
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-import numpy as np
```

#### Горячий участок 18

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:439-615
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1,65 +0,0 @@

-def _enrich_by_pairs (df_act :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ],log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame activity \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u044b (molecule_chembl_id, document_chembl_id)."
-    pairs_df =df_act [["molecule_chembl_id","document_chembl_id"]].astype ("string").copy ()
-    for column in pairs_df .columns :
-        pairs_df [column ]=pairs_df [column ].str .strip ().str .upper ()
-    pairs_df =pairs_df .dropna ()
-    pairs_df =pairs_df .drop_duplicates ()
-    pairs :set [tuple [str ,str ]]=set (map (tuple ,pairs_df .to_numpy ()))
-    if not pairs :
-        log .debug ("enrichment_by_pairs_skipped_no_valid_pairs")
-        return df_act
-    fields =cfg .get ("fields",["molecule_chembl_id","document_chembl_id","compound_name","compound_key","curated"])
-    page_limit =cfg .get ("page_limit",1000 )
-    log .info ("enrichment_fetching_compound_records_by_pairs",pairs_count =len (pairs ))
-    compound_records_dict :dict [tuple [str ,str ],dict [str ,Any ]]={}
-    try :
-        compound_records_dict =client .fetch_compound_records_by_pairs (pairs =pairs ,fields =list (fields ),page_limit =page_limit )or {}
-    except Exception as exc :
-        log .warning ("enrichment_fetch_error_by_pairs",pairs_count =len (pairs ),error =str (exc ),exc_info =True )
-        return df_act
-    enrichment_data :list [dict [str ,Any ]]=[]
-    pairs_found =0
-    pairs_not_found =0
-    for pair in pairs :
-        compound_record :dict [str ,Any ]|None =compound_records_dict .get (pair )
-        if compound_record :
-            record_mapping :Mapping [str ,Any ]=compound_record
-            compound_name_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("compound_name",("compound_name",)))
-            compound_key_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("compound_key",("compound_key",)))
-            curated_raw =_extract_first_present (record_mapping ,_COMPOUND_FIELD_ALIASES .get ("curated",("curated",)))
-            compound_name =None
-            if compound_name_raw is not None :
-                name_str =str (compound_name_raw ).strip ()
-                compound_name =name_str if name_str else None
-            compound_key =None
-            if compound_key_raw is not None :
-                key_str =str (compound_key_raw ).strip ()
-                compound_key =key_str if key_str else None
-            curated =curated_raw if curated_raw is not None else None
-            pairs_found +=1
-            enrichment_data .append ({"molecule_chembl_id":pair [0 ],"document_chembl_id":pair [1 ],"compound_name":compound_name ,"compound_key":compound_key ,"curated":curated ,"removed":None })
-        else :
-            pairs_not_found +=1
-            enrichment_data .append ({"molecule_chembl_id":pair [0 ],"document_chembl_id":pair [1 ],"compound_name":None ,"compound_key":None ,"curated":None ,"removed":None })
-    log .info ("enrichment_by_pairs_complete",pairs_requested =len (pairs ),pairs_found =pairs_found ,pairs_not_found =pairs_not_found ,records_returned =len (compound_records_dict ))
-    if pairs_not_found >0 :
-        log .warning ("enrichment_by_pairs_some_pairs_not_found",pairs_not_found =pairs_not_found ,pairs_total =len (pairs ),hint ="\u041f\u0440\u043e\u0432\u0435\u0440\u044c\u0442\u0435, \u0447\u0442\u043e \u043f\u0430\u0440\u044b (molecule_chembl_id, document_chembl_id) \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 ChEMBL API")
-    if not enrichment_data :
-        log .debug ("enrichment_by_pairs_no_records_found")
-        return df_act
-    df_enrich =pd .DataFrame (enrichment_data )
-    df_enrich ["molecule_chembl_id"]=df_enrich ["molecule_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_enrich ["document_chembl_id"]=df_enrich ["document_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_act ["molecule_chembl_id_normalized"]=df_act ["molecule_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_act ["document_chembl_id_normalized"]=df_act ["document_chembl_id"].astype ("string").str .strip ().str .upper ()
-    df_result =df_act .merge (df_enrich ,left_on =["molecule_chembl_id_normalized","document_chembl_id_normalized"],right_on =["molecule_chembl_id","document_chembl_id"],how ="left",suffixes =("","_enrich"))
-    df_result =df_result .drop (columns =["molecule_chembl_id_normalized","document_chembl_id_normalized"])
-    for col in ["compound_name","compound_key","curated"]:
-        if f'{col }_enrich'in df_result .columns :
-            if col not in df_result .columns :
-                df_result [col ]=df_result [f'{col }_enrich']
-            else :
-                df_result [col ]=df_result [col ].where (df_result [col ].notna (),df_result [f'{col }_enrich'])
-            df_result =df_result .drop (columns =[f'{col }_enrich'])
-    return df_result
```

#### Горячий участок 19

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:618-755
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1,67 +0,0 @@

-def _enrich_by_record_id (df_act :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ],log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame activity \u0447\u0435\u0440\u0435\u0437 record_id (fallback \u0434\u043b\u044f \u0441\u0442\u0440\u043e\u043a \u0431\u0435\u0437 document_chembl_id)."
-    record_ids :set [str ]=set ()
-    for _ ,row in df_act .iterrows ():
-        rec =row .get ("record_id")
-        if rec is not None and (not pd .isna (rec )):
-            rec_s =str (rec ).strip ()
-            if rec_s :
-                record_ids .add (rec_s )
-    if not record_ids :
-        log .debug ("enrichment_by_record_id_skipped_no_valid_ids")
-        return df_act
-    fields =["record_id","compound_name","compound_key"]
-    page_limit =cfg .get ("page_limit",1000 )
-    batch_size =int (cfg .get ("batch_size",100 ))or 100
-    log .info ("enrichment_fetching_compound_records_by_record_id",record_ids_count =len (record_ids ))
-    compound_records_dict :dict [str ,dict [str ,Any ]]={}
-    try :
-        unique_ids =list (record_ids )
-        all_records :list [dict [str ,Any ]]=[]
-        for i in range (0 ,len (unique_ids ),batch_size ):
-            chunk =unique_ids [i :i +batch_size ]
-            params :dict [str ,Any ]={"record_id__in":",".join (chunk ),"limit":page_limit ,"only":",".join (fields ),"order_by":"record_id"}
-            try :
-                for record in client .paginate ("/compound_record.json",params =params ,page_size =page_limit ,items_key ="compound_records"):
-                    all_records .append (dict (record ))
-            except Exception as exc :
-                log .warning ("enrichment_fetch_error_by_record_id",chunk_size =len (chunk ),error =str (exc ),exc_info =True )
-        for record in all_records :
-            rid_raw =record .get ("record_id")
-            if rid_raw is None :
-                continue
-            rid_str =str (rid_raw ).strip ()
-            if rid_str and rid_str not in compound_records_dict :
-                compound_records_dict [rid_str ]={"record_id":rid_str ,"compound_key":record .get ("compound_key"),"compound_name":record .get ("compound_name")}
-    except Exception as exc :
-        log .warning ("enrichment_fetch_error_by_record_id",record_ids_count =len (record_ids ),error =str (exc ),exc_info =True )
-        return df_act
-    if not compound_records_dict :
-        log .debug ("enrichment_by_record_id_no_records_found")
-        return df_act
-    compound_data :list [dict [str ,Any ]]=[]
-    for record_id ,record in compound_records_dict .items ():
-        compound_data .append ({"record_id":record_id ,"compound_key":record .get ("compound_key"),"compound_name":record .get ("compound_name")})
-    df_compound =pd .DataFrame (compound_data )if compound_data else pd .DataFrame (columns =["record_id","compound_key","compound_name"])
-    df_act_normalized =df_act .copy ()
-    if "record_id"in df_act_normalized .columns :
-        mask_na =df_act_normalized ["record_id"].isna ()
-        df_act_normalized ["record_id"]=df_act_normalized ["record_id"].astype (str )
-        df_act_normalized .loc [df_act_normalized ["record_id"]=="nan","record_id"]=pd .NA
-        df_act_normalized .loc [mask_na ,"record_id"]=pd .NA
-        if "record_id"in df_compound .columns and (not df_compound .empty ):
-            df_compound ["record_id"]=df_compound ["record_id"].astype (str )
-            df_compound .loc [df_compound ["record_id"]=="nan","record_id"]=pd .NA
-    df_result =df_act_normalized .merge (df_compound ,on =["record_id"],how ="left",suffixes =("","_compound"))
-    for col in ["compound_name","compound_key"]:
-        if f'{col }_compound'in df_result .columns :
-            if col not in df_result .columns :
-                df_result [col ]=df_result [f'{col }_compound']
-            else :
-                df_result [col ]=df_result [col ].where (df_result [col ].notna (),df_result [f'{col }_compound'])
-            df_result =df_result .drop (columns =[f'{col }_compound'])
-    if "curated"not in df_result .columns :
-        df_result ["curated"]=pd .NA
-    if "removed"not in df_result .columns :
-        df_result ["removed"]=pd .NA
-    return df_result
```

#### Горячий участок 20

- activity: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\activity\normalize.py:52-64
- testitem: нет в ветке

```diff
--- activity:normalize.py

+++ testitem:normalize.py

@@ -1,11 +0,0 @@

-def _extract_first_present (record :Mapping [str ,Any ],keys :Iterable [str ])->Any :
-    "\u0412\u043e\u0437\u0432\u0440\u0430\u0442\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u043e \u043f\u0435\u0440\u0432\u043e\u043c\u0443 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u043c\u0443 \u0430\u043b\u0438\u0430\u0441\u0443."
-    for key in keys :
-        if key in record :
-            return record [key ]
-    lowered_map ={str (k ).lower ():v for k ,v in record .items ()}
-    for key in keys :
-        candidate =str (key ).lower ()
-        if candidate in lowered_map :
-            return lowered_map [candidate ]
-    return None
```

---

## Пара: assay ↔ document

- AST hash: 4e65bc8b94391cb8c868cf6ad530c2c6 ↔ 3774d930e14eb5befd7fdffc255cb346

- Jaccard по токенам: 0.288

### Модуль run.py

Определение                                          | assay сигнатура                                                   | document сигнатура                        | Побочные эффекты                                                                                         | Исключения                             | Статус           
-----------------------------------------------------|-------------------------------------------------------------------|-------------------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------|------------------
ChemblAssayPipeline                                  | —                                                                 | —                                         | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
document: {} | assay: ['TypeError(msg)']
document: [] | только в assay   
ChemblAssayPipeline.__init__                         | self, config: PipelineConfig, run_id: str                         | —                                         | assay: {'logging': [], 'io': []}
document: {}                                                            | assay: []
document: []                 | только в assay   
ChemblAssayPipeline._add_row_metadata                | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: {'logging': ['log.debug'], 'io': []}
document: {}                                                 | assay: []
document: []                 | только в assay   
ChemblAssayPipeline._build_assay_descriptor          | self: SelfChemblAssayPipeline                                     | —                                         | assay: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
document: {}                      | assay: ['TypeError(msg)']
document: [] | только в assay   
ChemblAssayPipeline._check_missing_columns           | self, df: pd.DataFrame, log: Any, select_fields: list[str] | None | —                                         | assay: {'logging': ['log.debug', 'log.warning'], 'io': []}
document: {}                                  | assay: []
document: []                 | только в assay   
ChemblAssayPipeline._enrich_with_related_data        | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
document: {}                      | assay: []
document: []                 | только в assay   
ChemblAssayPipeline._harmonize_identifier_columns    | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: {'logging': ['log.debug'], 'io': []}
document: {}                                                 | assay: []
document: []                 | только в assay   
ChemblAssayPipeline._normalize_data_types            | self, df: pd.DataFrame, schema: Any, log: Any                     | —                                         | assay: {'logging': ['log.debug'], 'io': []}
document: {}                                                 | assay: []
document: []                 | только в assay   
ChemblAssayPipeline._normalize_identifiers           | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: {'logging': ['log.debug'], 'io': []}
document: {}                                                 | assay: []
document: []                 | только в assay   
ChemblAssayPipeline._normalize_nested_structures     | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: {'logging': ['log.debug'], 'io': []}
document: {}                                                 | assay: []
document: []                 | только в assay   
ChemblAssayPipeline._normalize_string_fields         | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: {'logging': ['log.debug', 'log.warning'], 'io': []}
document: {}                                  | assay: []
document: []                 | только в assay   
ChemblAssayPipeline._serialize_array_fields          | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: {'logging': ['log.debug'], 'io': []}
document: {}                                                 | assay: []
document: []                 | только в assay   
ChemblAssayPipeline.chembl_release                   | self                                                              | —                                         | assay: {'logging': [], 'io': []}
document: {}                                                            | assay: []
document: []                 | только в assay   
ChemblAssayPipeline.extract                          | self, *args, **kwargs                                             | —                                         | assay: {'logging': ['UnifiedLogger.get'], 'io': []}
document: {}                                         | assay: []
document: []                 | только в assay   
ChemblAssayPipeline.extract_all                      | self                                                              | —                                         | assay: {'logging': [], 'io': []}
document: {}                                                            | assay: []
document: []                 | только в assay   
ChemblAssayPipeline.extract_by_ids                   | self, ids: Sequence[str]                                          | —                                         | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
document: {} | assay: []
document: []                 | только в assay   
ChemblAssayPipeline.transform                        | self, df: pd.DataFrame                                            | —                                         | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}
document: {}                | assay: []
document: []                 | только в assay   
ChemblDocumentPipeline                               | —                                                                 | —                                         | assay: {}
document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []} | assay: []
document: ['TypeError(msg)'] | только в document
ChemblDocumentPipeline.__init__                      | —                                                                 | self, config: PipelineConfig, run_id: str | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline._add_system_fields            | —                                                                 | self, df: pd.DataFrame, log: Any          | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline._build_document_descriptor    | —                                                                 | self: SelfChemblDocumentPipeline          | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: ['TypeError(msg)'] | только в document
ChemblDocumentPipeline._check_document_id_uniqueness | —                                                                 | self, df: pd.DataFrame, log: Any          | assay: {}
document: {'logging': ['log.warning'], 'io': []}                                               | assay: []
document: []                 | только в document
ChemblDocumentPipeline._coerce_mapping               | —                                                                 | payload: Any                              | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline._enrich_document_terms        | —                                                                 | self, df: pd.DataFrame                    | assay: {}
document: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}                          | assay: []
document: []                 | только в document
ChemblDocumentPipeline._extract_nested_fields        | —                                                                 | self, record: dict[str, Any]              | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline._normalize_authors            | —                                                                 | authors: Any, separator: str              | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline._normalize_doi                | —                                                                 | doi: str | None                           | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline._normalize_identifiers        | —                                                                 | self, df: pd.DataFrame, log: Any          | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline._normalize_journal            | —                                                                 | value: Any, max_len: int                  | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline._normalize_numeric_fields     | —                                                                 | self, df: pd.DataFrame, log: Any          | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline._normalize_string_fields      | —                                                                 | self, df: pd.DataFrame, log: Any          | assay: {}
document: {'logging': ['log.debug'], 'io': []}                                                 | assay: []
document: []                 | только в document
ChemblDocumentPipeline._schema_column_specs          | —                                                                 | self                                      | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline._should_enrich_document_terms | —                                                                 | self                                      | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline.extract                       | —                                                                 | self, *args, **kwargs                     | assay: {}
document: {'logging': ['UnifiedLogger.get'], 'io': []}                                         | assay: []
document: []                 | только в document
ChemblDocumentPipeline.extract_all                   | —                                                                 | self                                      | assay: {}
document: {'logging': [], 'io': []}                                                            | assay: []
document: []                 | только в document
ChemblDocumentPipeline.extract_by_ids                | —                                                                 | self, ids: Sequence[str]                  | assay: {}
document: {'logging': ['UnifiedLogger.get', 'log.info'], 'io': []}                             | assay: []
document: []                 | только в document
ChemblDocumentPipeline.transform                     | —                                                                 | self, df: pd.DataFrame                    | assay: {}
document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []} | assay: []
document: []                 | только в document
ChemblDocumentPipeline.validate                      | —                                                                 | self, df: pd.DataFrame                    | assay: {}
document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                | assay: []
document: []                 | только в document
__module_block_0                                     | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_1                                     | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | совпадает        
__module_block_10                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_11                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_12                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_13                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | совпадает        
__module_block_14                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | совпадает        
__module_block_15                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | совпадает        
__module_block_16                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | совпадает        
__module_block_17                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | совпадает        
__module_block_18                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_19                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_2                                     | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_20                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_21                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {}                                                            | assay: []
document: []                 | только в assay   
__module_block_22                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {}                                                            | assay: []
document: []                 | только в assay   
__module_block_26                                    | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {}                                                            | assay: []
document: []                 | только в assay   
__module_block_3                                     | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_4                                     | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_5                                     | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_6                                     | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_7                                     | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
__module_block_8                                     | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | совпадает        
__module_block_9                                     | —                                                                 | —                                         | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                     | assay: []
document: []                 | отличается       
_extract_bao_ids_from_classifications                | node: Any                                                         | —                                         | assay: {'logging': [], 'io': []}
document: {}                                                            | assay: []
document: []                 | только в assay   
_iter_classification_mappings                        | node: Any                                                         | —                                         | assay: {'logging': [], 'io': []}
document: {}                                                            | assay: []
document: []                 | только в assay   
_normalize_bao_identifier                            | raw_value: Any                                                    | —                                         | assay: {'logging': [], 'io': []}
document: {}                                                            | assay: []
document: []                 | только в assay   

_Показаны первые 20 горячих участков из 58._

#### Горячий участок 1

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:126-952
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,324 +0,0 @@

-class ChemblAssayPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting assay records from the ChEMBL API."
-    actor ="assay_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all assay records from ChEMBL using pagination."
-        descriptor =self ._build_assay_descriptor ()
-        return self .run_extract_all (descriptor )
-    def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-        "Return the descriptor powering the shared extraction template."
-        def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-            if isinstance (pipeline ,ChemblAssayPipeline ):
-                return pipeline
-            msg ="ChemblAssayPipeline instance required"
-            raise TypeError (msg )
-        def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-            chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-            assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-            assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-            assay_pipeline ._chembl_release =assay_client .chembl_release
-            log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-            raw_source =assay_pipeline ._resolve_source_config ("chembl")
-            select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-            log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-            context =ChemblExtractionContext (source_config ,assay_client )
-            context .chembl_client =chembl_client
-            context .select_fields =tuple (select_fields )if select_fields else None
-            context .chembl_release =assay_pipeline ._chembl_release
-            context .extra_filters ={"max_url_length":source_config .max_url_length }
-            return context
-        def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-        def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            if df .empty :
-                return df
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in df .columns or df [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            select_fields =context .select_fields
-            if select_fields :
-                expected_fields =set (select_fields )
-                actual_fields =set (df .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-            return df
-        def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-            return pd .DataFrame ()
-        def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-        return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        self ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        if self .config .cli .dry_run :
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-            return pd .DataFrame ()
-        limit =self .config .cli .limit
-        resolved_select_fields =self ._resolve_select_fields (source_raw )
-        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-        log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-        def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield dict (item )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            if dataframe .empty :
-                return dataframe
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            if context .select_fields :
-                expected_fields =set (context .select_fields )
-                actual_fields =set (dataframe .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-            return dataframe
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw assay data by normalizing identifiers, types, and nested structures."
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._enrich_with_related_data (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._serialize_array_fields (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,AssaySchema ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Serialize array-of-object fields to header+rows format."
-        df =df .copy ()
-        arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-        if arrays_to_serialize :
-            df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-            for column in arrays_to_serialize :
-                if column in df_result .columns :
-                    column_as_string :Series =df_result [column ].astype ("string")
-                    filled_column :Series =column_as_string .copy ()
-                    filled_column [column_as_string .isna ()]=""
-                    empty_mask :Series =filled_column .eq ("")
-                    if bool (empty_mask .any ()):
-                        df_result .loc [empty_mask ,column ]=pd .NA
-            df =df_result
-            log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-        return df
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-            df ["target_chembl_id"]=df ["target_id"]
-            actions .append ("target_id->target_chembl_id")
-        alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize ChEMBL identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-        working_df =df .copy ()
-        string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-        rules ={column :StringRule ()for column in string_fields }
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if "curation_level"not in normalized_df .columns :
-            normalized_df ["curation_level"]=pd .NA
-            log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-        df =df .copy ()
-        if "assay_parameters"in df .columns :
-            log .debug ("validating_assay_parameters_truv")
-            df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-        if "assay_classifications"in df .columns :
-            if "assay_class_id"not in df .columns :
-                df ["assay_class_id"]=pd .NA
-            updated_rows =0
-            classifications_series =df ["assay_classifications"]
-            for row_index ,value in classifications_series .items ():
-                if value is None or value is pd .NA :
-                    continue
-                if isinstance (value ,float )and pd .isna (value ):
-                    continue
-                if isinstance (value ,str ):
-                    continue
-                extracted_ids =_extract_bao_ids_from_classifications (value )
-                if not extracted_ids :
-                    continue
-                joined_ids =";".join (extracted_ids )
-                row_label =cast (Any ,row_index )
-                current_value =df .at [row_label ,"assay_class_id"]
-                if pd .isna (current_value )or current_value !=joined_ids :
-                    df .at [row_label ,"assay_class_id"]=joined_ids
-                    updated_rows +=1
-            if updated_rows >0 :
-                log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-        return df
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_added",value ="assay")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_filled",value ="assay")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-        "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-        df =super ()._normalize_data_types (df ,schema ,log )
-        if "row_index"in df .columns and df ["row_index"].isna ().any ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-        df =df .copy ()
-        optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-        expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-        select_fields_set :set [str ]=set ()
-        if select_fields is not None :
-            select_fields_set =set (select_fields )
-        missing_in_response :list [str ]=[]
-        missing_in_select_fields :list [str ]=[]
-        for column in expected_api_fields :
-            if column not in df .columns :
-                missing_in_response .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-                else :
-                    log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-        missing_columns :list [str ]=[]
-        for column ,version in optional_columns .items ():
-            if column not in df .columns :
-                df [column ]=pd .NA
-                missing_columns .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-                else :
-                    log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-        if missing_in_response or missing_columns :
-            log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-        return df
-    def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-        if df .empty :
-            return df
-        try :
-            source_raw =self ._resolve_source_config ("chembl")
-        except KeyError as exc :
-            log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-            return df
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        chembl_config =getattr (self .config ,"chembl",None )
-        if chembl_config is None :
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-            return df
-        if not isinstance (chembl_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-            return df
-        assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-        if not isinstance (assay_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-            return df
-        enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-        if not isinstance (enrich_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-            return df
-        classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-        if classifications_cfg is not None :
-            log .info ("enrichment_classifications_started")
-            df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-            df =df_with_classifications
-            log .info ("enrichment_classifications_completed")
-            if "assay_class_id"in df .columns :
-                filled_count =int (df ["assay_class_id"].notna ().sum ())
-                total_count =len (df )
-                if filled_count ==0 :
-                    log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-                else :
-                    log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-            else :
-                log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-        else :
-            log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-        parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-        if parameters_cfg is not None :
-            log .info ("enrichment_parameters_started")
-            df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-            df =df_with_parameters
-            log .info ("enrichment_parameters_completed")
-        return df
```

#### Горячий участок 2

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:131-133
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,3 +0,0 @@

-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
```

#### Горячий участок 3

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:681-704
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,18 +0,0 @@

-def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_added",value ="assay")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_filled",value ="assay")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Горячий участок 4

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:167-323
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,51 +0,0 @@

-def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-    "Return the descriptor powering the shared extraction template."
-    def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-        if isinstance (pipeline ,ChemblAssayPipeline ):
-            return pipeline
-        msg ="ChemblAssayPipeline instance required"
-        raise TypeError (msg )
-    def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        assay_pipeline ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        raw_source =assay_pipeline ._resolve_source_config ("chembl")
-        select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-        log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-        context =ChemblExtractionContext (source_config ,assay_client )
-        context .chembl_client =chembl_client
-        context .select_fields =tuple (select_fields )if select_fields else None
-        context .chembl_release =assay_pipeline ._chembl_release
-        context .extra_filters ={"max_url_length":source_config .max_url_length }
-        return context
-    def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-    def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        if df .empty :
-            return df
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in df .columns or df [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        select_fields =context .select_fields
-        if select_fields :
-            expected_fields =set (select_fields )
-            actual_fields =set (df .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-        return df
-    def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-        return pd .DataFrame ()
-    def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-    return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
```

#### Горячий участок 5

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:720-835
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,31 +0,0 @@

-def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-    "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-    df =df .copy ()
-    optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-    expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-    select_fields_set :set [str ]=set ()
-    if select_fields is not None :
-        select_fields_set =set (select_fields )
-    missing_in_response :list [str ]=[]
-    missing_in_select_fields :list [str ]=[]
-    for column in expected_api_fields :
-        if column not in df .columns :
-            missing_in_response .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-            else :
-                log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-    missing_columns :list [str ]=[]
-    for column ,version in optional_columns .items ():
-        if column not in df .columns :
-            df [column ]=pd .NA
-            missing_columns .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-            else :
-                log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-    if missing_in_response or missing_columns :
-        log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-    return df
```

#### Горячий участок 6

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:837-952
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,53 +0,0 @@

-def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-    if df .empty :
-        return df
-    try :
-        source_raw =self ._resolve_source_config ("chembl")
-    except KeyError as exc :
-        log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-        return df
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    chembl_config =getattr (self .config ,"chembl",None )
-    if chembl_config is None :
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-        return df
-    if not isinstance (chembl_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-        return df
-    assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-    if not isinstance (assay_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-        return df
-    enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-    if not isinstance (enrich_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-        return df
-    classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-    if classifications_cfg is not None :
-        log .info ("enrichment_classifications_started")
-        df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-        df =df_with_classifications
-        log .info ("enrichment_classifications_completed")
-        if "assay_class_id"in df .columns :
-            filled_count =int (df ["assay_class_id"].notna ().sum ())
-            total_count =len (df )
-            if filled_count ==0 :
-                log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-            else :
-                log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-        else :
-            log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-    else :
-        log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-    parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-    if parameters_cfg is not None :
-        log .info ("enrichment_parameters_started")
-        df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-        df =df_with_parameters
-        log .info ("enrichment_parameters_completed")
-    return df
```

#### Горячий участок 7

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:526-548
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,17 +0,0 @@

-def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-    df =df .copy ()
-    actions :list [str ]=[]
-    if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-        df ["assay_chembl_id"]=df ["assay_id"]
-        actions .append ("assay_id->assay_chembl_id")
-    if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-        df ["target_chembl_id"]=df ["target_id"]
-        actions .append ("target_id->target_chembl_id")
-    alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-    if alias_columns :
-        df =df .drop (columns =alias_columns )
-        actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-    if actions :
-        log .debug ("identifier_harmonization",actions =actions )
-    return df
```

#### Горячий участок 8

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:706-718
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,7 +0,0 @@

-def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-    "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-    df =super ()._normalize_data_types (df ,schema ,log )
-    if "row_index"in df .columns and df ["row_index"].isna ().any ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Горячий участок 9

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:550-577
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,7 +0,0 @@

-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize ChEMBL identifiers with regex validation."
-    rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-    normalized_df ,stats =normalize_identifier_columns (df ,rules )
-    if stats .has_changes :
-        log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-    return normalized_df
```

#### Горячий участок 10

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:623-679
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,30 +0,0 @@

-def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-    df =df .copy ()
-    if "assay_parameters"in df .columns :
-        log .debug ("validating_assay_parameters_truv")
-        df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-    if "assay_classifications"in df .columns :
-        if "assay_class_id"not in df .columns :
-            df ["assay_class_id"]=pd .NA
-        updated_rows =0
-        classifications_series =df ["assay_classifications"]
-        for row_index ,value in classifications_series .items ():
-            if value is None or value is pd .NA :
-                continue
-            if isinstance (value ,float )and pd .isna (value ):
-                continue
-            if isinstance (value ,str ):
-                continue
-            extracted_ids =_extract_bao_ids_from_classifications (value )
-            if not extracted_ids :
-                continue
-            joined_ids =";".join (extracted_ids )
-            row_label =cast (Any ,row_index )
-            current_value =df .at [row_label ,"assay_class_id"]
-            if pd .isna (current_value )or current_value !=joined_ids :
-                df .at [row_label ,"assay_class_id"]=joined_ids
-                updated_rows +=1
-        if updated_rows >0 :
-            log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-    return df
```

#### Горячий участок 11

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:579-621
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,12 +0,0 @@

-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-    working_df =df .copy ()
-    string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-    rules ={column :StringRule ()for column in string_fields }
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if "curation_level"not in normalized_df .columns :
-        normalized_df ["curation_level"]=pd .NA
-        log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    return normalized_df
```

#### Горячий участок 12

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:504-524
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,17 +0,0 @@

-def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Serialize array-of-object fields to header+rows format."
-    df =df .copy ()
-    arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-    if arrays_to_serialize :
-        df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-        for column in arrays_to_serialize :
-            if column in df_result .columns :
-                column_as_string :Series =df_result [column ].astype ("string")
-                filled_column :Series =column_as_string .copy ()
-                filled_column [column_as_string .isna ()]=""
-                empty_mask :Series =filled_column .eq ("")
-                if bool (empty_mask .any ()):
-                    df_result .loc [empty_mask ,column ]=pd .NA
-        df =df_result
-        log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-    return df
```

#### Горячий участок 13

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:136-139
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,4 +0,0 @@

-@property
-def chembl_release (self )->str |None :
-    "Return the cached ChEMBL release captured during extraction."
-    return self ._chembl_release
```

#### Горячий участок 14

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:145-159
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,4 +0,0 @@

-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
```

#### Горячий участок 15

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:161-165
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,4 +0,0 @@

-def extract_all (self )->pd .DataFrame :
-    "Extract all assay records from ChEMBL using pagination."
-    descriptor =self ._build_assay_descriptor ()
-    return self .run_extract_all (descriptor )
```

#### Горячий участок 16

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:325-469
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,42 +0,0 @@

-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-    assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-    self ._chembl_release =assay_client .chembl_release
-    log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-    if self .config .cli .dry_run :
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-        return pd .DataFrame ()
-    limit =self .config .cli .limit
-    resolved_select_fields =self ._resolve_select_fields (source_raw )
-    merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-    log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-    def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield dict (item )
-    def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-        if dataframe .empty :
-            return dataframe
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        if context .select_fields :
-            expected_fields =set (context .select_fields )
-            actual_fields =set (dataframe .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-        return dataframe
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-    return dataframe
```

#### Горячий участок 17

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:471-498
- document: нет в ветке

```diff
--- assay:run.py

+++ document:run.py

@@ -1,21 +0,0 @@

-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw assay data by normalizing identifiers, types, and nested structures."
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-    df =df .copy ()
-    df =self ._harmonize_identifier_columns (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._enrich_with_related_data (df ,log )
-    df =self ._normalize_nested_structures (df ,log )
-    df =self ._serialize_array_fields (df ,log )
-    df =self ._add_row_metadata (df ,log )
-    df =self ._normalize_data_types (df ,AssaySchema ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

#### Горячий участок 18

- assay: нет в ветке
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:60-654

```diff
--- assay:run.py

+++ document:run.py

@@ -0,0 +1,324 @@

+class ChemblDocumentPipeline (ChemblPipelineBase ):
+    "ETL pipeline extracting document records from the ChEMBL API."
+    actor ="document_chembl"
+    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+        super ().__init__ (config ,run_id )
+        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
+    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
+        "Fetch document payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
+        return self ._dispatch_extract_mode (log ,event_name ="chembl_document.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="document_chembl_id")
+    def extract_all (self )->pd .DataFrame :
+        "Extract all document records from ChEMBL using pagination."
+        return self .run_extract_all (self ._build_document_descriptor ())
+    def _build_document_descriptor (self :SelfChemblDocumentPipeline )->ChemblExtractionDescriptor [SelfChemblDocumentPipeline ]:
+        "Return the descriptor powering the shared extraction routine."
+        def _require_document_pipeline (pipeline :ChemblPipelineBase )->ChemblDocumentPipeline :
+            if isinstance (pipeline ,ChemblDocumentPipeline ):
+                return pipeline
+            msg ="ChemblDocumentPipeline instance required"
+            raise TypeError (msg )
+        def build_context (pipeline :SelfChemblDocumentPipeline ,source_config :Any ,log :BoundLogger )->ChemblExtractionContext :
+            document_pipeline =_require_document_pipeline (pipeline )
+            typed_source_config =source_config if isinstance (source_config ,DocumentSourceConfig )else DocumentSourceConfig .from_source_config (cast (Any ,source_config ))
+            base_url =document_pipeline ._resolve_base_url (typed_source_config .parameters )
+            http_client ,_ =document_pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
+            chembl_client =ChemblClient (http_client )
+            document_client =ChemblDocumentClient (chembl_client ,batch_size =min (typed_source_config .batch_size ,25 ))
+            document_pipeline ._chembl_release =document_pipeline .fetch_chembl_release (chembl_client ,log )
+            select_fields =document_pipeline ._resolve_select_fields (cast (SourceConfig ,cast (Any ,typed_source_config )),default_fields =API_DOCUMENT_FIELDS )
+            context =ChemblExtractionContext (typed_source_config ,document_client )
+            context .chembl_client =chembl_client
+            context .select_fields =tuple (select_fields )if select_fields else None
+            context .chembl_release =document_pipeline ._chembl_release
+            return context
+        def empty_frame (_ :SelfChemblDocumentPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
+            return pd .DataFrame ({"document_chembl_id":pd .Series (dtype ="string")})
+        def record_transform (pipeline :SelfChemblDocumentPipeline ,payload :Mapping [str ,Any ],_ :ChemblExtractionContext )->Mapping [str ,Any ]:
+            document_pipeline =_require_document_pipeline (pipeline )
+            return document_pipeline ._extract_nested_fields (dict (payload ))
+        def summary_extra (pipeline :SelfChemblDocumentPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
+            _require_document_pipeline (pipeline )
+            page_size =context .page_size or 0
+            pages =0
+            if page_size >0 :
+                total_rows =int (df .shape [0 ])
+                pages =(total_rows +page_size -1 )//page_size
+            return {"pages":pages }
+        return ChemblExtractionDescriptor [SelfChemblDocumentPipeline ](name ="chembl_document",source_name ="chembl",source_config_factory =DocumentSourceConfig .from_source_config ,build_context =build_context ,id_column ="document_chembl_id",summary_event ="chembl_document.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =API_DOCUMENT_FIELDS ,record_transform =record_transform ,sort_by =("document_chembl_id",),empty_frame_factory =empty_frame ,summary_extra =summary_extra )
+    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
+        "Extract document records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of document_chembl_id values to extract (as strings).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted document records.\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
+        stage_start =time .perf_counter ()
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =DocumentSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
+        chembl_client =ChemblClient (http_client )
+        document_client =ChemblDocumentClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
+        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
+        resolved_select_fields =self ._resolve_select_fields (source_raw ,default_fields =list (API_DOCUMENT_FIELDS ))
+        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
+        limit =self .config .cli .limit
+        def fetch_documents (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
+            if "original_paginate"not in context .extra :
+                original_paginate =chembl_client .paginate
+                def counted_paginate (*args :Any ,**kwargs :Any )->Any :
+                    context .increment_api_calls ()
+                    return original_paginate (*args ,**kwargs )
+                chembl_client .paginate =counted_paginate
+                context .extra ["original_paginate"]=original_paginate
+            iterator =document_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
+            for item in iterator :
+                yield self ._extract_nested_fields (dict (item ))
+        def finalize_context (context :BatchExtractionContext )->None :
+            original =context .extra .pop ("original_paginate",None )
+            if original is not None :
+                chembl_client .paginate =original
+            api_calls_value =context .stats .api_calls if context .stats .api_calls is not None else 0
+            override ={"batches":context .stats .batches ,"api_calls":api_calls_value ,"cache_hits":context .stats .cache_hits }
+            context .extra ["stats_attribute_override"]=override
+        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="document_chembl_id",fetcher =fetch_documents ,select_fields =merged_select_fields or None ,batch_size =document_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":merged_select_fields }if merged_select_fields else None ,chembl_release =self ._chembl_release ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats")
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_document.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =stats .batches ,api_calls =stats .api_calls ,cache_hits =stats .cache_hits )
+        return dataframe
+    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
+        "Transform raw document data by normalizing fields and identifiers."
+        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
+        df =df .copy ()
+        if df .empty :
+            log .debug ("transform_empty_dataframe")
+            return df
+        log .info ("transform_started",rows =len (df ))
+        df =self ._normalize_identifiers (df ,log )
+        df =self ._normalize_string_fields (df ,log )
+        df =self ._normalize_numeric_fields (df ,log )
+        if self ._should_enrich_document_terms ():
+            df =self ._enrich_document_terms (df )
+        df =self ._add_system_fields (df ,log )
+        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
+        if "document_chembl_id"in df .columns and df ["document_chembl_id"].duplicated ().any ():
+            initial_count =len (df )
+            df =df .sort_values (by =list (df .columns )).drop_duplicates (subset =["document_chembl_id"],keep ="first")
+            deduped_count =len (df )
+            if deduped_count <initial_count :
+                log .warning ("document_deduplication_applied",initial_count =initial_count ,deduped_count =deduped_count ,removed_count =initial_count -deduped_count )
+        df =self ._order_schema_columns (df ,COLUMN_ORDER )
+        log .info ("transform_completed",rows =len (df ))
+        return df
+    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
+        "Validate payload against DocumentSchema with detailed error handling."
+        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
+        if df .empty :
+            log .debug ("validate_empty_dataframe")
+            return df
+        if self .config .validation .strict :
+            allowed_columns =set (COLUMN_ORDER )
+            extra_columns =[column for column in df .columns if column not in allowed_columns ]
+            if extra_columns :
+                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
+                df =df .drop (columns =extra_columns )
+        log .info ("validate_started",rows =len (df ))
+        self ._check_document_id_uniqueness (df ,log )
+        validated =super ().validate (df )
+        log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =self .config .validation .coerce )
+        return validated
+    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize identifier fields (DOI, PMID)."
+        df =df .copy ()
+        if "doi"in df .columns :
+            df ["doi_clean"]=df ["doi"].apply (self ._normalize_doi )
+        if "pubmed_id"in df .columns :
+            df ["pubmed_id"]=pd .to_numeric (df ["pubmed_id"],errors ="coerce").astype ("Int64")
+        return df
+    @staticmethod
+    def _normalize_doi (doi :str |None )->str :
+        "Normalize DOI by removing prefixes and validating format."
+        if not doi :
+            return ""
+        if not isinstance (doi ,str ):
+            return ""
+        doi =doi .strip ().lower ()
+        for prefix in ["doi:","https://doi.org/","http://dx.doi.org/","http://doi.org/"]:
+            if doi .startswith (prefix ):
+                doi =doi [len (prefix ):]
+        doi =doi .strip ()
+        doi_pattern =re .compile ("^10\\.\\d{4,9}/\\S+$")
+        if doi_pattern .match (doi ):
+            return doi
+        return ""
+    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize string fields (title, abstract, journal, authors)."
+        working_df =df .copy ()
+        rules ={"title":StringRule (max_length =1000 ),"abstract":StringRule (max_length =5000 )}
+        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        if stats .has_changes :
+            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        if "journal"in normalized_df .columns :
+            journal_series :pd .Series [Any ]=normalized_df ["journal"]
+            normalized_df ["journal"]=journal_series .map (lambda value :self ._normalize_journal (value ))
+        if "authors"in normalized_df .columns :
+            def _to_author_tuple (item :object )->tuple [str ,int ]|None :
+                if not isinstance (item ,tuple ):
+                    return None
+                tuple_item =cast (tuple [object ,...],item )
+                if len (tuple_item )!=2 :
+                    return None
+                name_raw ,count_raw =tuple_item
+                if not isinstance (name_raw ,str ):
+                    return None
+                name_value :str =name_raw
+                if isinstance (count_raw ,Integral ):
+                    count_value =int (count_raw )
+                elif isinstance (count_raw ,Real ):
+                    float_value =float (count_raw )
+                    if not float_value .is_integer ():
+                        return None
+                    count_value =int (float_value )
+                else :
+                    return None
+                if count_value <0 :
+                    return None
+                return (name_value ,count_value )
+            def _author_name_from_tuple (data :tuple [str ,int ]|None )->str :
+                return data [0 ]if data is not None else ""
+            def _author_count_from_tuple (data :tuple [str ,int ]|None )->int :
+                return data [1 ]if data is not None else 0
+            authors_series :pd .Series [Any ]=normalized_df ["authors"]
+            normalized_result =authors_series .apply (self ._normalize_authors )
+            normalized_tuples =normalized_result .apply (_to_author_tuple )
+            normalized_df ["authors"]=normalized_tuples .apply (_author_name_from_tuple )
+            normalized_df ["authors_count"]=normalized_tuples .apply (_author_count_from_tuple )
+        return normalized_df
+    @staticmethod
+    def _normalize_journal (value :Any ,max_len :int =255 )->str :
+        "Trim and collapse whitespace for journal name."
+        if pd .isna (value ):
+            return ""
+        text =str (value )
+        text =re .sub ("\\s+"," ",text ).strip ()
+        return text [:max_len ]if len (text )>max_len else text
+    @staticmethod
+    def _normalize_authors (authors :Any ,separator :str =", ")->tuple [str ,int ]:
+        "Normalize author separators and count."
+        if pd .isna (authors ):
+            return ("",0 )
+        text =str (authors ).strip ()
+        text =re .sub (";",",",text )
+        text =re .sub ("\\s+"," ",text )
+        if not text :
+            return ("",0 )
+        parts =text .split (",")
+        parts =[p .strip ()for p in parts if p .strip ()]
+        return (separator .join (parts ),len (parts ))
+    def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize numeric fields (year)."
+        df =df .copy ()
+        if "year"in df .columns :
+            def _coerce_year (value :object )->int |None :
+                if value is None or value is pd .NA :
+                    return None
+                if isinstance (value ,Integral ):
+                    year_int =int (value )
+                elif isinstance (value ,Real ):
+                    float_value =float (value )
+                    if not float_value .is_integer ():
+                        return None
+                    year_int =int (float_value )
+                elif isinstance (value ,str ):
+                    stripped =value .strip ()
+                    if not stripped :
+                        return None
+                    if not stripped .isdigit ():
+                        return None
+                    year_int =int (stripped )
+                else :
+                    return None
+                if 1500 <=year_int <=2100 :
+                    return year_int
+                return None
+            normalized_year =df ["year"].apply (_coerce_year )
+            df ["year"]=normalized_year .astype ("Int64")
+        return df
+    def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Add document-specific system fields (source)."
+        df =df .copy ()
+        df ["source"]="ChEMBL"
+        return df
+    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
+        specs =dict (super ()._schema_column_specs ())
+        specs ["source"]={"default":"ChEMBL"}
+        specs ["authors_count"]={"default":0 ,"dtype":"Int64"}
+        hashing_config =self .config .determinism .hashing
+        business_key_column =hashing_config .business_key_column
+        row_hash_column =hashing_config .row_hash_column
+        if business_key_column :
+            specs [business_key_column ]={"default":""}
+        if row_hash_column :
+            specs [row_hash_column ]={"default":""}
+        return specs
+    def _check_document_id_uniqueness (self ,df :pd .DataFrame ,log :Any )->None :
+        "Check that document_chembl_id is unique."
+        if df .empty :
+            return
+        if "document_chembl_id"not in df .columns :
+            return
+        duplicates =df ["document_chembl_id"].duplicated ()
+        if duplicates .any ():
+            duplicate_ids =df [df ["document_chembl_id"].duplicated ()]["document_chembl_id"].unique ().tolist ()
+            log .warning ("document_id_duplicates",duplicate_count =duplicates .sum (),duplicate_ids =duplicate_ids [:10 ])
+    def _should_enrich_document_terms (self )->bool :
+        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 document_term \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
+        if not self .config .chembl :
+            return False
+        try :
+            chembl_section =self .config .chembl
+            document_section :Any =chembl_section .get ("document")
+            if not isinstance (document_section ,Mapping ):
+                return False
+            document_section =cast (Mapping [str ,Any ],document_section )
+            enrich_section :Any =document_section .get ("enrich")
+            if not isinstance (enrich_section ,Mapping ):
+                return False
+            enrich_section =cast (Mapping [str ,Any ],enrich_section )
+            document_term_section :Any =enrich_section .get ("document_term")
+            if not isinstance (document_term_section ,Mapping ):
+                return False
+            document_term_section =cast (Mapping [str ,Any ],document_term_section )
+            enabled :Any =document_term_section .get ("enabled")
+            return bool (enabled )if enabled is not None else False
+        except (AttributeError ,KeyError ,TypeError ):
+            return False
+    def _enrich_document_terms (self ,df :pd .DataFrame )->pd .DataFrame :
+        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term."
+        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
+        enrich_cfg :dict [str ,Any ]={}
+        try :
+            if self .config .chembl :
+                chembl_section =self .config .chembl
+                document_section :Any =chembl_section .get ("document")
+                if isinstance (document_section ,Mapping ):
+                    document_section =cast (Mapping [str ,Any ],document_section )
+                    enrich_section :Any =document_section .get ("enrich")
+                    if isinstance (enrich_section ,Mapping ):
+                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
+                        document_term_section :Any =enrich_section .get ("document_term")
+                        if isinstance (document_term_section ,Mapping ):
+                            document_term_section =cast (Mapping [str ,Any ],document_term_section )
+                            enrich_cfg =dict (document_term_section )
+        except (AttributeError ,KeyError ,TypeError )as exc :
+            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =DocumentSourceConfig .from_source_config (source_raw )
+        api_client ,_ =self .prepare_chembl_client ("chembl",base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters ))),client_name ="chembl_enrichment_client")
+        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
+        return enrich_with_document_terms (df ,chembl_client ,enrich_cfg )
+    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
+        "Extract fields from nested objects in document records."
+        return record
+    @staticmethod
+    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
+        "Coerce payload to dictionary mapping."
+        if isinstance (payload ,Mapping ):
+            return cast (dict [str ,Any ],payload )
+        return {}
```

#### Горячий участок 19

- assay: нет в ветке
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:65-67

```diff
--- assay:run.py

+++ document:run.py

@@ -0,0 +1,3 @@

+def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+    super ().__init__ (config ,run_id )
+    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
```

#### Горячий участок 20

- assay: нет в ветке
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:532-539

```diff
--- assay:run.py

+++ document:run.py

@@ -0,0 +1,5 @@

+def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+    "Add document-specific system fields (source)."
+    df =df .copy ()
+    df ["source"]="ChEMBL"
+    return df
```

### Модуль transform.py

Определение                    | assay сигнатура                                | document сигнатура | Побочные эффекты                                                                                                      | Исключения                                    | Статус        
-------------------------------|------------------------------------------------|--------------------|-----------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|---------------
__module_block_0               | —                                              | —                  | assay: {'logging': [], 'io': []}
document: {}                                                                         | assay: []
document: []                        | только в assay
__module_block_1               | —                                              | —                  | assay: {'logging': [], 'io': []}
document: {}                                                                         | assay: []
document: []                        | только в assay
__module_block_2               | —                                              | —                  | assay: {'logging': [], 'io': []}
document: {}                                                                         | assay: []
document: []                        | только в assay
__module_block_3               | —                                              | —                  | assay: {'logging': [], 'io': []}
document: {}                                                                         | assay: []
document: []                        | только в assay
__module_block_4               | —                                              | —                  | assay: {'logging': [], 'io': []}
document: {}                                                                         | assay: []
document: []                        | только в assay
__module_block_5               | —                                              | —                  | assay: {'logging': [], 'io': []}
document: {}                                                                         | assay: []
document: []                        | только в assay
__module_block_6               | —                                              | —                  | assay: {'logging': [], 'io': []}
document: {}                                                                         | assay: []
document: []                        | только в assay
__module_block_7               | —                                              | —                  | assay: {'logging': [], 'io': []}
document: {}                                                                         | assay: []
document: []                        | только в assay
__module_block_8               | —                                              | —                  | assay: {'logging': [], 'io': []}
document: {}                                                                         | assay: []
document: []                        | только в assay
_is_null_like                  | value: Any                                     | —                  | assay: {'logging': [], 'io': []}
document: {}                                                                         | assay: []
document: []                        | только в assay
validate_assay_parameters_truv | df: pd.DataFrame, column: str, fail_fast: bool | —                  | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.warning'], 'io': ['json.loads']}
document: {} | assay: ['ValueError(error_msg)']
document: [] | только в assay

#### Горячий участок 1

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:1-1
- document: нет в ветке

```diff
--- assay:transform.py

+++ document:transform.py

@@ -1 +0,0 @@

-"Transform utilities for ChEMBL assay pipeline array serialization."
```

#### Горячий участок 2

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:3-3
- document: нет в ветке

```diff
--- assay:transform.py

+++ document:transform.py

@@ -1 +0,0 @@

-from __future__ import annotations
```

#### Горячий участок 3

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:10-10
- document: нет в ветке

```diff
--- assay:transform.py

+++ document:transform.py

@@ -1 +0,0 @@

-from bioetl .core .logger import UnifiedLogger
```

#### Горячий участок 4

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:11-11
- document: нет в ветке

```diff
--- assay:transform.py

+++ document:transform.py

@@ -1 +0,0 @@

-from bioetl .core .serialization import header_rows_serialize ,serialize_array_fields
```

#### Горячий участок 5

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:6-6
- document: нет в ветке

```diff
--- assay:transform.py

+++ document:transform.py

@@ -1 +0,0 @@

-from typing import Any ,cast
```

#### Горячий участок 6

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:5-5
- document: нет в ветке

```diff
--- assay:transform.py

+++ document:transform.py

@@ -1 +0,0 @@

-import json
```

#### Горячий участок 7

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:8-8
- document: нет в ветке

```diff
--- assay:transform.py

+++ document:transform.py

@@ -1 +0,0 @@

-import pandas as pd
```

#### Горячий участок 8

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:13-17
- document: нет в ветке

```diff
--- assay:transform.py

+++ document:transform.py

@@ -1 +0,0 @@

-__all__ =["header_rows_serialize","serialize_array_fields","validate_assay_parameters_truv"]
```

#### Горячий участок 9

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:19-19
- document: нет в ветке

```diff
--- assay:transform.py

+++ document:transform.py

@@ -1 +0,0 @@

-AssayParam =dict [str ,Any ]
```

#### Горячий участок 10

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:22-39
- document: нет в ветке

```diff
--- assay:transform.py

+++ document:transform.py

@@ -1,13 +0,0 @@

-def _is_null_like (value :Any )->bool :
-    "\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043c\u043e\u0436\u043d\u043e \u043b\u0438 \u0442\u0440\u0430\u043a\u0442\u043e\u0432\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u0430\u043a \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0435."
-    if value is None :
-        return True
-    if isinstance (value ,str ):
-        return value .strip ()==""
-    if isinstance (value ,float ):
-        return bool (pd .isna (value ))
-    try :
-        is_na_raw =cast (Any ,pd .isna (value ))
-    except TypeError :
-        return False
-    return bool (is_na_raw )if isinstance (is_na_raw ,bool )else False
```

#### Горячий участок 11

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:42-232
- document: нет в ветке

```diff
--- assay:transform.py

+++ document:transform.py

@@ -1,78 +0,0 @@

-def validate_assay_parameters_truv (df :pd .DataFrame ,column :str ="assay_parameters",fail_fast :bool =True )->pd .DataFrame :
-    "\u0412\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u0442\u044c TRUV-\u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0434\u043b\u044f assay_parameters.\n\n    \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n    - value IS NOT NULL XOR text_value IS NOT NULL (\u043d\u0435 \u043e\u0431\u0430 \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u043d\u0435 NULL)\n    - standard_value IS NOT NULL XOR standard_text_value IS NOT NULL\n    - active \u2208 {0, 1, NULL}\n    - relation \u2208 {'=', '<', '\u2264', '>', '\u2265', '~', NULL} (\u0441 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u0435\u043c \u0434\u043b\u044f \u043d\u0435\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0445)\n\n    Parameters\n    ----------\n    df:\n        DataFrame \u0441 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 assay_parameters (JSON-\u0441\u0442\u0440\u043e\u043a\u0430 \u0441 \u043c\u0430\u0441\u0441\u0438\u0432\u043e\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432).\n    column:\n        \u0418\u043c\u044f \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 (\u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \"assay_parameters\").\n    fail_fast:\n        \u0415\u0441\u043b\u0438 True, \u0432\u044b\u0431\u0440\u0430\u0441\u044b\u0432\u0430\u0435\u0442 ValueError \u043f\u0440\u0438 \u043d\u0430\u0440\u0443\u0448\u0435\u043d\u0438\u0438 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432.\n        \u0415\u0441\u043b\u0438 False, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0435\u0442.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0439 DataFrame (\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u043d\u0435 \u0438\u0437\u043c\u0435\u043d\u044f\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435).\n\n    Raises\n    ------\n    ValueError:\n        \u0415\u0441\u043b\u0438 fail_fast=True \u0438 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043e \u043d\u0430\u0440\u0443\u0448\u0435\u043d\u0438\u0435 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432.\n\n    Notes\n    -----\n    - \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442\u0441\u044f \u043d\u0430 \u044d\u0442\u0430\u043f\u0435 transform \u0434\u043b\u044f fail-fast \u043f\u043e\u0434\u0445\u043e\u0434\u0430\n    - \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u044b relation: '=', '<', '\u2264', '>', '\u2265', '~'\n    - \u041d\u0435\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u044b \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="assay_transform")
-    if column not in df .columns :
-        log .debug ("truv_validation_skipped_missing_column",column =column )
-        return df
-    STANDARD_RELATIONS ={"=","<","\u2264",">","\u2265","~"}
-    errors :list [str ]=[]
-    warnings :list [str ]=[]
-    for idx ,row in df .iterrows ():
-        params_str =row .get (column )
-        if _is_null_like (params_str ):
-            continue
-        try :
-            if isinstance (params_str ,str ):
-                params_raw =json .loads (params_str )
-            else :
-                params_raw =params_str
-        except (json .JSONDecodeError ,TypeError )as exc :
-            errors .append (f'Row {idx }: Invalid JSON in {column }: {exc }')
-            continue
-        if not isinstance (params_raw ,list ):
-            errors .append (f'Row {idx }: {column } must be a JSON array, got {type (params_raw ).__name__ }')
-            continue
-        params_candidates =cast (list [object ],params_raw )
-        for param_idx ,param_raw in enumerate (params_candidates ):
-            if not isinstance (param_raw ,dict ):
-                errors .append (f'Row {idx }, param {param_idx }: Parameter must be a dict, got {type (param_raw ).__name__ }')
-                continue
-            param_dict :AssayParam =cast (AssayParam ,param_raw )
-            value :Any =param_dict .get ("value")
-            text_value :Any =param_dict .get ("text_value")
-            value_is_null =value is None or (isinstance (value ,float )and pd .isna (value ))or (isinstance (value ,str )and value .strip ()=="")
-            text_value_is_null =text_value is None or (isinstance (text_value ,float )and pd .isna (text_value ))or (isinstance (text_value ,str )and text_value .strip ()=="")
-            if not value_is_null and (not text_value_is_null ):
-                errors .append (f"Row {idx }, param {param_idx }: Both 'value' and 'text_value' are not NULL (value={value }, text_value={text_value }). TRUV invariant violation: value and text_value must be mutually exclusive.")
-            standard_value :Any =param_dict .get ("standard_value")
-            standard_text_value :Any =param_dict .get ("standard_text_value")
-            standard_value_is_null =standard_value is None or (isinstance (standard_value ,float )and pd .isna (standard_value ))or (isinstance (standard_value ,str )and standard_value .strip ()=="")
-            standard_text_value_is_null =standard_text_value is None or (isinstance (standard_text_value ,float )and pd .isna (standard_text_value ))or (isinstance (standard_text_value ,str )and standard_text_value .strip ()=="")
-            if not standard_value_is_null and (not standard_text_value_is_null ):
-                errors .append (f"Row {idx }, param {param_idx }: Both 'standard_value' and 'standard_text_value' are not NULL (standard_value={standard_value }, standard_text_value={standard_text_value }). TRUV invariant violation: standard_value and standard_text_value must be mutually exclusive.")
-            active :Any =param_dict .get ("active")
-            if active is not None :
-                if isinstance (active ,bool ):
-                    active_int =1 if active else 0
-                elif isinstance (active ,(int ,float )):
-                    active_int =int (active )
-                elif isinstance (active ,str ):
-                    try :
-                        active_int =int (active )
-                    except ValueError :
-                        errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' value: {active !r }. Must be 0, 1, or NULL.")
-                        continue
-                else :
-                    errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' type: {type (active ).__name__ }. Must be 0, 1, or NULL.")
-                    continue
-                if active_int not in {0 ,1 }:
-                    errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' value: {active_int }. Must be 0, 1, or NULL.")
-            relation :Any =param_dict .get ("relation")
-            if relation is not None and (not (isinstance (relation ,float )and pd .isna (relation ))):
-                relation_str =str (relation ).strip ()
-                if relation_str and relation_str not in STANDARD_RELATIONS :
-                    warnings .append (f"Row {idx }, param {param_idx }: Non-standard 'relation' value: {relation_str !r }. Standard operators: {", ".join (sorted (STANDARD_RELATIONS ))}.")
-    if warnings :
-        for warning in warnings :
-            log .warning ("truv_validation_warning",message =warning )
-    if errors :
-        error_msg =f'TRUV validation failed for {column }:\n'+"\n".join (errors )
-        if fail_fast :
-            log .error ("truv_validation_failed",error_count =len (errors ))
-            raise ValueError (error_msg )
-        else :
-            for error in errors :
-                log .warning ("truv_validation_error",message =error )
-    if not errors and (not warnings ):
-        log .debug ("truv_validation_passed",rows_checked =len (df ))
-    return df
```

### Модуль normalize.py

Определение                       | assay сигнатура                                                      | document сигнатура                                                  | Побочные эффекты                                                                                                     | Исключения             | Статус           
----------------------------------|----------------------------------------------------------------------|---------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|------------------------|------------------
__module_block_0                  | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | отличается       
__module_block_1                  | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | совпадает        
__module_block_10                 | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | отличается       
__module_block_11                 | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | совпадает        
__module_block_12                 | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {}                                                                        | assay: []
document: [] | только в assay   
__module_block_13                 | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | отличается       
__module_block_2                  | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | совпадает        
__module_block_3                  | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | совпадает        
__module_block_4                  | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | совпадает        
__module_block_5                  | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | отличается       
__module_block_6                  | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | отличается       
__module_block_7                  | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | отличается       
__module_block_8                  | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | отличается       
__module_block_9                  | —                                                                    | —                                                                   | assay: {'logging': [], 'io': []}
document: {'logging': [], 'io': []}                                                 | assay: []
document: [] | совпадает        
_escape_pipe                      | —                                                                    | value: str | Any                                                    | assay: {}
document: {'logging': [], 'io': []}                                                                        | assay: []
document: [] | только в document
_is_numeric                       | —                                                                    | value: Any                                                          | assay: {}
document: {'logging': [], 'io': []}                                                                        | assay: []
document: [] | только в document
_should_nullify_string_value      | value: Any                                                           | —                                                                   | assay: {'logging': [], 'io': []}
document: {}                                                                        | assay: []
document: [] | только в assay   
aggregate_terms                   | —                                                                    | rows: Iterable[dict[str, Any]], sort: str                           | assay: {}
document: {'logging': [], 'io': []}                                                                        | assay: []
document: [] | только в document
enrich_with_assay_classifications | df_assay: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | —                                                                   | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}
document: {} | assay: []
document: [] | только в assay   
enrich_with_assay_parameters      | df_assay: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | —                                                                   | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}
document: {} | assay: []
document: [] | только в assay   
enrich_with_document_terms        | —                                                                    | df_docs: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | assay: {}
document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}             | assay: []
document: [] | только в document

#### Горячий участок 1

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:1-1
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:1-1

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-"Enrichment functions for Assay pipeline."
+"Enrichment functions for Document pipeline."
```

#### Горячий участок 2

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:19-22
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:16-16

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-__all__ =["enrich_with_assay_classifications","enrich_with_assay_parameters"]
+__all__ =["enrich_with_document_terms","aggregate_terms","_escape_pipe"]
```

#### Горячий участок 3

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:28-31
- document: нет в ветке

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -1 +0,0 @@

-_CLASSIFICATION_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_classifications","string"),("assay_class_id","string"))
```

#### Горячий участок 4

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:33-33
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:46-49

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-_PARAMETERS_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_parameters","string"),)
+_DOCUMENT_TERM_COLUMNS :tuple [tuple [str ,str ],...]=(("term","string"),("weight","string"))
```

#### Горячий участок 5

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:14-17
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:14-14

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-from bioetl .schemas .assay import ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA ,ASSAY_PARAMETERS_ENRICHMENT_SCHEMA
+from bioetl .schemas .document import DOCUMENT_TERMS_ENRICHMENT_SCHEMA
```

#### Горячий участок 6

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:6-6
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:5-5

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-from collections .abc import Mapping
+from collections import defaultdict
```

#### Горячий участок 7

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:7-7
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:6-6

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-from typing import Any
+from collections .abc import Iterable ,Mapping
```

#### Горячий участок 8

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:5-5
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:7-7

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -1 +1 @@

-import json
+from typing import Any
```

#### Горячий участок 9

- assay: нет в ветке
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:22-43

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -0,0 +1,8 @@

+def _escape_pipe (value :str |Any )->str :
+    "Escape pipe and backslash delimiters in string values.\n\n    Parameters\n    ----------\n    value:\n        Input value to escape. ``None`` \u0438 NA \u043f\u0440\u0438\u0432\u043e\u0434\u044f\u0442\u0441\u044f \u043a \u043f\u0443\u0441\u0442\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435.\n\n    Returns\n    -------\n    str:\n        \u0421\u0442\u0440\u043e\u043a\u0430 \u0441 \u044d\u043a\u0440\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u0435\u043b\u044f\u043c\u0438: ``|`` \u2192 ``\\|``, ``\\`` \u2192 ``\\\\``.\n    "
+    if value is None or pd .isna (value ):
+        return ""
+    text =str (value )
+    if not text :
+        return ""
+    return text .replace ("\\","\\\\").replace ("|","\\|")
```

#### Горячий участок 10

- assay: нет в ветке
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:122-128

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -0,0 +1,7 @@

+def _is_numeric (value :Any )->bool :
+    "Check if value can be converted to float."
+    try :
+        float (value )
+        return True
+    except (ValueError ,TypeError ):
+        return False
```

#### Горячий участок 11

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:36-44
- document: нет в ветке

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -1,9 +0,0 @@

-def _should_nullify_string_value (value :Any )->bool :
-    "\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043d\u0443\u0436\u043d\u043e \u043b\u0438 \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 NA \u0432 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0435."
-    if value is None :
-        return False
-    if value is pd .NA :
-        return False
-    if isinstance (value ,float )and pd .isna (value ):
-        return False
-    return not isinstance (value ,str )
```

#### Горячий участок 12

- assay: нет в ветке
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:52-119

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -0,0 +1,26 @@

+def aggregate_terms (rows :Iterable [dict [str ,Any ]],sort :str ="weight_desc")->dict [str ,dict [str ,str ]]:
+    "Aggregate document terms by document_chembl_id.\n\n    Parameters\n    ----------\n    rows:\n        Iterable of document_term records, each with 'document_chembl_id', 'term', 'weight'.\n    sort:\n        Sort order: 'weight_desc' (default) sorts by weight descending, None preserves order.\n\n    Returns\n    -------\n    dict[str, dict[str, str]]:\n        Dictionary keyed by document_chembl_id -> {'term': 't1|t2|...', 'weight': 'w1|w2|...'}.\n        Terms and weights are serialized with \"|\" separator, order is synchronized.\n    "
+    bucket :dict [str ,list [tuple [str ,Any ]]]=defaultdict (list )
+    for r in rows :
+        did =r .get ("document_chembl_id")
+        if not did :
+            continue
+        term_value =r .get ("term")
+        weight_value =r .get ("weight")
+        term_str =str (term_value )if term_value is not None else ""
+        bucket [did ].append ((term_str ,weight_value ))
+    result :dict [str ,dict [str ,str ]]={}
+    for did ,items in bucket .items ():
+        if sort =="weight_desc":
+            items .sort (key =lambda x :float (x [1 ])if x [1 ]not in (None ,"")and _is_numeric (x [1 ])else float ("-inf"),reverse =True )
+        terms_list :list [str ]=[]
+        weights_list :list [str ]=[]
+        for term ,weight in items :
+            escaped_term =_escape_pipe (term or "")
+            terms_list .append (escaped_term )
+            if weight in (None ,""):
+                weights_list .append ("")
+            else :
+                weights_list .append (str (weight ))
+        result [did ]={"term":"|".join (terms_list ),"weight":"|".join (weights_list )}
+    return result
```

#### Горячий участок 13

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:47-219
- document: нет в ветке

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -1,86 +0,0 @@

-def enrich_with_assay_classifications (df_assay :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame assay \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 ASSAY_CLASS_MAP \u0438 ASSAY_CLASSIFICATION.\n\n    Parameters\n    ----------\n    df_assay:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.assay.enrich.classifications.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438/\u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438:\n        - assay_classifications (string, nullable) - \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u043c\u0430\u0441\u0441\u0438\u0432 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439\n        - assay_class_id (string, nullable) - \u0441\u043f\u0438\u0441\u043e\u043a assay_class_id \u0447\u0435\u0440\u0435\u0437 \";\"\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="assay_enrichment")
-    df_assay =_ensure_columns (df_assay ,_CLASSIFICATION_COLUMNS )
-    if "assay_classifications"in df_assay .columns :
-        invalid_mask =df_assay ["assay_classifications"].map (_should_nullify_string_value )
-        if bool (invalid_mask .any ()):
-            log .warning ("assay_classifications_reset_non_string",rows =int (invalid_mask .sum ()))
-            df_assay .loc [invalid_mask ,"assay_classifications"]=pd .NA
-    if df_assay .empty :
-        log .debug ("enrichment_skipped_empty_dataframe")
-        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    required_cols =["assay_chembl_id"]
-    missing_cols =[col for col in required_cols if col not in df_assay .columns ]
-    if missing_cols :
-        log .warning ("enrichment_skipped_missing_columns",missing_columns =missing_cols )
-        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    assay_ids :list [str ]=[]
-    for _ ,row in df_assay .iterrows ():
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        if assay_id_str :
-            assay_ids .append (assay_id_str )
-    if not assay_ids :
-        log .debug ("enrichment_skipped_no_valid_ids")
-        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    class_map_fields =cfg .get ("class_map_fields",["assay_chembl_id","assay_class_id"])
-    classification_fields =cfg .get ("classification_fields",["assay_class_id","l1","l2","l3","pref_name"])
-    page_limit =cfg .get ("page_limit",1000 )
-    log .info ("enrichment_fetching_assay_class_map",ids_count =len (set (assay_ids )))
-    class_map_dict =client .fetch_assay_class_map_by_assay_ids (assay_ids ,list (class_map_fields ),page_limit )
-    all_class_ids :set [str ]=set ()
-    for mappings in class_map_dict .values ():
-        for mapping in mappings :
-            class_id =mapping .get ("assay_class_id")
-            if class_id and (not (isinstance (class_id ,float )and pd .isna (class_id ))):
-                all_class_ids .add (str (class_id ).strip ())
-    classification_dict :dict [str ,dict [str ,Any ]]={}
-    if all_class_ids :
-        log .info ("enrichment_fetching_assay_classifications",class_ids_count =len (all_class_ids ))
-        classification_dict =client .fetch_assay_classifications_by_class_ids (list (all_class_ids ),list (classification_fields ),page_limit )
-    df_assay =df_assay .copy ()
-    if "assay_classifications"not in df_assay .columns :
-        df_assay ["assay_classifications"]=pd .NA
-    if "assay_class_id"not in df_assay .columns :
-        df_assay ["assay_class_id"]=pd .NA
-    for idx ,row in df_assay .iterrows ():
-        row_key :Any =idx
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        mappings =class_map_dict .get (assay_id_str ,[])
-        if not mappings :
-            df_assay .at [row_key ,"assay_classifications"]=pd .NA
-            df_assay .at [row_key ,"assay_class_id"]=pd .NA
-            continue
-        classifications :list [dict [str ,Any ]]=[]
-        class_ids :list [str ]=[]
-        for mapping in mappings :
-            class_id =mapping .get ("assay_class_id")
-            if not class_id or (isinstance (class_id ,float )and pd .isna (class_id )):
-                continue
-            class_id_str =str (class_id ).strip ()
-            if not class_id_str :
-                continue
-            classification_data =classification_dict .get (class_id_str )
-            if classification_data :
-                class_record :dict [str ,Any ]={"assay_class_id":class_id_str }
-                for field in classification_fields :
-                    if field !="assay_class_id":
-                        class_record [field ]=classification_data .get (field )
-                classifications .append (class_record )
-            else :
-                class_record ={"assay_class_id":class_id_str }
-                classifications .append (class_record )
-            class_ids .append (class_id_str )
-        if classifications :
-            serialized =json .dumps (classifications ,ensure_ascii =False )
-            class_id_joined =";".join (class_ids )
-            df_assay .at [row_key ,"assay_classifications"]=serialized
-            df_assay .at [row_key ,"assay_class_id"]=class_id_joined
-    log .info ("enrichment_classifications_complete",assays_with_classifications =len (df_assay [df_assay ["assay_classifications"].notna ()]))
-    return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
```

#### Горячий участок 14

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:222-378
- document: нет в ветке

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -1,57 +0,0 @@

-def enrich_with_assay_parameters (df_assay :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame assay \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 ASSAY_PARAMETERS.\n\n    \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442 \u043f\u043e\u043b\u043d\u044b\u0439 TRUV-\u043d\u0430\u0431\u043e\u0440 \u043f\u043e\u043b\u0435\u0439 (TYPE, RELATION, VALUE, UNITS, TEXT_VALUE),\n    \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (standard_*), \u0441\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (active) \u0438 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435\n    \u043f\u043e\u043b\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 (type_normalized, type_fixed).\n\n    Parameters\n    ----------\n    df_assay:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.assay.enrich.parameters.\n        \u0414\u043e\u043b\u0436\u043d\u0430 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c fields (\u0441\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439 \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f), page_limit \u0438 active_only.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0439/\u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439:\n        - assay_parameters (string, nullable) - \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 JSON-\u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\n          \u0441 \u043f\u043e\u043b\u044f\u043c\u0438: type, relation, value, units, text_value, standard_*,\n          active, type_normalized, type_fixed (\u0435\u0441\u043b\u0438 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 \u0434\u0430\u043c\u043f\u0435)\n\n    Notes\n    -----\n    - \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u0435\u0441\u0442\u044c, \u043d\u0435 \u043a\u043e\u043f\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0432 standard_* \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\n    - \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (type_normalized, type_fixed) \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438\n      \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 \u0434\u0430\u043c\u043f\u0435 ChEMBL\n    - \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0444\u0438\u043b\u044c\u0442\u0440\u0443\u044e\u0442\u0441\u044f \u043f\u043e active=1, \u0435\u0441\u043b\u0438 active_only=True \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="assay_enrichment")
-    df_assay =_ensure_columns (df_assay ,_PARAMETERS_COLUMNS )
-    if "assay_parameters"in df_assay .columns :
-        invalid_mask =df_assay ["assay_parameters"].map (_should_nullify_string_value )
-        if bool (invalid_mask .any ()):
-            log .warning ("assay_parameters_reset_non_string",rows =int (invalid_mask .sum ()))
-            df_assay .loc [invalid_mask ,"assay_parameters"]=pd .NA
-        df_assay ["assay_parameters"]=df_assay ["assay_parameters"].astype ("string")
-    if df_assay .empty :
-        log .debug ("enrichment_skipped_empty_dataframe")
-        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    required_cols =["assay_chembl_id"]
-    missing_cols =[col for col in required_cols if col not in df_assay .columns ]
-    if missing_cols :
-        log .warning ("enrichment_skipped_missing_columns",missing_columns =missing_cols )
-        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    assay_ids :list [str ]=[]
-    for _ ,row in df_assay .iterrows ():
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        if assay_id_str :
-            assay_ids .append (assay_id_str )
-    if not assay_ids :
-        log .debug ("enrichment_skipped_no_valid_ids")
-        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    fields =cfg .get ("fields",["assay_chembl_id","type","relation","value","units","text_value","standard_type","standard_relation","standard_value","standard_units","standard_text_value","active"])
-    page_limit =cfg .get ("page_limit",1000 )
-    active_only =cfg .get ("active_only",True )
-    log .info ("enrichment_fetching_assay_parameters",ids_count =len (set (assay_ids )))
-    parameters_dict =client .fetch_assay_parameters_by_assay_ids (assay_ids ,list (fields ),page_limit ,active_only )
-    df_assay =df_assay .copy ()
-    if "assay_parameters"not in df_assay .columns :
-        df_assay ["assay_parameters"]=pd .NA
-    for row_position ,(_ ,row )in enumerate (df_assay .iterrows ()):
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        parameters =parameters_dict .get (assay_id_str ,[])
-        if not parameters :
-            continue
-        params_list :list [dict [str ,Any ]]=[]
-        for param in parameters :
-            param_record :dict [str ,Any ]={}
-            for field in fields :
-                if field !="assay_chembl_id":
-                    param_record [field ]=param .get (field )
-            params_list .append (param_record )
-        if params_list :
-            index_label =df_assay .index [row_position ]
-            df_assay .loc [index_label ,"assay_parameters"]=json .dumps (params_list ,ensure_ascii =False )
-    log .info ("enrichment_parameters_complete",assays_with_parameters =len (df_assay [df_assay ["assay_parameters"].notna ()]))
-    return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
```

#### Горячий участок 15

- assay: нет в ветке
- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:131-294

```diff
--- assay:normalize.py

+++ document:normalize.py

@@ -0,0 +1,76 @@

+def enrich_with_document_terms (df_docs :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
+    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term.\n\n    Parameters\n    ----------\n    df_docs:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c document_chembl_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.document.enrich.document_term.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438:\n        - term (nullable string, pipe-separated terms)\n        - weight (nullable string, pipe-separated weights)\n    "
+    log =UnifiedLogger .get (__name__ ).bind (component ="document_enrichment")
+    def _ensure_term_columns (df_input :pd .DataFrame )->pd .DataFrame :
+        result_frame =df_input .copy ()
+        for column_name in ("term","weight"):
+            if column_name not in result_frame .columns :
+                result_frame [column_name ]=pd .Series ([""for _ in range (len (result_frame ))],index =result_frame .index ,dtype ="string")
+            else :
+                result_frame [column_name ]=result_frame [column_name ].astype ("string")
+            na_mask =result_frame [column_name ].isna ()
+            if bool (na_mask .any ()):
+                result_frame .loc [na_mask ,column_name ]=""
+            result_frame [column_name ]=result_frame [column_name ].astype ("string")
+        return result_frame
+    df_docs =_ensure_columns (df_docs ,_DOCUMENT_TERM_COLUMNS )
+    if df_docs .empty :
+        log .debug ("enrichment_skipped_empty_dataframe")
+        prepared_empty =_ensure_term_columns (df_docs )
+        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (prepared_empty ,lazy =True )
+    required_cols =["document_chembl_id"]
+    missing_cols =[col for col in required_cols if col not in df_docs .columns ]
+    if missing_cols :
+        log .warning ("enrichment_skipped_missing_columns",missing_columns =missing_cols )
+        prepared_missing =_ensure_term_columns (df_docs )
+        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (prepared_missing ,lazy =True )
+    doc_ids :list [str ]=[]
+    for _ ,row in df_docs .iterrows ():
+        doc_id =row .get ("document_chembl_id")
+        if pd .isna (doc_id )or doc_id is None :
+            continue
+        doc_id_str =str (doc_id ).strip ()
+        if doc_id_str :
+            doc_ids .append (doc_id_str )
+    if not doc_ids :
+        log .debug ("enrichment_skipped_no_valid_ids")
+        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (df_docs ,lazy =True )
+    fields =cfg .get ("select_fields",["document_chembl_id","term","weight"])
+    page_limit =cfg .get ("page_limit",1000 )
+    sort =cfg .get ("sort","weight_desc")
+    log .info ("enrichment_fetching_terms",ids_count =len (set (doc_ids )))
+    records_dict =client .fetch_document_terms_by_ids (ids =doc_ids ,fields =list (fields ),page_limit =page_limit )
+    all_records :list [dict [str ,Any ]]=[]
+    for records_list in records_dict .values ():
+        all_records .extend (records_list )
+    agg_result =aggregate_terms (all_records ,sort =sort )
+    enrichment_data :list [dict [str ,Any ]]=[]
+    for doc_id ,term_weight in agg_result .items ():
+        enrichment_data .append ({"document_chembl_id":doc_id ,"term":term_weight ["term"],"weight":term_weight ["weight"]})
+    if not enrichment_data :
+        log .debug ("enrichment_no_records_found")
+        prepared =_ensure_term_columns (df_docs )
+        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (prepared ,lazy =True )
+    df_enrich =pd .DataFrame (enrichment_data )
+    original_index =df_docs .index .copy ()
+    df_result =df_docs .merge (df_enrich ,on =["document_chembl_id"],how ="left",suffixes =("","_enrich"))
+    for col in ["term","weight"]:
+        enrich_col =f'{col }_enrich'
+        if enrich_col in df_result .columns :
+            base_series =df_result [col ]if col in df_result .columns else pd .Series ([pd .NA ]*len (df_result ),index =df_result .index ,dtype ="object")
+            enrich_series =df_result [enrich_col ]
+            df_result [col ]=base_series .where (pd .notna (base_series ),enrich_series )
+            df_result =df_result .drop (columns =[enrich_col ])
+    for col in ["term","weight"]:
+        if col not in df_result .columns :
+            df_result [col ]=pd .Series ([""for _ in range (len (df_result ))],index =df_result .index ,dtype ="string")
+        else :
+            df_result [col ]=df_result [col ].astype ("string")
+        na_mask =df_result [col ].isna ()
+        if bool (na_mask .any ()):
+            df_result .loc [na_mask ,col ]=""
+        df_result [col ]=df_result [col ].astype ("string")
+    df_result =df_result .reindex (original_index )
+    df_result =_ensure_term_columns (df_result )
+    log .info ("enrichment_completed",rows_enriched =df_result .shape [0 ],documents_with_terms =len (agg_result ))
+    return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (df_result ,lazy =True )
```

---

## Пара: assay ↔ target

- AST hash: 4e65bc8b94391cb8c868cf6ad530c2c6 ↔ 107171553e4f1c509bba122a3d0ccb96

- Jaccard по токенам: 0.295

### Модуль run.py

Определение                                          | assay сигнатура                                                   | target сигнатура                                     | Побочные эффекты                                                                                                   | Исключения                           | Статус         
-----------------------------------------------------|-------------------------------------------------------------------|------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|--------------------------------------|----------------
ChemblAssayPipeline                                  | —                                                                 | —                                                    | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
target: {}             | assay: ['TypeError(msg)']
target: [] | только в assay 
ChemblAssayPipeline.__init__                         | self, config: PipelineConfig, run_id: str                         | —                                                    | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: []                 | только в assay 
ChemblAssayPipeline._add_row_metadata                | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: {'logging': ['log.debug'], 'io': []}
target: {}                                                             | assay: []
target: []                 | только в assay 
ChemblAssayPipeline._build_assay_descriptor          | self: SelfChemblAssayPipeline                                     | —                                                    | assay: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
target: {}                                  | assay: ['TypeError(msg)']
target: [] | только в assay 
ChemblAssayPipeline._check_missing_columns           | self, df: pd.DataFrame, log: Any, select_fields: list[str] | None | —                                                    | assay: {'logging': ['log.debug', 'log.warning'], 'io': []}
target: {}                                              | assay: []
target: []                 | только в assay 
ChemblAssayPipeline._enrich_with_related_data        | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
target: {}                                  | assay: []
target: []                 | только в assay 
ChemblAssayPipeline._harmonize_identifier_columns    | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: {'logging': ['log.debug'], 'io': []}
target: {}                                                             | assay: []
target: []                 | только в assay 
ChemblAssayPipeline._normalize_data_types            | self, df: pd.DataFrame, schema: Any, log: Any                     | —                                                    | assay: {'logging': ['log.debug'], 'io': []}
target: {}                                                             | assay: []
target: []                 | только в assay 
ChemblAssayPipeline._normalize_identifiers           | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: {'logging': ['log.debug'], 'io': []}
target: {}                                                             | assay: []
target: []                 | только в assay 
ChemblAssayPipeline._normalize_nested_structures     | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: {'logging': ['log.debug'], 'io': []}
target: {}                                                             | assay: []
target: []                 | только в assay 
ChemblAssayPipeline._normalize_string_fields         | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: {'logging': ['log.debug', 'log.warning'], 'io': []}
target: {}                                              | assay: []
target: []                 | только в assay 
ChemblAssayPipeline._serialize_array_fields          | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: {'logging': ['log.debug'], 'io': []}
target: {}                                                             | assay: []
target: []                 | только в assay 
ChemblAssayPipeline.chembl_release                   | self                                                              | —                                                    | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: []                 | только в assay 
ChemblAssayPipeline.extract                          | self, *args, **kwargs                                             | —                                                    | assay: {'logging': ['UnifiedLogger.get'], 'io': []}
target: {}                                                     | assay: []
target: []                 | только в assay 
ChemblAssayPipeline.extract_all                      | self                                                              | —                                                    | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: []                 | только в assay 
ChemblAssayPipeline.extract_by_ids                   | self, ids: Sequence[str]                                          | —                                                    | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
target: {}             | assay: []
target: []                 | только в assay 
ChemblAssayPipeline.transform                        | self, df: pd.DataFrame                                            | —                                                    | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}
target: {}                            | assay: []
target: []                 | только в assay 
ChemblTargetPipeline                                 | —                                                                 | —                                                    | assay: {}
target: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']} | assay: []
target: []                 | только в target
ChemblTargetPipeline.__init__                        | —                                                                 | self, config: PipelineConfig, run_id: str            | assay: {}
target: {'logging': [], 'io': []}                                                                        | assay: []
target: []                 | только в target
ChemblTargetPipeline._build_target_descriptor        | —                                                                 | self                                                 | assay: {}
target: {'logging': ['log.info'], 'io': []}                                                              | assay: []
target: []                 | только в target
ChemblTargetPipeline._enrich_protein_classifications | —                                                                 | self, df: pd.DataFrame, log: Any                     | assay: {}
target: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}                      | assay: []
target: []                 | только в target
ChemblTargetPipeline._enrich_target_components       | —                                                                 | self, df: pd.DataFrame, log: Any                     | assay: {}
target: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}                      | assay: []
target: []                 | только в target
ChemblTargetPipeline._harmonize_identifier_columns   | —                                                                 | self, df: pd.DataFrame, log: Any                     | assay: {}
target: {'logging': ['log.debug'], 'io': []}                                                             | assay: []
target: []                 | только в target
ChemblTargetPipeline._normalize_data_types           | —                                                                 | self, df: pd.DataFrame, schema: Any | None, log: Any | assay: {}
target: {'logging': ['log.warning'], 'io': []}                                                           | assay: []
target: []                 | только в target
ChemblTargetPipeline._normalize_identifiers          | —                                                                 | self, df: pd.DataFrame, log: Any                     | assay: {}
target: {'logging': ['log.warning'], 'io': []}                                                           | assay: []
target: []                 | только в target
ChemblTargetPipeline._normalize_string_fields        | —                                                                 | self, df: pd.DataFrame, log: Any                     | assay: {}
target: {'logging': ['log.debug'], 'io': []}                                                             | assay: []
target: []                 | только в target
ChemblTargetPipeline.extract                         | —                                                                 | self, *args, **kwargs                                | assay: {}
target: {'logging': ['UnifiedLogger.get'], 'io': []}                                                     | assay: []
target: []                 | только в target
ChemblTargetPipeline.extract_all                     | —                                                                 | self                                                 | assay: {}
target: {'logging': [], 'io': []}                                                                        | assay: []
target: []                 | только в target
ChemblTargetPipeline.extract_by_ids                  | —                                                                 | self, ids: Sequence[str]                             | assay: {}
target: {'logging': ['UnifiedLogger.get', 'log.info'], 'io': []}                                         | assay: []
target: []                 | только в target
ChemblTargetPipeline.transform                       | —                                                                 | self, df: pd.DataFrame                               | assay: {}
target: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                            | assay: []
target: []                 | только в target
__module_block_0                                     | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_1                                     | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | совпадает      
__module_block_10                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_11                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_12                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_13                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_14                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_15                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_16                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_17                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_18                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_19                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_2                                     | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_20                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: []                 | только в assay 
__module_block_21                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: []                 | только в assay 
__module_block_22                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: []                 | только в assay 
__module_block_26                                    | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: []                 | только в assay 
__module_block_3                                     | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_4                                     | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_5                                     | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_6                                     | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_7                                     | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_8                                     | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
__module_block_9                                     | —                                                                 | —                                                    | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | assay: []
target: []                 | отличается     
_extract_bao_ids_from_classifications                | node: Any                                                         | —                                                    | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: []                 | только в assay 
_iter_classification_mappings                        | node: Any                                                         | —                                                    | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: []                 | только в assay 
_normalize_bao_identifier                            | raw_value: Any                                                    | —                                                    | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: []                 | только в assay 

_Показаны первые 20 горячих участков из 56._

#### Горячий участок 1

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:126-952
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,324 +0,0 @@

-class ChemblAssayPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting assay records from the ChEMBL API."
-    actor ="assay_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all assay records from ChEMBL using pagination."
-        descriptor =self ._build_assay_descriptor ()
-        return self .run_extract_all (descriptor )
-    def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-        "Return the descriptor powering the shared extraction template."
-        def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-            if isinstance (pipeline ,ChemblAssayPipeline ):
-                return pipeline
-            msg ="ChemblAssayPipeline instance required"
-            raise TypeError (msg )
-        def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-            chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-            assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-            assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-            assay_pipeline ._chembl_release =assay_client .chembl_release
-            log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-            raw_source =assay_pipeline ._resolve_source_config ("chembl")
-            select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-            log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-            context =ChemblExtractionContext (source_config ,assay_client )
-            context .chembl_client =chembl_client
-            context .select_fields =tuple (select_fields )if select_fields else None
-            context .chembl_release =assay_pipeline ._chembl_release
-            context .extra_filters ={"max_url_length":source_config .max_url_length }
-            return context
-        def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-        def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            if df .empty :
-                return df
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in df .columns or df [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            select_fields =context .select_fields
-            if select_fields :
-                expected_fields =set (select_fields )
-                actual_fields =set (df .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-            return df
-        def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-            return pd .DataFrame ()
-        def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-        return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        self ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        if self .config .cli .dry_run :
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-            return pd .DataFrame ()
-        limit =self .config .cli .limit
-        resolved_select_fields =self ._resolve_select_fields (source_raw )
-        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-        log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-        def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield dict (item )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            if dataframe .empty :
-                return dataframe
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            if context .select_fields :
-                expected_fields =set (context .select_fields )
-                actual_fields =set (dataframe .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-            return dataframe
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw assay data by normalizing identifiers, types, and nested structures."
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._enrich_with_related_data (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._serialize_array_fields (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,AssaySchema ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Serialize array-of-object fields to header+rows format."
-        df =df .copy ()
-        arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-        if arrays_to_serialize :
-            df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-            for column in arrays_to_serialize :
-                if column in df_result .columns :
-                    column_as_string :Series =df_result [column ].astype ("string")
-                    filled_column :Series =column_as_string .copy ()
-                    filled_column [column_as_string .isna ()]=""
-                    empty_mask :Series =filled_column .eq ("")
-                    if bool (empty_mask .any ()):
-                        df_result .loc [empty_mask ,column ]=pd .NA
-            df =df_result
-            log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-        return df
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-            df ["target_chembl_id"]=df ["target_id"]
-            actions .append ("target_id->target_chembl_id")
-        alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize ChEMBL identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-        working_df =df .copy ()
-        string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-        rules ={column :StringRule ()for column in string_fields }
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if "curation_level"not in normalized_df .columns :
-            normalized_df ["curation_level"]=pd .NA
-            log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-        df =df .copy ()
-        if "assay_parameters"in df .columns :
-            log .debug ("validating_assay_parameters_truv")
-            df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-        if "assay_classifications"in df .columns :
-            if "assay_class_id"not in df .columns :
-                df ["assay_class_id"]=pd .NA
-            updated_rows =0
-            classifications_series =df ["assay_classifications"]
-            for row_index ,value in classifications_series .items ():
-                if value is None or value is pd .NA :
-                    continue
-                if isinstance (value ,float )and pd .isna (value ):
-                    continue
-                if isinstance (value ,str ):
-                    continue
-                extracted_ids =_extract_bao_ids_from_classifications (value )
-                if not extracted_ids :
-                    continue
-                joined_ids =";".join (extracted_ids )
-                row_label =cast (Any ,row_index )
-                current_value =df .at [row_label ,"assay_class_id"]
-                if pd .isna (current_value )or current_value !=joined_ids :
-                    df .at [row_label ,"assay_class_id"]=joined_ids
-                    updated_rows +=1
-            if updated_rows >0 :
-                log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-        return df
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_added",value ="assay")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_filled",value ="assay")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-        "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-        df =super ()._normalize_data_types (df ,schema ,log )
-        if "row_index"in df .columns and df ["row_index"].isna ().any ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-        df =df .copy ()
-        optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-        expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-        select_fields_set :set [str ]=set ()
-        if select_fields is not None :
-            select_fields_set =set (select_fields )
-        missing_in_response :list [str ]=[]
-        missing_in_select_fields :list [str ]=[]
-        for column in expected_api_fields :
-            if column not in df .columns :
-                missing_in_response .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-                else :
-                    log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-        missing_columns :list [str ]=[]
-        for column ,version in optional_columns .items ():
-            if column not in df .columns :
-                df [column ]=pd .NA
-                missing_columns .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-                else :
-                    log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-        if missing_in_response or missing_columns :
-            log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-        return df
-    def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-        if df .empty :
-            return df
-        try :
-            source_raw =self ._resolve_source_config ("chembl")
-        except KeyError as exc :
-            log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-            return df
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        chembl_config =getattr (self .config ,"chembl",None )
-        if chembl_config is None :
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-            return df
-        if not isinstance (chembl_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-            return df
-        assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-        if not isinstance (assay_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-            return df
-        enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-        if not isinstance (enrich_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-            return df
-        classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-        if classifications_cfg is not None :
-            log .info ("enrichment_classifications_started")
-            df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-            df =df_with_classifications
-            log .info ("enrichment_classifications_completed")
-            if "assay_class_id"in df .columns :
-                filled_count =int (df ["assay_class_id"].notna ().sum ())
-                total_count =len (df )
-                if filled_count ==0 :
-                    log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-                else :
-                    log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-            else :
-                log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-        else :
-            log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-        parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-        if parameters_cfg is not None :
-            log .info ("enrichment_parameters_started")
-            df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-            df =df_with_parameters
-            log .info ("enrichment_parameters_completed")
-        return df
```

#### Горячий участок 2

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:131-133
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,3 +0,0 @@

-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
```

#### Горячий участок 3

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:681-704
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,18 +0,0 @@

-def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_added",value ="assay")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_filled",value ="assay")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Горячий участок 4

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:167-323
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,51 +0,0 @@

-def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-    "Return the descriptor powering the shared extraction template."
-    def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-        if isinstance (pipeline ,ChemblAssayPipeline ):
-            return pipeline
-        msg ="ChemblAssayPipeline instance required"
-        raise TypeError (msg )
-    def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        assay_pipeline ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        raw_source =assay_pipeline ._resolve_source_config ("chembl")
-        select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-        log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-        context =ChemblExtractionContext (source_config ,assay_client )
-        context .chembl_client =chembl_client
-        context .select_fields =tuple (select_fields )if select_fields else None
-        context .chembl_release =assay_pipeline ._chembl_release
-        context .extra_filters ={"max_url_length":source_config .max_url_length }
-        return context
-    def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-    def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        if df .empty :
-            return df
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in df .columns or df [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        select_fields =context .select_fields
-        if select_fields :
-            expected_fields =set (select_fields )
-            actual_fields =set (df .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-        return df
-    def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-        return pd .DataFrame ()
-    def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-    return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
```

#### Горячий участок 5

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:720-835
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,31 +0,0 @@

-def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-    "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-    df =df .copy ()
-    optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-    expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-    select_fields_set :set [str ]=set ()
-    if select_fields is not None :
-        select_fields_set =set (select_fields )
-    missing_in_response :list [str ]=[]
-    missing_in_select_fields :list [str ]=[]
-    for column in expected_api_fields :
-        if column not in df .columns :
-            missing_in_response .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-            else :
-                log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-    missing_columns :list [str ]=[]
-    for column ,version in optional_columns .items ():
-        if column not in df .columns :
-            df [column ]=pd .NA
-            missing_columns .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-            else :
-                log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-    if missing_in_response or missing_columns :
-        log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-    return df
```

#### Горячий участок 6

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:837-952
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,53 +0,0 @@

-def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-    if df .empty :
-        return df
-    try :
-        source_raw =self ._resolve_source_config ("chembl")
-    except KeyError as exc :
-        log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-        return df
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    chembl_config =getattr (self .config ,"chembl",None )
-    if chembl_config is None :
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-        return df
-    if not isinstance (chembl_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-        return df
-    assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-    if not isinstance (assay_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-        return df
-    enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-    if not isinstance (enrich_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-        return df
-    classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-    if classifications_cfg is not None :
-        log .info ("enrichment_classifications_started")
-        df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-        df =df_with_classifications
-        log .info ("enrichment_classifications_completed")
-        if "assay_class_id"in df .columns :
-            filled_count =int (df ["assay_class_id"].notna ().sum ())
-            total_count =len (df )
-            if filled_count ==0 :
-                log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-            else :
-                log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-        else :
-            log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-    else :
-        log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-    parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-    if parameters_cfg is not None :
-        log .info ("enrichment_parameters_started")
-        df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-        df =df_with_parameters
-        log .info ("enrichment_parameters_completed")
-    return df
```

#### Горячий участок 7

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:526-548
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,17 +0,0 @@

-def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-    df =df .copy ()
-    actions :list [str ]=[]
-    if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-        df ["assay_chembl_id"]=df ["assay_id"]
-        actions .append ("assay_id->assay_chembl_id")
-    if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-        df ["target_chembl_id"]=df ["target_id"]
-        actions .append ("target_id->target_chembl_id")
-    alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-    if alias_columns :
-        df =df .drop (columns =alias_columns )
-        actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-    if actions :
-        log .debug ("identifier_harmonization",actions =actions )
-    return df
```

#### Горячий участок 8

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:706-718
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,7 +0,0 @@

-def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-    "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-    df =super ()._normalize_data_types (df ,schema ,log )
-    if "row_index"in df .columns and df ["row_index"].isna ().any ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Горячий участок 9

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:550-577
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,7 +0,0 @@

-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize ChEMBL identifiers with regex validation."
-    rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-    normalized_df ,stats =normalize_identifier_columns (df ,rules )
-    if stats .has_changes :
-        log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-    return normalized_df
```

#### Горячий участок 10

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:623-679
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,30 +0,0 @@

-def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-    df =df .copy ()
-    if "assay_parameters"in df .columns :
-        log .debug ("validating_assay_parameters_truv")
-        df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-    if "assay_classifications"in df .columns :
-        if "assay_class_id"not in df .columns :
-            df ["assay_class_id"]=pd .NA
-        updated_rows =0
-        classifications_series =df ["assay_classifications"]
-        for row_index ,value in classifications_series .items ():
-            if value is None or value is pd .NA :
-                continue
-            if isinstance (value ,float )and pd .isna (value ):
-                continue
-            if isinstance (value ,str ):
-                continue
-            extracted_ids =_extract_bao_ids_from_classifications (value )
-            if not extracted_ids :
-                continue
-            joined_ids =";".join (extracted_ids )
-            row_label =cast (Any ,row_index )
-            current_value =df .at [row_label ,"assay_class_id"]
-            if pd .isna (current_value )or current_value !=joined_ids :
-                df .at [row_label ,"assay_class_id"]=joined_ids
-                updated_rows +=1
-        if updated_rows >0 :
-            log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-    return df
```

#### Горячий участок 11

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:579-621
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,12 +0,0 @@

-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-    working_df =df .copy ()
-    string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-    rules ={column :StringRule ()for column in string_fields }
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if "curation_level"not in normalized_df .columns :
-        normalized_df ["curation_level"]=pd .NA
-        log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    return normalized_df
```

#### Горячий участок 12

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:504-524
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,17 +0,0 @@

-def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Serialize array-of-object fields to header+rows format."
-    df =df .copy ()
-    arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-    if arrays_to_serialize :
-        df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-        for column in arrays_to_serialize :
-            if column in df_result .columns :
-                column_as_string :Series =df_result [column ].astype ("string")
-                filled_column :Series =column_as_string .copy ()
-                filled_column [column_as_string .isna ()]=""
-                empty_mask :Series =filled_column .eq ("")
-                if bool (empty_mask .any ()):
-                    df_result .loc [empty_mask ,column ]=pd .NA
-        df =df_result
-        log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-    return df
```

#### Горячий участок 13

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:136-139
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,4 +0,0 @@

-@property
-def chembl_release (self )->str |None :
-    "Return the cached ChEMBL release captured during extraction."
-    return self ._chembl_release
```

#### Горячий участок 14

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:145-159
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,4 +0,0 @@

-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
```

#### Горячий участок 15

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:161-165
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,4 +0,0 @@

-def extract_all (self )->pd .DataFrame :
-    "Extract all assay records from ChEMBL using pagination."
-    descriptor =self ._build_assay_descriptor ()
-    return self .run_extract_all (descriptor )
```

#### Горячий участок 16

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:325-469
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,42 +0,0 @@

-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-    assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-    self ._chembl_release =assay_client .chembl_release
-    log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-    if self .config .cli .dry_run :
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-        return pd .DataFrame ()
-    limit =self .config .cli .limit
-    resolved_select_fields =self ._resolve_select_fields (source_raw )
-    merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-    log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-    def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield dict (item )
-    def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-        if dataframe .empty :
-            return dataframe
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        if context .select_fields :
-            expected_fields =set (context .select_fields )
-            actual_fields =set (dataframe .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-        return dataframe
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-    return dataframe
```

#### Горячий участок 17

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:471-498
- target: нет в ветке

```diff
--- assay:run.py

+++ target:run.py

@@ -1,21 +0,0 @@

-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw assay data by normalizing identifiers, types, and nested structures."
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-    df =df .copy ()
-    df =self ._harmonize_identifier_columns (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._enrich_with_related_data (df ,log )
-    df =self ._normalize_nested_structures (df ,log )
-    df =self ._serialize_array_fields (df ,log )
-    df =self ._add_row_metadata (df ,log )
-    df =self ._normalize_data_types (df ,AssaySchema ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

#### Горячий участок 18

- assay: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:38-783

```diff
--- assay:run.py

+++ target:run.py

@@ -0,0 +1,342 @@

+class ChemblTargetPipeline (ChemblPipelineBase ):
+    "ETL pipeline extracting target records from the ChEMBL API."
+    actor ="target_chembl"
+    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+        super ().__init__ (config ,run_id )
+    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
+        "Fetch target payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        return self ._dispatch_extract_mode (log ,event_name ="chembl_target.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="target_chembl_id")
+    def extract_all (self )->pd .DataFrame :
+        "Extract all target records from ChEMBL using pagination."
+        descriptor =self ._build_target_descriptor ()
+        return self .run_extract_all (descriptor )
+    def _build_target_descriptor (self )->ChemblExtractionDescriptor ["ChemblTargetPipeline"]:
+        "Return the descriptor powering target extraction."
+        def build_context (pipeline :"ChemblTargetPipeline",source_config :TargetSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+            base_url =pipeline ._resolve_base_url (source_config .parameters )
+            http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
+            chembl_client =ChemblClient (http_client )
+            pipeline ._chembl_release =pipeline .fetch_chembl_release (chembl_client ,log )
+            target_client =ChemblTargetClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
+            select_fields =source_config .parameters .select_fields
+            return ChemblExtractionContext (source_config =source_config ,iterator =target_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =pipeline ._chembl_release ,extra_filters ={"batch_size":source_config .batch_size })
+        def empty_frame (_ :"ChemblTargetPipeline",__ :ChemblExtractionContext )->pd .DataFrame :
+            return pd .DataFrame ({"target_chembl_id":pd .Series (dtype ="string")})
+        def dry_run_handler (pipeline :"ChemblTargetPipeline",_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =pipeline ._chembl_release )
+            return pd .DataFrame ()
+        def summary_extra (pipeline :"ChemblTargetPipeline",_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+            return {"limit":pipeline .config .cli .limit }
+        return ChemblExtractionDescriptor [ChemblTargetPipeline ](name ="chembl_target",source_name ="chembl",source_config_factory =TargetSourceConfig .from_source_config ,build_context =build_context ,id_column ="target_chembl_id",summary_event ="chembl_target.extract_summary",sort_by =("target_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
+    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
+        "Extract target records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of target_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted target records.\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        stage_start =time .perf_counter ()
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =TargetSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
+        chembl_client =ChemblClient (http_client )
+        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
+        if self .config .cli .dry_run :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
+            return pd .DataFrame ()
+        batch_size =source_config .batch_size
+        limit =self .config .cli .limit
+        select_fields =source_config .parameters .select_fields
+        target_client =ChemblTargetClient (chembl_client ,batch_size =min (batch_size ,25 ))
+        def fetch_targets (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
+            iterator =target_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
+            for item in iterator :
+                yield dict (item )
+        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="target_chembl_id",fetcher =fetch_targets ,select_fields =select_fields ,batch_size =batch_size ,chunk_size =min (batch_size ,100 ),max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release )
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_target.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
+        return dataframe
+    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
+        "Transform raw target data by normalizing fields and enriching with component/classification data."
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
+        df =df .copy ()
+        df =self ._harmonize_identifier_columns (df ,log )
+        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
+        if df .empty :
+            log .debug ("transform_empty_dataframe")
+            return df
+        log .info ("transform_started",rows =len (df ))
+        df =self ._normalize_identifiers (df ,log )
+        df =serialize_target_arrays (df ,self .config )
+        if not self .config .cli .dry_run :
+            df =self ._enrich_target_components (df ,log )
+        if not self .config .cli .dry_run :
+            df =self ._enrich_protein_classifications (df ,log )
+        df =self ._normalize_string_fields (df ,log )
+        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
+        df =self ._normalize_data_types (df ,TargetSchema ,log )
+        df =self ._order_schema_columns (df ,COLUMN_ORDER )
+        log .info ("transform_completed",rows =len (df ))
+        return df
+    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Harmonize identifier column names."
+        df =df .copy ()
+        actions :list [str ]=[]
+        if "target_id"in df .columns and "target_chembl_id"not in df .columns :
+            df ["target_chembl_id"]=df ["target_id"]
+            actions .append ("target_id->target_chembl_id")
+        alias_columns =[column for column in ("target_id",)if column in df .columns ]
+        if alias_columns :
+            df =df .drop (columns =alias_columns )
+            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
+        if actions :
+            log .debug ("identifier_harmonization",actions =actions )
+        return df
+    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize ChEMBL identifiers with regex validation."
+        rules =[IdentifierRule (name ="target_chembl",columns =["target_chembl_id"],pattern ="^CHEMBL\\d+$")]
+        normalized_df ,stats =normalize_identifier_columns (df ,rules )
+        invalid_info =stats .per_column .get ("target_chembl_id")
+        if invalid_info and invalid_info ["invalid"]>0 :
+            log .warning ("invalid_target_chembl_id",count =invalid_info ["invalid"])
+        return normalized_df
+    def _enrich_target_components (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Enrich targets with component data from /target_component endpoint.\n\n        Only enriches targets where uniprot_accessions or component_count are missing.\n        If data is already present from the main query (via serialize_target_arrays),\n        it will not be overwritten.\n        "
+        df =df .copy ()
+        if df .empty or "target_chembl_id"not in df .columns :
+            return df
+        needs_enrichment =df ["target_chembl_id"].notna ()
+        if "uniprot_accessions"in df .columns :
+            needs_enrichment =needs_enrichment &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
+        if "component_count"in df .columns :
+            needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["component_count"].isna ()|(df ["component_count"]==0 ))
+        target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
+        if not target_ids_to_enrich :
+            log .debug ("enrich_target_components_skipped",reason ="all_data_present")
+            return df
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =TargetSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
+        chembl_client =ChemblClient (http_client )
+        if "target_chembl_id"not in df .columns :
+            return df
+        component_map :dict [str ,list [str ]]={}
+        target_ids_set :set [str ]=set (target_ids_to_enrich )
+        def _is_target (value :object )->bool :
+            if value is None or value is pd .NA :
+                return False
+            if isinstance (value ,Real )and math .isnan (float (value )):
+                return False
+            normalized =str (value ).strip ()
+            if not normalized :
+                return False
+            return normalized in target_ids_set
+        target_membership =df ["target_chembl_id"].map (_is_target )
+        log .info ("enrich_target_components_start",target_count =len (target_ids_to_enrich ))
+        for target_id in target_ids_to_enrich :
+            try :
+                components :list [str ]=[]
+                for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
+                    accession =item .get ("accession")
+                    if isinstance (accession ,str )and accession .strip ():
+                        components .append (accession .strip ())
+                if components :
+                    component_map [target_id ]=components
+            except Exception as exc :
+                log .warning ("target_component_fetch_error",target_chembl_id =target_id ,error =str (exc ))
+        if "uniprot_accessions"in df .columns :
+            mask =target_membership &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
+            if mask .any ():
+                df .loc [mask ,"uniprot_accessions"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
+        if "component_count"in df .columns :
+            mask =target_membership &(df ["component_count"].isna ()|(df ["component_count"]==0 ))
+            if mask .any ():
+                df .loc [mask ,"component_count"]=df .loc [mask ,"target_chembl_id"].map (lambda x :len (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
+        log .info ("enrich_target_components_complete",enriched_count =len (component_map ))
+        return df
+    def _enrich_protein_classifications (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Enrich targets with full protein classification hierarchy.\n\n        Extracts complete protein classification hierarchy with tree nodes and expanded paths l1..l8.\n        Algorithm:\n        1. For each target, get components via /target_component.json\n        2. Filter only PROTEIN components (component_type = 'PROTEIN')\n        3. For each protein component, get classes via /component_class.json \u2192 protein_class_id\n        4. For each protein_class_id, get node metadata via /protein_classification.json\n        5. For each protein_class_id, get expanded path via /protein_family_classification.json \u2192 l1..l8\n        6. Aggregate at TID level: protein_class_list (array) and protein_class_top (min class_level)\n\n        Only enriches targets where protein_class_list or protein_class_top are missing.\n        If data is already present from the main query, it will not be overwritten.\n        "
+        df =df .copy ()
+        if df .empty or "target_chembl_id"not in df .columns :
+            return df
+        needs_enrichment =df ["target_chembl_id"].notna ()
+        if "protein_class_list"in df .columns :
+            needs_enrichment =needs_enrichment &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
+        if "protein_class_top"in df .columns :
+            needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
+        target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
+        if not target_ids_to_enrich :
+            log .debug ("enrich_protein_classifications_skipped",reason ="all_data_present")
+            return df
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =TargetSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
+        chembl_client =ChemblClient (http_client )
+        if "protein_class_list"not in df .columns :
+            df ["protein_class_list"]=pd .NA
+        if "protein_class_top"not in df .columns :
+            df ["protein_class_top"]=pd .NA
+        if "target_chembl_id"not in df .columns :
+            return df
+        classification_list_map :dict [str ,list [dict [str ,Any ]]]={}
+        classification_top_map :dict [str ,dict [str ,Any ]]={}
+        target_ids_set :set [str ]=set (target_ids_to_enrich )
+        def _is_target (value :object )->bool :
+            if value is None or value is pd .NA :
+                return False
+            if isinstance (value ,Real )and math .isnan (float (value )):
+                return False
+            normalized =str (value ).strip ()
+            if not normalized :
+                return False
+            return normalized in target_ids_set
+        target_membership =df ["target_chembl_id"].map (_is_target )
+        log .info ("enrich_protein_classifications_start",target_count =len (target_ids_to_enrich ))
+        for target_id in target_ids_to_enrich :
+            try :
+                component_ids :list [str ]=[]
+                for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
+                    component_id =item .get ("component_id")
+                    if component_id is not None :
+                        component_ids .append (str (component_id ))
+                if not component_ids :
+                    continue
+                protein_component_ids :list [str ]=[]
+                for component_id in component_ids :
+                    try :
+                        for seq_item in chembl_client .paginate ("/component_sequence.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_sequences"):
+                            component_type =seq_item .get ("component_type")
+                            if isinstance (component_type ,str )and component_type .upper ()=="PROTEIN":
+                                protein_component_ids .append (component_id )
+                                break
+                    except Exception as exc :
+                        log .debug ("component_sequence_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
+                if not protein_component_ids :
+                    continue
+                protein_class_ids :set [str ]=set ()
+                for component_id in protein_component_ids :
+                    try :
+                        for class_item in chembl_client .paginate ("/component_class.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_classes"):
+                            protein_class_id =class_item .get ("protein_class_id")
+                            if protein_class_id is not None :
+                                protein_class_ids .add (str (protein_class_id ))
+                    except Exception as exc :
+                        log .debug ("component_class_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
+                if not protein_class_ids :
+                    continue
+                protein_classes :list [dict [str ,Any ]]=[]
+                for protein_class_id in protein_class_ids :
+                    try :
+                        node_metadata :dict [str ,Any ]|None =None
+                        for node_item in chembl_client .paginate ("/protein_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_classifications"):
+                            node_metadata ={"protein_class_id":str (protein_class_id ),"pref_name":node_item .get ("pref_name"),"short_name":node_item .get ("short_name"),"class_level":node_item .get ("class_level"),"parent_id":node_item .get ("parent_id"),"protein_class_desc":node_item .get ("protein_class_desc")}
+                            break
+                        path_levels :list [str |None ]=[None ]*8
+                        try :
+                            for path_item in chembl_client .paginate ("/protein_family_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_family_classifications"):
+                                for i in range (1 ,9 ):
+                                    level_key =f'l{i }'
+                                    level_value =path_item .get (level_key )
+                                    if level_value is not None :
+                                        if isinstance (level_value ,(float ,int )):
+                                            if pd .isna (level_value ):
+                                                path_levels [i -1 ]=None
+                                            else :
+                                                path_levels [i -1 ]=str (level_value )
+                                        else :
+                                            path_levels [i -1 ]=str (level_value )
+                                break
+                        except Exception as exc :
+                            log .debug ("protein_family_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
+                        if node_metadata :
+                            class_obj :dict [str ,Any ]={"protein_class_id":node_metadata ["protein_class_id"],"pref_name":node_metadata .get ("pref_name"),"short_name":node_metadata .get ("short_name"),"class_level":node_metadata .get ("class_level"),"parent_id":node_metadata .get ("parent_id"),"protein_class_desc":node_metadata .get ("protein_class_desc"),"path":[level for level in path_levels if level is not None ]}
+                            protein_classes .append (class_obj )
+                    except Exception as exc :
+                        log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
+                if protein_classes :
+                    seen_ids :set [str ]=set ()
+                    unique_classes :list [dict [str ,Any ]]=[]
+                    for class_obj in protein_classes :
+                        class_id =class_obj .get ("protein_class_id")
+                        if class_id and class_id not in seen_ids :
+                            seen_ids .add (class_id )
+                            unique_classes .append (class_obj )
+                    unique_classes .sort (key =lambda x :(x .get ("class_level")is None ,x .get ("class_level")or 0 ))
+                    classification_list_map [target_id ]=unique_classes
+                    top_class :dict [str ,Any ]|None =None
+                    min_level :int |None =None
+                    for class_obj in unique_classes :
+                        level =class_obj .get ("class_level")
+                        if level is not None :
+                            try :
+                                level_int =int (level )if not isinstance (level ,int )else level
+                                if min_level is None or level_int <min_level :
+                                    min_level =level_int
+                                    top_class =class_obj
+                            except (ValueError ,TypeError ):
+                                continue
+                    if top_class :
+                        classification_top_map [target_id ]=top_class
+            except Exception as exc :
+                log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,error =str (exc ))
+        if "protein_class_list"in df .columns :
+            mask =target_membership &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
+            if mask .any ():
+                df .loc [mask ,"protein_class_list"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_list_map .get (str (x ),[]),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_list_map else pd .NA )
+        if "protein_class_top"in df .columns :
+            mask =target_membership &(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
+            if mask .any ():
+                df .loc [mask ,"protein_class_top"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_top_map .get (str (x ),{}),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_top_map else pd .NA )
+        log .info ("enrich_protein_classifications_complete",enriched_list_count =len (classification_list_map ),enriched_top_count =len (classification_top_map ))
+        return df
+    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize string fields by trimming whitespace."
+        working_df =df .copy ()
+        rules ={"pref_name":StringRule (),"target_type":StringRule (),"organism":StringRule (),"tax_id":StringRule ()}
+        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        if stats .has_changes :
+            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        return normalized_df
+    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any |None ,log :Any )->pd .DataFrame :
+        "Normalize data types to match schema expectations.\n\n        Overrides base implementation to handle component_count and species_group_flag specially.\n        "
+        df =super ()._normalize_data_types (df ,schema ,log )
+        def _coerce_nullable_int (value :object )->object :
+            if value is None or value is pd .NA :
+                return pd .NA
+            if isinstance (value ,bool ):
+                return int (value )
+            if isinstance (value ,Integral ):
+                return int (value )
+            if isinstance (value ,Decimal ):
+                if value .is_nan ()or value %1 !=0 :
+                    return pd .NA
+                return int (value )
+            if isinstance (value ,Real ):
+                float_value =float (value )
+                if not math .isfinite (float_value )or not float_value .is_integer ():
+                    return pd .NA
+                return int (float_value )
+            if isinstance (value ,str ):
+                stripped =value .strip ()
+                if not stripped :
+                    return pd .NA
+                try :
+                    decimal_value =Decimal (stripped )
+                except (InvalidOperation ,ValueError ):
+                    return pd .NA
+                if decimal_value .is_nan ()or decimal_value %1 !=0 :
+                    return pd .NA
+                return int (decimal_value )
+            return pd .NA
+        if "component_count"in df .columns :
+            coerced_component_count =df ["component_count"].map (_coerce_nullable_int )
+            df ["component_count"]=coerced_component_count .astype ("Int64")
+        if "species_group_flag"in df .columns :
+            try :
+                coerced_species_group_flag =df ["species_group_flag"].map (_coerce_nullable_int )
+                df ["species_group_flag"]=coerced_species_group_flag .astype ("Int64")
+            except (ValueError ,TypeError )as exc :
+                log .warning ("species_group_flag_conversion_failed",error =str (exc ))
+        return df
```

#### Горячий участок 19

- assay: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:43-44

```diff
--- assay:run.py

+++ target:run.py

@@ -0,0 +1,2 @@

+def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+    super ().__init__ (config ,run_id )
```

#### Горячий участок 20

- assay: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:72-141

```diff
--- assay:run.py

+++ target:run.py

@@ -0,0 +1,19 @@

+def _build_target_descriptor (self )->ChemblExtractionDescriptor ["ChemblTargetPipeline"]:
+    "Return the descriptor powering target extraction."
+    def build_context (pipeline :"ChemblTargetPipeline",source_config :TargetSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+        base_url =pipeline ._resolve_base_url (source_config .parameters )
+        http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
+        chembl_client =ChemblClient (http_client )
+        pipeline ._chembl_release =pipeline .fetch_chembl_release (chembl_client ,log )
+        target_client =ChemblTargetClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
+        select_fields =source_config .parameters .select_fields
+        return ChemblExtractionContext (source_config =source_config ,iterator =target_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =pipeline ._chembl_release ,extra_filters ={"batch_size":source_config .batch_size })
+    def empty_frame (_ :"ChemblTargetPipeline",__ :ChemblExtractionContext )->pd .DataFrame :
+        return pd .DataFrame ({"target_chembl_id":pd .Series (dtype ="string")})
+    def dry_run_handler (pipeline :"ChemblTargetPipeline",_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =pipeline ._chembl_release )
+        return pd .DataFrame ()
+    def summary_extra (pipeline :"ChemblTargetPipeline",_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+        return {"limit":pipeline .config .cli .limit }
+    return ChemblExtractionDescriptor [ChemblTargetPipeline ](name ="chembl_target",source_name ="chembl",source_config_factory =TargetSourceConfig .from_source_config ,build_context =build_context ,id_column ="target_chembl_id",summary_event ="chembl_target.extract_summary",sort_by =("target_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
```

### Модуль transform.py

Определение                              | assay сигнатура                                | target сигнатура              | Побочные эффекты                                                                                                    | Исключения                                  | Статус         
-----------------------------------------|------------------------------------------------|-------------------------------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------|----------------
__module_block_0                         | —                                              | —                             | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                  | assay: []
target: []                        | отличается     
__module_block_1                         | —                                              | —                             | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                  | assay: []
target: []                        | совпадает      
__module_block_10                        | —                                              | —                             | assay: {}
target: {'logging': [], 'io': []}                                                                         | assay: []
target: []                        | только в target
__module_block_2                         | —                                              | —                             | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                  | assay: []
target: []                        | отличается     
__module_block_3                         | —                                              | —                             | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                  | assay: []
target: []                        | отличается     
__module_block_4                         | —                                              | —                             | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                  | assay: []
target: []                        | отличается     
__module_block_5                         | —                                              | —                             | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                  | assay: []
target: []                        | совпадает      
__module_block_6                         | —                                              | —                             | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                  | assay: []
target: []                        | отличается     
__module_block_7                         | —                                              | —                             | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                  | assay: []
target: []                        | отличается     
__module_block_8                         | —                                              | —                             | assay: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                  | assay: []
target: []                        | отличается     
__module_block_9                         | —                                              | —                             | assay: {}
target: {'logging': [], 'io': []}                                                                         | assay: []
target: []                        | только в target
_collect_dicts                           | —                                              | source: Any                   | assay: {}
target: {'logging': [], 'io': []}                                                                         | assay: []
target: []                        | только в target
_is_iterable_of_objects                  | —                                              | value: Any                    | assay: {}
target: {'logging': [], 'io': []}                                                                         | assay: []
target: []                        | только в target
_is_json_dict                            | —                                              | value: Any                    | assay: {}
target: {'logging': [], 'io': []}                                                                         | assay: []
target: []                        | только в target
_is_null_like                            | value: Any                                     | —                             | assay: {'logging': [], 'io': []}
target: {}                                                                         | assay: []
target: []                        | только в assay 
extract_and_serialize_component_synonyms | —                                              | target_components: Any        | assay: {}
target: {'logging': [], 'io': []}                                                                         | assay: []
target: []                        | только в target
flatten_target_components                | —                                              | rec: dict[str, Any]           | assay: {}
target: {'logging': [], 'io': ['json.dumps']}                                                             | assay: []
target: []                        | только в target
serialize_target_arrays                  | —                                              | df: pd.DataFrame, config: Any | assay: {}
target: {'logging': [], 'io': []}                                                                         | assay: []
target: []                        | только в target
validate_assay_parameters_truv           | df: pd.DataFrame, column: str, fail_fast: bool | —                             | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.warning'], 'io': ['json.loads']}
target: {} | assay: ['ValueError(error_msg)']
target: [] | только в assay 

#### Горячий участок 1

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:1-1
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:1-1

```diff
--- assay:transform.py

+++ target:transform.py

@@ -1 +1 @@

-"Transform utilities for ChEMBL assay pipeline array serialization."
+"Transform utilities for ChEMBL target pipeline array serialization."
```

#### Горячий участок 2

- assay: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:22-22

```diff
--- assay:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+JsonDict =dict [str ,Any ]
```

#### Горячий участок 3

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:10-10
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:13-13

```diff
--- assay:transform.py

+++ target:transform.py

@@ -1 +1 @@

-from bioetl .core .logger import UnifiedLogger
+from bioetl .core .serialization import header_rows_serialize
```

#### Горячий участок 4

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:11-11
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:6-6

```diff
--- assay:transform.py

+++ target:transform.py

@@ -1 +1 @@

-from bioetl .core .serialization import header_rows_serialize ,serialize_array_fields
+from collections .abc import Iterable
```

#### Горячий участок 5

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:6-6
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:7-7

```diff
--- assay:transform.py

+++ target:transform.py

@@ -1 +1 @@

-from typing import Any ,cast
+from typing import Any ,TypeGuard ,cast
```

#### Горячий участок 6

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:8-8
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:9-9

```diff
--- assay:transform.py

+++ target:transform.py

@@ -1 +1 @@

-import pandas as pd
+import numpy as np
```

#### Горячий участок 7

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:13-17
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:10-10

```diff
--- assay:transform.py

+++ target:transform.py

@@ -1 +1 @@

-__all__ =["header_rows_serialize","serialize_array_fields","validate_assay_parameters_truv"]
+import numpy .typing as npt
```

#### Горячий участок 8

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:19-19
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:11-11

```diff
--- assay:transform.py

+++ target:transform.py

@@ -1 +1 @@

-AssayParam =dict [str ,Any ]
+import pandas as pd
```

#### Горячий участок 9

- assay: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:15-19

```diff
--- assay:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+__all__ =["serialize_target_arrays","extract_and_serialize_component_synonyms","flatten_target_components"]
```

#### Горячий участок 10

- assay: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:33-47

```diff
--- assay:transform.py

+++ target:transform.py

@@ -0,0 +1,12 @@

+def _collect_dicts (source :Any )->list [JsonDict ]:
+    "Collect dictionary entries from arbitrary source keeping order."
+    result :list [JsonDict ]=[]
+    if _is_json_dict (source ):
+        result .append (source )
+        return result
+    if _is_iterable_of_objects (source ):
+        for element in source :
+            element_any :Any =element
+            if _is_json_dict (element_any ):
+                result .append (element_any )
+    return result
```

#### Горячий участок 11

- assay: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:29-30

```diff
--- assay:transform.py

+++ target:transform.py

@@ -0,0 +1,2 @@

+def _is_iterable_of_objects (value :Any )->TypeGuard [Iterable [Any ]]:
+    return isinstance (value ,Iterable )and (not isinstance (value ,(str ,bytes )))
```

#### Горячий участок 12

- assay: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:25-26

```diff
--- assay:transform.py

+++ target:transform.py

@@ -0,0 +1,2 @@

+def _is_json_dict (value :Any )->TypeGuard [JsonDict ]:
+    return isinstance (value ,dict )
```

#### Горячий участок 13

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:22-39
- target: нет в ветке

```diff
--- assay:transform.py

+++ target:transform.py

@@ -1,13 +0,0 @@

-def _is_null_like (value :Any )->bool :
-    "\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043c\u043e\u0436\u043d\u043e \u043b\u0438 \u0442\u0440\u0430\u043a\u0442\u043e\u0432\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u0430\u043a \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0435."
-    if value is None :
-        return True
-    if isinstance (value ,str ):
-        return value .strip ()==""
-    if isinstance (value ,float ):
-        return bool (pd .isna (value ))
-    try :
-        is_na_raw =cast (Any ,pd .isna (value ))
-    except TypeError :
-        return False
-    return bool (is_na_raw )if isinstance (is_na_raw ,bool )else False
```

#### Горячий участок 14

- assay: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:133-162

```diff
--- assay:transform.py

+++ target:transform.py

@@ -0,0 +1,15 @@

+def extract_and_serialize_component_synonyms (target_components :Any )->str :
+    "Extract target_component_synonyms from target_components and serialize.\n\n    Parameters\n    ----------\n    target_components:\n        List of target component dicts, None, or empty list.\n\n    Returns\n    -------\n    str:\n        Serialized string in header+rows format, or empty string for None/empty.\n    "
+    if target_components is None :
+        return ""
+    components :list [dict [str ,Any ]]=_collect_dicts (target_components )
+    if not components :
+        return ""
+    all_synonyms :list [dict [str ,Any ]]=[]
+    for component in components :
+        syns_item :Any =component .get ("target_component_synonyms")
+        if syns_item :
+            all_synonyms .extend (_collect_dicts (syns_item ))
+    if not all_synonyms :
+        return ""
+    return header_rows_serialize (all_synonyms )
```

#### Горячий участок 15

- assay: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:50-130

```diff
--- assay:transform.py

+++ target:transform.py

@@ -0,0 +1,34 @@

+def flatten_target_components (rec :dict [str ,Any ])->dict [str ,Any ]:
+    "Flatten nested target_components data into flat columns.\n\n    Extracts:\n    - uniprot_accessions from target_components[*].accession\n    - target_component_synonyms__flat from target_components[*].target_component_synonyms[*].component_synonym\n    - target_components__flat (serialized container)\n    - cross_references__flat (serialized from top-level)\n    - component_count (counted from accessions or from top-level)\n\n    Parameters\n    ----------\n    rec:\n        Target record dict from ChEMBL API.\n\n    Returns\n    -------\n    dict[str, Any]:\n        Dictionary with flattened fields:\n        - uniprot_accessions: sorted list of unique UniProt accessions (as JSON string)\n        - target_component_synonyms__flat: serialized synonyms\n        - target_components__flat: serialized components\n        - cross_references__flat: serialized cross-references\n        - component_count: count of unique accessions\n    "
+    result :dict [str ,Any ]={"uniprot_accessions":"","target_component_synonyms__flat":"","target_components__flat":"","cross_references__flat":"","component_count":None }
+    comps_raw :Any =rec .get ("target_components")or []
+    comps :list [dict [str ,Any ]]=_collect_dicts (comps_raw )
+    accessions :list [str ]=[]
+    all_synonyms :list [dict [str ,Any ]]=[]
+    for component in comps :
+        accession :Any =component .get ("accession")
+        if isinstance (accession ,str )and accession .strip ():
+            accessions .append (accession .strip ())
+        syns :Any =component .get ("target_component_synonyms")
+        if syns :
+            all_synonyms .extend (_collect_dicts (syns ))
+    unique_accessions =sorted (set (accessions ))
+    if unique_accessions :
+        result ["uniprot_accessions"]=json .dumps (unique_accessions ,ensure_ascii =False )
+        result ["component_count"]=len (unique_accessions )
+    else :
+        top_level_count =rec .get ("component_count")
+        if top_level_count is not None :
+            try :
+                result ["component_count"]=int (top_level_count )
+            except (ValueError ,TypeError ):
+                result ["component_count"]=None
+    if all_synonyms :
+        result ["target_component_synonyms__flat"]=header_rows_serialize (all_synonyms )
+    if comps :
+        result ["target_components__flat"]=header_rows_serialize (comps )
+    xrefs_raw :Any =rec .get ("cross_references")or []
+    xrefs :list [dict [str ,Any ]]=_collect_dicts (xrefs_raw )
+    if xrefs :
+        result ["cross_references__flat"]=header_rows_serialize (xrefs )
+    return result
```

#### Горячий участок 16

- assay: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:165-251

```diff
--- assay:transform.py

+++ target:transform.py

@@ -0,0 +1,46 @@

+def serialize_target_arrays (df :pd .DataFrame ,config :Any )->pd .DataFrame :
+    "Serialize array fields for target pipeline.\n\n    Uses flatten_target_components() to extract and serialize nested data\n    from target_components and cross_references.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    config:\n        Pipeline config with transform.arrays_to_header_rows.\n\n    Returns\n    -------\n    pd.DataFrame:\n        DataFrame with serialized array fields.\n    "
+    df =df .copy ()
+    arrays_to_serialize :list [str ]=[]
+    try :
+        if hasattr (config ,"transform")and config .transform is not None :
+            if hasattr (config .transform ,"arrays_to_header_rows"):
+                arrays_to_serialize =list (config .transform .arrays_to_header_rows )
+    except (AttributeError ,TypeError ):
+        pass
+    if not df .empty :
+        flattened_data :list [dict [str ,Any ]]=[]
+        for _ ,row in df .iterrows ():
+            row_dict :dict [str ,Any ]=row .to_dict ()
+            for key ,value in row_dict .items ():
+                if isinstance (value ,np .ndarray ):
+                    array_value =cast (npt .NDArray [Any ],value )
+                    if array_value .size ==0 :
+                        row_dict [key ]=None
+                    else :
+                        try :
+                            nan_mask =np .asarray (pd .isna (array_value ),dtype =bool )
+                            if bool (np .all (nan_mask )):
+                                row_dict [key ]=None
+                        except (TypeError ,ValueError ):
+                            pass
+                elif pd .api .types .is_scalar (value ):
+                    try :
+                        if pd .isna (value ):
+                            row_dict [key ]=None
+                    except (TypeError ,ValueError ):
+                        pass
+            flattened =flatten_target_components (row_dict )
+            row_dict .update (flattened )
+            flattened_data .append (row_dict )
+        df =pd .DataFrame (flattened_data )
+    else :
+        df ["cross_references__flat"]=""
+        df ["target_components__flat"]=""
+        df ["target_component_synonyms__flat"]=""
+        df ["uniprot_accessions"]=""
+        df ["component_count"]=None
+    for col in arrays_to_serialize :
+        if col in df .columns and f'{col }__flat'in df .columns :
+            df =df .drop (columns =[col ])
+    return df
```

#### Горячий участок 17

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:42-232
- target: нет в ветке

```diff
--- assay:transform.py

+++ target:transform.py

@@ -1,78 +0,0 @@

-def validate_assay_parameters_truv (df :pd .DataFrame ,column :str ="assay_parameters",fail_fast :bool =True )->pd .DataFrame :
-    "\u0412\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u0442\u044c TRUV-\u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0434\u043b\u044f assay_parameters.\n\n    \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n    - value IS NOT NULL XOR text_value IS NOT NULL (\u043d\u0435 \u043e\u0431\u0430 \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u043d\u0435 NULL)\n    - standard_value IS NOT NULL XOR standard_text_value IS NOT NULL\n    - active \u2208 {0, 1, NULL}\n    - relation \u2208 {'=', '<', '\u2264', '>', '\u2265', '~', NULL} (\u0441 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u0435\u043c \u0434\u043b\u044f \u043d\u0435\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0445)\n\n    Parameters\n    ----------\n    df:\n        DataFrame \u0441 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 assay_parameters (JSON-\u0441\u0442\u0440\u043e\u043a\u0430 \u0441 \u043c\u0430\u0441\u0441\u0438\u0432\u043e\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432).\n    column:\n        \u0418\u043c\u044f \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 (\u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \"assay_parameters\").\n    fail_fast:\n        \u0415\u0441\u043b\u0438 True, \u0432\u044b\u0431\u0440\u0430\u0441\u044b\u0432\u0430\u0435\u0442 ValueError \u043f\u0440\u0438 \u043d\u0430\u0440\u0443\u0448\u0435\u043d\u0438\u0438 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432.\n        \u0415\u0441\u043b\u0438 False, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0435\u0442.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0439 DataFrame (\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u043d\u0435 \u0438\u0437\u043c\u0435\u043d\u044f\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435).\n\n    Raises\n    ------\n    ValueError:\n        \u0415\u0441\u043b\u0438 fail_fast=True \u0438 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043e \u043d\u0430\u0440\u0443\u0448\u0435\u043d\u0438\u0435 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432.\n\n    Notes\n    -----\n    - \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442\u0441\u044f \u043d\u0430 \u044d\u0442\u0430\u043f\u0435 transform \u0434\u043b\u044f fail-fast \u043f\u043e\u0434\u0445\u043e\u0434\u0430\n    - \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u044b relation: '=', '<', '\u2264', '>', '\u2265', '~'\n    - \u041d\u0435\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u044b \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="assay_transform")
-    if column not in df .columns :
-        log .debug ("truv_validation_skipped_missing_column",column =column )
-        return df
-    STANDARD_RELATIONS ={"=","<","\u2264",">","\u2265","~"}
-    errors :list [str ]=[]
-    warnings :list [str ]=[]
-    for idx ,row in df .iterrows ():
-        params_str =row .get (column )
-        if _is_null_like (params_str ):
-            continue
-        try :
-            if isinstance (params_str ,str ):
-                params_raw =json .loads (params_str )
-            else :
-                params_raw =params_str
-        except (json .JSONDecodeError ,TypeError )as exc :
-            errors .append (f'Row {idx }: Invalid JSON in {column }: {exc }')
-            continue
-        if not isinstance (params_raw ,list ):
-            errors .append (f'Row {idx }: {column } must be a JSON array, got {type (params_raw ).__name__ }')
-            continue
-        params_candidates =cast (list [object ],params_raw )
-        for param_idx ,param_raw in enumerate (params_candidates ):
-            if not isinstance (param_raw ,dict ):
-                errors .append (f'Row {idx }, param {param_idx }: Parameter must be a dict, got {type (param_raw ).__name__ }')
-                continue
-            param_dict :AssayParam =cast (AssayParam ,param_raw )
-            value :Any =param_dict .get ("value")
-            text_value :Any =param_dict .get ("text_value")
-            value_is_null =value is None or (isinstance (value ,float )and pd .isna (value ))or (isinstance (value ,str )and value .strip ()=="")
-            text_value_is_null =text_value is None or (isinstance (text_value ,float )and pd .isna (text_value ))or (isinstance (text_value ,str )and text_value .strip ()=="")
-            if not value_is_null and (not text_value_is_null ):
-                errors .append (f"Row {idx }, param {param_idx }: Both 'value' and 'text_value' are not NULL (value={value }, text_value={text_value }). TRUV invariant violation: value and text_value must be mutually exclusive.")
-            standard_value :Any =param_dict .get ("standard_value")
-            standard_text_value :Any =param_dict .get ("standard_text_value")
-            standard_value_is_null =standard_value is None or (isinstance (standard_value ,float )and pd .isna (standard_value ))or (isinstance (standard_value ,str )and standard_value .strip ()=="")
-            standard_text_value_is_null =standard_text_value is None or (isinstance (standard_text_value ,float )and pd .isna (standard_text_value ))or (isinstance (standard_text_value ,str )and standard_text_value .strip ()=="")
-            if not standard_value_is_null and (not standard_text_value_is_null ):
-                errors .append (f"Row {idx }, param {param_idx }: Both 'standard_value' and 'standard_text_value' are not NULL (standard_value={standard_value }, standard_text_value={standard_text_value }). TRUV invariant violation: standard_value and standard_text_value must be mutually exclusive.")
-            active :Any =param_dict .get ("active")
-            if active is not None :
-                if isinstance (active ,bool ):
-                    active_int =1 if active else 0
-                elif isinstance (active ,(int ,float )):
-                    active_int =int (active )
-                elif isinstance (active ,str ):
-                    try :
-                        active_int =int (active )
-                    except ValueError :
-                        errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' value: {active !r }. Must be 0, 1, or NULL.")
-                        continue
-                else :
-                    errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' type: {type (active ).__name__ }. Must be 0, 1, or NULL.")
-                    continue
-                if active_int not in {0 ,1 }:
-                    errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' value: {active_int }. Must be 0, 1, or NULL.")
-            relation :Any =param_dict .get ("relation")
-            if relation is not None and (not (isinstance (relation ,float )and pd .isna (relation ))):
-                relation_str =str (relation ).strip ()
-                if relation_str and relation_str not in STANDARD_RELATIONS :
-                    warnings .append (f"Row {idx }, param {param_idx }: Non-standard 'relation' value: {relation_str !r }. Standard operators: {", ".join (sorted (STANDARD_RELATIONS ))}.")
-    if warnings :
-        for warning in warnings :
-            log .warning ("truv_validation_warning",message =warning )
-    if errors :
-        error_msg =f'TRUV validation failed for {column }:\n'+"\n".join (errors )
-        if fail_fast :
-            log .error ("truv_validation_failed",error_count =len (errors ))
-            raise ValueError (error_msg )
-        else :
-            for error in errors :
-                log .warning ("truv_validation_error",message =error )
-    if not errors and (not warnings ):
-        log .debug ("truv_validation_passed",rows_checked =len (df ))
-    return df
```

### Модуль normalize.py

Определение                       | assay сигнатура                                                      | target сигнатура | Побочные эффекты                                                                                                   | Исключения           | Статус        
----------------------------------|----------------------------------------------------------------------|------------------|--------------------------------------------------------------------------------------------------------------------|----------------------|---------------
__module_block_0                  | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_1                  | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_10                 | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_11                 | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_12                 | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_13                 | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_2                  | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_3                  | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_4                  | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_5                  | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_6                  | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_7                  | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_8                  | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
__module_block_9                  | —                                                                    | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
_should_nullify_string_value      | value: Any                                                           | —                | assay: {'logging': [], 'io': []}
target: {}                                                                        | assay: []
target: [] | только в assay
enrich_with_assay_classifications | df_assay: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | —                | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}
target: {} | assay: []
target: [] | только в assay
enrich_with_assay_parameters      | df_assay: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | —                | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}
target: {} | assay: []
target: [] | только в assay

#### Горячий участок 1

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:1-1
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-"Enrichment functions for Assay pipeline."
```

#### Горячий участок 2

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:3-3
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from __future__ import annotations
```

#### Горячий участок 3

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:19-22
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-__all__ =["enrich_with_assay_classifications","enrich_with_assay_parameters"]
```

#### Горячий участок 4

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:25-25
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-_ensure_columns =ensure_columns
```

#### Горячий участок 5

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:28-31
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-_CLASSIFICATION_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_classifications","string"),("assay_class_id","string"))
```

#### Горячий участок 6

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:33-33
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-_PARAMETERS_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_parameters","string"),)
```

#### Горячий участок 7

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:11-11
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .clients .chembl import ChemblClient
```

#### Горячий участок 8

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:12-12
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .frame import ensure_columns
```

#### Горячий участок 9

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:13-13
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .logger import UnifiedLogger
```

#### Горячий участок 10

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:14-17
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .schemas .assay import ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA ,ASSAY_PARAMETERS_ENRICHMENT_SCHEMA
```

#### Горячий участок 11

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:6-6
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from collections .abc import Mapping
```

#### Горячий участок 12

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:7-7
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from typing import Any
```

#### Горячий участок 13

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:5-5
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-import json
```

#### Горячий участок 14

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:9-9
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-import pandas as pd
```

#### Горячий участок 15

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:36-44
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1,9 +0,0 @@

-def _should_nullify_string_value (value :Any )->bool :
-    "\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043d\u0443\u0436\u043d\u043e \u043b\u0438 \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 NA \u0432 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0435."
-    if value is None :
-        return False
-    if value is pd .NA :
-        return False
-    if isinstance (value ,float )and pd .isna (value ):
-        return False
-    return not isinstance (value ,str )
```

#### Горячий участок 16

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:47-219
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1,86 +0,0 @@

-def enrich_with_assay_classifications (df_assay :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame assay \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 ASSAY_CLASS_MAP \u0438 ASSAY_CLASSIFICATION.\n\n    Parameters\n    ----------\n    df_assay:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.assay.enrich.classifications.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438/\u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438:\n        - assay_classifications (string, nullable) - \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u043c\u0430\u0441\u0441\u0438\u0432 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439\n        - assay_class_id (string, nullable) - \u0441\u043f\u0438\u0441\u043e\u043a assay_class_id \u0447\u0435\u0440\u0435\u0437 \";\"\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="assay_enrichment")
-    df_assay =_ensure_columns (df_assay ,_CLASSIFICATION_COLUMNS )
-    if "assay_classifications"in df_assay .columns :
-        invalid_mask =df_assay ["assay_classifications"].map (_should_nullify_string_value )
-        if bool (invalid_mask .any ()):
-            log .warning ("assay_classifications_reset_non_string",rows =int (invalid_mask .sum ()))
-            df_assay .loc [invalid_mask ,"assay_classifications"]=pd .NA
-    if df_assay .empty :
-        log .debug ("enrichment_skipped_empty_dataframe")
-        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    required_cols =["assay_chembl_id"]
-    missing_cols =[col for col in required_cols if col not in df_assay .columns ]
-    if missing_cols :
-        log .warning ("enrichment_skipped_missing_columns",missing_columns =missing_cols )
-        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    assay_ids :list [str ]=[]
-    for _ ,row in df_assay .iterrows ():
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        if assay_id_str :
-            assay_ids .append (assay_id_str )
-    if not assay_ids :
-        log .debug ("enrichment_skipped_no_valid_ids")
-        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    class_map_fields =cfg .get ("class_map_fields",["assay_chembl_id","assay_class_id"])
-    classification_fields =cfg .get ("classification_fields",["assay_class_id","l1","l2","l3","pref_name"])
-    page_limit =cfg .get ("page_limit",1000 )
-    log .info ("enrichment_fetching_assay_class_map",ids_count =len (set (assay_ids )))
-    class_map_dict =client .fetch_assay_class_map_by_assay_ids (assay_ids ,list (class_map_fields ),page_limit )
-    all_class_ids :set [str ]=set ()
-    for mappings in class_map_dict .values ():
-        for mapping in mappings :
-            class_id =mapping .get ("assay_class_id")
-            if class_id and (not (isinstance (class_id ,float )and pd .isna (class_id ))):
-                all_class_ids .add (str (class_id ).strip ())
-    classification_dict :dict [str ,dict [str ,Any ]]={}
-    if all_class_ids :
-        log .info ("enrichment_fetching_assay_classifications",class_ids_count =len (all_class_ids ))
-        classification_dict =client .fetch_assay_classifications_by_class_ids (list (all_class_ids ),list (classification_fields ),page_limit )
-    df_assay =df_assay .copy ()
-    if "assay_classifications"not in df_assay .columns :
-        df_assay ["assay_classifications"]=pd .NA
-    if "assay_class_id"not in df_assay .columns :
-        df_assay ["assay_class_id"]=pd .NA
-    for idx ,row in df_assay .iterrows ():
-        row_key :Any =idx
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        mappings =class_map_dict .get (assay_id_str ,[])
-        if not mappings :
-            df_assay .at [row_key ,"assay_classifications"]=pd .NA
-            df_assay .at [row_key ,"assay_class_id"]=pd .NA
-            continue
-        classifications :list [dict [str ,Any ]]=[]
-        class_ids :list [str ]=[]
-        for mapping in mappings :
-            class_id =mapping .get ("assay_class_id")
-            if not class_id or (isinstance (class_id ,float )and pd .isna (class_id )):
-                continue
-            class_id_str =str (class_id ).strip ()
-            if not class_id_str :
-                continue
-            classification_data =classification_dict .get (class_id_str )
-            if classification_data :
-                class_record :dict [str ,Any ]={"assay_class_id":class_id_str }
-                for field in classification_fields :
-                    if field !="assay_class_id":
-                        class_record [field ]=classification_data .get (field )
-                classifications .append (class_record )
-            else :
-                class_record ={"assay_class_id":class_id_str }
-                classifications .append (class_record )
-            class_ids .append (class_id_str )
-        if classifications :
-            serialized =json .dumps (classifications ,ensure_ascii =False )
-            class_id_joined =";".join (class_ids )
-            df_assay .at [row_key ,"assay_classifications"]=serialized
-            df_assay .at [row_key ,"assay_class_id"]=class_id_joined
-    log .info ("enrichment_classifications_complete",assays_with_classifications =len (df_assay [df_assay ["assay_classifications"].notna ()]))
-    return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
```

#### Горячий участок 17

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:222-378
- target: нет в ветке

```diff
--- assay:normalize.py

+++ target:normalize.py

@@ -1,57 +0,0 @@

-def enrich_with_assay_parameters (df_assay :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame assay \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 ASSAY_PARAMETERS.\n\n    \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442 \u043f\u043e\u043b\u043d\u044b\u0439 TRUV-\u043d\u0430\u0431\u043e\u0440 \u043f\u043e\u043b\u0435\u0439 (TYPE, RELATION, VALUE, UNITS, TEXT_VALUE),\n    \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (standard_*), \u0441\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (active) \u0438 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435\n    \u043f\u043e\u043b\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 (type_normalized, type_fixed).\n\n    Parameters\n    ----------\n    df_assay:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.assay.enrich.parameters.\n        \u0414\u043e\u043b\u0436\u043d\u0430 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c fields (\u0441\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439 \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f), page_limit \u0438 active_only.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0439/\u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439:\n        - assay_parameters (string, nullable) - \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 JSON-\u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\n          \u0441 \u043f\u043e\u043b\u044f\u043c\u0438: type, relation, value, units, text_value, standard_*,\n          active, type_normalized, type_fixed (\u0435\u0441\u043b\u0438 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 \u0434\u0430\u043c\u043f\u0435)\n\n    Notes\n    -----\n    - \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u0435\u0441\u0442\u044c, \u043d\u0435 \u043a\u043e\u043f\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0432 standard_* \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\n    - \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (type_normalized, type_fixed) \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438\n      \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 \u0434\u0430\u043c\u043f\u0435 ChEMBL\n    - \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0444\u0438\u043b\u044c\u0442\u0440\u0443\u044e\u0442\u0441\u044f \u043f\u043e active=1, \u0435\u0441\u043b\u0438 active_only=True \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="assay_enrichment")
-    df_assay =_ensure_columns (df_assay ,_PARAMETERS_COLUMNS )
-    if "assay_parameters"in df_assay .columns :
-        invalid_mask =df_assay ["assay_parameters"].map (_should_nullify_string_value )
-        if bool (invalid_mask .any ()):
-            log .warning ("assay_parameters_reset_non_string",rows =int (invalid_mask .sum ()))
-            df_assay .loc [invalid_mask ,"assay_parameters"]=pd .NA
-        df_assay ["assay_parameters"]=df_assay ["assay_parameters"].astype ("string")
-    if df_assay .empty :
-        log .debug ("enrichment_skipped_empty_dataframe")
-        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    required_cols =["assay_chembl_id"]
-    missing_cols =[col for col in required_cols if col not in df_assay .columns ]
-    if missing_cols :
-        log .warning ("enrichment_skipped_missing_columns",missing_columns =missing_cols )
-        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    assay_ids :list [str ]=[]
-    for _ ,row in df_assay .iterrows ():
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        if assay_id_str :
-            assay_ids .append (assay_id_str )
-    if not assay_ids :
-        log .debug ("enrichment_skipped_no_valid_ids")
-        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    fields =cfg .get ("fields",["assay_chembl_id","type","relation","value","units","text_value","standard_type","standard_relation","standard_value","standard_units","standard_text_value","active"])
-    page_limit =cfg .get ("page_limit",1000 )
-    active_only =cfg .get ("active_only",True )
-    log .info ("enrichment_fetching_assay_parameters",ids_count =len (set (assay_ids )))
-    parameters_dict =client .fetch_assay_parameters_by_assay_ids (assay_ids ,list (fields ),page_limit ,active_only )
-    df_assay =df_assay .copy ()
-    if "assay_parameters"not in df_assay .columns :
-        df_assay ["assay_parameters"]=pd .NA
-    for row_position ,(_ ,row )in enumerate (df_assay .iterrows ()):
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        parameters =parameters_dict .get (assay_id_str ,[])
-        if not parameters :
-            continue
-        params_list :list [dict [str ,Any ]]=[]
-        for param in parameters :
-            param_record :dict [str ,Any ]={}
-            for field in fields :
-                if field !="assay_chembl_id":
-                    param_record [field ]=param .get (field )
-            params_list .append (param_record )
-        if params_list :
-            index_label =df_assay .index [row_position ]
-            df_assay .loc [index_label ,"assay_parameters"]=json .dumps (params_list ,ensure_ascii =False )
-    log .info ("enrichment_parameters_complete",assays_with_parameters =len (df_assay [df_assay ["assay_parameters"].notna ()]))
-    return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
```

---

## Пара: assay ↔ testitem

- AST hash: 4e65bc8b94391cb8c868cf6ad530c2c6 ↔ 729263c98934cfcb08473bcacfb20e9e

- Jaccard по токенам: 0.259

### Модуль run.py

Определение                                       | assay сигнатура                                                   | testitem сигнатура                                                           | Побочные эффекты                                                                                                                                                    | Исключения                             | Статус           
--------------------------------------------------|-------------------------------------------------------------------|------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------|------------------
ChemblAssayPipeline                               | —                                                                 | —                                                                            | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {}                                                            | assay: ['TypeError(msg)']
testitem: [] | только в assay   
ChemblAssayPipeline.__init__                      | self, config: PipelineConfig, run_id: str                         | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline._add_row_metadata             | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                            | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline._build_assay_descriptor       | self: SelfChemblAssayPipeline                                     | —                                                                            | assay: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {}                                                                                 | assay: ['TypeError(msg)']
testitem: [] | только в assay   
ChemblAssayPipeline._check_missing_columns        | self, df: pd.DataFrame, log: Any, select_fields: list[str] | None | —                                                                            | assay: {'logging': ['log.debug', 'log.warning'], 'io': []}
testitem: {}                                                                                             | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline._enrich_with_related_data     | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {}                                                                                 | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline._harmonize_identifier_columns | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                            | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline._normalize_data_types         | self, df: pd.DataFrame, schema: Any, log: Any                     | —                                                                            | assay: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                            | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline._normalize_identifiers        | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                            | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline._normalize_nested_structures  | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                            | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline._normalize_string_fields      | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: {'logging': ['log.debug', 'log.warning'], 'io': []}
testitem: {}                                                                                             | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline._serialize_array_fields       | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                            | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline.chembl_release                | self                                                              | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline.extract                       | self, *args, **kwargs                                             | —                                                                            | assay: {'logging': ['UnifiedLogger.get'], 'io': []}
testitem: {}                                                                                                    | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline.extract_all                   | self                                                              | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline.extract_by_ids                | self, ids: Sequence[str]                                          | —                                                                            | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {}                                                            | assay: []
testitem: []                 | только в assay   
ChemblAssayPipeline.transform                     | self, df: pd.DataFrame                                            | —                                                                            | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}
testitem: {}                                                                           | assay: []
testitem: []                 | только в assay   
TestItemChemblPipeline                            | —                                                                 | —                                                                            | assay: {}
testitem: {'logging': ['UnifiedLogger.get', 'bound_log.info', 'bound_log.warning', 'log.debug', 'log.info', 'log.warning'], 'io': ['status_payload.get']} | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline.__init__                   | —                                                                 | self, config: PipelineConfig, run_id: str                                    | assay: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline._build_testitem_descriptor | —                                                                 | self: SelfTestitemChemblPipeline                                             | assay: {}
testitem: {'logging': ['log.debug', 'log.info'], 'io': []}                                                                                                | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline._check_empty_columns       | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: {}
testitem: {'logging': ['log.warning'], 'io': []}                                                                                                          | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline._deduplicate_molecules     | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: {}
testitem: {'logging': ['log.info'], 'io': []}                                                                                                             | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline._fetch_chembl_release      | —                                                                 | self, client: UnifiedAPIClient | ChemblClient | Any, log: BoundLogger | None | assay: {}
testitem: {'logging': ['UnifiedLogger.get', 'bound_log.info', 'bound_log.warning'], 'io': ['status_payload.get']}                                         | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline._flatten_nested_structures | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline._normalize_identifiers     | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline._normalize_numeric_fields  | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline._normalize_string_fields   | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline._remove_extra_columns      | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline._schema_column_specs       | —                                                                 | self                                                                         | assay: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline.api_version                | —                                                                 | self                                                                         | assay: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline.augment_metadata           | —                                                                 | self, metadata: Mapping[str, object], df: pd.DataFrame                       | assay: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline.chembl_db_version          | —                                                                 | self                                                                         | assay: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline.extract                    | —                                                                 | self, *args, **kwargs                                                        | assay: {}
testitem: {'logging': ['UnifiedLogger.get'], 'io': []}                                                                                                    | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline.extract_all                | —                                                                 | self                                                                         | assay: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline.extract_by_ids             | —                                                                 | self, ids: Sequence[str]                                                     | assay: {}
testitem: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                                                                           | assay: []
testitem: []                 | только в testitem
TestItemChemblPipeline.transform                  | —                                                                 | self, df: pd.DataFrame                                                       | assay: {}
testitem: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                                                                           | assay: []
testitem: []                 | только в testitem
__module_block_0                                  | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_1                                  | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | совпадает        
__module_block_10                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_11                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_12                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_13                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | совпадает        
__module_block_14                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | совпадает        
__module_block_15                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | совпадает        
__module_block_16                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_17                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_18                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_19                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | assay: []
testitem: []                 | только в assay   
__module_block_2                                  | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_20                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | assay: []
testitem: []                 | только в assay   
__module_block_21                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | assay: []
testitem: []                 | только в assay   
__module_block_22                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | assay: []
testitem: []                 | только в assay   
__module_block_26                                 | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | assay: []
testitem: []                 | только в assay   
__module_block_3                                  | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_4                                  | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_5                                  | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_6                                  | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_7                                  | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_8                                  | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
__module_block_9                                  | —                                                                 | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | assay: []
testitem: []                 | отличается       
_extract_bao_ids_from_classifications             | node: Any                                                         | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | assay: []
testitem: []                 | только в assay   
_iter_classification_mappings                     | node: Any                                                         | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | assay: []
testitem: []                 | только в assay   
_normalize_bao_identifier                         | raw_value: Any                                                    | —                                                                            | assay: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | assay: []
testitem: []                 | только в assay   

_Показаны первые 20 горячих участков из 59._

#### Горячий участок 1

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:126-952
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,324 +0,0 @@

-class ChemblAssayPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting assay records from the ChEMBL API."
-    actor ="assay_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all assay records from ChEMBL using pagination."
-        descriptor =self ._build_assay_descriptor ()
-        return self .run_extract_all (descriptor )
-    def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-        "Return the descriptor powering the shared extraction template."
-        def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-            if isinstance (pipeline ,ChemblAssayPipeline ):
-                return pipeline
-            msg ="ChemblAssayPipeline instance required"
-            raise TypeError (msg )
-        def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-            chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-            assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-            assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-            assay_pipeline ._chembl_release =assay_client .chembl_release
-            log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-            raw_source =assay_pipeline ._resolve_source_config ("chembl")
-            select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-            log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-            context =ChemblExtractionContext (source_config ,assay_client )
-            context .chembl_client =chembl_client
-            context .select_fields =tuple (select_fields )if select_fields else None
-            context .chembl_release =assay_pipeline ._chembl_release
-            context .extra_filters ={"max_url_length":source_config .max_url_length }
-            return context
-        def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-        def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            if df .empty :
-                return df
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in df .columns or df [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            select_fields =context .select_fields
-            if select_fields :
-                expected_fields =set (select_fields )
-                actual_fields =set (df .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-            return df
-        def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-            return pd .DataFrame ()
-        def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-        return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        self ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        if self .config .cli .dry_run :
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-            return pd .DataFrame ()
-        limit =self .config .cli .limit
-        resolved_select_fields =self ._resolve_select_fields (source_raw )
-        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-        log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-        def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield dict (item )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            if dataframe .empty :
-                return dataframe
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            if context .select_fields :
-                expected_fields =set (context .select_fields )
-                actual_fields =set (dataframe .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-            return dataframe
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw assay data by normalizing identifiers, types, and nested structures."
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._enrich_with_related_data (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._serialize_array_fields (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,AssaySchema ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Serialize array-of-object fields to header+rows format."
-        df =df .copy ()
-        arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-        if arrays_to_serialize :
-            df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-            for column in arrays_to_serialize :
-                if column in df_result .columns :
-                    column_as_string :Series =df_result [column ].astype ("string")
-                    filled_column :Series =column_as_string .copy ()
-                    filled_column [column_as_string .isna ()]=""
-                    empty_mask :Series =filled_column .eq ("")
-                    if bool (empty_mask .any ()):
-                        df_result .loc [empty_mask ,column ]=pd .NA
-            df =df_result
-            log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-        return df
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-            df ["target_chembl_id"]=df ["target_id"]
-            actions .append ("target_id->target_chembl_id")
-        alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize ChEMBL identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-        working_df =df .copy ()
-        string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-        rules ={column :StringRule ()for column in string_fields }
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if "curation_level"not in normalized_df .columns :
-            normalized_df ["curation_level"]=pd .NA
-            log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-        df =df .copy ()
-        if "assay_parameters"in df .columns :
-            log .debug ("validating_assay_parameters_truv")
-            df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-        if "assay_classifications"in df .columns :
-            if "assay_class_id"not in df .columns :
-                df ["assay_class_id"]=pd .NA
-            updated_rows =0
-            classifications_series =df ["assay_classifications"]
-            for row_index ,value in classifications_series .items ():
-                if value is None or value is pd .NA :
-                    continue
-                if isinstance (value ,float )and pd .isna (value ):
-                    continue
-                if isinstance (value ,str ):
-                    continue
-                extracted_ids =_extract_bao_ids_from_classifications (value )
-                if not extracted_ids :
-                    continue
-                joined_ids =";".join (extracted_ids )
-                row_label =cast (Any ,row_index )
-                current_value =df .at [row_label ,"assay_class_id"]
-                if pd .isna (current_value )or current_value !=joined_ids :
-                    df .at [row_label ,"assay_class_id"]=joined_ids
-                    updated_rows +=1
-            if updated_rows >0 :
-                log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-        return df
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_added",value ="assay")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_filled",value ="assay")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-        "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-        df =super ()._normalize_data_types (df ,schema ,log )
-        if "row_index"in df .columns and df ["row_index"].isna ().any ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-        df =df .copy ()
-        optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-        expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-        select_fields_set :set [str ]=set ()
-        if select_fields is not None :
-            select_fields_set =set (select_fields )
-        missing_in_response :list [str ]=[]
-        missing_in_select_fields :list [str ]=[]
-        for column in expected_api_fields :
-            if column not in df .columns :
-                missing_in_response .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-                else :
-                    log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-        missing_columns :list [str ]=[]
-        for column ,version in optional_columns .items ():
-            if column not in df .columns :
-                df [column ]=pd .NA
-                missing_columns .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-                else :
-                    log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-        if missing_in_response or missing_columns :
-            log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-        return df
-    def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-        if df .empty :
-            return df
-        try :
-            source_raw =self ._resolve_source_config ("chembl")
-        except KeyError as exc :
-            log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-            return df
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        chembl_config =getattr (self .config ,"chembl",None )
-        if chembl_config is None :
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-            return df
-        if not isinstance (chembl_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-            return df
-        assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-        if not isinstance (assay_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-            return df
-        enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-        if not isinstance (enrich_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-            return df
-        classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-        if classifications_cfg is not None :
-            log .info ("enrichment_classifications_started")
-            df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-            df =df_with_classifications
-            log .info ("enrichment_classifications_completed")
-            if "assay_class_id"in df .columns :
-                filled_count =int (df ["assay_class_id"].notna ().sum ())
-                total_count =len (df )
-                if filled_count ==0 :
-                    log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-                else :
-                    log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-            else :
-                log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-        else :
-            log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-        parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-        if parameters_cfg is not None :
-            log .info ("enrichment_parameters_started")
-            df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-            df =df_with_parameters
-            log .info ("enrichment_parameters_completed")
-        return df
```

#### Горячий участок 2

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:131-133
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,3 +0,0 @@

-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
```

#### Горячий участок 3

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:681-704
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,18 +0,0 @@

-def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_added",value ="assay")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_filled",value ="assay")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Горячий участок 4

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:167-323
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,51 +0,0 @@

-def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-    "Return the descriptor powering the shared extraction template."
-    def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-        if isinstance (pipeline ,ChemblAssayPipeline ):
-            return pipeline
-        msg ="ChemblAssayPipeline instance required"
-        raise TypeError (msg )
-    def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        assay_pipeline ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        raw_source =assay_pipeline ._resolve_source_config ("chembl")
-        select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-        log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-        context =ChemblExtractionContext (source_config ,assay_client )
-        context .chembl_client =chembl_client
-        context .select_fields =tuple (select_fields )if select_fields else None
-        context .chembl_release =assay_pipeline ._chembl_release
-        context .extra_filters ={"max_url_length":source_config .max_url_length }
-        return context
-    def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-    def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        if df .empty :
-            return df
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in df .columns or df [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        select_fields =context .select_fields
-        if select_fields :
-            expected_fields =set (select_fields )
-            actual_fields =set (df .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-        return df
-    def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-        return pd .DataFrame ()
-    def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-    return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
```

#### Горячий участок 5

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:720-835
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,31 +0,0 @@

-def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-    "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-    df =df .copy ()
-    optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-    expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-    select_fields_set :set [str ]=set ()
-    if select_fields is not None :
-        select_fields_set =set (select_fields )
-    missing_in_response :list [str ]=[]
-    missing_in_select_fields :list [str ]=[]
-    for column in expected_api_fields :
-        if column not in df .columns :
-            missing_in_response .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-            else :
-                log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-    missing_columns :list [str ]=[]
-    for column ,version in optional_columns .items ():
-        if column not in df .columns :
-            df [column ]=pd .NA
-            missing_columns .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-            else :
-                log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-    if missing_in_response or missing_columns :
-        log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-    return df
```

#### Горячий участок 6

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:837-952
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,53 +0,0 @@

-def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-    if df .empty :
-        return df
-    try :
-        source_raw =self ._resolve_source_config ("chembl")
-    except KeyError as exc :
-        log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-        return df
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    chembl_config =getattr (self .config ,"chembl",None )
-    if chembl_config is None :
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-        return df
-    if not isinstance (chembl_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-        return df
-    assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-    if not isinstance (assay_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-        return df
-    enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-    if not isinstance (enrich_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-        return df
-    classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-    if classifications_cfg is not None :
-        log .info ("enrichment_classifications_started")
-        df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-        df =df_with_classifications
-        log .info ("enrichment_classifications_completed")
-        if "assay_class_id"in df .columns :
-            filled_count =int (df ["assay_class_id"].notna ().sum ())
-            total_count =len (df )
-            if filled_count ==0 :
-                log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-            else :
-                log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-        else :
-            log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-    else :
-        log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-    parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-    if parameters_cfg is not None :
-        log .info ("enrichment_parameters_started")
-        df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-        df =df_with_parameters
-        log .info ("enrichment_parameters_completed")
-    return df
```

#### Горячий участок 7

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:526-548
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,17 +0,0 @@

-def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-    df =df .copy ()
-    actions :list [str ]=[]
-    if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-        df ["assay_chembl_id"]=df ["assay_id"]
-        actions .append ("assay_id->assay_chembl_id")
-    if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-        df ["target_chembl_id"]=df ["target_id"]
-        actions .append ("target_id->target_chembl_id")
-    alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-    if alias_columns :
-        df =df .drop (columns =alias_columns )
-        actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-    if actions :
-        log .debug ("identifier_harmonization",actions =actions )
-    return df
```

#### Горячий участок 8

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:706-718
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,7 +0,0 @@

-def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-    "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-    df =super ()._normalize_data_types (df ,schema ,log )
-    if "row_index"in df .columns and df ["row_index"].isna ().any ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Горячий участок 9

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:550-577
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,7 +0,0 @@

-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize ChEMBL identifiers with regex validation."
-    rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-    normalized_df ,stats =normalize_identifier_columns (df ,rules )
-    if stats .has_changes :
-        log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-    return normalized_df
```

#### Горячий участок 10

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:623-679
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,30 +0,0 @@

-def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-    df =df .copy ()
-    if "assay_parameters"in df .columns :
-        log .debug ("validating_assay_parameters_truv")
-        df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-    if "assay_classifications"in df .columns :
-        if "assay_class_id"not in df .columns :
-            df ["assay_class_id"]=pd .NA
-        updated_rows =0
-        classifications_series =df ["assay_classifications"]
-        for row_index ,value in classifications_series .items ():
-            if value is None or value is pd .NA :
-                continue
-            if isinstance (value ,float )and pd .isna (value ):
-                continue
-            if isinstance (value ,str ):
-                continue
-            extracted_ids =_extract_bao_ids_from_classifications (value )
-            if not extracted_ids :
-                continue
-            joined_ids =";".join (extracted_ids )
-            row_label =cast (Any ,row_index )
-            current_value =df .at [row_label ,"assay_class_id"]
-            if pd .isna (current_value )or current_value !=joined_ids :
-                df .at [row_label ,"assay_class_id"]=joined_ids
-                updated_rows +=1
-        if updated_rows >0 :
-            log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-    return df
```

#### Горячий участок 11

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:579-621
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,12 +0,0 @@

-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-    working_df =df .copy ()
-    string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-    rules ={column :StringRule ()for column in string_fields }
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if "curation_level"not in normalized_df .columns :
-        normalized_df ["curation_level"]=pd .NA
-        log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    return normalized_df
```

#### Горячий участок 12

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:504-524
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,17 +0,0 @@

-def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Serialize array-of-object fields to header+rows format."
-    df =df .copy ()
-    arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-    if arrays_to_serialize :
-        df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-        for column in arrays_to_serialize :
-            if column in df_result .columns :
-                column_as_string :Series =df_result [column ].astype ("string")
-                filled_column :Series =column_as_string .copy ()
-                filled_column [column_as_string .isna ()]=""
-                empty_mask :Series =filled_column .eq ("")
-                if bool (empty_mask .any ()):
-                    df_result .loc [empty_mask ,column ]=pd .NA
-        df =df_result
-        log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-    return df
```

#### Горячий участок 13

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:136-139
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,4 +0,0 @@

-@property
-def chembl_release (self )->str |None :
-    "Return the cached ChEMBL release captured during extraction."
-    return self ._chembl_release
```

#### Горячий участок 14

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:145-159
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,4 +0,0 @@

-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
```

#### Горячий участок 15

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:161-165
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,4 +0,0 @@

-def extract_all (self )->pd .DataFrame :
-    "Extract all assay records from ChEMBL using pagination."
-    descriptor =self ._build_assay_descriptor ()
-    return self .run_extract_all (descriptor )
```

#### Горячий участок 16

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:325-469
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,42 +0,0 @@

-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-    assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-    self ._chembl_release =assay_client .chembl_release
-    log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-    if self .config .cli .dry_run :
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-        return pd .DataFrame ()
-    limit =self .config .cli .limit
-    resolved_select_fields =self ._resolve_select_fields (source_raw )
-    merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-    log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-    def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield dict (item )
-    def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-        if dataframe .empty :
-            return dataframe
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        if context .select_fields :
-            expected_fields =set (context .select_fields )
-            actual_fields =set (dataframe .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-        return dataframe
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-    return dataframe
```

#### Горячий участок 17

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\run.py:471-498
- testitem: нет в ветке

```diff
--- assay:run.py

+++ testitem:run.py

@@ -1,21 +0,0 @@

-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw assay data by normalizing identifiers, types, and nested structures."
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-    df =df .copy ()
-    df =self ._harmonize_identifier_columns (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._enrich_with_related_data (df ,log )
-    df =self ._normalize_nested_structures (df ,log )
-    df =self ._serialize_array_fields (df ,log )
-    df =self ._add_row_metadata (df ,log )
-    df =self ._normalize_data_types (df ,AssaySchema ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

#### Горячий участок 18

- assay: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\run.py:52-807

```diff
--- assay:run.py

+++ testitem:run.py

@@ -0,0 +1,296 @@

+class TestItemChemblPipeline (ChemblPipelineBase ):
+    "ETL pipeline extracting molecule records from the ChEMBL API."
+    actor ="testitem_chembl"
+    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+        super ().__init__ (config ,run_id )
+        self ._chembl_db_version :str |None =None
+        self ._api_version :str |None =None
+    @property
+    def chembl_db_version (self )->str |None :
+        "Return the cached ChEMBL DB version captured during extraction."
+        return self ._chembl_db_version
+    @property
+    def api_version (self )->str |None :
+        "Return the cached ChEMBL API version captured during extraction."
+        return self ._api_version
+    def _fetch_chembl_release (self ,client :UnifiedAPIClient |ChemblClient |Any ,log :BoundLogger |None =None )->str |None :
+        "Capture ChEMBL release and API version from status endpoint."
+        if log is None :
+            bound_log :BoundLogger =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
+        else :
+            bound_log =log
+        request_timestamp =datetime .now (timezone .utc )
+        release_value :str |None =None
+        api_version :str |None =None
+        try :
+            status_payload :dict [str ,Any ]={}
+            handshake_candidate =getattr (client ,"handshake",None )
+            if callable (handshake_candidate ):
+                status_payload =self ._coerce_mapping (handshake_candidate ("/status"))
+            else :
+                get_candidate =getattr (client ,"get",None )
+                if callable (get_candidate ):
+                    response =get_candidate ("/status.json")
+                    json_candidate =getattr (response ,"json",None )
+                    if callable (json_candidate ):
+                        status_payload =self ._coerce_mapping (json_candidate ())
+            if status_payload :
+                release_value =self ._extract_chembl_release (status_payload )
+                api_candidate =status_payload .get ("api_version")
+                if isinstance (api_candidate ,str )and api_candidate .strip ():
+                    api_version =api_candidate
+                bound_log .info ("chembl_testitem.status",chembl_db_version =release_value ,api_version =api_version )
+        except Exception as exc :
+            bound_log .warning ("chembl_testitem.status_failed",error =str (exc ))
+        finally :
+            self ._chembl_db_version =release_value
+            self ._api_version =api_version
+            self .record_extract_metadata (chembl_release =release_value ,requested_at_utc =request_timestamp )
+        return release_value
+    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
+        "Fetch molecule payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        return self ._dispatch_extract_mode (log ,event_name ="chembl_testitem.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="molecule_chembl_id")
+    def extract_all (self )->pd .DataFrame :
+        "Extract all molecule records from ChEMBL using pagination."
+        descriptor =self ._build_testitem_descriptor ()
+        return self .run_extract_all (descriptor )
+    def _build_testitem_descriptor (self :SelfTestitemChemblPipeline )->ChemblExtractionDescriptor [SelfTestitemChemblPipeline ]:
+        "Return the descriptor powering testitem extraction."
+        def build_context (pipeline :SelfTestitemChemblPipeline ,source_config :TestItemSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+            base_url =pipeline ._resolve_base_url (source_config .parameters )
+            http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+            chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
+            pipeline ._fetch_chembl_release (chembl_client ,log )
+            select_fields =source_config .parameters .select_fields
+            log .debug ("chembl_testitem.select_fields",fields =select_fields )
+            testitem_client =ChemblTestitemClient (chembl_client ,batch_size =min (source_config .page_size ,25 ))
+            return ChemblExtractionContext (source_config =source_config ,iterator =testitem_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,page_size =source_config .page_size ,chembl_release =pipeline ._chembl_db_version ,metadata ={"api_version":pipeline ._api_version })
+        def empty_frame (_ :SelfTestitemChemblPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
+            return pd .DataFrame ({"molecule_chembl_id":pd .Series (dtype ="string")})
+        def dry_run_handler (pipeline :SelfTestitemChemblPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =pipeline ._chembl_db_version ,api_version =pipeline ._api_version )
+            return pd .DataFrame ()
+        def summary_extra (pipeline :SelfTestitemChemblPipeline ,_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+            return {"chembl_db_version":pipeline ._chembl_db_version ,"api_version":pipeline ._api_version ,"limit":pipeline .config .cli .limit }
+        return ChemblExtractionDescriptor [SelfTestitemChemblPipeline ](name ="chembl_testitem",source_name ="chembl",source_config_factory =TestItemSourceConfig .from_source_config ,build_context =build_context ,id_column ="molecule_chembl_id",summary_event ="chembl_testitem.extract_summary",must_have_fields =tuple (MUST_HAVE_FIELDS ),default_select_fields =MUST_HAVE_FIELDS ,sort_by =("molecule_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra ,hard_page_size_cap =None )
+    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
+        "Extract molecule records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of molecule_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted molecule records.\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        stage_start =time .perf_counter ()
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =TestItemSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
+        self ._fetch_chembl_release (chembl_client ,log )
+        if self .config .cli .dry_run :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =self ._chembl_db_version ,api_version =self ._api_version )
+            return pd .DataFrame ()
+        page_size =min (source_config .page_size ,25 )
+        limit =self .config .cli .limit
+        resolved_select_fields =source_config .parameters .select_fields
+        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
+        log .debug ("chembl_testitem.select_fields",fields =merged_select_fields )
+        testitem_client =ChemblTestitemClient (chembl_client ,batch_size =page_size )
+        def fetch_testitems (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
+            iterator =testitem_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
+            for item in iterator :
+                yield dict (item )
+        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="molecule_chembl_id",fetcher =fetch_testitems ,select_fields =merged_select_fields or None ,batch_size =page_size ,chunk_size =min (page_size ,25 ),max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None },chembl_release =self ._chembl_release )
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_testitem.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_db_version =self ._chembl_db_version ,api_version =self ._api_version ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
+        return dataframe
+    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
+        "Transform raw molecule data by normalizing fields and extracting nested properties."
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
+        df =df .copy ()
+        if df .empty :
+            log .debug ("transform_empty_dataframe")
+            return df
+        log .info ("transform_started",rows =len (df ))
+        df =transform_testitem (df ,self .config )
+        df =self ._normalize_identifiers (df ,log )
+        df =self ._normalize_string_fields (df ,log )
+        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
+        df =self ._normalize_numeric_fields (df ,log )
+        self ._check_empty_columns (df ,log )
+        df =self ._remove_extra_columns (df ,log )
+        df ["_chembl_db_version"]=self ._chembl_db_version or ""
+        df ["_api_version"]=self ._api_version or ""
+        df =self ._deduplicate_molecules (df ,log )
+        if "molecule_chembl_id"in df .columns :
+            df =df .sort_values ("molecule_chembl_id").reset_index (drop =True )
+        df =self ._order_schema_columns (df ,COLUMN_ORDER )
+        log .info ("transform_completed",rows =len (df ))
+        return df
+    def augment_metadata (self ,metadata :Mapping [str ,object ],df :pd .DataFrame )->Mapping [str ,object ]:
+        "Enrich metadata with ChEMBL versions."
+        enriched =dict (super ().augment_metadata (metadata ,df ))
+        if self ._chembl_db_version :
+            enriched ["chembl_db_version"]=self ._chembl_db_version
+        if self ._api_version :
+            enriched ["api_version"]=self ._api_version
+        return enriched
+    def _flatten_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Flatten nested molecule_structures and molecule_properties into flat columns."
+        if df .empty :
+            return df
+        if "molecule_structures"in df .columns :
+            structures_df =pd .json_normalize (df ["molecule_structures"].tolist ())
+            if "canonical_smiles"in structures_df .columns :
+                df ["canonical_smiles"]=structures_df ["canonical_smiles"]
+            if "standard_inchi_key"in structures_df .columns :
+                df ["standard_inchi_key"]=structures_df ["standard_inchi_key"]
+        if "molecule_properties"in df .columns :
+            properties_df =pd .json_normalize (df ["molecule_properties"].tolist ())
+            property_columns =["full_mwt","mw_freebase","alogp","hbd","hba","psa","aromatic_rings","rtb","num_ro5_violations"]
+            for col in property_columns :
+                if col in properties_df .columns :
+                    df [col ]=properties_df [col ]
+        log .debug ("flatten_nested_structures_completed",columns =list (df .columns ))
+        return df
+    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize ChEMBL identifiers and InChI keys."
+        if df .empty :
+            return df
+        working_df =df .copy ()
+        inchi_key_col ="molecule_structures__standard_inchi_key"if "molecule_structures__standard_inchi_key"in working_df .columns else "standard_inchi_key"
+        rules :dict [str ,StringRule ]={}
+        if "molecule_chembl_id"in working_df .columns :
+            rules ["molecule_chembl_id"]=StringRule ()
+        if inchi_key_col in working_df .columns :
+            rules [inchi_key_col ]=StringRule (uppercase =True )
+        if rules :
+            normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        else :
+            normalized_df =working_df
+            stats =StringStats ()
+        if stats .has_changes :
+            log .debug ("normalize_identifiers_completed",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        else :
+            log .debug ("normalize_identifiers_completed")
+        return normalized_df
+    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize string fields: trim, replace empty strings with NaN."
+        if df .empty :
+            return df
+        working_df =df .copy ()
+        rules ={"pref_name":StringRule (),"molecule_type":StringRule (),"molecule_structures__canonical_smiles":StringRule (),"canonical_smiles":StringRule ()}
+        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        if stats .has_changes :
+            log .debug ("normalize_string_fields_completed",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        else :
+            log .debug ("normalize_string_fields_completed")
+        return normalized_df
+    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
+        specs =dict (super ()._schema_column_specs ())
+        int_columns =["max_phase","first_approval","first_in_class","availability_type","black_box_warning","chirality","dosed_ingredient","inorganic_flag","natural_product","prodrug","therapeutic_flag","molecule_properties__aromatic_rings","molecule_properties__hba","molecule_properties__hba_lipinski","molecule_properties__hbd","molecule_properties__hbd_lipinski","molecule_properties__heavy_atoms","molecule_properties__num_lipinski_ro5_violations","molecule_properties__num_ro5_violations","molecule_properties__ro3_pass","molecule_properties__rtb"]
+        float_columns =["molecule_properties__alogp","molecule_properties__cx_logd","molecule_properties__cx_logp","molecule_properties__cx_most_apka","molecule_properties__cx_most_bpka","molecule_properties__full_mwt","molecule_properties__mw_freebase","molecule_properties__mw_monoisotopic","molecule_properties__psa","molecule_properties__qed_weighted"]
+        for column in int_columns :
+            specs [column ]={"dtype":"Int64","default":pd .NA }
+        for column in float_columns :
+            specs [column ]={"dtype":"Float64","default":pd .NA }
+        for column in COLUMN_ORDER :
+            if column not in specs :
+                specs [column ]={"dtype":"string","default":pd .NA }
+        return specs
+    def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize numeric fields: convert types, replace negative values with None."
+        if df .empty :
+            return df
+        int_ge_zero_columns =["chirality","availability_type","hbd","hba","aromatic_rings","rtb","num_ro5_violations"]
+        int_columns =["max_phase","first_approval","black_box_warning"]
+        boolean_columns =["first_in_class","inorganic_flag","natural_product","prodrug","therapeutic_flag","dosed_ingredient"]
+        for col in int_ge_zero_columns +int_columns +boolean_columns :
+            if col in df .columns :
+                numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                if col in int_ge_zero_columns :
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                elif col in boolean_columns :
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                    numeric_series =numeric_series .where ((numeric_series ==0 )|(numeric_series ==1 )|numeric_series .isna ())
+                df [col ]=numeric_series .astype ("Int64")
+        for col in df .columns :
+            if col .startswith ("molecule_properties__"):
+                prop_name =col .replace ("molecule_properties__","")
+                if prop_name =="ro3_pass":
+                    mask =df [col ].notna ()
+                    if mask .any ():
+                        col_str =df .loc [mask ,col ].astype (str ).str .upper ().str .strip ()
+                        converted =df [col ].copy ()
+                        converted .loc [mask &(col_str =="Y")]=1
+                        converted .loc [mask &(col_str =="N")]=0
+                        other_mask =mask &~col_str .isin (["Y","N"])
+                        if other_mask .any ():
+                            numeric_vals =pd .to_numeric (df .loc [other_mask ,col ],errors ="coerce")
+                            valid_numeric =numeric_vals .notna ()&numeric_vals .isin ([0 ,1 ])
+                            converted .loc [other_mask ]=pd .NA
+                            if valid_numeric .any ():
+                                valid_indices =numeric_vals .index [valid_numeric ]
+                                converted .loc [valid_indices ]=numeric_vals .loc [valid_indices ]
+                        df [col ]=converted
+                    numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                    numeric_series =numeric_series .where ((numeric_series ==0 )|(numeric_series ==1 )|numeric_series .isna ())
+                    df [col ]=numeric_series .astype ("Int64")
+                elif prop_name in ["aromatic_rings","hba","hba_lipinski","hbd","hbd_lipinski","heavy_atoms","num_lipinski_ro5_violations","num_ro5_violations","rtb"]:
+                    numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                    df [col ]=numeric_series .astype ("Int64")
+        log .debug ("normalize_numeric_fields_completed")
+        return df
+    def _check_empty_columns (self ,df :pd .DataFrame ,log :Any )->None :
+        "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043f\u0443\u0441\u0442\u044b\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n        \u041b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u0435, \u0435\u0441\u043b\u0438 \u043f\u0440\u043e\u0446\u0435\u043d\u0442 \u043f\u0443\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u043e \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u043c \u043f\u043e\u043b\u044f\u043c > 95%.\n        \u042d\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443 \u0441 select_fields \u0438\u043b\u0438 flatten_objects.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        "
+        if df .empty :
+            return
+        key_fields =["molecule_chembl_id","pref_name","molecule_type","availability_type","chirality","first_approval","first_in_class","indication_class","helm_notation","molecule_properties__cx_logp","molecule_properties__cx_logd","molecule_properties__mw_monoisotopic","molecule_properties__hba_lipinski","molecule_properties__hbd_lipinski","molecule_properties__molecular_species","molecule_properties__num_lipinski_ro5_violations"]
+        available_key_fields =[col for col in key_fields if col in df .columns ]
+        if not available_key_fields :
+            return
+        empty_percentages :dict [str ,float ]={}
+        for col in available_key_fields :
+            if len (df )>0 :
+                empty_count =df [col ].isna ().sum ()
+                empty_percentage =empty_count /len (df )*100.0
+                empty_percentages [col ]=empty_percentage
+        highly_empty_fields ={col :pct for col ,pct in empty_percentages .items ()if pct >95.0 }
+        if highly_empty_fields :
+            log .warning ("highly_empty_columns_detected",empty_fields =highly_empty_fields ,message =f'Fields with >95% empty values detected: {highly_empty_fields }. This may indicate missing fields in select_fields or flatten_objects configuration.')
+    def _remove_extra_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Remove columns that are not in the schema."
+        if df .empty :
+            return df
+        from bioetl .schemas .testitem import COLUMN_ORDER
+        schema_columns =set (COLUMN_ORDER )
+        existing_columns =set (df .columns )
+        extra_columns =existing_columns -schema_columns
+        if extra_columns :
+            df =df .drop (columns =list (extra_columns ))
+            log .debug ("remove_extra_columns_completed",removed_columns =list (extra_columns ),remaining_columns =list (df .columns ))
+        return df
+    def _deduplicate_molecules (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Deduplicate molecules by standard_inchi_key (fallback to molecule_chembl_id)."
+        if df .empty :
+            return df
+        rows_before =len (df )
+        inchi_key_col ="molecule_structures__standard_inchi_key"if "molecule_structures__standard_inchi_key"in df .columns else "standard_inchi_key"
+        canonical_smiles_col ="molecule_structures__canonical_smiles"if "molecule_structures__canonical_smiles"in df .columns else "canonical_smiles"
+        full_mwt_col ="molecule_properties__full_mwt"if "molecule_properties__full_mwt"in df .columns else "full_mwt"
+        alogp_col ="molecule_properties__alogp"if "molecule_properties__alogp"in df .columns else "alogp"
+        if inchi_key_col in df .columns :
+            completeness_cols =[canonical_smiles_col ,full_mwt_col ,alogp_col ]
+            available_completeness =[col for col in completeness_cols if col in df .columns ]
+            if available_completeness :
+                df ["_completeness"]=df [available_completeness ].notna ().sum (axis =1 )
+                df =df .sort_values (["_completeness",canonical_smiles_col ],ascending =[False ,False ],na_position ="last")
+                df =df .drop (columns =["_completeness"])
+            df =df .drop_duplicates (subset =[inchi_key_col ],keep ="first")
+        else :
+            df =df .drop_duplicates (subset =["molecule_chembl_id"],keep ="first")
+        rows_after =len (df )
+        dropped =rows_before -rows_after
+        if dropped >0 :
+            log .info ("deduplicate_molecules_completed",rows_before =rows_before ,rows_after =rows_after ,dropped =dropped )
+        return df
```

#### Горячий участок 19

- assay: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\run.py:57-60

```diff
--- assay:run.py

+++ testitem:run.py

@@ -0,0 +1,4 @@

+def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+    super ().__init__ (config ,run_id )
+    self ._chembl_db_version :str |None =None
+    self ._api_version :str |None =None
```

#### Горячий участок 20

- assay: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\run.py:151-234

```diff
--- assay:run.py

+++ testitem:run.py

@@ -0,0 +1,20 @@

+def _build_testitem_descriptor (self :SelfTestitemChemblPipeline )->ChemblExtractionDescriptor [SelfTestitemChemblPipeline ]:
+    "Return the descriptor powering testitem extraction."
+    def build_context (pipeline :SelfTestitemChemblPipeline ,source_config :TestItemSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+        base_url =pipeline ._resolve_base_url (source_config .parameters )
+        http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+        chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
+        pipeline ._fetch_chembl_release (chembl_client ,log )
+        select_fields =source_config .parameters .select_fields
+        log .debug ("chembl_testitem.select_fields",fields =select_fields )
+        testitem_client =ChemblTestitemClient (chembl_client ,batch_size =min (source_config .page_size ,25 ))
+        return ChemblExtractionContext (source_config =source_config ,iterator =testitem_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,page_size =source_config .page_size ,chembl_release =pipeline ._chembl_db_version ,metadata ={"api_version":pipeline ._api_version })
+    def empty_frame (_ :SelfTestitemChemblPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
+        return pd .DataFrame ({"molecule_chembl_id":pd .Series (dtype ="string")})
+    def dry_run_handler (pipeline :SelfTestitemChemblPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =pipeline ._chembl_db_version ,api_version =pipeline ._api_version )
+        return pd .DataFrame ()
+    def summary_extra (pipeline :SelfTestitemChemblPipeline ,_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+        return {"chembl_db_version":pipeline ._chembl_db_version ,"api_version":pipeline ._api_version ,"limit":pipeline .config .cli .limit }
+    return ChemblExtractionDescriptor [SelfTestitemChemblPipeline ](name ="chembl_testitem",source_name ="chembl",source_config_factory =TestItemSourceConfig .from_source_config ,build_context =build_context ,id_column ="molecule_chembl_id",summary_event ="chembl_testitem.extract_summary",must_have_fields =tuple (MUST_HAVE_FIELDS ),default_select_fields =MUST_HAVE_FIELDS ,sort_by =("molecule_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra ,hard_page_size_cap =None )
```

### Модуль transform.py

Определение                    | assay сигнатура                                | testitem сигнатура                                             | Побочные эффекты                                                                                                      | Исключения                                    | Статус           
-------------------------------|------------------------------------------------|----------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|------------------
__module_block_0               | —                                              | —                                                              | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                  | assay: []
testitem: []                        | отличается       
__module_block_1               | —                                              | —                                                              | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                  | assay: []
testitem: []                        | совпадает        
__module_block_2               | —                                              | —                                                              | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                  | assay: []
testitem: []                        | отличается       
__module_block_3               | —                                              | —                                                              | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                  | assay: []
testitem: []                        | отличается       
__module_block_4               | —                                              | —                                                              | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                  | assay: []
testitem: []                        | отличается       
__module_block_5               | —                                              | —                                                              | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                  | assay: []
testitem: []                        | отличается       
__module_block_6               | —                                              | —                                                              | assay: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                  | assay: []
testitem: []                        | отличается       
__module_block_7               | —                                              | —                                                              | assay: {'logging': [], 'io': []}
testitem: {}                                                                         | assay: []
testitem: []                        | только в assay   
__module_block_8               | —                                              | —                                                              | assay: {'logging': [], 'io': []}
testitem: {}                                                                         | assay: []
testitem: []                        | только в assay   
_is_null_like                  | value: Any                                     | —                                                              | assay: {'logging': [], 'io': []}
testitem: {}                                                                         | assay: []
testitem: []                        | только в assay   
flatten_object_col             | —                                              | df: pd.DataFrame, col: str, fields: Sequence[str], prefix: str | assay: {}
testitem: {'logging': [], 'io': []}                                                                         | assay: []
testitem: []                        | только в testitem
transform                      | —                                              | df: pd.DataFrame, cfg: Any                                     | assay: {}
testitem: {'logging': [], 'io': []}                                                                         | assay: []
testitem: []                        | только в testitem
validate_assay_parameters_truv | df: pd.DataFrame, column: str, fail_fast: bool | —                                                              | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.error', 'log.warning'], 'io': ['json.loads']}
testitem: {} | assay: ['ValueError(error_msg)']
testitem: [] | только в assay   

#### Горячий участок 1

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:1-1
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:1-1

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-"Transform utilities for ChEMBL assay pipeline array serialization."
+"Transform utilities for ChEMBL testitem pipeline array serialization and flattening."
```

#### Горячий участок 2

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:10-10
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:10-10

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-from bioetl .core .logger import UnifiedLogger
+from bioetl .core .serialization import serialize_objects ,serialize_simple_list
```

#### Горячий участок 3

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:11-11
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:5-5

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-from bioetl .core .serialization import header_rows_serialize ,serialize_array_fields
+from collections .abc import Sequence
```

#### Горячий участок 4

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:6-6
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:6-6

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-from typing import Any ,cast
+from typing import Any
```

#### Горячий участок 5

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:5-5
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:8-8

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-import json
+import pandas as pd
```

#### Горячий участок 6

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:8-8
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:12-17

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-import pandas as pd
+__all__ =["serialize_simple_list","serialize_objects","flatten_object_col","transform"]
```

#### Горячий участок 7

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:13-17
- testitem: нет в ветке

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -1 +0,0 @@

-__all__ =["header_rows_serialize","serialize_array_fields","validate_assay_parameters_truv"]
```

#### Горячий участок 8

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:19-19
- testitem: нет в ветке

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -1 +0,0 @@

-AssayParam =dict [str ,Any ]
```

#### Горячий участок 9

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:22-39
- testitem: нет в ветке

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -1,13 +0,0 @@

-def _is_null_like (value :Any )->bool :
-    "\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043c\u043e\u0436\u043d\u043e \u043b\u0438 \u0442\u0440\u0430\u043a\u0442\u043e\u0432\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u0430\u043a \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0435."
-    if value is None :
-        return True
-    if isinstance (value ,str ):
-        return value .strip ()==""
-    if isinstance (value ,float ):
-        return bool (pd .isna (value ))
-    try :
-        is_na_raw =cast (Any ,pd .isna (value ))
-    except TypeError :
-        return False
-    return bool (is_na_raw )if isinstance (is_na_raw ,bool )else False
```

#### Горячий участок 10

- assay: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:20-75

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -0,0 +1,16 @@

+def flatten_object_col (df :pd .DataFrame ,col :str ,fields :Sequence [str ],prefix :str )->pd .DataFrame :
+    "Flatten nested object column into flat columns with prefix.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    col:\n        Column name containing nested objects.\n    fields:\n        List of field names to extract from nested objects.\n    prefix:\n        Prefix to add to flattened column names (e.g., \"molecule_hierarchy__\").\n\n    Returns\n    -------\n    pd.DataFrame:\n        DataFrame with flattened columns added and original column removed.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame({\n    ...     \"molecule_chembl_id\": [\"CHEMBL1\"],\n    ...     \"molecule_hierarchy\": [{\"molecule_chembl_id\": \"CHEMBL1\", \"parent_chembl_id\": \"CHEMBL2\"}],\n    ... })\n    >>> result = flatten_object_col(df, \"molecule_hierarchy\", [\"molecule_chembl_id\", \"parent_chembl_id\"], \"molecule_hierarchy__\")\n    >>> \"molecule_hierarchy__molecule_chembl_id\" in result.columns\n    True\n    >>> \"molecule_hierarchy__parent_chembl_id\" in result.columns\n    True\n    >>> \"molecule_hierarchy\" not in result.columns\n    True\n    "
+    df =df .copy ()
+    if col not in df .columns :
+        for f in fields :
+            df [f'{prefix }{f }']=None
+        return df
+    def row_to_dict (obj :Any )->dict [str ,Any ]:
+        "Extract fields from nested object."
+        if not isinstance (obj ,dict ):
+            return dict .fromkeys (fields )
+        return {f :obj .get (f )for f in fields }
+    expanded =df [col ].map (row_to_dict ).apply (pd .Series )
+    expanded .columns =[f'{prefix }{c }'for c in expanded .columns ]
+    df =df .drop (columns =[col ])
+    return pd .concat ([df ,expanded ],axis =1 )
```

#### Горячий участок 11

- assay: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:78-138

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -0,0 +1,25 @@

+def transform (df :pd .DataFrame ,cfg :Any )->pd .DataFrame :
+    "Transform testitem DataFrame by flattening nested objects and serializing arrays.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    cfg:\n        Pipeline config with transform section (enable_flatten, enable_serialization,\n        arrays_simple_to_pipe, arrays_objects_to_header_rows, flatten_objects).\n\n    Returns\n    -------\n    pd.DataFrame:\n        Transformed DataFrame with flattened objects and serialized arrays.\n    "
+    df =df .copy ()
+    enable_flatten =getattr (cfg .transform ,"enable_flatten",True )if hasattr (cfg ,"transform")else True
+    enable_serialization =getattr (cfg .transform ,"enable_serialization",True )if hasattr (cfg ,"transform")else True
+    if enable_flatten and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"flatten_objects"):
+        flatten_objects =cfg .transform .flatten_objects
+        if isinstance (flatten_objects ,dict ):
+            for obj_col ,fields in flatten_objects .items ():
+                if isinstance (fields ,Sequence )and (not isinstance (fields ,(str ,bytes ))):
+                    df =flatten_object_col (df ,obj_col ,fields ,prefix =f'{obj_col }__')
+    if enable_serialization and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"arrays_simple_to_pipe"):
+        arrays_simple =cfg .transform .arrays_simple_to_pipe
+        if isinstance (arrays_simple ,Sequence )and (not isinstance (arrays_simple ,(str ,bytes ))):
+            for col in arrays_simple :
+                if col in df .columns :
+                    df [col ]=df [col ].map (serialize_simple_list )
+    if enable_serialization and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"arrays_objects_to_header_rows"):
+        arrays_objects =cfg .transform .arrays_objects_to_header_rows
+        if isinstance (arrays_objects ,Sequence )and (not isinstance (arrays_objects ,(str ,bytes ))):
+            for col in arrays_objects :
+                if col in df .columns :
+                    df [f'{col }__flat']=df [col ].map (serialize_objects )
+                    df =df .drop (columns =[col ])
+    return df
```

#### Горячий участок 12

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\transform.py:42-232
- testitem: нет в ветке

```diff
--- assay:transform.py

+++ testitem:transform.py

@@ -1,78 +0,0 @@

-def validate_assay_parameters_truv (df :pd .DataFrame ,column :str ="assay_parameters",fail_fast :bool =True )->pd .DataFrame :
-    "\u0412\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u0442\u044c TRUV-\u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0434\u043b\u044f assay_parameters.\n\n    \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n    - value IS NOT NULL XOR text_value IS NOT NULL (\u043d\u0435 \u043e\u0431\u0430 \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u043d\u0435 NULL)\n    - standard_value IS NOT NULL XOR standard_text_value IS NOT NULL\n    - active \u2208 {0, 1, NULL}\n    - relation \u2208 {'=', '<', '\u2264', '>', '\u2265', '~', NULL} (\u0441 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u0435\u043c \u0434\u043b\u044f \u043d\u0435\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0445)\n\n    Parameters\n    ----------\n    df:\n        DataFrame \u0441 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 assay_parameters (JSON-\u0441\u0442\u0440\u043e\u043a\u0430 \u0441 \u043c\u0430\u0441\u0441\u0438\u0432\u043e\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432).\n    column:\n        \u0418\u043c\u044f \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 (\u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \"assay_parameters\").\n    fail_fast:\n        \u0415\u0441\u043b\u0438 True, \u0432\u044b\u0431\u0440\u0430\u0441\u044b\u0432\u0430\u0435\u0442 ValueError \u043f\u0440\u0438 \u043d\u0430\u0440\u0443\u0448\u0435\u043d\u0438\u0438 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432.\n        \u0415\u0441\u043b\u0438 False, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0435\u0442.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0439 DataFrame (\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u043d\u0435 \u0438\u0437\u043c\u0435\u043d\u044f\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435).\n\n    Raises\n    ------\n    ValueError:\n        \u0415\u0441\u043b\u0438 fail_fast=True \u0438 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043e \u043d\u0430\u0440\u0443\u0448\u0435\u043d\u0438\u0435 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432.\n\n    Notes\n    -----\n    - \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442\u0441\u044f \u043d\u0430 \u044d\u0442\u0430\u043f\u0435 transform \u0434\u043b\u044f fail-fast \u043f\u043e\u0434\u0445\u043e\u0434\u0430\n    - \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u044b relation: '=', '<', '\u2264', '>', '\u2265', '~'\n    - \u041d\u0435\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u044b \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="assay_transform")
-    if column not in df .columns :
-        log .debug ("truv_validation_skipped_missing_column",column =column )
-        return df
-    STANDARD_RELATIONS ={"=","<","\u2264",">","\u2265","~"}
-    errors :list [str ]=[]
-    warnings :list [str ]=[]
-    for idx ,row in df .iterrows ():
-        params_str =row .get (column )
-        if _is_null_like (params_str ):
-            continue
-        try :
-            if isinstance (params_str ,str ):
-                params_raw =json .loads (params_str )
-            else :
-                params_raw =params_str
-        except (json .JSONDecodeError ,TypeError )as exc :
-            errors .append (f'Row {idx }: Invalid JSON in {column }: {exc }')
-            continue
-        if not isinstance (params_raw ,list ):
-            errors .append (f'Row {idx }: {column } must be a JSON array, got {type (params_raw ).__name__ }')
-            continue
-        params_candidates =cast (list [object ],params_raw )
-        for param_idx ,param_raw in enumerate (params_candidates ):
-            if not isinstance (param_raw ,dict ):
-                errors .append (f'Row {idx }, param {param_idx }: Parameter must be a dict, got {type (param_raw ).__name__ }')
-                continue
-            param_dict :AssayParam =cast (AssayParam ,param_raw )
-            value :Any =param_dict .get ("value")
-            text_value :Any =param_dict .get ("text_value")
-            value_is_null =value is None or (isinstance (value ,float )and pd .isna (value ))or (isinstance (value ,str )and value .strip ()=="")
-            text_value_is_null =text_value is None or (isinstance (text_value ,float )and pd .isna (text_value ))or (isinstance (text_value ,str )and text_value .strip ()=="")
-            if not value_is_null and (not text_value_is_null ):
-                errors .append (f"Row {idx }, param {param_idx }: Both 'value' and 'text_value' are not NULL (value={value }, text_value={text_value }). TRUV invariant violation: value and text_value must be mutually exclusive.")
-            standard_value :Any =param_dict .get ("standard_value")
-            standard_text_value :Any =param_dict .get ("standard_text_value")
-            standard_value_is_null =standard_value is None or (isinstance (standard_value ,float )and pd .isna (standard_value ))or (isinstance (standard_value ,str )and standard_value .strip ()=="")
-            standard_text_value_is_null =standard_text_value is None or (isinstance (standard_text_value ,float )and pd .isna (standard_text_value ))or (isinstance (standard_text_value ,str )and standard_text_value .strip ()=="")
-            if not standard_value_is_null and (not standard_text_value_is_null ):
-                errors .append (f"Row {idx }, param {param_idx }: Both 'standard_value' and 'standard_text_value' are not NULL (standard_value={standard_value }, standard_text_value={standard_text_value }). TRUV invariant violation: standard_value and standard_text_value must be mutually exclusive.")
-            active :Any =param_dict .get ("active")
-            if active is not None :
-                if isinstance (active ,bool ):
-                    active_int =1 if active else 0
-                elif isinstance (active ,(int ,float )):
-                    active_int =int (active )
-                elif isinstance (active ,str ):
-                    try :
-                        active_int =int (active )
-                    except ValueError :
-                        errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' value: {active !r }. Must be 0, 1, or NULL.")
-                        continue
-                else :
-                    errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' type: {type (active ).__name__ }. Must be 0, 1, or NULL.")
-                    continue
-                if active_int not in {0 ,1 }:
-                    errors .append (f"Row {idx }, param {param_idx }: Invalid 'active' value: {active_int }. Must be 0, 1, or NULL.")
-            relation :Any =param_dict .get ("relation")
-            if relation is not None and (not (isinstance (relation ,float )and pd .isna (relation ))):
-                relation_str =str (relation ).strip ()
-                if relation_str and relation_str not in STANDARD_RELATIONS :
-                    warnings .append (f"Row {idx }, param {param_idx }: Non-standard 'relation' value: {relation_str !r }. Standard operators: {", ".join (sorted (STANDARD_RELATIONS ))}.")
-    if warnings :
-        for warning in warnings :
-            log .warning ("truv_validation_warning",message =warning )
-    if errors :
-        error_msg =f'TRUV validation failed for {column }:\n'+"\n".join (errors )
-        if fail_fast :
-            log .error ("truv_validation_failed",error_count =len (errors ))
-            raise ValueError (error_msg )
-        else :
-            for error in errors :
-                log .warning ("truv_validation_error",message =error )
-    if not errors and (not warnings ):
-        log .debug ("truv_validation_passed",rows_checked =len (df ))
-    return df
```

### Модуль normalize.py

Определение                       | assay сигнатура                                                      | testitem сигнатура | Побочные эффекты                                                                                                     | Исключения             | Статус        
----------------------------------|----------------------------------------------------------------------|--------------------|----------------------------------------------------------------------------------------------------------------------|------------------------|---------------
__module_block_0                  | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_1                  | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_10                 | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_11                 | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_12                 | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_13                 | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_2                  | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_3                  | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_4                  | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_5                  | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_6                  | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_7                  | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_8                  | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
__module_block_9                  | —                                                                    | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
_should_nullify_string_value      | value: Any                                                           | —                  | assay: {'logging': [], 'io': []}
testitem: {}                                                                        | assay: []
testitem: [] | только в assay
enrich_with_assay_classifications | df_assay: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | —                  | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}
testitem: {} | assay: []
testitem: [] | только в assay
enrich_with_assay_parameters      | df_assay: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | —                  | assay: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}
testitem: {} | assay: []
testitem: [] | только в assay

#### Горячий участок 1

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:1-1
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-"Enrichment functions for Assay pipeline."
```

#### Горячий участок 2

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:3-3
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from __future__ import annotations
```

#### Горячий участок 3

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:19-22
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-__all__ =["enrich_with_assay_classifications","enrich_with_assay_parameters"]
```

#### Горячий участок 4

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:25-25
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-_ensure_columns =ensure_columns
```

#### Горячий участок 5

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:28-31
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-_CLASSIFICATION_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_classifications","string"),("assay_class_id","string"))
```

#### Горячий участок 6

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:33-33
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-_PARAMETERS_COLUMNS :tuple [tuple [str ,str ],...]=(("assay_parameters","string"),)
```

#### Горячий участок 7

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:11-11
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .clients .chembl import ChemblClient
```

#### Горячий участок 8

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:12-12
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .frame import ensure_columns
```

#### Горячий участок 9

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:13-13
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .logger import UnifiedLogger
```

#### Горячий участок 10

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:14-17
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .schemas .assay import ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA ,ASSAY_PARAMETERS_ENRICHMENT_SCHEMA
```

#### Горячий участок 11

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:6-6
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from collections .abc import Mapping
```

#### Горячий участок 12

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:7-7
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from typing import Any
```

#### Горячий участок 13

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:5-5
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-import json
```

#### Горячий участок 14

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:9-9
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-import pandas as pd
```

#### Горячий участок 15

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:36-44
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1,9 +0,0 @@

-def _should_nullify_string_value (value :Any )->bool :
-    "\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043d\u0443\u0436\u043d\u043e \u043b\u0438 \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 NA \u0432 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0435."
-    if value is None :
-        return False
-    if value is pd .NA :
-        return False
-    if isinstance (value ,float )and pd .isna (value ):
-        return False
-    return not isinstance (value ,str )
```

#### Горячий участок 16

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:47-219
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1,86 +0,0 @@

-def enrich_with_assay_classifications (df_assay :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame assay \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 ASSAY_CLASS_MAP \u0438 ASSAY_CLASSIFICATION.\n\n    Parameters\n    ----------\n    df_assay:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.assay.enrich.classifications.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438/\u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438:\n        - assay_classifications (string, nullable) - \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u043c\u0430\u0441\u0441\u0438\u0432 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439\n        - assay_class_id (string, nullable) - \u0441\u043f\u0438\u0441\u043e\u043a assay_class_id \u0447\u0435\u0440\u0435\u0437 \";\"\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="assay_enrichment")
-    df_assay =_ensure_columns (df_assay ,_CLASSIFICATION_COLUMNS )
-    if "assay_classifications"in df_assay .columns :
-        invalid_mask =df_assay ["assay_classifications"].map (_should_nullify_string_value )
-        if bool (invalid_mask .any ()):
-            log .warning ("assay_classifications_reset_non_string",rows =int (invalid_mask .sum ()))
-            df_assay .loc [invalid_mask ,"assay_classifications"]=pd .NA
-    if df_assay .empty :
-        log .debug ("enrichment_skipped_empty_dataframe")
-        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    required_cols =["assay_chembl_id"]
-    missing_cols =[col for col in required_cols if col not in df_assay .columns ]
-    if missing_cols :
-        log .warning ("enrichment_skipped_missing_columns",missing_columns =missing_cols )
-        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    assay_ids :list [str ]=[]
-    for _ ,row in df_assay .iterrows ():
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        if assay_id_str :
-            assay_ids .append (assay_id_str )
-    if not assay_ids :
-        log .debug ("enrichment_skipped_no_valid_ids")
-        return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    class_map_fields =cfg .get ("class_map_fields",["assay_chembl_id","assay_class_id"])
-    classification_fields =cfg .get ("classification_fields",["assay_class_id","l1","l2","l3","pref_name"])
-    page_limit =cfg .get ("page_limit",1000 )
-    log .info ("enrichment_fetching_assay_class_map",ids_count =len (set (assay_ids )))
-    class_map_dict =client .fetch_assay_class_map_by_assay_ids (assay_ids ,list (class_map_fields ),page_limit )
-    all_class_ids :set [str ]=set ()
-    for mappings in class_map_dict .values ():
-        for mapping in mappings :
-            class_id =mapping .get ("assay_class_id")
-            if class_id and (not (isinstance (class_id ,float )and pd .isna (class_id ))):
-                all_class_ids .add (str (class_id ).strip ())
-    classification_dict :dict [str ,dict [str ,Any ]]={}
-    if all_class_ids :
-        log .info ("enrichment_fetching_assay_classifications",class_ids_count =len (all_class_ids ))
-        classification_dict =client .fetch_assay_classifications_by_class_ids (list (all_class_ids ),list (classification_fields ),page_limit )
-    df_assay =df_assay .copy ()
-    if "assay_classifications"not in df_assay .columns :
-        df_assay ["assay_classifications"]=pd .NA
-    if "assay_class_id"not in df_assay .columns :
-        df_assay ["assay_class_id"]=pd .NA
-    for idx ,row in df_assay .iterrows ():
-        row_key :Any =idx
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        mappings =class_map_dict .get (assay_id_str ,[])
-        if not mappings :
-            df_assay .at [row_key ,"assay_classifications"]=pd .NA
-            df_assay .at [row_key ,"assay_class_id"]=pd .NA
-            continue
-        classifications :list [dict [str ,Any ]]=[]
-        class_ids :list [str ]=[]
-        for mapping in mappings :
-            class_id =mapping .get ("assay_class_id")
-            if not class_id or (isinstance (class_id ,float )and pd .isna (class_id )):
-                continue
-            class_id_str =str (class_id ).strip ()
-            if not class_id_str :
-                continue
-            classification_data =classification_dict .get (class_id_str )
-            if classification_data :
-                class_record :dict [str ,Any ]={"assay_class_id":class_id_str }
-                for field in classification_fields :
-                    if field !="assay_class_id":
-                        class_record [field ]=classification_data .get (field )
-                classifications .append (class_record )
-            else :
-                class_record ={"assay_class_id":class_id_str }
-                classifications .append (class_record )
-            class_ids .append (class_id_str )
-        if classifications :
-            serialized =json .dumps (classifications ,ensure_ascii =False )
-            class_id_joined =";".join (class_ids )
-            df_assay .at [row_key ,"assay_classifications"]=serialized
-            df_assay .at [row_key ,"assay_class_id"]=class_id_joined
-    log .info ("enrichment_classifications_complete",assays_with_classifications =len (df_assay [df_assay ["assay_classifications"].notna ()]))
-    return ASSAY_CLASSIFICATION_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
```

#### Горячий участок 17

- assay: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\assay\normalize.py:222-378
- testitem: нет в ветке

```diff
--- assay:normalize.py

+++ testitem:normalize.py

@@ -1,57 +0,0 @@

-def enrich_with_assay_parameters (df_assay :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame assay \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 ASSAY_PARAMETERS.\n\n    \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442 \u043f\u043e\u043b\u043d\u044b\u0439 TRUV-\u043d\u0430\u0431\u043e\u0440 \u043f\u043e\u043b\u0435\u0439 (TYPE, RELATION, VALUE, UNITS, TEXT_VALUE),\n    \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (standard_*), \u0441\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (active) \u0438 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435\n    \u043f\u043e\u043b\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 (type_normalized, type_fixed).\n\n    Parameters\n    ----------\n    df_assay:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.assay.enrich.parameters.\n        \u0414\u043e\u043b\u0436\u043d\u0430 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c fields (\u0441\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439 \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f), page_limit \u0438 active_only.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0439/\u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439:\n        - assay_parameters (string, nullable) - \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 JSON-\u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\n          \u0441 \u043f\u043e\u043b\u044f\u043c\u0438: type, relation, value, units, text_value, standard_*,\n          active, type_normalized, type_fixed (\u0435\u0441\u043b\u0438 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 \u0434\u0430\u043c\u043f\u0435)\n\n    Notes\n    -----\n    - \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u0435\u0441\u0442\u044c, \u043d\u0435 \u043a\u043e\u043f\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0432 standard_* \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\n    - \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u043e\u043b\u044f (type_normalized, type_fixed) \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438\n      \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 \u0434\u0430\u043c\u043f\u0435 ChEMBL\n    - \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0444\u0438\u043b\u044c\u0442\u0440\u0443\u044e\u0442\u0441\u044f \u043f\u043e active=1, \u0435\u0441\u043b\u0438 active_only=True \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="assay_enrichment")
-    df_assay =_ensure_columns (df_assay ,_PARAMETERS_COLUMNS )
-    if "assay_parameters"in df_assay .columns :
-        invalid_mask =df_assay ["assay_parameters"].map (_should_nullify_string_value )
-        if bool (invalid_mask .any ()):
-            log .warning ("assay_parameters_reset_non_string",rows =int (invalid_mask .sum ()))
-            df_assay .loc [invalid_mask ,"assay_parameters"]=pd .NA
-        df_assay ["assay_parameters"]=df_assay ["assay_parameters"].astype ("string")
-    if df_assay .empty :
-        log .debug ("enrichment_skipped_empty_dataframe")
-        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    required_cols =["assay_chembl_id"]
-    missing_cols =[col for col in required_cols if col not in df_assay .columns ]
-    if missing_cols :
-        log .warning ("enrichment_skipped_missing_columns",missing_columns =missing_cols )
-        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    assay_ids :list [str ]=[]
-    for _ ,row in df_assay .iterrows ():
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        if assay_id_str :
-            assay_ids .append (assay_id_str )
-    if not assay_ids :
-        log .debug ("enrichment_skipped_no_valid_ids")
-        return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
-    fields =cfg .get ("fields",["assay_chembl_id","type","relation","value","units","text_value","standard_type","standard_relation","standard_value","standard_units","standard_text_value","active"])
-    page_limit =cfg .get ("page_limit",1000 )
-    active_only =cfg .get ("active_only",True )
-    log .info ("enrichment_fetching_assay_parameters",ids_count =len (set (assay_ids )))
-    parameters_dict =client .fetch_assay_parameters_by_assay_ids (assay_ids ,list (fields ),page_limit ,active_only )
-    df_assay =df_assay .copy ()
-    if "assay_parameters"not in df_assay .columns :
-        df_assay ["assay_parameters"]=pd .NA
-    for row_position ,(_ ,row )in enumerate (df_assay .iterrows ()):
-        assay_id =row .get ("assay_chembl_id")
-        if pd .isna (assay_id )or assay_id is None :
-            continue
-        assay_id_str =str (assay_id ).strip ()
-        parameters =parameters_dict .get (assay_id_str ,[])
-        if not parameters :
-            continue
-        params_list :list [dict [str ,Any ]]=[]
-        for param in parameters :
-            param_record :dict [str ,Any ]={}
-            for field in fields :
-                if field !="assay_chembl_id":
-                    param_record [field ]=param .get (field )
-            params_list .append (param_record )
-        if params_list :
-            index_label =df_assay .index [row_position ]
-            df_assay .loc [index_label ,"assay_parameters"]=json .dumps (params_list ,ensure_ascii =False )
-    log .info ("enrichment_parameters_complete",assays_with_parameters =len (df_assay [df_assay ["assay_parameters"].notna ()]))
-    return ASSAY_PARAMETERS_ENRICHMENT_SCHEMA .validate (df_assay ,lazy =True )
```

---

## Пара: document ↔ target

- AST hash: 3774d930e14eb5befd7fdffc255cb346 ↔ 107171553e4f1c509bba122a3d0ccb96

- Jaccard по токенам: 0.298

### Модуль run.py

Определение                                          | document сигнатура                        | target сигнатура                                     | Побочные эффекты                                                                                                      | Исключения                              | Статус           
-----------------------------------------------------|-------------------------------------------|------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|-----------------------------------------|------------------
ChemblDocumentPipeline                               | —                                         | —                                                    | document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
target: {}             | document: ['TypeError(msg)']
target: [] | только в document
ChemblDocumentPipeline.__init__                      | self, config: PipelineConfig, run_id: str | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline._add_system_fields            | self, df: pd.DataFrame, log: Any          | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline._build_document_descriptor    | self: SelfChemblDocumentPipeline          | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: ['TypeError(msg)']
target: [] | только в document
ChemblDocumentPipeline._check_document_id_uniqueness | self, df: pd.DataFrame, log: Any          | —                                                    | document: {'logging': ['log.warning'], 'io': []}
target: {}                                                           | document: []
target: []                 | только в document
ChemblDocumentPipeline._coerce_mapping               | payload: Any                              | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline._enrich_document_terms        | self, df: pd.DataFrame                    | —                                                    | document: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}
target: {}                                      | document: []
target: []                 | только в document
ChemblDocumentPipeline._extract_nested_fields        | self, record: dict[str, Any]              | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline._normalize_authors            | authors: Any, separator: str              | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline._normalize_doi                | doi: str | None                           | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline._normalize_identifiers        | self, df: pd.DataFrame, log: Any          | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline._normalize_journal            | value: Any, max_len: int                  | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline._normalize_numeric_fields     | self, df: pd.DataFrame, log: Any          | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline._normalize_string_fields      | self, df: pd.DataFrame, log: Any          | —                                                    | document: {'logging': ['log.debug'], 'io': []}
target: {}                                                             | document: []
target: []                 | только в document
ChemblDocumentPipeline._schema_column_specs          | self                                      | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline._should_enrich_document_terms | self                                      | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline.extract                       | self, *args, **kwargs                     | —                                                    | document: {'logging': ['UnifiedLogger.get'], 'io': []}
target: {}                                                     | document: []
target: []                 | только в document
ChemblDocumentPipeline.extract_all                   | self                                      | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
ChemblDocumentPipeline.extract_by_ids                | self, ids: Sequence[str]                  | —                                                    | document: {'logging': ['UnifiedLogger.get', 'log.info'], 'io': []}
target: {}                                         | document: []
target: []                 | только в document
ChemblDocumentPipeline.transform                     | self, df: pd.DataFrame                    | —                                                    | document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
target: {}             | document: []
target: []                 | только в document
ChemblDocumentPipeline.validate                      | self, df: pd.DataFrame                    | —                                                    | document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}
target: {}                            | document: []
target: []                 | только в document
ChemblTargetPipeline                                 | —                                         | —                                                    | document: {}
target: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']} | document: []
target: []                 | только в target  
ChemblTargetPipeline.__init__                        | —                                         | self, config: PipelineConfig, run_id: str            | document: {}
target: {'logging': [], 'io': []}                                                                        | document: []
target: []                 | только в target  
ChemblTargetPipeline._build_target_descriptor        | —                                         | self                                                 | document: {}
target: {'logging': ['log.info'], 'io': []}                                                              | document: []
target: []                 | только в target  
ChemblTargetPipeline._enrich_protein_classifications | —                                         | self, df: pd.DataFrame, log: Any                     | document: {}
target: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}                      | document: []
target: []                 | только в target  
ChemblTargetPipeline._enrich_target_components       | —                                         | self, df: pd.DataFrame, log: Any                     | document: {}
target: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}                      | document: []
target: []                 | только в target  
ChemblTargetPipeline._harmonize_identifier_columns   | —                                         | self, df: pd.DataFrame, log: Any                     | document: {}
target: {'logging': ['log.debug'], 'io': []}                                                             | document: []
target: []                 | только в target  
ChemblTargetPipeline._normalize_data_types           | —                                         | self, df: pd.DataFrame, schema: Any | None, log: Any | document: {}
target: {'logging': ['log.warning'], 'io': []}                                                           | document: []
target: []                 | только в target  
ChemblTargetPipeline._normalize_identifiers          | —                                         | self, df: pd.DataFrame, log: Any                     | document: {}
target: {'logging': ['log.warning'], 'io': []}                                                           | document: []
target: []                 | только в target  
ChemblTargetPipeline._normalize_string_fields        | —                                         | self, df: pd.DataFrame, log: Any                     | document: {}
target: {'logging': ['log.debug'], 'io': []}                                                             | document: []
target: []                 | только в target  
ChemblTargetPipeline.extract                         | —                                         | self, *args, **kwargs                                | document: {}
target: {'logging': ['UnifiedLogger.get'], 'io': []}                                                     | document: []
target: []                 | только в target  
ChemblTargetPipeline.extract_all                     | —                                         | self                                                 | document: {}
target: {'logging': [], 'io': []}                                                                        | document: []
target: []                 | только в target  
ChemblTargetPipeline.extract_by_ids                  | —                                         | self, ids: Sequence[str]                             | document: {}
target: {'logging': ['UnifiedLogger.get', 'log.info'], 'io': []}                                         | document: []
target: []                 | только в target  
ChemblTargetPipeline.transform                       | —                                         | self, df: pd.DataFrame                               | document: {}
target: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                            | document: []
target: []                 | только в target  
__module_block_0                                     | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_1                                     | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | совпадает        
__module_block_10                                    | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_11                                    | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_12                                    | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_13                                    | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_14                                    | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_15                                    | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_16                                    | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_17                                    | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_18                                    | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_19                                    | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_2                                     | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_20                                    | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {}                                                                        | document: []
target: []                 | только в document
__module_block_3                                     | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | совпадает        
__module_block_4                                     | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | совпадает        
__module_block_5                                     | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_6                                     | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_7                                     | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_8                                     | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       
__module_block_9                                     | —                                         | —                                                    | document: {'logging': [], 'io': []}
target: {'logging': [], 'io': []}                                                 | document: []
target: []                 | отличается       

_Показаны первые 20 горячих участков из 52._

#### Горячий участок 1

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:60-654
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,324 +0,0 @@

-class ChemblDocumentPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting document records from the ChEMBL API."
-    actor ="document_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch document payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_document.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="document_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all document records from ChEMBL using pagination."
-        return self .run_extract_all (self ._build_document_descriptor ())
-    def _build_document_descriptor (self :SelfChemblDocumentPipeline )->ChemblExtractionDescriptor [SelfChemblDocumentPipeline ]:
-        "Return the descriptor powering the shared extraction routine."
-        def _require_document_pipeline (pipeline :ChemblPipelineBase )->ChemblDocumentPipeline :
-            if isinstance (pipeline ,ChemblDocumentPipeline ):
-                return pipeline
-            msg ="ChemblDocumentPipeline instance required"
-            raise TypeError (msg )
-        def build_context (pipeline :SelfChemblDocumentPipeline ,source_config :Any ,log :BoundLogger )->ChemblExtractionContext :
-            document_pipeline =_require_document_pipeline (pipeline )
-            typed_source_config =source_config if isinstance (source_config ,DocumentSourceConfig )else DocumentSourceConfig .from_source_config (cast (Any ,source_config ))
-            base_url =document_pipeline ._resolve_base_url (typed_source_config .parameters )
-            http_client ,_ =document_pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-            chembl_client =ChemblClient (http_client )
-            document_client =ChemblDocumentClient (chembl_client ,batch_size =min (typed_source_config .batch_size ,25 ))
-            document_pipeline ._chembl_release =document_pipeline .fetch_chembl_release (chembl_client ,log )
-            select_fields =document_pipeline ._resolve_select_fields (cast (SourceConfig ,cast (Any ,typed_source_config )),default_fields =API_DOCUMENT_FIELDS )
-            context =ChemblExtractionContext (typed_source_config ,document_client )
-            context .chembl_client =chembl_client
-            context .select_fields =tuple (select_fields )if select_fields else None
-            context .chembl_release =document_pipeline ._chembl_release
-            return context
-        def empty_frame (_ :SelfChemblDocumentPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"document_chembl_id":pd .Series (dtype ="string")})
-        def record_transform (pipeline :SelfChemblDocumentPipeline ,payload :Mapping [str ,Any ],_ :ChemblExtractionContext )->Mapping [str ,Any ]:
-            document_pipeline =_require_document_pipeline (pipeline )
-            return document_pipeline ._extract_nested_fields (dict (payload ))
-        def summary_extra (pipeline :SelfChemblDocumentPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            _require_document_pipeline (pipeline )
-            page_size =context .page_size or 0
-            pages =0
-            if page_size >0 :
-                total_rows =int (df .shape [0 ])
-                pages =(total_rows +page_size -1 )//page_size
-            return {"pages":pages }
-        return ChemblExtractionDescriptor [SelfChemblDocumentPipeline ](name ="chembl_document",source_name ="chembl",source_config_factory =DocumentSourceConfig .from_source_config ,build_context =build_context ,id_column ="document_chembl_id",summary_event ="chembl_document.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =API_DOCUMENT_FIELDS ,record_transform =record_transform ,sort_by =("document_chembl_id",),empty_frame_factory =empty_frame ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract document records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of document_chembl_id values to extract (as strings).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted document records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =DocumentSourceConfig .from_source_config (source_raw )
-        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-        chembl_client =ChemblClient (http_client )
-        document_client =ChemblDocumentClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        resolved_select_fields =self ._resolve_select_fields (source_raw ,default_fields =list (API_DOCUMENT_FIELDS ))
-        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-        limit =self .config .cli .limit
-        def fetch_documents (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            if "original_paginate"not in context .extra :
-                original_paginate =chembl_client .paginate
-                def counted_paginate (*args :Any ,**kwargs :Any )->Any :
-                    context .increment_api_calls ()
-                    return original_paginate (*args ,**kwargs )
-                chembl_client .paginate =counted_paginate
-                context .extra ["original_paginate"]=original_paginate
-            iterator =document_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield self ._extract_nested_fields (dict (item ))
-        def finalize_context (context :BatchExtractionContext )->None :
-            original =context .extra .pop ("original_paginate",None )
-            if original is not None :
-                chembl_client .paginate =original
-            api_calls_value =context .stats .api_calls if context .stats .api_calls is not None else 0
-            override ={"batches":context .stats .batches ,"api_calls":api_calls_value ,"cache_hits":context .stats .cache_hits }
-            context .extra ["stats_attribute_override"]=override
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="document_chembl_id",fetcher =fetch_documents ,select_fields =merged_select_fields or None ,batch_size =document_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":merged_select_fields }if merged_select_fields else None ,chembl_release =self ._chembl_release ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats")
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_document.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =stats .batches ,api_calls =stats .api_calls ,cache_hits =stats .cache_hits )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw document data by normalizing fields and identifiers."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_numeric_fields (df ,log )
-        if self ._should_enrich_document_terms ():
-            df =self ._enrich_document_terms (df )
-        df =self ._add_system_fields (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if "document_chembl_id"in df .columns and df ["document_chembl_id"].duplicated ().any ():
-            initial_count =len (df )
-            df =df .sort_values (by =list (df .columns )).drop_duplicates (subset =["document_chembl_id"],keep ="first")
-            deduped_count =len (df )
-            if deduped_count <initial_count :
-                log .warning ("document_deduplication_applied",initial_count =initial_count ,deduped_count =deduped_count ,removed_count =initial_count -deduped_count )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against DocumentSchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        self ._check_document_id_uniqueness (df ,log )
-        validated =super ().validate (df )
-        log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =self .config .validation .coerce )
-        return validated
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize identifier fields (DOI, PMID)."
-        df =df .copy ()
-        if "doi"in df .columns :
-            df ["doi_clean"]=df ["doi"].apply (self ._normalize_doi )
-        if "pubmed_id"in df .columns :
-            df ["pubmed_id"]=pd .to_numeric (df ["pubmed_id"],errors ="coerce").astype ("Int64")
-        return df
-    @staticmethod
-    def _normalize_doi (doi :str |None )->str :
-        "Normalize DOI by removing prefixes and validating format."
-        if not doi :
-            return ""
-        if not isinstance (doi ,str ):
-            return ""
-        doi =doi .strip ().lower ()
-        for prefix in ["doi:","https://doi.org/","http://dx.doi.org/","http://doi.org/"]:
-            if doi .startswith (prefix ):
-                doi =doi [len (prefix ):]
-        doi =doi .strip ()
-        doi_pattern =re .compile ("^10\\.\\d{4,9}/\\S+$")
-        if doi_pattern .match (doi ):
-            return doi
-        return ""
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields (title, abstract, journal, authors)."
-        working_df =df .copy ()
-        rules ={"title":StringRule (max_length =1000 ),"abstract":StringRule (max_length =5000 )}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        if "journal"in normalized_df .columns :
-            journal_series :pd .Series [Any ]=normalized_df ["journal"]
-            normalized_df ["journal"]=journal_series .map (lambda value :self ._normalize_journal (value ))
-        if "authors"in normalized_df .columns :
-            def _to_author_tuple (item :object )->tuple [str ,int ]|None :
-                if not isinstance (item ,tuple ):
-                    return None
-                tuple_item =cast (tuple [object ,...],item )
-                if len (tuple_item )!=2 :
-                    return None
-                name_raw ,count_raw =tuple_item
-                if not isinstance (name_raw ,str ):
-                    return None
-                name_value :str =name_raw
-                if isinstance (count_raw ,Integral ):
-                    count_value =int (count_raw )
-                elif isinstance (count_raw ,Real ):
-                    float_value =float (count_raw )
-                    if not float_value .is_integer ():
-                        return None
-                    count_value =int (float_value )
-                else :
-                    return None
-                if count_value <0 :
-                    return None
-                return (name_value ,count_value )
-            def _author_name_from_tuple (data :tuple [str ,int ]|None )->str :
-                return data [0 ]if data is not None else ""
-            def _author_count_from_tuple (data :tuple [str ,int ]|None )->int :
-                return data [1 ]if data is not None else 0
-            authors_series :pd .Series [Any ]=normalized_df ["authors"]
-            normalized_result =authors_series .apply (self ._normalize_authors )
-            normalized_tuples =normalized_result .apply (_to_author_tuple )
-            normalized_df ["authors"]=normalized_tuples .apply (_author_name_from_tuple )
-            normalized_df ["authors_count"]=normalized_tuples .apply (_author_count_from_tuple )
-        return normalized_df
-    @staticmethod
-    def _normalize_journal (value :Any ,max_len :int =255 )->str :
-        "Trim and collapse whitespace for journal name."
-        if pd .isna (value ):
-            return ""
-        text =str (value )
-        text =re .sub ("\\s+"," ",text ).strip ()
-        return text [:max_len ]if len (text )>max_len else text
-    @staticmethod
-    def _normalize_authors (authors :Any ,separator :str =", ")->tuple [str ,int ]:
-        "Normalize author separators and count."
-        if pd .isna (authors ):
-            return ("",0 )
-        text =str (authors ).strip ()
-        text =re .sub (";",",",text )
-        text =re .sub ("\\s+"," ",text )
-        if not text :
-            return ("",0 )
-        parts =text .split (",")
-        parts =[p .strip ()for p in parts if p .strip ()]
-        return (separator .join (parts ),len (parts ))
-    def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize numeric fields (year)."
-        df =df .copy ()
-        if "year"in df .columns :
-            def _coerce_year (value :object )->int |None :
-                if value is None or value is pd .NA :
-                    return None
-                if isinstance (value ,Integral ):
-                    year_int =int (value )
-                elif isinstance (value ,Real ):
-                    float_value =float (value )
-                    if not float_value .is_integer ():
-                        return None
-                    year_int =int (float_value )
-                elif isinstance (value ,str ):
-                    stripped =value .strip ()
-                    if not stripped :
-                        return None
-                    if not stripped .isdigit ():
-                        return None
-                    year_int =int (stripped )
-                else :
-                    return None
-                if 1500 <=year_int <=2100 :
-                    return year_int
-                return None
-            normalized_year =df ["year"].apply (_coerce_year )
-            df ["year"]=normalized_year .astype ("Int64")
-        return df
-    def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Add document-specific system fields (source)."
-        df =df .copy ()
-        df ["source"]="ChEMBL"
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        specs ["source"]={"default":"ChEMBL"}
-        specs ["authors_count"]={"default":0 ,"dtype":"Int64"}
-        hashing_config =self .config .determinism .hashing
-        business_key_column =hashing_config .business_key_column
-        row_hash_column =hashing_config .row_hash_column
-        if business_key_column :
-            specs [business_key_column ]={"default":""}
-        if row_hash_column :
-            specs [row_hash_column ]={"default":""}
-        return specs
-    def _check_document_id_uniqueness (self ,df :pd .DataFrame ,log :Any )->None :
-        "Check that document_chembl_id is unique."
-        if df .empty :
-            return
-        if "document_chembl_id"not in df .columns :
-            return
-        duplicates =df ["document_chembl_id"].duplicated ()
-        if duplicates .any ():
-            duplicate_ids =df [df ["document_chembl_id"].duplicated ()]["document_chembl_id"].unique ().tolist ()
-            log .warning ("document_id_duplicates",duplicate_count =duplicates .sum (),duplicate_ids =duplicate_ids [:10 ])
-    def _should_enrich_document_terms (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 document_term \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            document_section :Any =chembl_section .get ("document")
-            if not isinstance (document_section ,Mapping ):
-                return False
-            document_section =cast (Mapping [str ,Any ],document_section )
-            enrich_section :Any =document_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            document_term_section :Any =enrich_section .get ("document_term")
-            if not isinstance (document_term_section ,Mapping ):
-                return False
-            document_term_section =cast (Mapping [str ,Any ],document_term_section )
-            enabled :Any =document_term_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_document_terms (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                document_section :Any =chembl_section .get ("document")
-                if isinstance (document_section ,Mapping ):
-                    document_section =cast (Mapping [str ,Any ],document_section )
-                    enrich_section :Any =document_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        document_term_section :Any =enrich_section .get ("document_term")
-                        if isinstance (document_term_section ,Mapping ):
-                            document_term_section =cast (Mapping [str ,Any ],document_term_section )
-                            enrich_cfg =dict (document_term_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =DocumentSourceConfig .from_source_config (source_raw )
-        api_client ,_ =self .prepare_chembl_client ("chembl",base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters ))),client_name ="chembl_enrichment_client")
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_document_terms (df ,chembl_client ,enrich_cfg )
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested objects in document records."
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        "Coerce payload to dictionary mapping."
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
```

#### Горячий участок 2

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:65-67
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,3 +0,0 @@

-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
```

#### Горячий участок 3

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:532-539
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,5 +0,0 @@

-def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Add document-specific system fields (source)."
-    df =df .copy ()
-    df ["source"]="ChEMBL"
-    return df
```

#### Горячий участок 4

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:90-178
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,35 +0,0 @@

-def _build_document_descriptor (self :SelfChemblDocumentPipeline )->ChemblExtractionDescriptor [SelfChemblDocumentPipeline ]:
-    "Return the descriptor powering the shared extraction routine."
-    def _require_document_pipeline (pipeline :ChemblPipelineBase )->ChemblDocumentPipeline :
-        if isinstance (pipeline ,ChemblDocumentPipeline ):
-            return pipeline
-        msg ="ChemblDocumentPipeline instance required"
-        raise TypeError (msg )
-    def build_context (pipeline :SelfChemblDocumentPipeline ,source_config :Any ,log :BoundLogger )->ChemblExtractionContext :
-        document_pipeline =_require_document_pipeline (pipeline )
-        typed_source_config =source_config if isinstance (source_config ,DocumentSourceConfig )else DocumentSourceConfig .from_source_config (cast (Any ,source_config ))
-        base_url =document_pipeline ._resolve_base_url (typed_source_config .parameters )
-        http_client ,_ =document_pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-        chembl_client =ChemblClient (http_client )
-        document_client =ChemblDocumentClient (chembl_client ,batch_size =min (typed_source_config .batch_size ,25 ))
-        document_pipeline ._chembl_release =document_pipeline .fetch_chembl_release (chembl_client ,log )
-        select_fields =document_pipeline ._resolve_select_fields (cast (SourceConfig ,cast (Any ,typed_source_config )),default_fields =API_DOCUMENT_FIELDS )
-        context =ChemblExtractionContext (typed_source_config ,document_client )
-        context .chembl_client =chembl_client
-        context .select_fields =tuple (select_fields )if select_fields else None
-        context .chembl_release =document_pipeline ._chembl_release
-        return context
-    def empty_frame (_ :SelfChemblDocumentPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"document_chembl_id":pd .Series (dtype ="string")})
-    def record_transform (pipeline :SelfChemblDocumentPipeline ,payload :Mapping [str ,Any ],_ :ChemblExtractionContext )->Mapping [str ,Any ]:
-        document_pipeline =_require_document_pipeline (pipeline )
-        return document_pipeline ._extract_nested_fields (dict (payload ))
-    def summary_extra (pipeline :SelfChemblDocumentPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        _require_document_pipeline (pipeline )
-        page_size =context .page_size or 0
-        pages =0
-        if page_size >0 :
-            total_rows =int (df .shape [0 ])
-            pages =(total_rows +page_size -1 )//page_size
-        return {"pages":pages }
-    return ChemblExtractionDescriptor [SelfChemblDocumentPipeline ](name ="chembl_document",source_name ="chembl",source_config_factory =DocumentSourceConfig .from_source_config ,build_context =build_context ,id_column ="document_chembl_id",summary_event ="chembl_document.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =API_DOCUMENT_FIELDS ,record_transform =record_transform ,sort_by =("document_chembl_id",),empty_frame_factory =empty_frame ,summary_extra =summary_extra )
```

#### Горячий участок 5

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:557-572
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,10 +0,0 @@

-def _check_document_id_uniqueness (self ,df :pd .DataFrame ,log :Any )->None :
-    "Check that document_chembl_id is unique."
-    if df .empty :
-        return
-    if "document_chembl_id"not in df .columns :
-        return
-    duplicates =df ["document_chembl_id"].duplicated ()
-    if duplicates .any ():
-        duplicate_ids =df [df ["document_chembl_id"].duplicated ()]["document_chembl_id"].unique ().tolist ()
-        log .warning ("document_id_duplicates",duplicate_count =duplicates .sum (),duplicate_ids =duplicate_ids [:10 ])
```

#### Горячий участок 6

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:650-654
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,6 +0,0 @@

-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    "Coerce payload to dictionary mapping."
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Горячий участок 7

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:597-641
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,24 +0,0 @@

-def _enrich_document_terms (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            document_section :Any =chembl_section .get ("document")
-            if isinstance (document_section ,Mapping ):
-                document_section =cast (Mapping [str ,Any ],document_section )
-                enrich_section :Any =document_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    document_term_section :Any =enrich_section .get ("document_term")
-                    if isinstance (document_term_section ,Mapping ):
-                        document_term_section =cast (Mapping [str ,Any ],document_term_section )
-                        enrich_cfg =dict (document_term_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =DocumentSourceConfig .from_source_config (source_raw )
-    api_client ,_ =self .prepare_chembl_client ("chembl",base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters ))),client_name ="chembl_enrichment_client")
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_document_terms (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 8

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:643-647
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,3 +0,0 @@

-def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-    "Extract fields from nested objects in document records."
-    return record
```

#### Горячий участок 9

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:484-495
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,13 +0,0 @@

-@staticmethod
-def _normalize_authors (authors :Any ,separator :str =", ")->tuple [str ,int ]:
-    "Normalize author separators and count."
-    if pd .isna (authors ):
-        return ("",0 )
-    text =str (authors ).strip ()
-    text =re .sub (";",",",text )
-    text =re .sub ("\\s+"," ",text )
-    if not text :
-        return ("",0 )
-    parts =text .split (",")
-    parts =[p .strip ()for p in parts if p .strip ()]
-    return (separator .join (parts ),len (parts ))
```

#### Горячий участок 10

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:392-408
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,16 +0,0 @@

-@staticmethod
-def _normalize_doi (doi :str |None )->str :
-    "Normalize DOI by removing prefixes and validating format."
-    if not doi :
-        return ""
-    if not isinstance (doi ,str ):
-        return ""
-    doi =doi .strip ().lower ()
-    for prefix in ["doi:","https://doi.org/","http://dx.doi.org/","http://doi.org/"]:
-        if doi .startswith (prefix ):
-            doi =doi [len (prefix ):]
-    doi =doi .strip ()
-    doi_pattern =re .compile ("^10\\.\\d{4,9}/\\S+$")
-    if doi_pattern .match (doi ):
-        return doi
-    return ""
```

#### Горячий участок 11

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:377-389
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,8 +0,0 @@

-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize identifier fields (DOI, PMID)."
-    df =df .copy ()
-    if "doi"in df .columns :
-        df ["doi_clean"]=df ["doi"].apply (self ._normalize_doi )
-    if "pubmed_id"in df .columns :
-        df ["pubmed_id"]=pd .to_numeric (df ["pubmed_id"],errors ="coerce").astype ("Int64")
-    return df
```

#### Горячий участок 12

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:475-481
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,8 +0,0 @@

-@staticmethod
-def _normalize_journal (value :Any ,max_len :int =255 )->str :
-    "Trim and collapse whitespace for journal name."
-    if pd .isna (value ):
-        return ""
-    text =str (value )
-    text =re .sub ("\\s+"," ",text ).strip ()
-    return text [:max_len ]if len (text )>max_len else text
```

#### Горячий участок 13

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:497-530
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,29 +0,0 @@

-def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize numeric fields (year)."
-    df =df .copy ()
-    if "year"in df .columns :
-        def _coerce_year (value :object )->int |None :
-            if value is None or value is pd .NA :
-                return None
-            if isinstance (value ,Integral ):
-                year_int =int (value )
-            elif isinstance (value ,Real ):
-                float_value =float (value )
-                if not float_value .is_integer ():
-                    return None
-                year_int =int (float_value )
-            elif isinstance (value ,str ):
-                stripped =value .strip ()
-                if not stripped :
-                    return None
-                if not stripped .isdigit ():
-                    return None
-                year_int =int (stripped )
-            else :
-                return None
-            if 1500 <=year_int <=2100 :
-                return year_int
-            return None
-        normalized_year =df ["year"].apply (_coerce_year )
-        df ["year"]=normalized_year .astype ("Int64")
-    return df
```

#### Горячий участок 14

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:410-472
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,43 +0,0 @@

-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields (title, abstract, journal, authors)."
-    working_df =df .copy ()
-    rules ={"title":StringRule (max_length =1000 ),"abstract":StringRule (max_length =5000 )}
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    if "journal"in normalized_df .columns :
-        journal_series :pd .Series [Any ]=normalized_df ["journal"]
-        normalized_df ["journal"]=journal_series .map (lambda value :self ._normalize_journal (value ))
-    if "authors"in normalized_df .columns :
-        def _to_author_tuple (item :object )->tuple [str ,int ]|None :
-            if not isinstance (item ,tuple ):
-                return None
-            tuple_item =cast (tuple [object ,...],item )
-            if len (tuple_item )!=2 :
-                return None
-            name_raw ,count_raw =tuple_item
-            if not isinstance (name_raw ,str ):
-                return None
-            name_value :str =name_raw
-            if isinstance (count_raw ,Integral ):
-                count_value =int (count_raw )
-            elif isinstance (count_raw ,Real ):
-                float_value =float (count_raw )
-                if not float_value .is_integer ():
-                    return None
-                count_value =int (float_value )
-            else :
-                return None
-            if count_value <0 :
-                return None
-            return (name_value ,count_value )
-        def _author_name_from_tuple (data :tuple [str ,int ]|None )->str :
-            return data [0 ]if data is not None else ""
-        def _author_count_from_tuple (data :tuple [str ,int ]|None )->int :
-            return data [1 ]if data is not None else 0
-        authors_series :pd .Series [Any ]=normalized_df ["authors"]
-        normalized_result =authors_series .apply (self ._normalize_authors )
-        normalized_tuples =normalized_result .apply (_to_author_tuple )
-        normalized_df ["authors"]=normalized_tuples .apply (_author_name_from_tuple )
-        normalized_df ["authors_count"]=normalized_tuples .apply (_author_count_from_tuple )
-    return normalized_df
```

#### Горячий участок 15

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:541-555
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,12 +0,0 @@

-def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-    specs =dict (super ()._schema_column_specs ())
-    specs ["source"]={"default":"ChEMBL"}
-    specs ["authors_count"]={"default":0 ,"dtype":"Int64"}
-    hashing_config =self .config .determinism .hashing
-    business_key_column =hashing_config .business_key_column
-    row_hash_column =hashing_config .row_hash_column
-    if business_key_column :
-        specs [business_key_column ]={"default":""}
-    if row_hash_column :
-        specs [row_hash_column ]={"default":""}
-    return specs
```

#### Горячий участок 16

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:574-595
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,22 +0,0 @@

-def _should_enrich_document_terms (self )->bool :
-    "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 document_term \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-    if not self .config .chembl :
-        return False
-    try :
-        chembl_section =self .config .chembl
-        document_section :Any =chembl_section .get ("document")
-        if not isinstance (document_section ,Mapping ):
-            return False
-        document_section =cast (Mapping [str ,Any ],document_section )
-        enrich_section :Any =document_section .get ("enrich")
-        if not isinstance (enrich_section ,Mapping ):
-            return False
-        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-        document_term_section :Any =enrich_section .get ("document_term")
-        if not isinstance (document_term_section ,Mapping ):
-            return False
-        document_term_section =cast (Mapping [str ,Any ],document_term_section )
-        enabled :Any =document_term_section .get ("enabled")
-        return bool (enabled )if enabled is not None else False
-    except (AttributeError ,KeyError ,TypeError ):
-        return False
```

#### Горячий участок 17

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:69-83
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,4 +0,0 @@

-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch document payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_document.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="document_chembl_id")
```

#### Горячий участок 18

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:85-88
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,3 +0,0 @@

-def extract_all (self )->pd .DataFrame :
-    "Extract all document records from ChEMBL using pagination."
-    return self .run_extract_all (self ._build_document_descriptor ())
```

#### Горячий участок 19

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:180-280
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,36 +0,0 @@

-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract document records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of document_chembl_id values to extract (as strings).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted document records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =DocumentSourceConfig .from_source_config (source_raw )
-    base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-    http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-    chembl_client =ChemblClient (http_client )
-    document_client =ChemblDocumentClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-    self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-    resolved_select_fields =self ._resolve_select_fields (source_raw ,default_fields =list (API_DOCUMENT_FIELDS ))
-    merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-    limit =self .config .cli .limit
-    def fetch_documents (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        if "original_paginate"not in context .extra :
-            original_paginate =chembl_client .paginate
-            def counted_paginate (*args :Any ,**kwargs :Any )->Any :
-                context .increment_api_calls ()
-                return original_paginate (*args ,**kwargs )
-            chembl_client .paginate =counted_paginate
-            context .extra ["original_paginate"]=original_paginate
-        iterator =document_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield self ._extract_nested_fields (dict (item ))
-    def finalize_context (context :BatchExtractionContext )->None :
-        original =context .extra .pop ("original_paginate",None )
-        if original is not None :
-            chembl_client .paginate =original
-        api_calls_value =context .stats .api_calls if context .stats .api_calls is not None else 0
-        override ={"batches":context .stats .batches ,"api_calls":api_calls_value ,"cache_hits":context .stats .cache_hits }
-        context .extra ["stats_attribute_override"]=override
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="document_chembl_id",fetcher =fetch_documents ,select_fields =merged_select_fields or None ,batch_size =document_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":merged_select_fields }if merged_select_fields else None ,chembl_release =self ._chembl_release ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats")
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_document.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =stats .batches ,api_calls =stats .api_calls ,cache_hits =stats .cache_hits )
-    return dataframe
```

#### Горячий участок 20

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:282-336
- target: нет в ветке

```diff
--- document:run.py

+++ target:run.py

@@ -1,24 +0,0 @@

-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw document data by normalizing fields and identifiers."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-    df =df .copy ()
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._normalize_numeric_fields (df ,log )
-    if self ._should_enrich_document_terms ():
-        df =self ._enrich_document_terms (df )
-    df =self ._add_system_fields (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if "document_chembl_id"in df .columns and df ["document_chembl_id"].duplicated ().any ():
-        initial_count =len (df )
-        df =df .sort_values (by =list (df .columns )).drop_duplicates (subset =["document_chembl_id"],keep ="first")
-        deduped_count =len (df )
-        if deduped_count <initial_count :
-            log .warning ("document_deduplication_applied",initial_count =initial_count ,deduped_count =deduped_count ,removed_count =initial_count -deduped_count )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

### Модуль transform.py

Определение                              | document сигнатура | target сигнатура              | Побочные эффекты                                           | Исключения              | Статус         
-----------------------------------------|--------------------|-------------------------------|------------------------------------------------------------|-------------------------|----------------
__module_block_0                         | —                  | —                             | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
__module_block_1                         | —                  | —                             | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
__module_block_10                        | —                  | —                             | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
__module_block_2                         | —                  | —                             | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
__module_block_3                         | —                  | —                             | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
__module_block_4                         | —                  | —                             | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
__module_block_5                         | —                  | —                             | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
__module_block_6                         | —                  | —                             | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
__module_block_7                         | —                  | —                             | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
__module_block_8                         | —                  | —                             | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
__module_block_9                         | —                  | —                             | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
_collect_dicts                           | —                  | source: Any                   | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
_is_iterable_of_objects                  | —                  | value: Any                    | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
_is_json_dict                            | —                  | value: Any                    | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
extract_and_serialize_component_synonyms | —                  | target_components: Any        | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target
flatten_target_components                | —                  | rec: dict[str, Any]           | document: {}
target: {'logging': [], 'io': ['json.dumps']} | document: []
target: [] | только в target
serialize_target_arrays                  | —                  | df: pd.DataFrame, config: Any | document: {}
target: {'logging': [], 'io': []}             | document: []
target: [] | только в target

#### Горячий участок 1

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:1-1

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+"Transform utilities for ChEMBL target pipeline array serialization."
```

#### Горячий участок 2

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:3-3

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+from __future__ import annotations
```

#### Горячий участок 3

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:22-22

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+JsonDict =dict [str ,Any ]
```

#### Горячий участок 4

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:13-13

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+from bioetl .core .serialization import header_rows_serialize
```

#### Горячий участок 5

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:6-6

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+from collections .abc import Iterable
```

#### Горячий участок 6

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:7-7

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+from typing import Any ,TypeGuard ,cast
```

#### Горячий участок 7

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:5-5

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+import json
```

#### Горячий участок 8

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:9-9

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+import numpy as np
```

#### Горячий участок 9

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:10-10

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+import numpy .typing as npt
```

#### Горячий участок 10

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:11-11

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+import pandas as pd
```

#### Горячий участок 11

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:15-19

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1 @@

+__all__ =["serialize_target_arrays","extract_and_serialize_component_synonyms","flatten_target_components"]
```

#### Горячий участок 12

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:33-47

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1,12 @@

+def _collect_dicts (source :Any )->list [JsonDict ]:
+    "Collect dictionary entries from arbitrary source keeping order."
+    result :list [JsonDict ]=[]
+    if _is_json_dict (source ):
+        result .append (source )
+        return result
+    if _is_iterable_of_objects (source ):
+        for element in source :
+            element_any :Any =element
+            if _is_json_dict (element_any ):
+                result .append (element_any )
+    return result
```

#### Горячий участок 13

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:29-30

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1,2 @@

+def _is_iterable_of_objects (value :Any )->TypeGuard [Iterable [Any ]]:
+    return isinstance (value ,Iterable )and (not isinstance (value ,(str ,bytes )))
```

#### Горячий участок 14

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:25-26

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1,2 @@

+def _is_json_dict (value :Any )->TypeGuard [JsonDict ]:
+    return isinstance (value ,dict )
```

#### Горячий участок 15

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:133-162

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1,15 @@

+def extract_and_serialize_component_synonyms (target_components :Any )->str :
+    "Extract target_component_synonyms from target_components and serialize.\n\n    Parameters\n    ----------\n    target_components:\n        List of target component dicts, None, or empty list.\n\n    Returns\n    -------\n    str:\n        Serialized string in header+rows format, or empty string for None/empty.\n    "
+    if target_components is None :
+        return ""
+    components :list [dict [str ,Any ]]=_collect_dicts (target_components )
+    if not components :
+        return ""
+    all_synonyms :list [dict [str ,Any ]]=[]
+    for component in components :
+        syns_item :Any =component .get ("target_component_synonyms")
+        if syns_item :
+            all_synonyms .extend (_collect_dicts (syns_item ))
+    if not all_synonyms :
+        return ""
+    return header_rows_serialize (all_synonyms )
```

#### Горячий участок 16

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:50-130

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1,34 @@

+def flatten_target_components (rec :dict [str ,Any ])->dict [str ,Any ]:
+    "Flatten nested target_components data into flat columns.\n\n    Extracts:\n    - uniprot_accessions from target_components[*].accession\n    - target_component_synonyms__flat from target_components[*].target_component_synonyms[*].component_synonym\n    - target_components__flat (serialized container)\n    - cross_references__flat (serialized from top-level)\n    - component_count (counted from accessions or from top-level)\n\n    Parameters\n    ----------\n    rec:\n        Target record dict from ChEMBL API.\n\n    Returns\n    -------\n    dict[str, Any]:\n        Dictionary with flattened fields:\n        - uniprot_accessions: sorted list of unique UniProt accessions (as JSON string)\n        - target_component_synonyms__flat: serialized synonyms\n        - target_components__flat: serialized components\n        - cross_references__flat: serialized cross-references\n        - component_count: count of unique accessions\n    "
+    result :dict [str ,Any ]={"uniprot_accessions":"","target_component_synonyms__flat":"","target_components__flat":"","cross_references__flat":"","component_count":None }
+    comps_raw :Any =rec .get ("target_components")or []
+    comps :list [dict [str ,Any ]]=_collect_dicts (comps_raw )
+    accessions :list [str ]=[]
+    all_synonyms :list [dict [str ,Any ]]=[]
+    for component in comps :
+        accession :Any =component .get ("accession")
+        if isinstance (accession ,str )and accession .strip ():
+            accessions .append (accession .strip ())
+        syns :Any =component .get ("target_component_synonyms")
+        if syns :
+            all_synonyms .extend (_collect_dicts (syns ))
+    unique_accessions =sorted (set (accessions ))
+    if unique_accessions :
+        result ["uniprot_accessions"]=json .dumps (unique_accessions ,ensure_ascii =False )
+        result ["component_count"]=len (unique_accessions )
+    else :
+        top_level_count =rec .get ("component_count")
+        if top_level_count is not None :
+            try :
+                result ["component_count"]=int (top_level_count )
+            except (ValueError ,TypeError ):
+                result ["component_count"]=None
+    if all_synonyms :
+        result ["target_component_synonyms__flat"]=header_rows_serialize (all_synonyms )
+    if comps :
+        result ["target_components__flat"]=header_rows_serialize (comps )
+    xrefs_raw :Any =rec .get ("cross_references")or []
+    xrefs :list [dict [str ,Any ]]=_collect_dicts (xrefs_raw )
+    if xrefs :
+        result ["cross_references__flat"]=header_rows_serialize (xrefs )
+    return result
```

#### Горячий участок 17

- document: нет в ветке
- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:165-251

```diff
--- document:transform.py

+++ target:transform.py

@@ -0,0 +1,46 @@

+def serialize_target_arrays (df :pd .DataFrame ,config :Any )->pd .DataFrame :
+    "Serialize array fields for target pipeline.\n\n    Uses flatten_target_components() to extract and serialize nested data\n    from target_components and cross_references.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    config:\n        Pipeline config with transform.arrays_to_header_rows.\n\n    Returns\n    -------\n    pd.DataFrame:\n        DataFrame with serialized array fields.\n    "
+    df =df .copy ()
+    arrays_to_serialize :list [str ]=[]
+    try :
+        if hasattr (config ,"transform")and config .transform is not None :
+            if hasattr (config .transform ,"arrays_to_header_rows"):
+                arrays_to_serialize =list (config .transform .arrays_to_header_rows )
+    except (AttributeError ,TypeError ):
+        pass
+    if not df .empty :
+        flattened_data :list [dict [str ,Any ]]=[]
+        for _ ,row in df .iterrows ():
+            row_dict :dict [str ,Any ]=row .to_dict ()
+            for key ,value in row_dict .items ():
+                if isinstance (value ,np .ndarray ):
+                    array_value =cast (npt .NDArray [Any ],value )
+                    if array_value .size ==0 :
+                        row_dict [key ]=None
+                    else :
+                        try :
+                            nan_mask =np .asarray (pd .isna (array_value ),dtype =bool )
+                            if bool (np .all (nan_mask )):
+                                row_dict [key ]=None
+                        except (TypeError ,ValueError ):
+                            pass
+                elif pd .api .types .is_scalar (value ):
+                    try :
+                        if pd .isna (value ):
+                            row_dict [key ]=None
+                    except (TypeError ,ValueError ):
+                        pass
+            flattened =flatten_target_components (row_dict )
+            row_dict .update (flattened )
+            flattened_data .append (row_dict )
+        df =pd .DataFrame (flattened_data )
+    else :
+        df ["cross_references__flat"]=""
+        df ["target_components__flat"]=""
+        df ["target_component_synonyms__flat"]=""
+        df ["uniprot_accessions"]=""
+        df ["component_count"]=None
+    for col in arrays_to_serialize :
+        if col in df .columns and f'{col }__flat'in df .columns :
+            df =df .drop (columns =[col ])
+    return df
```

### Модуль normalize.py

Определение                | document сигнатура                                                  | target сигнатура | Побочные эффекты                                                                                          | Исключения              | Статус           
---------------------------|---------------------------------------------------------------------|------------------|-----------------------------------------------------------------------------------------------------------|-------------------------|------------------
__module_block_0           | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_1           | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_10          | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_11          | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_13          | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_2           | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_3           | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_4           | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_5           | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_6           | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_7           | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_8           | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
__module_block_9           | —                                                                   | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
_escape_pipe               | value: str | Any                                                    | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
_is_numeric                | value: Any                                                          | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
aggregate_terms            | rows: Iterable[dict[str, Any]], sort: str                           | —                | document: {'logging': [], 'io': []}
target: {}                                                            | document: []
target: [] | только в document
enrich_with_document_terms | df_docs: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | —                | document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
target: {} | document: []
target: [] | только в document

#### Горячий участок 1

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:1-1
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-"Enrichment functions for Document pipeline."
```

#### Горячий участок 2

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:3-3
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from __future__ import annotations
```

#### Горячий участок 3

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:16-16
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-__all__ =["enrich_with_document_terms","aggregate_terms","_escape_pipe"]
```

#### Горячий участок 4

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:19-19
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-_ensure_columns =ensure_columns
```

#### Горячий участок 5

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:46-49
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-_DOCUMENT_TERM_COLUMNS :tuple [tuple [str ,str ],...]=(("term","string"),("weight","string"))
```

#### Горячий участок 6

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:11-11
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .clients .chembl import ChemblClient
```

#### Горячий участок 7

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:12-12
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .frame import ensure_columns
```

#### Горячий участок 8

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:13-13
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .logger import UnifiedLogger
```

#### Горячий участок 9

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:14-14
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from bioetl .schemas .document import DOCUMENT_TERMS_ENRICHMENT_SCHEMA
```

#### Горячий участок 10

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:5-5
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from collections import defaultdict
```

#### Горячий участок 11

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:6-6
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from collections .abc import Iterable ,Mapping
```

#### Горячий участок 12

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:7-7
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-from typing import Any
```

#### Горячий участок 13

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:9-9
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1 +0,0 @@

-import pandas as pd
```

#### Горячий участок 14

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:22-43
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1,8 +0,0 @@

-def _escape_pipe (value :str |Any )->str :
-    "Escape pipe and backslash delimiters in string values.\n\n    Parameters\n    ----------\n    value:\n        Input value to escape. ``None`` \u0438 NA \u043f\u0440\u0438\u0432\u043e\u0434\u044f\u0442\u0441\u044f \u043a \u043f\u0443\u0441\u0442\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435.\n\n    Returns\n    -------\n    str:\n        \u0421\u0442\u0440\u043e\u043a\u0430 \u0441 \u044d\u043a\u0440\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u0435\u043b\u044f\u043c\u0438: ``|`` \u2192 ``\\|``, ``\\`` \u2192 ``\\\\``.\n    "
-    if value is None or pd .isna (value ):
-        return ""
-    text =str (value )
-    if not text :
-        return ""
-    return text .replace ("\\","\\\\").replace ("|","\\|")
```

#### Горячий участок 15

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:122-128
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1,7 +0,0 @@

-def _is_numeric (value :Any )->bool :
-    "Check if value can be converted to float."
-    try :
-        float (value )
-        return True
-    except (ValueError ,TypeError ):
-        return False
```

#### Горячий участок 16

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:52-119
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1,26 +0,0 @@

-def aggregate_terms (rows :Iterable [dict [str ,Any ]],sort :str ="weight_desc")->dict [str ,dict [str ,str ]]:
-    "Aggregate document terms by document_chembl_id.\n\n    Parameters\n    ----------\n    rows:\n        Iterable of document_term records, each with 'document_chembl_id', 'term', 'weight'.\n    sort:\n        Sort order: 'weight_desc' (default) sorts by weight descending, None preserves order.\n\n    Returns\n    -------\n    dict[str, dict[str, str]]:\n        Dictionary keyed by document_chembl_id -> {'term': 't1|t2|...', 'weight': 'w1|w2|...'}.\n        Terms and weights are serialized with \"|\" separator, order is synchronized.\n    "
-    bucket :dict [str ,list [tuple [str ,Any ]]]=defaultdict (list )
-    for r in rows :
-        did =r .get ("document_chembl_id")
-        if not did :
-            continue
-        term_value =r .get ("term")
-        weight_value =r .get ("weight")
-        term_str =str (term_value )if term_value is not None else ""
-        bucket [did ].append ((term_str ,weight_value ))
-    result :dict [str ,dict [str ,str ]]={}
-    for did ,items in bucket .items ():
-        if sort =="weight_desc":
-            items .sort (key =lambda x :float (x [1 ])if x [1 ]not in (None ,"")and _is_numeric (x [1 ])else float ("-inf"),reverse =True )
-        terms_list :list [str ]=[]
-        weights_list :list [str ]=[]
-        for term ,weight in items :
-            escaped_term =_escape_pipe (term or "")
-            terms_list .append (escaped_term )
-            if weight in (None ,""):
-                weights_list .append ("")
-            else :
-                weights_list .append (str (weight ))
-        result [did ]={"term":"|".join (terms_list ),"weight":"|".join (weights_list )}
-    return result
```

#### Горячий участок 17

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:131-294
- target: нет в ветке

```diff
--- document:normalize.py

+++ target:normalize.py

@@ -1,76 +0,0 @@

-def enrich_with_document_terms (df_docs :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term.\n\n    Parameters\n    ----------\n    df_docs:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c document_chembl_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.document.enrich.document_term.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438:\n        - term (nullable string, pipe-separated terms)\n        - weight (nullable string, pipe-separated weights)\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="document_enrichment")
-    def _ensure_term_columns (df_input :pd .DataFrame )->pd .DataFrame :
-        result_frame =df_input .copy ()
-        for column_name in ("term","weight"):
-            if column_name not in result_frame .columns :
-                result_frame [column_name ]=pd .Series ([""for _ in range (len (result_frame ))],index =result_frame .index ,dtype ="string")
-            else :
-                result_frame [column_name ]=result_frame [column_name ].astype ("string")
-            na_mask =result_frame [column_name ].isna ()
-            if bool (na_mask .any ()):
-                result_frame .loc [na_mask ,column_name ]=""
-            result_frame [column_name ]=result_frame [column_name ].astype ("string")
-        return result_frame
-    df_docs =_ensure_columns (df_docs ,_DOCUMENT_TERM_COLUMNS )
-    if df_docs .empty :
-        log .debug ("enrichment_skipped_empty_dataframe")
-        prepared_empty =_ensure_term_columns (df_docs )
-        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (prepared_empty ,lazy =True )
-    required_cols =["document_chembl_id"]
-    missing_cols =[col for col in required_cols if col not in df_docs .columns ]
-    if missing_cols :
-        log .warning ("enrichment_skipped_missing_columns",missing_columns =missing_cols )
-        prepared_missing =_ensure_term_columns (df_docs )
-        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (prepared_missing ,lazy =True )
-    doc_ids :list [str ]=[]
-    for _ ,row in df_docs .iterrows ():
-        doc_id =row .get ("document_chembl_id")
-        if pd .isna (doc_id )or doc_id is None :
-            continue
-        doc_id_str =str (doc_id ).strip ()
-        if doc_id_str :
-            doc_ids .append (doc_id_str )
-    if not doc_ids :
-        log .debug ("enrichment_skipped_no_valid_ids")
-        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (df_docs ,lazy =True )
-    fields =cfg .get ("select_fields",["document_chembl_id","term","weight"])
-    page_limit =cfg .get ("page_limit",1000 )
-    sort =cfg .get ("sort","weight_desc")
-    log .info ("enrichment_fetching_terms",ids_count =len (set (doc_ids )))
-    records_dict =client .fetch_document_terms_by_ids (ids =doc_ids ,fields =list (fields ),page_limit =page_limit )
-    all_records :list [dict [str ,Any ]]=[]
-    for records_list in records_dict .values ():
-        all_records .extend (records_list )
-    agg_result =aggregate_terms (all_records ,sort =sort )
-    enrichment_data :list [dict [str ,Any ]]=[]
-    for doc_id ,term_weight in agg_result .items ():
-        enrichment_data .append ({"document_chembl_id":doc_id ,"term":term_weight ["term"],"weight":term_weight ["weight"]})
-    if not enrichment_data :
-        log .debug ("enrichment_no_records_found")
-        prepared =_ensure_term_columns (df_docs )
-        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (prepared ,lazy =True )
-    df_enrich =pd .DataFrame (enrichment_data )
-    original_index =df_docs .index .copy ()
-    df_result =df_docs .merge (df_enrich ,on =["document_chembl_id"],how ="left",suffixes =("","_enrich"))
-    for col in ["term","weight"]:
-        enrich_col =f'{col }_enrich'
-        if enrich_col in df_result .columns :
-            base_series =df_result [col ]if col in df_result .columns else pd .Series ([pd .NA ]*len (df_result ),index =df_result .index ,dtype ="object")
-            enrich_series =df_result [enrich_col ]
-            df_result [col ]=base_series .where (pd .notna (base_series ),enrich_series )
-            df_result =df_result .drop (columns =[enrich_col ])
-    for col in ["term","weight"]:
-        if col not in df_result .columns :
-            df_result [col ]=pd .Series ([""for _ in range (len (df_result ))],index =df_result .index ,dtype ="string")
-        else :
-            df_result [col ]=df_result [col ].astype ("string")
-        na_mask =df_result [col ].isna ()
-        if bool (na_mask .any ()):
-            df_result .loc [na_mask ,col ]=""
-        df_result [col ]=df_result [col ].astype ("string")
-    df_result =df_result .reindex (original_index )
-    df_result =_ensure_term_columns (df_result )
-    log .info ("enrichment_completed",rows_enriched =df_result .shape [0 ],documents_with_terms =len (agg_result ))
-    return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (df_result ,lazy =True )
```

---

## Пара: document ↔ testitem

- AST hash: 3774d930e14eb5befd7fdffc255cb346 ↔ 729263c98934cfcb08473bcacfb20e9e

- Jaccard по токенам: 0.307

### Модуль run.py

Определение                                          | document сигнатура                        | testitem сигнатура                                                           | Побочные эффекты                                                                                                                                                       | Исключения                                | Статус           
-----------------------------------------------------|-------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------|------------------
ChemblDocumentPipeline                               | —                                         | —                                                                            | document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {}                                                            | document: ['TypeError(msg)']
testitem: [] | только в document
ChemblDocumentPipeline.__init__                      | self, config: PipelineConfig, run_id: str | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._add_system_fields            | self, df: pd.DataFrame, log: Any          | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._build_document_descriptor    | self: SelfChemblDocumentPipeline          | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: ['TypeError(msg)']
testitem: [] | только в document
ChemblDocumentPipeline._check_document_id_uniqueness | self, df: pd.DataFrame, log: Any          | —                                                                            | document: {'logging': ['log.warning'], 'io': []}
testitem: {}                                                                                                          | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._coerce_mapping               | payload: Any                              | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._enrich_document_terms        | self, df: pd.DataFrame                    | —                                                                            | document: {'logging': ['UnifiedLogger.get', 'log.warning'], 'io': []}
testitem: {}                                                                                     | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._extract_nested_fields        | self, record: dict[str, Any]              | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._normalize_authors            | authors: Any, separator: str              | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._normalize_doi                | doi: str | None                           | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._normalize_identifiers        | self, df: pd.DataFrame, log: Any          | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._normalize_journal            | value: Any, max_len: int                  | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._normalize_numeric_fields     | self, df: pd.DataFrame, log: Any          | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._normalize_string_fields      | self, df: pd.DataFrame, log: Any          | —                                                                            | document: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                            | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._schema_column_specs          | self                                      | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline._should_enrich_document_terms | self                                      | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline.extract                       | self, *args, **kwargs                     | —                                                                            | document: {'logging': ['UnifiedLogger.get'], 'io': []}
testitem: {}                                                                                                    | document: []
testitem: []                 | только в document
ChemblDocumentPipeline.extract_all                   | self                                      | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
ChemblDocumentPipeline.extract_by_ids                | self, ids: Sequence[str]                  | —                                                                            | document: {'logging': ['UnifiedLogger.get', 'log.info'], 'io': []}
testitem: {}                                                                                        | document: []
testitem: []                 | только в document
ChemblDocumentPipeline.transform                     | self, df: pd.DataFrame                    | —                                                                            | document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {}                                                            | document: []
testitem: []                 | только в document
ChemblDocumentPipeline.validate                      | self, df: pd.DataFrame                    | —                                                                            | document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}
testitem: {}                                                                           | document: []
testitem: []                 | только в document
TestItemChemblPipeline                               | —                                         | —                                                                            | document: {}
testitem: {'logging': ['UnifiedLogger.get', 'bound_log.info', 'bound_log.warning', 'log.debug', 'log.info', 'log.warning'], 'io': ['status_payload.get']} | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline.__init__                      | —                                         | self, config: PipelineConfig, run_id: str                                    | document: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline._build_testitem_descriptor    | —                                         | self: SelfTestitemChemblPipeline                                             | document: {}
testitem: {'logging': ['log.debug', 'log.info'], 'io': []}                                                                                                | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline._check_empty_columns          | —                                         | self, df: pd.DataFrame, log: Any                                             | document: {}
testitem: {'logging': ['log.warning'], 'io': []}                                                                                                          | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline._deduplicate_molecules        | —                                         | self, df: pd.DataFrame, log: Any                                             | document: {}
testitem: {'logging': ['log.info'], 'io': []}                                                                                                             | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline._fetch_chembl_release         | —                                         | self, client: UnifiedAPIClient | ChemblClient | Any, log: BoundLogger | None | document: {}
testitem: {'logging': ['UnifiedLogger.get', 'bound_log.info', 'bound_log.warning'], 'io': ['status_payload.get']}                                         | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline._flatten_nested_structures    | —                                         | self, df: pd.DataFrame, log: Any                                             | document: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline._normalize_identifiers        | —                                         | self, df: pd.DataFrame, log: Any                                             | document: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline._normalize_numeric_fields     | —                                         | self, df: pd.DataFrame, log: Any                                             | document: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline._normalize_string_fields      | —                                         | self, df: pd.DataFrame, log: Any                                             | document: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline._remove_extra_columns         | —                                         | self, df: pd.DataFrame, log: Any                                             | document: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline._schema_column_specs          | —                                         | self                                                                         | document: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline.api_version                   | —                                         | self                                                                         | document: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline.augment_metadata              | —                                         | self, metadata: Mapping[str, object], df: pd.DataFrame                       | document: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline.chembl_db_version             | —                                         | self                                                                         | document: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline.extract                       | —                                         | self, *args, **kwargs                                                        | document: {}
testitem: {'logging': ['UnifiedLogger.get'], 'io': []}                                                                                                    | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline.extract_all                   | —                                         | self                                                                         | document: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline.extract_by_ids                | —                                         | self, ids: Sequence[str]                                                     | document: {}
testitem: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                                                                           | document: []
testitem: []                 | только в testitem
TestItemChemblPipeline.transform                     | —                                         | self, df: pd.DataFrame                                                       | document: {}
testitem: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                                                                           | document: []
testitem: []                 | только в testitem
__module_block_0                                     | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       
__module_block_1                                     | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | совпадает        
__module_block_10                                    | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       
__module_block_11                                    | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | совпадает        
__module_block_12                                    | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       
__module_block_13                                    | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | совпадает        
__module_block_14                                    | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | совпадает        
__module_block_15                                    | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | совпадает        
__module_block_16                                    | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       
__module_block_17                                    | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       
__module_block_18                                    | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       
__module_block_19                                    | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
__module_block_2                                     | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       
__module_block_20                                    | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | document: []
testitem: []                 | только в document
__module_block_3                                     | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | совпадает        
__module_block_4                                     | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | совпадает        
__module_block_5                                     | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       
__module_block_6                                     | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       
__module_block_7                                     | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       
__module_block_8                                     | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       
__module_block_9                                     | —                                         | —                                                                            | document: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | document: []
testitem: []                 | отличается       

_Показаны первые 20 горячих участков из 54._

#### Горячий участок 1

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:60-654
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,324 +0,0 @@

-class ChemblDocumentPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting document records from the ChEMBL API."
-    actor ="document_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch document payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_document.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="document_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all document records from ChEMBL using pagination."
-        return self .run_extract_all (self ._build_document_descriptor ())
-    def _build_document_descriptor (self :SelfChemblDocumentPipeline )->ChemblExtractionDescriptor [SelfChemblDocumentPipeline ]:
-        "Return the descriptor powering the shared extraction routine."
-        def _require_document_pipeline (pipeline :ChemblPipelineBase )->ChemblDocumentPipeline :
-            if isinstance (pipeline ,ChemblDocumentPipeline ):
-                return pipeline
-            msg ="ChemblDocumentPipeline instance required"
-            raise TypeError (msg )
-        def build_context (pipeline :SelfChemblDocumentPipeline ,source_config :Any ,log :BoundLogger )->ChemblExtractionContext :
-            document_pipeline =_require_document_pipeline (pipeline )
-            typed_source_config =source_config if isinstance (source_config ,DocumentSourceConfig )else DocumentSourceConfig .from_source_config (cast (Any ,source_config ))
-            base_url =document_pipeline ._resolve_base_url (typed_source_config .parameters )
-            http_client ,_ =document_pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-            chembl_client =ChemblClient (http_client )
-            document_client =ChemblDocumentClient (chembl_client ,batch_size =min (typed_source_config .batch_size ,25 ))
-            document_pipeline ._chembl_release =document_pipeline .fetch_chembl_release (chembl_client ,log )
-            select_fields =document_pipeline ._resolve_select_fields (cast (SourceConfig ,cast (Any ,typed_source_config )),default_fields =API_DOCUMENT_FIELDS )
-            context =ChemblExtractionContext (typed_source_config ,document_client )
-            context .chembl_client =chembl_client
-            context .select_fields =tuple (select_fields )if select_fields else None
-            context .chembl_release =document_pipeline ._chembl_release
-            return context
-        def empty_frame (_ :SelfChemblDocumentPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"document_chembl_id":pd .Series (dtype ="string")})
-        def record_transform (pipeline :SelfChemblDocumentPipeline ,payload :Mapping [str ,Any ],_ :ChemblExtractionContext )->Mapping [str ,Any ]:
-            document_pipeline =_require_document_pipeline (pipeline )
-            return document_pipeline ._extract_nested_fields (dict (payload ))
-        def summary_extra (pipeline :SelfChemblDocumentPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            _require_document_pipeline (pipeline )
-            page_size =context .page_size or 0
-            pages =0
-            if page_size >0 :
-                total_rows =int (df .shape [0 ])
-                pages =(total_rows +page_size -1 )//page_size
-            return {"pages":pages }
-        return ChemblExtractionDescriptor [SelfChemblDocumentPipeline ](name ="chembl_document",source_name ="chembl",source_config_factory =DocumentSourceConfig .from_source_config ,build_context =build_context ,id_column ="document_chembl_id",summary_event ="chembl_document.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =API_DOCUMENT_FIELDS ,record_transform =record_transform ,sort_by =("document_chembl_id",),empty_frame_factory =empty_frame ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract document records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of document_chembl_id values to extract (as strings).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted document records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =DocumentSourceConfig .from_source_config (source_raw )
-        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-        chembl_client =ChemblClient (http_client )
-        document_client =ChemblDocumentClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        resolved_select_fields =self ._resolve_select_fields (source_raw ,default_fields =list (API_DOCUMENT_FIELDS ))
-        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-        limit =self .config .cli .limit
-        def fetch_documents (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            if "original_paginate"not in context .extra :
-                original_paginate =chembl_client .paginate
-                def counted_paginate (*args :Any ,**kwargs :Any )->Any :
-                    context .increment_api_calls ()
-                    return original_paginate (*args ,**kwargs )
-                chembl_client .paginate =counted_paginate
-                context .extra ["original_paginate"]=original_paginate
-            iterator =document_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield self ._extract_nested_fields (dict (item ))
-        def finalize_context (context :BatchExtractionContext )->None :
-            original =context .extra .pop ("original_paginate",None )
-            if original is not None :
-                chembl_client .paginate =original
-            api_calls_value =context .stats .api_calls if context .stats .api_calls is not None else 0
-            override ={"batches":context .stats .batches ,"api_calls":api_calls_value ,"cache_hits":context .stats .cache_hits }
-            context .extra ["stats_attribute_override"]=override
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="document_chembl_id",fetcher =fetch_documents ,select_fields =merged_select_fields or None ,batch_size =document_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":merged_select_fields }if merged_select_fields else None ,chembl_release =self ._chembl_release ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats")
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_document.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =stats .batches ,api_calls =stats .api_calls ,cache_hits =stats .cache_hits )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw document data by normalizing fields and identifiers."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_numeric_fields (df ,log )
-        if self ._should_enrich_document_terms ():
-            df =self ._enrich_document_terms (df )
-        df =self ._add_system_fields (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if "document_chembl_id"in df .columns and df ["document_chembl_id"].duplicated ().any ():
-            initial_count =len (df )
-            df =df .sort_values (by =list (df .columns )).drop_duplicates (subset =["document_chembl_id"],keep ="first")
-            deduped_count =len (df )
-            if deduped_count <initial_count :
-                log .warning ("document_deduplication_applied",initial_count =initial_count ,deduped_count =deduped_count ,removed_count =initial_count -deduped_count )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against DocumentSchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        self ._check_document_id_uniqueness (df ,log )
-        validated =super ().validate (df )
-        log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =self .config .validation .coerce )
-        return validated
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize identifier fields (DOI, PMID)."
-        df =df .copy ()
-        if "doi"in df .columns :
-            df ["doi_clean"]=df ["doi"].apply (self ._normalize_doi )
-        if "pubmed_id"in df .columns :
-            df ["pubmed_id"]=pd .to_numeric (df ["pubmed_id"],errors ="coerce").astype ("Int64")
-        return df
-    @staticmethod
-    def _normalize_doi (doi :str |None )->str :
-        "Normalize DOI by removing prefixes and validating format."
-        if not doi :
-            return ""
-        if not isinstance (doi ,str ):
-            return ""
-        doi =doi .strip ().lower ()
-        for prefix in ["doi:","https://doi.org/","http://dx.doi.org/","http://doi.org/"]:
-            if doi .startswith (prefix ):
-                doi =doi [len (prefix ):]
-        doi =doi .strip ()
-        doi_pattern =re .compile ("^10\\.\\d{4,9}/\\S+$")
-        if doi_pattern .match (doi ):
-            return doi
-        return ""
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields (title, abstract, journal, authors)."
-        working_df =df .copy ()
-        rules ={"title":StringRule (max_length =1000 ),"abstract":StringRule (max_length =5000 )}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        if "journal"in normalized_df .columns :
-            journal_series :pd .Series [Any ]=normalized_df ["journal"]
-            normalized_df ["journal"]=journal_series .map (lambda value :self ._normalize_journal (value ))
-        if "authors"in normalized_df .columns :
-            def _to_author_tuple (item :object )->tuple [str ,int ]|None :
-                if not isinstance (item ,tuple ):
-                    return None
-                tuple_item =cast (tuple [object ,...],item )
-                if len (tuple_item )!=2 :
-                    return None
-                name_raw ,count_raw =tuple_item
-                if not isinstance (name_raw ,str ):
-                    return None
-                name_value :str =name_raw
-                if isinstance (count_raw ,Integral ):
-                    count_value =int (count_raw )
-                elif isinstance (count_raw ,Real ):
-                    float_value =float (count_raw )
-                    if not float_value .is_integer ():
-                        return None
-                    count_value =int (float_value )
-                else :
-                    return None
-                if count_value <0 :
-                    return None
-                return (name_value ,count_value )
-            def _author_name_from_tuple (data :tuple [str ,int ]|None )->str :
-                return data [0 ]if data is not None else ""
-            def _author_count_from_tuple (data :tuple [str ,int ]|None )->int :
-                return data [1 ]if data is not None else 0
-            authors_series :pd .Series [Any ]=normalized_df ["authors"]
-            normalized_result =authors_series .apply (self ._normalize_authors )
-            normalized_tuples =normalized_result .apply (_to_author_tuple )
-            normalized_df ["authors"]=normalized_tuples .apply (_author_name_from_tuple )
-            normalized_df ["authors_count"]=normalized_tuples .apply (_author_count_from_tuple )
-        return normalized_df
-    @staticmethod
-    def _normalize_journal (value :Any ,max_len :int =255 )->str :
-        "Trim and collapse whitespace for journal name."
-        if pd .isna (value ):
-            return ""
-        text =str (value )
-        text =re .sub ("\\s+"," ",text ).strip ()
-        return text [:max_len ]if len (text )>max_len else text
-    @staticmethod
-    def _normalize_authors (authors :Any ,separator :str =", ")->tuple [str ,int ]:
-        "Normalize author separators and count."
-        if pd .isna (authors ):
-            return ("",0 )
-        text =str (authors ).strip ()
-        text =re .sub (";",",",text )
-        text =re .sub ("\\s+"," ",text )
-        if not text :
-            return ("",0 )
-        parts =text .split (",")
-        parts =[p .strip ()for p in parts if p .strip ()]
-        return (separator .join (parts ),len (parts ))
-    def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize numeric fields (year)."
-        df =df .copy ()
-        if "year"in df .columns :
-            def _coerce_year (value :object )->int |None :
-                if value is None or value is pd .NA :
-                    return None
-                if isinstance (value ,Integral ):
-                    year_int =int (value )
-                elif isinstance (value ,Real ):
-                    float_value =float (value )
-                    if not float_value .is_integer ():
-                        return None
-                    year_int =int (float_value )
-                elif isinstance (value ,str ):
-                    stripped =value .strip ()
-                    if not stripped :
-                        return None
-                    if not stripped .isdigit ():
-                        return None
-                    year_int =int (stripped )
-                else :
-                    return None
-                if 1500 <=year_int <=2100 :
-                    return year_int
-                return None
-            normalized_year =df ["year"].apply (_coerce_year )
-            df ["year"]=normalized_year .astype ("Int64")
-        return df
-    def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Add document-specific system fields (source)."
-        df =df .copy ()
-        df ["source"]="ChEMBL"
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        specs ["source"]={"default":"ChEMBL"}
-        specs ["authors_count"]={"default":0 ,"dtype":"Int64"}
-        hashing_config =self .config .determinism .hashing
-        business_key_column =hashing_config .business_key_column
-        row_hash_column =hashing_config .row_hash_column
-        if business_key_column :
-            specs [business_key_column ]={"default":""}
-        if row_hash_column :
-            specs [row_hash_column ]={"default":""}
-        return specs
-    def _check_document_id_uniqueness (self ,df :pd .DataFrame ,log :Any )->None :
-        "Check that document_chembl_id is unique."
-        if df .empty :
-            return
-        if "document_chembl_id"not in df .columns :
-            return
-        duplicates =df ["document_chembl_id"].duplicated ()
-        if duplicates .any ():
-            duplicate_ids =df [df ["document_chembl_id"].duplicated ()]["document_chembl_id"].unique ().tolist ()
-            log .warning ("document_id_duplicates",duplicate_count =duplicates .sum (),duplicate_ids =duplicate_ids [:10 ])
-    def _should_enrich_document_terms (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 document_term \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            document_section :Any =chembl_section .get ("document")
-            if not isinstance (document_section ,Mapping ):
-                return False
-            document_section =cast (Mapping [str ,Any ],document_section )
-            enrich_section :Any =document_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            document_term_section :Any =enrich_section .get ("document_term")
-            if not isinstance (document_term_section ,Mapping ):
-                return False
-            document_term_section =cast (Mapping [str ,Any ],document_term_section )
-            enabled :Any =document_term_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_document_terms (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                document_section :Any =chembl_section .get ("document")
-                if isinstance (document_section ,Mapping ):
-                    document_section =cast (Mapping [str ,Any ],document_section )
-                    enrich_section :Any =document_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        document_term_section :Any =enrich_section .get ("document_term")
-                        if isinstance (document_term_section ,Mapping ):
-                            document_term_section =cast (Mapping [str ,Any ],document_term_section )
-                            enrich_cfg =dict (document_term_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =DocumentSourceConfig .from_source_config (source_raw )
-        api_client ,_ =self .prepare_chembl_client ("chembl",base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters ))),client_name ="chembl_enrichment_client")
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_document_terms (df ,chembl_client ,enrich_cfg )
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested objects in document records."
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        "Coerce payload to dictionary mapping."
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
```

#### Горячий участок 2

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:65-67
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,3 +0,0 @@

-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
```

#### Горячий участок 3

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:532-539
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,5 +0,0 @@

-def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Add document-specific system fields (source)."
-    df =df .copy ()
-    df ["source"]="ChEMBL"
-    return df
```

#### Горячий участок 4

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:90-178
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,35 +0,0 @@

-def _build_document_descriptor (self :SelfChemblDocumentPipeline )->ChemblExtractionDescriptor [SelfChemblDocumentPipeline ]:
-    "Return the descriptor powering the shared extraction routine."
-    def _require_document_pipeline (pipeline :ChemblPipelineBase )->ChemblDocumentPipeline :
-        if isinstance (pipeline ,ChemblDocumentPipeline ):
-            return pipeline
-        msg ="ChemblDocumentPipeline instance required"
-        raise TypeError (msg )
-    def build_context (pipeline :SelfChemblDocumentPipeline ,source_config :Any ,log :BoundLogger )->ChemblExtractionContext :
-        document_pipeline =_require_document_pipeline (pipeline )
-        typed_source_config =source_config if isinstance (source_config ,DocumentSourceConfig )else DocumentSourceConfig .from_source_config (cast (Any ,source_config ))
-        base_url =document_pipeline ._resolve_base_url (typed_source_config .parameters )
-        http_client ,_ =document_pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-        chembl_client =ChemblClient (http_client )
-        document_client =ChemblDocumentClient (chembl_client ,batch_size =min (typed_source_config .batch_size ,25 ))
-        document_pipeline ._chembl_release =document_pipeline .fetch_chembl_release (chembl_client ,log )
-        select_fields =document_pipeline ._resolve_select_fields (cast (SourceConfig ,cast (Any ,typed_source_config )),default_fields =API_DOCUMENT_FIELDS )
-        context =ChemblExtractionContext (typed_source_config ,document_client )
-        context .chembl_client =chembl_client
-        context .select_fields =tuple (select_fields )if select_fields else None
-        context .chembl_release =document_pipeline ._chembl_release
-        return context
-    def empty_frame (_ :SelfChemblDocumentPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"document_chembl_id":pd .Series (dtype ="string")})
-    def record_transform (pipeline :SelfChemblDocumentPipeline ,payload :Mapping [str ,Any ],_ :ChemblExtractionContext )->Mapping [str ,Any ]:
-        document_pipeline =_require_document_pipeline (pipeline )
-        return document_pipeline ._extract_nested_fields (dict (payload ))
-    def summary_extra (pipeline :SelfChemblDocumentPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        _require_document_pipeline (pipeline )
-        page_size =context .page_size or 0
-        pages =0
-        if page_size >0 :
-            total_rows =int (df .shape [0 ])
-            pages =(total_rows +page_size -1 )//page_size
-        return {"pages":pages }
-    return ChemblExtractionDescriptor [SelfChemblDocumentPipeline ](name ="chembl_document",source_name ="chembl",source_config_factory =DocumentSourceConfig .from_source_config ,build_context =build_context ,id_column ="document_chembl_id",summary_event ="chembl_document.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =API_DOCUMENT_FIELDS ,record_transform =record_transform ,sort_by =("document_chembl_id",),empty_frame_factory =empty_frame ,summary_extra =summary_extra )
```

#### Горячий участок 5

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:557-572
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,10 +0,0 @@

-def _check_document_id_uniqueness (self ,df :pd .DataFrame ,log :Any )->None :
-    "Check that document_chembl_id is unique."
-    if df .empty :
-        return
-    if "document_chembl_id"not in df .columns :
-        return
-    duplicates =df ["document_chembl_id"].duplicated ()
-    if duplicates .any ():
-        duplicate_ids =df [df ["document_chembl_id"].duplicated ()]["document_chembl_id"].unique ().tolist ()
-        log .warning ("document_id_duplicates",duplicate_count =duplicates .sum (),duplicate_ids =duplicate_ids [:10 ])
```

#### Горячий участок 6

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:650-654
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,6 +0,0 @@

-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    "Coerce payload to dictionary mapping."
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Горячий участок 7

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:597-641
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,24 +0,0 @@

-def _enrich_document_terms (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            document_section :Any =chembl_section .get ("document")
-            if isinstance (document_section ,Mapping ):
-                document_section =cast (Mapping [str ,Any ],document_section )
-                enrich_section :Any =document_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    document_term_section :Any =enrich_section .get ("document_term")
-                    if isinstance (document_term_section ,Mapping ):
-                        document_term_section =cast (Mapping [str ,Any ],document_term_section )
-                        enrich_cfg =dict (document_term_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =DocumentSourceConfig .from_source_config (source_raw )
-    api_client ,_ =self .prepare_chembl_client ("chembl",base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters ))),client_name ="chembl_enrichment_client")
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_document_terms (df ,chembl_client ,enrich_cfg )
```

#### Горячий участок 8

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:643-647
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,3 +0,0 @@

-def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-    "Extract fields from nested objects in document records."
-    return record
```

#### Горячий участок 9

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:484-495
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,13 +0,0 @@

-@staticmethod
-def _normalize_authors (authors :Any ,separator :str =", ")->tuple [str ,int ]:
-    "Normalize author separators and count."
-    if pd .isna (authors ):
-        return ("",0 )
-    text =str (authors ).strip ()
-    text =re .sub (";",",",text )
-    text =re .sub ("\\s+"," ",text )
-    if not text :
-        return ("",0 )
-    parts =text .split (",")
-    parts =[p .strip ()for p in parts if p .strip ()]
-    return (separator .join (parts ),len (parts ))
```

#### Горячий участок 10

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:392-408
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,16 +0,0 @@

-@staticmethod
-def _normalize_doi (doi :str |None )->str :
-    "Normalize DOI by removing prefixes and validating format."
-    if not doi :
-        return ""
-    if not isinstance (doi ,str ):
-        return ""
-    doi =doi .strip ().lower ()
-    for prefix in ["doi:","https://doi.org/","http://dx.doi.org/","http://doi.org/"]:
-        if doi .startswith (prefix ):
-            doi =doi [len (prefix ):]
-    doi =doi .strip ()
-    doi_pattern =re .compile ("^10\\.\\d{4,9}/\\S+$")
-    if doi_pattern .match (doi ):
-        return doi
-    return ""
```

#### Горячий участок 11

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:377-389
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,8 +0,0 @@

-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize identifier fields (DOI, PMID)."
-    df =df .copy ()
-    if "doi"in df .columns :
-        df ["doi_clean"]=df ["doi"].apply (self ._normalize_doi )
-    if "pubmed_id"in df .columns :
-        df ["pubmed_id"]=pd .to_numeric (df ["pubmed_id"],errors ="coerce").astype ("Int64")
-    return df
```

#### Горячий участок 12

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:475-481
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,8 +0,0 @@

-@staticmethod
-def _normalize_journal (value :Any ,max_len :int =255 )->str :
-    "Trim and collapse whitespace for journal name."
-    if pd .isna (value ):
-        return ""
-    text =str (value )
-    text =re .sub ("\\s+"," ",text ).strip ()
-    return text [:max_len ]if len (text )>max_len else text
```

#### Горячий участок 13

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:497-530
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,29 +0,0 @@

-def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize numeric fields (year)."
-    df =df .copy ()
-    if "year"in df .columns :
-        def _coerce_year (value :object )->int |None :
-            if value is None or value is pd .NA :
-                return None
-            if isinstance (value ,Integral ):
-                year_int =int (value )
-            elif isinstance (value ,Real ):
-                float_value =float (value )
-                if not float_value .is_integer ():
-                    return None
-                year_int =int (float_value )
-            elif isinstance (value ,str ):
-                stripped =value .strip ()
-                if not stripped :
-                    return None
-                if not stripped .isdigit ():
-                    return None
-                year_int =int (stripped )
-            else :
-                return None
-            if 1500 <=year_int <=2100 :
-                return year_int
-            return None
-        normalized_year =df ["year"].apply (_coerce_year )
-        df ["year"]=normalized_year .astype ("Int64")
-    return df
```

#### Горячий участок 14

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:410-472
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,43 +0,0 @@

-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields (title, abstract, journal, authors)."
-    working_df =df .copy ()
-    rules ={"title":StringRule (max_length =1000 ),"abstract":StringRule (max_length =5000 )}
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    if "journal"in normalized_df .columns :
-        journal_series :pd .Series [Any ]=normalized_df ["journal"]
-        normalized_df ["journal"]=journal_series .map (lambda value :self ._normalize_journal (value ))
-    if "authors"in normalized_df .columns :
-        def _to_author_tuple (item :object )->tuple [str ,int ]|None :
-            if not isinstance (item ,tuple ):
-                return None
-            tuple_item =cast (tuple [object ,...],item )
-            if len (tuple_item )!=2 :
-                return None
-            name_raw ,count_raw =tuple_item
-            if not isinstance (name_raw ,str ):
-                return None
-            name_value :str =name_raw
-            if isinstance (count_raw ,Integral ):
-                count_value =int (count_raw )
-            elif isinstance (count_raw ,Real ):
-                float_value =float (count_raw )
-                if not float_value .is_integer ():
-                    return None
-                count_value =int (float_value )
-            else :
-                return None
-            if count_value <0 :
-                return None
-            return (name_value ,count_value )
-        def _author_name_from_tuple (data :tuple [str ,int ]|None )->str :
-            return data [0 ]if data is not None else ""
-        def _author_count_from_tuple (data :tuple [str ,int ]|None )->int :
-            return data [1 ]if data is not None else 0
-        authors_series :pd .Series [Any ]=normalized_df ["authors"]
-        normalized_result =authors_series .apply (self ._normalize_authors )
-        normalized_tuples =normalized_result .apply (_to_author_tuple )
-        normalized_df ["authors"]=normalized_tuples .apply (_author_name_from_tuple )
-        normalized_df ["authors_count"]=normalized_tuples .apply (_author_count_from_tuple )
-    return normalized_df
```

#### Горячий участок 15

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:541-555
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,12 +0,0 @@

-def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-    specs =dict (super ()._schema_column_specs ())
-    specs ["source"]={"default":"ChEMBL"}
-    specs ["authors_count"]={"default":0 ,"dtype":"Int64"}
-    hashing_config =self .config .determinism .hashing
-    business_key_column =hashing_config .business_key_column
-    row_hash_column =hashing_config .row_hash_column
-    if business_key_column :
-        specs [business_key_column ]={"default":""}
-    if row_hash_column :
-        specs [row_hash_column ]={"default":""}
-    return specs
```

#### Горячий участок 16

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:574-595
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,22 +0,0 @@

-def _should_enrich_document_terms (self )->bool :
-    "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 document_term \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-    if not self .config .chembl :
-        return False
-    try :
-        chembl_section =self .config .chembl
-        document_section :Any =chembl_section .get ("document")
-        if not isinstance (document_section ,Mapping ):
-            return False
-        document_section =cast (Mapping [str ,Any ],document_section )
-        enrich_section :Any =document_section .get ("enrich")
-        if not isinstance (enrich_section ,Mapping ):
-            return False
-        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-        document_term_section :Any =enrich_section .get ("document_term")
-        if not isinstance (document_term_section ,Mapping ):
-            return False
-        document_term_section =cast (Mapping [str ,Any ],document_term_section )
-        enabled :Any =document_term_section .get ("enabled")
-        return bool (enabled )if enabled is not None else False
-    except (AttributeError ,KeyError ,TypeError ):
-        return False
```

#### Горячий участок 17

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:69-83
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,4 +0,0 @@

-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch document payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_document.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="document_chembl_id")
```

#### Горячий участок 18

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:85-88
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,3 +0,0 @@

-def extract_all (self )->pd .DataFrame :
-    "Extract all document records from ChEMBL using pagination."
-    return self .run_extract_all (self ._build_document_descriptor ())
```

#### Горячий участок 19

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:180-280
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,36 +0,0 @@

-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract document records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of document_chembl_id values to extract (as strings).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted document records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =DocumentSourceConfig .from_source_config (source_raw )
-    base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-    http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-    chembl_client =ChemblClient (http_client )
-    document_client =ChemblDocumentClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-    self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-    resolved_select_fields =self ._resolve_select_fields (source_raw ,default_fields =list (API_DOCUMENT_FIELDS ))
-    merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-    limit =self .config .cli .limit
-    def fetch_documents (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        if "original_paginate"not in context .extra :
-            original_paginate =chembl_client .paginate
-            def counted_paginate (*args :Any ,**kwargs :Any )->Any :
-                context .increment_api_calls ()
-                return original_paginate (*args ,**kwargs )
-            chembl_client .paginate =counted_paginate
-            context .extra ["original_paginate"]=original_paginate
-        iterator =document_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield self ._extract_nested_fields (dict (item ))
-    def finalize_context (context :BatchExtractionContext )->None :
-        original =context .extra .pop ("original_paginate",None )
-        if original is not None :
-            chembl_client .paginate =original
-        api_calls_value =context .stats .api_calls if context .stats .api_calls is not None else 0
-        override ={"batches":context .stats .batches ,"api_calls":api_calls_value ,"cache_hits":context .stats .cache_hits }
-        context .extra ["stats_attribute_override"]=override
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="document_chembl_id",fetcher =fetch_documents ,select_fields =merged_select_fields or None ,batch_size =document_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":merged_select_fields }if merged_select_fields else None ,chembl_release =self ._chembl_release ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats")
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_document.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =stats .batches ,api_calls =stats .api_calls ,cache_hits =stats .cache_hits )
-    return dataframe
```

#### Горячий участок 20

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\run.py:282-336
- testitem: нет в ветке

```diff
--- document:run.py

+++ testitem:run.py

@@ -1,24 +0,0 @@

-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw document data by normalizing fields and identifiers."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-    df =df .copy ()
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._normalize_numeric_fields (df ,log )
-    if self ._should_enrich_document_terms ():
-        df =self ._enrich_document_terms (df )
-    df =self ._add_system_fields (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if "document_chembl_id"in df .columns and df ["document_chembl_id"].duplicated ().any ():
-        initial_count =len (df )
-        df =df .sort_values (by =list (df .columns )).drop_duplicates (subset =["document_chembl_id"],keep ="first")
-        deduped_count =len (df )
-        if deduped_count <initial_count :
-            log .warning ("document_deduplication_applied",initial_count =initial_count ,deduped_count =deduped_count ,removed_count =initial_count -deduped_count )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

### Модуль transform.py

Определение        | document сигнатура | testitem сигнатура                                             | Побочные эффекты                                 | Исключения                | Статус           
-------------------|--------------------|----------------------------------------------------------------|--------------------------------------------------|---------------------------|------------------
__module_block_0   | —                  | —                                                              | document: {}
testitem: {'logging': [], 'io': []} | document: []
testitem: [] | только в testitem
__module_block_1   | —                  | —                                                              | document: {}
testitem: {'logging': [], 'io': []} | document: []
testitem: [] | только в testitem
__module_block_2   | —                  | —                                                              | document: {}
testitem: {'logging': [], 'io': []} | document: []
testitem: [] | только в testitem
__module_block_3   | —                  | —                                                              | document: {}
testitem: {'logging': [], 'io': []} | document: []
testitem: [] | только в testitem
__module_block_4   | —                  | —                                                              | document: {}
testitem: {'logging': [], 'io': []} | document: []
testitem: [] | только в testitem
__module_block_5   | —                  | —                                                              | document: {}
testitem: {'logging': [], 'io': []} | document: []
testitem: [] | только в testitem
__module_block_6   | —                  | —                                                              | document: {}
testitem: {'logging': [], 'io': []} | document: []
testitem: [] | только в testitem
flatten_object_col | —                  | df: pd.DataFrame, col: str, fields: Sequence[str], prefix: str | document: {}
testitem: {'logging': [], 'io': []} | document: []
testitem: [] | только в testitem
transform          | —                  | df: pd.DataFrame, cfg: Any                                     | document: {}
testitem: {'logging': [], 'io': []} | document: []
testitem: [] | только в testitem

#### Горячий участок 1

- document: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:1-1

```diff
--- document:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+"Transform utilities for ChEMBL testitem pipeline array serialization and flattening."
```

#### Горячий участок 2

- document: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:3-3

```diff
--- document:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+from __future__ import annotations
```

#### Горячий участок 3

- document: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:10-10

```diff
--- document:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+from bioetl .core .serialization import serialize_objects ,serialize_simple_list
```

#### Горячий участок 4

- document: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:5-5

```diff
--- document:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+from collections .abc import Sequence
```

#### Горячий участок 5

- document: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:6-6

```diff
--- document:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+from typing import Any
```

#### Горячий участок 6

- document: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:8-8

```diff
--- document:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+import pandas as pd
```

#### Горячий участок 7

- document: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:12-17

```diff
--- document:transform.py

+++ testitem:transform.py

@@ -0,0 +1 @@

+__all__ =["serialize_simple_list","serialize_objects","flatten_object_col","transform"]
```

#### Горячий участок 8

- document: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:20-75

```diff
--- document:transform.py

+++ testitem:transform.py

@@ -0,0 +1,16 @@

+def flatten_object_col (df :pd .DataFrame ,col :str ,fields :Sequence [str ],prefix :str )->pd .DataFrame :
+    "Flatten nested object column into flat columns with prefix.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    col:\n        Column name containing nested objects.\n    fields:\n        List of field names to extract from nested objects.\n    prefix:\n        Prefix to add to flattened column names (e.g., \"molecule_hierarchy__\").\n\n    Returns\n    -------\n    pd.DataFrame:\n        DataFrame with flattened columns added and original column removed.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame({\n    ...     \"molecule_chembl_id\": [\"CHEMBL1\"],\n    ...     \"molecule_hierarchy\": [{\"molecule_chembl_id\": \"CHEMBL1\", \"parent_chembl_id\": \"CHEMBL2\"}],\n    ... })\n    >>> result = flatten_object_col(df, \"molecule_hierarchy\", [\"molecule_chembl_id\", \"parent_chembl_id\"], \"molecule_hierarchy__\")\n    >>> \"molecule_hierarchy__molecule_chembl_id\" in result.columns\n    True\n    >>> \"molecule_hierarchy__parent_chembl_id\" in result.columns\n    True\n    >>> \"molecule_hierarchy\" not in result.columns\n    True\n    "
+    df =df .copy ()
+    if col not in df .columns :
+        for f in fields :
+            df [f'{prefix }{f }']=None
+        return df
+    def row_to_dict (obj :Any )->dict [str ,Any ]:
+        "Extract fields from nested object."
+        if not isinstance (obj ,dict ):
+            return dict .fromkeys (fields )
+        return {f :obj .get (f )for f in fields }
+    expanded =df [col ].map (row_to_dict ).apply (pd .Series )
+    expanded .columns =[f'{prefix }{c }'for c in expanded .columns ]
+    df =df .drop (columns =[col ])
+    return pd .concat ([df ,expanded ],axis =1 )
```

#### Горячий участок 9

- document: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:78-138

```diff
--- document:transform.py

+++ testitem:transform.py

@@ -0,0 +1,25 @@

+def transform (df :pd .DataFrame ,cfg :Any )->pd .DataFrame :
+    "Transform testitem DataFrame by flattening nested objects and serializing arrays.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    cfg:\n        Pipeline config with transform section (enable_flatten, enable_serialization,\n        arrays_simple_to_pipe, arrays_objects_to_header_rows, flatten_objects).\n\n    Returns\n    -------\n    pd.DataFrame:\n        Transformed DataFrame with flattened objects and serialized arrays.\n    "
+    df =df .copy ()
+    enable_flatten =getattr (cfg .transform ,"enable_flatten",True )if hasattr (cfg ,"transform")else True
+    enable_serialization =getattr (cfg .transform ,"enable_serialization",True )if hasattr (cfg ,"transform")else True
+    if enable_flatten and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"flatten_objects"):
+        flatten_objects =cfg .transform .flatten_objects
+        if isinstance (flatten_objects ,dict ):
+            for obj_col ,fields in flatten_objects .items ():
+                if isinstance (fields ,Sequence )and (not isinstance (fields ,(str ,bytes ))):
+                    df =flatten_object_col (df ,obj_col ,fields ,prefix =f'{obj_col }__')
+    if enable_serialization and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"arrays_simple_to_pipe"):
+        arrays_simple =cfg .transform .arrays_simple_to_pipe
+        if isinstance (arrays_simple ,Sequence )and (not isinstance (arrays_simple ,(str ,bytes ))):
+            for col in arrays_simple :
+                if col in df .columns :
+                    df [col ]=df [col ].map (serialize_simple_list )
+    if enable_serialization and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"arrays_objects_to_header_rows"):
+        arrays_objects =cfg .transform .arrays_objects_to_header_rows
+        if isinstance (arrays_objects ,Sequence )and (not isinstance (arrays_objects ,(str ,bytes ))):
+            for col in arrays_objects :
+                if col in df .columns :
+                    df [f'{col }__flat']=df [col ].map (serialize_objects )
+                    df =df .drop (columns =[col ])
+    return df
```

### Модуль normalize.py

Определение                | document сигнатура                                                  | testitem сигнатура | Побочные эффекты                                                                                            | Исключения                | Статус           
---------------------------|---------------------------------------------------------------------|--------------------|-------------------------------------------------------------------------------------------------------------|---------------------------|------------------
__module_block_0           | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_1           | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_10          | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_11          | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_13          | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_2           | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_3           | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_4           | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_5           | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_6           | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_7           | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_8           | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
__module_block_9           | —                                                                   | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
_escape_pipe               | value: str | Any                                                    | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
_is_numeric                | value: Any                                                          | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
aggregate_terms            | rows: Iterable[dict[str, Any]], sort: str                           | —                  | document: {'logging': [], 'io': []}
testitem: {}                                                            | document: []
testitem: [] | только в document
enrich_with_document_terms | df_docs: pd.DataFrame, client: ChemblClient, cfg: Mapping[str, Any] | —                  | document: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': []}
testitem: {} | document: []
testitem: [] | только в document

#### Горячий участок 1

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:1-1
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-"Enrichment functions for Document pipeline."
```

#### Горячий участок 2

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:3-3
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from __future__ import annotations
```

#### Горячий участок 3

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:16-16
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-__all__ =["enrich_with_document_terms","aggregate_terms","_escape_pipe"]
```

#### Горячий участок 4

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:19-19
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-_ensure_columns =ensure_columns
```

#### Горячий участок 5

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:46-49
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-_DOCUMENT_TERM_COLUMNS :tuple [tuple [str ,str ],...]=(("term","string"),("weight","string"))
```

#### Горячий участок 6

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:11-11
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .clients .chembl import ChemblClient
```

#### Горячий участок 7

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:12-12
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .frame import ensure_columns
```

#### Горячий участок 8

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:13-13
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .core .logger import UnifiedLogger
```

#### Горячий участок 9

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:14-14
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from bioetl .schemas .document import DOCUMENT_TERMS_ENRICHMENT_SCHEMA
```

#### Горячий участок 10

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:5-5
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from collections import defaultdict
```

#### Горячий участок 11

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:6-6
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from collections .abc import Iterable ,Mapping
```

#### Горячий участок 12

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:7-7
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-from typing import Any
```

#### Горячий участок 13

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:9-9
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1 +0,0 @@

-import pandas as pd
```

#### Горячий участок 14

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:22-43
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1,8 +0,0 @@

-def _escape_pipe (value :str |Any )->str :
-    "Escape pipe and backslash delimiters in string values.\n\n    Parameters\n    ----------\n    value:\n        Input value to escape. ``None`` \u0438 NA \u043f\u0440\u0438\u0432\u043e\u0434\u044f\u0442\u0441\u044f \u043a \u043f\u0443\u0441\u0442\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435.\n\n    Returns\n    -------\n    str:\n        \u0421\u0442\u0440\u043e\u043a\u0430 \u0441 \u044d\u043a\u0440\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u0435\u043b\u044f\u043c\u0438: ``|`` \u2192 ``\\|``, ``\\`` \u2192 ``\\\\``.\n    "
-    if value is None or pd .isna (value ):
-        return ""
-    text =str (value )
-    if not text :
-        return ""
-    return text .replace ("\\","\\\\").replace ("|","\\|")
```

#### Горячий участок 15

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:122-128
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1,7 +0,0 @@

-def _is_numeric (value :Any )->bool :
-    "Check if value can be converted to float."
-    try :
-        float (value )
-        return True
-    except (ValueError ,TypeError ):
-        return False
```

#### Горячий участок 16

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:52-119
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1,26 +0,0 @@

-def aggregate_terms (rows :Iterable [dict [str ,Any ]],sort :str ="weight_desc")->dict [str ,dict [str ,str ]]:
-    "Aggregate document terms by document_chembl_id.\n\n    Parameters\n    ----------\n    rows:\n        Iterable of document_term records, each with 'document_chembl_id', 'term', 'weight'.\n    sort:\n        Sort order: 'weight_desc' (default) sorts by weight descending, None preserves order.\n\n    Returns\n    -------\n    dict[str, dict[str, str]]:\n        Dictionary keyed by document_chembl_id -> {'term': 't1|t2|...', 'weight': 'w1|w2|...'}.\n        Terms and weights are serialized with \"|\" separator, order is synchronized.\n    "
-    bucket :dict [str ,list [tuple [str ,Any ]]]=defaultdict (list )
-    for r in rows :
-        did =r .get ("document_chembl_id")
-        if not did :
-            continue
-        term_value =r .get ("term")
-        weight_value =r .get ("weight")
-        term_str =str (term_value )if term_value is not None else ""
-        bucket [did ].append ((term_str ,weight_value ))
-    result :dict [str ,dict [str ,str ]]={}
-    for did ,items in bucket .items ():
-        if sort =="weight_desc":
-            items .sort (key =lambda x :float (x [1 ])if x [1 ]not in (None ,"")and _is_numeric (x [1 ])else float ("-inf"),reverse =True )
-        terms_list :list [str ]=[]
-        weights_list :list [str ]=[]
-        for term ,weight in items :
-            escaped_term =_escape_pipe (term or "")
-            terms_list .append (escaped_term )
-            if weight in (None ,""):
-                weights_list .append ("")
-            else :
-                weights_list .append (str (weight ))
-        result [did ]={"term":"|".join (terms_list ),"weight":"|".join (weights_list )}
-    return result
```

#### Горячий участок 17

- document: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\document\normalize.py:131-294
- testitem: нет в ветке

```diff
--- document:normalize.py

+++ testitem:normalize.py

@@ -1,76 +0,0 @@

-def enrich_with_document_terms (df_docs :pd .DataFrame ,client :ChemblClient ,cfg :Mapping [str ,Any ])->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term.\n\n    Parameters\n    ----------\n    df_docs:\n        DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c document_chembl_id.\n    client:\n        ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n    cfg:\n        \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0437 config.chembl.document.enrich.document_term.\n\n    Returns\n    -------\n    pd.DataFrame:\n        \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438:\n        - term (nullable string, pipe-separated terms)\n        - weight (nullable string, pipe-separated weights)\n    "
-    log =UnifiedLogger .get (__name__ ).bind (component ="document_enrichment")
-    def _ensure_term_columns (df_input :pd .DataFrame )->pd .DataFrame :
-        result_frame =df_input .copy ()
-        for column_name in ("term","weight"):
-            if column_name not in result_frame .columns :
-                result_frame [column_name ]=pd .Series ([""for _ in range (len (result_frame ))],index =result_frame .index ,dtype ="string")
-            else :
-                result_frame [column_name ]=result_frame [column_name ].astype ("string")
-            na_mask =result_frame [column_name ].isna ()
-            if bool (na_mask .any ()):
-                result_frame .loc [na_mask ,column_name ]=""
-            result_frame [column_name ]=result_frame [column_name ].astype ("string")
-        return result_frame
-    df_docs =_ensure_columns (df_docs ,_DOCUMENT_TERM_COLUMNS )
-    if df_docs .empty :
-        log .debug ("enrichment_skipped_empty_dataframe")
-        prepared_empty =_ensure_term_columns (df_docs )
-        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (prepared_empty ,lazy =True )
-    required_cols =["document_chembl_id"]
-    missing_cols =[col for col in required_cols if col not in df_docs .columns ]
-    if missing_cols :
-        log .warning ("enrichment_skipped_missing_columns",missing_columns =missing_cols )
-        prepared_missing =_ensure_term_columns (df_docs )
-        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (prepared_missing ,lazy =True )
-    doc_ids :list [str ]=[]
-    for _ ,row in df_docs .iterrows ():
-        doc_id =row .get ("document_chembl_id")
-        if pd .isna (doc_id )or doc_id is None :
-            continue
-        doc_id_str =str (doc_id ).strip ()
-        if doc_id_str :
-            doc_ids .append (doc_id_str )
-    if not doc_ids :
-        log .debug ("enrichment_skipped_no_valid_ids")
-        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (df_docs ,lazy =True )
-    fields =cfg .get ("select_fields",["document_chembl_id","term","weight"])
-    page_limit =cfg .get ("page_limit",1000 )
-    sort =cfg .get ("sort","weight_desc")
-    log .info ("enrichment_fetching_terms",ids_count =len (set (doc_ids )))
-    records_dict =client .fetch_document_terms_by_ids (ids =doc_ids ,fields =list (fields ),page_limit =page_limit )
-    all_records :list [dict [str ,Any ]]=[]
-    for records_list in records_dict .values ():
-        all_records .extend (records_list )
-    agg_result =aggregate_terms (all_records ,sort =sort )
-    enrichment_data :list [dict [str ,Any ]]=[]
-    for doc_id ,term_weight in agg_result .items ():
-        enrichment_data .append ({"document_chembl_id":doc_id ,"term":term_weight ["term"],"weight":term_weight ["weight"]})
-    if not enrichment_data :
-        log .debug ("enrichment_no_records_found")
-        prepared =_ensure_term_columns (df_docs )
-        return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (prepared ,lazy =True )
-    df_enrich =pd .DataFrame (enrichment_data )
-    original_index =df_docs .index .copy ()
-    df_result =df_docs .merge (df_enrich ,on =["document_chembl_id"],how ="left",suffixes =("","_enrich"))
-    for col in ["term","weight"]:
-        enrich_col =f'{col }_enrich'
-        if enrich_col in df_result .columns :
-            base_series =df_result [col ]if col in df_result .columns else pd .Series ([pd .NA ]*len (df_result ),index =df_result .index ,dtype ="object")
-            enrich_series =df_result [enrich_col ]
-            df_result [col ]=base_series .where (pd .notna (base_series ),enrich_series )
-            df_result =df_result .drop (columns =[enrich_col ])
-    for col in ["term","weight"]:
-        if col not in df_result .columns :
-            df_result [col ]=pd .Series ([""for _ in range (len (df_result ))],index =df_result .index ,dtype ="string")
-        else :
-            df_result [col ]=df_result [col ].astype ("string")
-        na_mask =df_result [col ].isna ()
-        if bool (na_mask .any ()):
-            df_result .loc [na_mask ,col ]=""
-        df_result [col ]=df_result [col ].astype ("string")
-    df_result =df_result .reindex (original_index )
-    df_result =_ensure_term_columns (df_result )
-    log .info ("enrichment_completed",rows_enriched =df_result .shape [0 ],documents_with_terms =len (agg_result ))
-    return DOCUMENT_TERMS_ENRICHMENT_SCHEMA .validate (df_result ,lazy =True )
```

---

## Пара: target ↔ testitem

- AST hash: 107171553e4f1c509bba122a3d0ccb96 ↔ 729263c98934cfcb08473bcacfb20e9e

- Jaccard по токенам: 0.302

### Модуль run.py

Определение                                          | target сигнатура                                     | testitem сигнатура                                                           | Побочные эффекты                                                                                                                                                     | Исключения              | Статус           
-----------------------------------------------------|------------------------------------------------------|------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|------------------
ChemblTargetPipeline                                 | —                                                    | —                                                                            | target: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}
testitem: {}                                                | target: []
testitem: [] | только в target  
ChemblTargetPipeline.__init__                        | self, config: PipelineConfig, run_id: str            | —                                                                            | target: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | target: []
testitem: [] | только в target  
ChemblTargetPipeline._build_target_descriptor        | self                                                 | —                                                                            | target: {'logging': ['log.info'], 'io': []}
testitem: {}                                                                                                             | target: []
testitem: [] | только в target  
ChemblTargetPipeline._enrich_protein_classifications | self, df: pd.DataFrame, log: Any                     | —                                                                            | target: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}
testitem: {}                                                                     | target: []
testitem: [] | только в target  
ChemblTargetPipeline._enrich_target_components       | self, df: pd.DataFrame, log: Any                     | —                                                                            | target: {'logging': ['log.debug', 'log.info', 'log.warning'], 'io': ['json.dumps']}
testitem: {}                                                                     | target: []
testitem: [] | только в target  
ChemblTargetPipeline._harmonize_identifier_columns   | self, df: pd.DataFrame, log: Any                     | —                                                                            | target: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                            | target: []
testitem: [] | только в target  
ChemblTargetPipeline._normalize_data_types           | self, df: pd.DataFrame, schema: Any | None, log: Any | —                                                                            | target: {'logging': ['log.warning'], 'io': []}
testitem: {}                                                                                                          | target: []
testitem: [] | только в target  
ChemblTargetPipeline._normalize_identifiers          | self, df: pd.DataFrame, log: Any                     | —                                                                            | target: {'logging': ['log.warning'], 'io': []}
testitem: {}                                                                                                          | target: []
testitem: [] | только в target  
ChemblTargetPipeline._normalize_string_fields        | self, df: pd.DataFrame, log: Any                     | —                                                                            | target: {'logging': ['log.debug'], 'io': []}
testitem: {}                                                                                                            | target: []
testitem: [] | только в target  
ChemblTargetPipeline.extract                         | self, *args, **kwargs                                | —                                                                            | target: {'logging': ['UnifiedLogger.get'], 'io': []}
testitem: {}                                                                                                    | target: []
testitem: [] | только в target  
ChemblTargetPipeline.extract_all                     | self                                                 | —                                                                            | target: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | target: []
testitem: [] | только в target  
ChemblTargetPipeline.extract_by_ids                  | self, ids: Sequence[str]                             | —                                                                            | target: {'logging': ['UnifiedLogger.get', 'log.info'], 'io': []}
testitem: {}                                                                                        | target: []
testitem: [] | только в target  
ChemblTargetPipeline.transform                       | self, df: pd.DataFrame                               | —                                                                            | target: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}
testitem: {}                                                                           | target: []
testitem: [] | только в target  
TestItemChemblPipeline                               | —                                                    | —                                                                            | target: {}
testitem: {'logging': ['UnifiedLogger.get', 'bound_log.info', 'bound_log.warning', 'log.debug', 'log.info', 'log.warning'], 'io': ['status_payload.get']} | target: []
testitem: [] | только в testitem
TestItemChemblPipeline.__init__                      | —                                                    | self, config: PipelineConfig, run_id: str                                    | target: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | target: []
testitem: [] | только в testitem
TestItemChemblPipeline._build_testitem_descriptor    | —                                                    | self: SelfTestitemChemblPipeline                                             | target: {}
testitem: {'logging': ['log.debug', 'log.info'], 'io': []}                                                                                                | target: []
testitem: [] | только в testitem
TestItemChemblPipeline._check_empty_columns          | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: {}
testitem: {'logging': ['log.warning'], 'io': []}                                                                                                          | target: []
testitem: [] | только в testitem
TestItemChemblPipeline._deduplicate_molecules        | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: {}
testitem: {'logging': ['log.info'], 'io': []}                                                                                                             | target: []
testitem: [] | только в testitem
TestItemChemblPipeline._fetch_chembl_release         | —                                                    | self, client: UnifiedAPIClient | ChemblClient | Any, log: BoundLogger | None | target: {}
testitem: {'logging': ['UnifiedLogger.get', 'bound_log.info', 'bound_log.warning'], 'io': ['status_payload.get']}                                         | target: []
testitem: [] | только в testitem
TestItemChemblPipeline._flatten_nested_structures    | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | target: []
testitem: [] | только в testitem
TestItemChemblPipeline._normalize_identifiers        | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | target: []
testitem: [] | только в testitem
TestItemChemblPipeline._normalize_numeric_fields     | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | target: []
testitem: [] | только в testitem
TestItemChemblPipeline._normalize_string_fields      | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | target: []
testitem: [] | только в testitem
TestItemChemblPipeline._remove_extra_columns         | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: {}
testitem: {'logging': ['log.debug'], 'io': []}                                                                                                            | target: []
testitem: [] | только в testitem
TestItemChemblPipeline._schema_column_specs          | —                                                    | self                                                                         | target: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | target: []
testitem: [] | только в testitem
TestItemChemblPipeline.api_version                   | —                                                    | self                                                                         | target: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | target: []
testitem: [] | только в testitem
TestItemChemblPipeline.augment_metadata              | —                                                    | self, metadata: Mapping[str, object], df: pd.DataFrame                       | target: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | target: []
testitem: [] | только в testitem
TestItemChemblPipeline.chembl_db_version             | —                                                    | self                                                                         | target: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | target: []
testitem: [] | только в testitem
TestItemChemblPipeline.extract                       | —                                                    | self, *args, **kwargs                                                        | target: {}
testitem: {'logging': ['UnifiedLogger.get'], 'io': []}                                                                                                    | target: []
testitem: [] | только в testitem
TestItemChemblPipeline.extract_all                   | —                                                    | self                                                                         | target: {}
testitem: {'logging': [], 'io': []}                                                                                                                       | target: []
testitem: [] | только в testitem
TestItemChemblPipeline.extract_by_ids                | —                                                    | self, ids: Sequence[str]                                                     | target: {}
testitem: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                                                                           | target: []
testitem: [] | только в testitem
TestItemChemblPipeline.transform                     | —                                                    | self, df: pd.DataFrame                                                       | target: {}
testitem: {'logging': ['UnifiedLogger.get', 'log.debug', 'log.info'], 'io': []}                                                                           | target: []
testitem: [] | только в testitem
__module_block_0                                     | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_1                                     | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | совпадает        
__module_block_10                                    | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_11                                    | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_12                                    | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_13                                    | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_14                                    | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_15                                    | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_16                                    | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_17                                    | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_18                                    | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_19                                    | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {}                                                                                                                       | target: []
testitem: [] | только в target  
__module_block_2                                     | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_3                                     | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | совпадает        
__module_block_4                                     | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | совпадает        
__module_block_5                                     | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_6                                     | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_7                                     | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | совпадает        
__module_block_8                                     | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       
__module_block_9                                     | —                                                    | —                                                                            | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []}                                                                                                | target: []
testitem: [] | отличается       

_Показаны первые 20 горячих участков из 48._

#### Горячий участок 1

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:38-783
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,342 +0,0 @@

-class ChemblTargetPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting target records from the ChEMBL API."
-    actor ="target_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch target payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_target.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="target_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all target records from ChEMBL using pagination."
-        descriptor =self ._build_target_descriptor ()
-        return self .run_extract_all (descriptor )
-    def _build_target_descriptor (self )->ChemblExtractionDescriptor ["ChemblTargetPipeline"]:
-        "Return the descriptor powering target extraction."
-        def build_context (pipeline :"ChemblTargetPipeline",source_config :TargetSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            base_url =pipeline ._resolve_base_url (source_config .parameters )
-            http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
-            chembl_client =ChemblClient (http_client )
-            pipeline ._chembl_release =pipeline .fetch_chembl_release (chembl_client ,log )
-            target_client =ChemblTargetClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-            select_fields =source_config .parameters .select_fields
-            return ChemblExtractionContext (source_config =source_config ,iterator =target_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =pipeline ._chembl_release ,extra_filters ={"batch_size":source_config .batch_size })
-        def empty_frame (_ :"ChemblTargetPipeline",__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"target_chembl_id":pd .Series (dtype ="string")})
-        def dry_run_handler (pipeline :"ChemblTargetPipeline",_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =pipeline ._chembl_release )
-            return pd .DataFrame ()
-        def summary_extra (pipeline :"ChemblTargetPipeline",_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
-            return {"limit":pipeline .config .cli .limit }
-        return ChemblExtractionDescriptor [ChemblTargetPipeline ](name ="chembl_target",source_name ="chembl",source_config_factory =TargetSourceConfig .from_source_config ,build_context =build_context ,id_column ="target_chembl_id",summary_event ="chembl_target.extract_summary",sort_by =("target_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract target records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of target_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted target records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =TargetSourceConfig .from_source_config (source_raw )
-        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
-        chembl_client =ChemblClient (http_client )
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        if self .config .cli .dry_run :
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-            return pd .DataFrame ()
-        batch_size =source_config .batch_size
-        limit =self .config .cli .limit
-        select_fields =source_config .parameters .select_fields
-        target_client =ChemblTargetClient (chembl_client ,batch_size =min (batch_size ,25 ))
-        def fetch_targets (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            iterator =target_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield dict (item )
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="target_chembl_id",fetcher =fetch_targets ,select_fields =select_fields ,batch_size =batch_size ,chunk_size =min (batch_size ,100 ),max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_target.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw target data by normalizing fields and enriching with component/classification data."
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =serialize_target_arrays (df ,self .config )
-        if not self .config .cli .dry_run :
-            df =self ._enrich_target_components (df ,log )
-        if not self .config .cli .dry_run :
-            df =self ._enrich_protein_classifications (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        df =self ._normalize_data_types (df ,TargetSchema ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Harmonize identifier column names."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-            df ["target_chembl_id"]=df ["target_id"]
-            actions .append ("target_id->target_chembl_id")
-        alias_columns =[column for column in ("target_id",)if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize ChEMBL identifiers with regex validation."
-        rules =[IdentifierRule (name ="target_chembl",columns =["target_chembl_id"],pattern ="^CHEMBL\\d+$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        invalid_info =stats .per_column .get ("target_chembl_id")
-        if invalid_info and invalid_info ["invalid"]>0 :
-            log .warning ("invalid_target_chembl_id",count =invalid_info ["invalid"])
-        return normalized_df
-    def _enrich_target_components (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Enrich targets with component data from /target_component endpoint.\n\n        Only enriches targets where uniprot_accessions or component_count are missing.\n        If data is already present from the main query (via serialize_target_arrays),\n        it will not be overwritten.\n        "
-        df =df .copy ()
-        if df .empty or "target_chembl_id"not in df .columns :
-            return df
-        needs_enrichment =df ["target_chembl_id"].notna ()
-        if "uniprot_accessions"in df .columns :
-            needs_enrichment =needs_enrichment &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
-        if "component_count"in df .columns :
-            needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["component_count"].isna ()|(df ["component_count"]==0 ))
-        target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
-        if not target_ids_to_enrich :
-            log .debug ("enrich_target_components_skipped",reason ="all_data_present")
-            return df
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =TargetSourceConfig .from_source_config (source_raw )
-        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
-        chembl_client =ChemblClient (http_client )
-        if "target_chembl_id"not in df .columns :
-            return df
-        component_map :dict [str ,list [str ]]={}
-        target_ids_set :set [str ]=set (target_ids_to_enrich )
-        def _is_target (value :object )->bool :
-            if value is None or value is pd .NA :
-                return False
-            if isinstance (value ,Real )and math .isnan (float (value )):
-                return False
-            normalized =str (value ).strip ()
-            if not normalized :
-                return False
-            return normalized in target_ids_set
-        target_membership =df ["target_chembl_id"].map (_is_target )
-        log .info ("enrich_target_components_start",target_count =len (target_ids_to_enrich ))
-        for target_id in target_ids_to_enrich :
-            try :
-                components :list [str ]=[]
-                for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
-                    accession =item .get ("accession")
-                    if isinstance (accession ,str )and accession .strip ():
-                        components .append (accession .strip ())
-                if components :
-                    component_map [target_id ]=components
-            except Exception as exc :
-                log .warning ("target_component_fetch_error",target_chembl_id =target_id ,error =str (exc ))
-        if "uniprot_accessions"in df .columns :
-            mask =target_membership &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
-            if mask .any ():
-                df .loc [mask ,"uniprot_accessions"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
-        if "component_count"in df .columns :
-            mask =target_membership &(df ["component_count"].isna ()|(df ["component_count"]==0 ))
-            if mask .any ():
-                df .loc [mask ,"component_count"]=df .loc [mask ,"target_chembl_id"].map (lambda x :len (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
-        log .info ("enrich_target_components_complete",enriched_count =len (component_map ))
-        return df
-    def _enrich_protein_classifications (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Enrich targets with full protein classification hierarchy.\n\n        Extracts complete protein classification hierarchy with tree nodes and expanded paths l1..l8.\n        Algorithm:\n        1. For each target, get components via /target_component.json\n        2. Filter only PROTEIN components (component_type = 'PROTEIN')\n        3. For each protein component, get classes via /component_class.json \u2192 protein_class_id\n        4. For each protein_class_id, get node metadata via /protein_classification.json\n        5. For each protein_class_id, get expanded path via /protein_family_classification.json \u2192 l1..l8\n        6. Aggregate at TID level: protein_class_list (array) and protein_class_top (min class_level)\n\n        Only enriches targets where protein_class_list or protein_class_top are missing.\n        If data is already present from the main query, it will not be overwritten.\n        "
-        df =df .copy ()
-        if df .empty or "target_chembl_id"not in df .columns :
-            return df
-        needs_enrichment =df ["target_chembl_id"].notna ()
-        if "protein_class_list"in df .columns :
-            needs_enrichment =needs_enrichment &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
-        if "protein_class_top"in df .columns :
-            needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
-        target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
-        if not target_ids_to_enrich :
-            log .debug ("enrich_protein_classifications_skipped",reason ="all_data_present")
-            return df
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =TargetSourceConfig .from_source_config (source_raw )
-        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
-        chembl_client =ChemblClient (http_client )
-        if "protein_class_list"not in df .columns :
-            df ["protein_class_list"]=pd .NA
-        if "protein_class_top"not in df .columns :
-            df ["protein_class_top"]=pd .NA
-        if "target_chembl_id"not in df .columns :
-            return df
-        classification_list_map :dict [str ,list [dict [str ,Any ]]]={}
-        classification_top_map :dict [str ,dict [str ,Any ]]={}
-        target_ids_set :set [str ]=set (target_ids_to_enrich )
-        def _is_target (value :object )->bool :
-            if value is None or value is pd .NA :
-                return False
-            if isinstance (value ,Real )and math .isnan (float (value )):
-                return False
-            normalized =str (value ).strip ()
-            if not normalized :
-                return False
-            return normalized in target_ids_set
-        target_membership =df ["target_chembl_id"].map (_is_target )
-        log .info ("enrich_protein_classifications_start",target_count =len (target_ids_to_enrich ))
-        for target_id in target_ids_to_enrich :
-            try :
-                component_ids :list [str ]=[]
-                for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
-                    component_id =item .get ("component_id")
-                    if component_id is not None :
-                        component_ids .append (str (component_id ))
-                if not component_ids :
-                    continue
-                protein_component_ids :list [str ]=[]
-                for component_id in component_ids :
-                    try :
-                        for seq_item in chembl_client .paginate ("/component_sequence.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_sequences"):
-                            component_type =seq_item .get ("component_type")
-                            if isinstance (component_type ,str )and component_type .upper ()=="PROTEIN":
-                                protein_component_ids .append (component_id )
-                                break
-                    except Exception as exc :
-                        log .debug ("component_sequence_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
-                if not protein_component_ids :
-                    continue
-                protein_class_ids :set [str ]=set ()
-                for component_id in protein_component_ids :
-                    try :
-                        for class_item in chembl_client .paginate ("/component_class.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_classes"):
-                            protein_class_id =class_item .get ("protein_class_id")
-                            if protein_class_id is not None :
-                                protein_class_ids .add (str (protein_class_id ))
-                    except Exception as exc :
-                        log .debug ("component_class_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
-                if not protein_class_ids :
-                    continue
-                protein_classes :list [dict [str ,Any ]]=[]
-                for protein_class_id in protein_class_ids :
-                    try :
-                        node_metadata :dict [str ,Any ]|None =None
-                        for node_item in chembl_client .paginate ("/protein_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_classifications"):
-                            node_metadata ={"protein_class_id":str (protein_class_id ),"pref_name":node_item .get ("pref_name"),"short_name":node_item .get ("short_name"),"class_level":node_item .get ("class_level"),"parent_id":node_item .get ("parent_id"),"protein_class_desc":node_item .get ("protein_class_desc")}
-                            break
-                        path_levels :list [str |None ]=[None ]*8
-                        try :
-                            for path_item in chembl_client .paginate ("/protein_family_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_family_classifications"):
-                                for i in range (1 ,9 ):
-                                    level_key =f'l{i }'
-                                    level_value =path_item .get (level_key )
-                                    if level_value is not None :
-                                        if isinstance (level_value ,(float ,int )):
-                                            if pd .isna (level_value ):
-                                                path_levels [i -1 ]=None
-                                            else :
-                                                path_levels [i -1 ]=str (level_value )
-                                        else :
-                                            path_levels [i -1 ]=str (level_value )
-                                break
-                        except Exception as exc :
-                            log .debug ("protein_family_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
-                        if node_metadata :
-                            class_obj :dict [str ,Any ]={"protein_class_id":node_metadata ["protein_class_id"],"pref_name":node_metadata .get ("pref_name"),"short_name":node_metadata .get ("short_name"),"class_level":node_metadata .get ("class_level"),"parent_id":node_metadata .get ("parent_id"),"protein_class_desc":node_metadata .get ("protein_class_desc"),"path":[level for level in path_levels if level is not None ]}
-                            protein_classes .append (class_obj )
-                    except Exception as exc :
-                        log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
-                if protein_classes :
-                    seen_ids :set [str ]=set ()
-                    unique_classes :list [dict [str ,Any ]]=[]
-                    for class_obj in protein_classes :
-                        class_id =class_obj .get ("protein_class_id")
-                        if class_id and class_id not in seen_ids :
-                            seen_ids .add (class_id )
-                            unique_classes .append (class_obj )
-                    unique_classes .sort (key =lambda x :(x .get ("class_level")is None ,x .get ("class_level")or 0 ))
-                    classification_list_map [target_id ]=unique_classes
-                    top_class :dict [str ,Any ]|None =None
-                    min_level :int |None =None
-                    for class_obj in unique_classes :
-                        level =class_obj .get ("class_level")
-                        if level is not None :
-                            try :
-                                level_int =int (level )if not isinstance (level ,int )else level
-                                if min_level is None or level_int <min_level :
-                                    min_level =level_int
-                                    top_class =class_obj
-                            except (ValueError ,TypeError ):
-                                continue
-                    if top_class :
-                        classification_top_map [target_id ]=top_class
-            except Exception as exc :
-                log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,error =str (exc ))
-        if "protein_class_list"in df .columns :
-            mask =target_membership &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
-            if mask .any ():
-                df .loc [mask ,"protein_class_list"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_list_map .get (str (x ),[]),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_list_map else pd .NA )
-        if "protein_class_top"in df .columns :
-            mask =target_membership &(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
-            if mask .any ():
-                df .loc [mask ,"protein_class_top"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_top_map .get (str (x ),{}),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_top_map else pd .NA )
-        log .info ("enrich_protein_classifications_complete",enriched_list_count =len (classification_list_map ),enriched_top_count =len (classification_top_map ))
-        return df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields by trimming whitespace."
-        working_df =df .copy ()
-        rules ={"pref_name":StringRule (),"target_type":StringRule (),"organism":StringRule (),"tax_id":StringRule ()}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any |None ,log :Any )->pd .DataFrame :
-        "Normalize data types to match schema expectations.\n\n        Overrides base implementation to handle component_count and species_group_flag specially.\n        "
-        df =super ()._normalize_data_types (df ,schema ,log )
-        def _coerce_nullable_int (value :object )->object :
-            if value is None or value is pd .NA :
-                return pd .NA
-            if isinstance (value ,bool ):
-                return int (value )
-            if isinstance (value ,Integral ):
-                return int (value )
-            if isinstance (value ,Decimal ):
-                if value .is_nan ()or value %1 !=0 :
-                    return pd .NA
-                return int (value )
-            if isinstance (value ,Real ):
-                float_value =float (value )
-                if not math .isfinite (float_value )or not float_value .is_integer ():
-                    return pd .NA
-                return int (float_value )
-            if isinstance (value ,str ):
-                stripped =value .strip ()
-                if not stripped :
-                    return pd .NA
-                try :
-                    decimal_value =Decimal (stripped )
-                except (InvalidOperation ,ValueError ):
-                    return pd .NA
-                if decimal_value .is_nan ()or decimal_value %1 !=0 :
-                    return pd .NA
-                return int (decimal_value )
-            return pd .NA
-        if "component_count"in df .columns :
-            coerced_component_count =df ["component_count"].map (_coerce_nullable_int )
-            df ["component_count"]=coerced_component_count .astype ("Int64")
-        if "species_group_flag"in df .columns :
-            try :
-                coerced_species_group_flag =df ["species_group_flag"].map (_coerce_nullable_int )
-                df ["species_group_flag"]=coerced_species_group_flag .astype ("Int64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("species_group_flag_conversion_failed",error =str (exc ))
-        return df
```

#### Горячий участок 2

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:43-44
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,2 +0,0 @@

-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
```

#### Горячий участок 3

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:72-141
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,19 +0,0 @@

-def _build_target_descriptor (self )->ChemblExtractionDescriptor ["ChemblTargetPipeline"]:
-    "Return the descriptor powering target extraction."
-    def build_context (pipeline :"ChemblTargetPipeline",source_config :TargetSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        base_url =pipeline ._resolve_base_url (source_config .parameters )
-        http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
-        chembl_client =ChemblClient (http_client )
-        pipeline ._chembl_release =pipeline .fetch_chembl_release (chembl_client ,log )
-        target_client =ChemblTargetClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-        select_fields =source_config .parameters .select_fields
-        return ChemblExtractionContext (source_config =source_config ,iterator =target_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =pipeline ._chembl_release ,extra_filters ={"batch_size":source_config .batch_size })
-    def empty_frame (_ :"ChemblTargetPipeline",__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"target_chembl_id":pd .Series (dtype ="string")})
-    def dry_run_handler (pipeline :"ChemblTargetPipeline",_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =pipeline ._chembl_release )
-        return pd .DataFrame ()
-    def summary_extra (pipeline :"ChemblTargetPipeline",_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
-        return {"limit":pipeline .config .cli .limit }
-    return ChemblExtractionDescriptor [ChemblTargetPipeline ](name ="chembl_target",source_name ="chembl",source_config_factory =TargetSourceConfig .from_source_config ,build_context =build_context ,id_column ="target_chembl_id",summary_event ="chembl_target.extract_summary",sort_by =("target_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
```

#### Горячий участок 4

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:418-708
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,136 +0,0 @@

-def _enrich_protein_classifications (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Enrich targets with full protein classification hierarchy.\n\n        Extracts complete protein classification hierarchy with tree nodes and expanded paths l1..l8.\n        Algorithm:\n        1. For each target, get components via /target_component.json\n        2. Filter only PROTEIN components (component_type = 'PROTEIN')\n        3. For each protein component, get classes via /component_class.json \u2192 protein_class_id\n        4. For each protein_class_id, get node metadata via /protein_classification.json\n        5. For each protein_class_id, get expanded path via /protein_family_classification.json \u2192 l1..l8\n        6. Aggregate at TID level: protein_class_list (array) and protein_class_top (min class_level)\n\n        Only enriches targets where protein_class_list or protein_class_top are missing.\n        If data is already present from the main query, it will not be overwritten.\n        "
-    df =df .copy ()
-    if df .empty or "target_chembl_id"not in df .columns :
-        return df
-    needs_enrichment =df ["target_chembl_id"].notna ()
-    if "protein_class_list"in df .columns :
-        needs_enrichment =needs_enrichment &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
-    if "protein_class_top"in df .columns :
-        needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
-    target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
-    if not target_ids_to_enrich :
-        log .debug ("enrich_protein_classifications_skipped",reason ="all_data_present")
-        return df
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =TargetSourceConfig .from_source_config (source_raw )
-    base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-    http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
-    chembl_client =ChemblClient (http_client )
-    if "protein_class_list"not in df .columns :
-        df ["protein_class_list"]=pd .NA
-    if "protein_class_top"not in df .columns :
-        df ["protein_class_top"]=pd .NA
-    if "target_chembl_id"not in df .columns :
-        return df
-    classification_list_map :dict [str ,list [dict [str ,Any ]]]={}
-    classification_top_map :dict [str ,dict [str ,Any ]]={}
-    target_ids_set :set [str ]=set (target_ids_to_enrich )
-    def _is_target (value :object )->bool :
-        if value is None or value is pd .NA :
-            return False
-        if isinstance (value ,Real )and math .isnan (float (value )):
-            return False
-        normalized =str (value ).strip ()
-        if not normalized :
-            return False
-        return normalized in target_ids_set
-    target_membership =df ["target_chembl_id"].map (_is_target )
-    log .info ("enrich_protein_classifications_start",target_count =len (target_ids_to_enrich ))
-    for target_id in target_ids_to_enrich :
-        try :
-            component_ids :list [str ]=[]
-            for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
-                component_id =item .get ("component_id")
-                if component_id is not None :
-                    component_ids .append (str (component_id ))
-            if not component_ids :
-                continue
-            protein_component_ids :list [str ]=[]
-            for component_id in component_ids :
-                try :
-                    for seq_item in chembl_client .paginate ("/component_sequence.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_sequences"):
-                        component_type =seq_item .get ("component_type")
-                        if isinstance (component_type ,str )and component_type .upper ()=="PROTEIN":
-                            protein_component_ids .append (component_id )
-                            break
-                except Exception as exc :
-                    log .debug ("component_sequence_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
-            if not protein_component_ids :
-                continue
-            protein_class_ids :set [str ]=set ()
-            for component_id in protein_component_ids :
-                try :
-                    for class_item in chembl_client .paginate ("/component_class.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_classes"):
-                        protein_class_id =class_item .get ("protein_class_id")
-                        if protein_class_id is not None :
-                            protein_class_ids .add (str (protein_class_id ))
-                except Exception as exc :
-                    log .debug ("component_class_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
-            if not protein_class_ids :
-                continue
-            protein_classes :list [dict [str ,Any ]]=[]
-            for protein_class_id in protein_class_ids :
-                try :
-                    node_metadata :dict [str ,Any ]|None =None
-                    for node_item in chembl_client .paginate ("/protein_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_classifications"):
-                        node_metadata ={"protein_class_id":str (protein_class_id ),"pref_name":node_item .get ("pref_name"),"short_name":node_item .get ("short_name"),"class_level":node_item .get ("class_level"),"parent_id":node_item .get ("parent_id"),"protein_class_desc":node_item .get ("protein_class_desc")}
-                        break
-                    path_levels :list [str |None ]=[None ]*8
-                    try :
-                        for path_item in chembl_client .paginate ("/protein_family_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_family_classifications"):
-                            for i in range (1 ,9 ):
-                                level_key =f'l{i }'
-                                level_value =path_item .get (level_key )
-                                if level_value is not None :
-                                    if isinstance (level_value ,(float ,int )):
-                                        if pd .isna (level_value ):
-                                            path_levels [i -1 ]=None
-                                        else :
-                                            path_levels [i -1 ]=str (level_value )
-                                    else :
-                                        path_levels [i -1 ]=str (level_value )
-                            break
-                    except Exception as exc :
-                        log .debug ("protein_family_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
-                    if node_metadata :
-                        class_obj :dict [str ,Any ]={"protein_class_id":node_metadata ["protein_class_id"],"pref_name":node_metadata .get ("pref_name"),"short_name":node_metadata .get ("short_name"),"class_level":node_metadata .get ("class_level"),"parent_id":node_metadata .get ("parent_id"),"protein_class_desc":node_metadata .get ("protein_class_desc"),"path":[level for level in path_levels if level is not None ]}
-                        protein_classes .append (class_obj )
-                except Exception as exc :
-                    log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
-            if protein_classes :
-                seen_ids :set [str ]=set ()
-                unique_classes :list [dict [str ,Any ]]=[]
-                for class_obj in protein_classes :
-                    class_id =class_obj .get ("protein_class_id")
-                    if class_id and class_id not in seen_ids :
-                        seen_ids .add (class_id )
-                        unique_classes .append (class_obj )
-                unique_classes .sort (key =lambda x :(x .get ("class_level")is None ,x .get ("class_level")or 0 ))
-                classification_list_map [target_id ]=unique_classes
-                top_class :dict [str ,Any ]|None =None
-                min_level :int |None =None
-                for class_obj in unique_classes :
-                    level =class_obj .get ("class_level")
-                    if level is not None :
-                        try :
-                            level_int =int (level )if not isinstance (level ,int )else level
-                            if min_level is None or level_int <min_level :
-                                min_level =level_int
-                                top_class =class_obj
-                        except (ValueError ,TypeError ):
-                            continue
-                if top_class :
-                    classification_top_map [target_id ]=top_class
-        except Exception as exc :
-            log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,error =str (exc ))
-    if "protein_class_list"in df .columns :
-        mask =target_membership &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
-        if mask .any ():
-            df .loc [mask ,"protein_class_list"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_list_map .get (str (x ),[]),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_list_map else pd .NA )
-    if "protein_class_top"in df .columns :
-        mask =target_membership &(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
-        if mask .any ():
-            df .loc [mask ,"protein_class_top"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_top_map .get (str (x ),{}),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_top_map else pd .NA )
-    log .info ("enrich_protein_classifications_complete",enriched_list_count =len (classification_list_map ),enriched_top_count =len (classification_top_map ))
-    return df
```

#### Горячий участок 5

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:312-416
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,55 +0,0 @@

-def _enrich_target_components (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Enrich targets with component data from /target_component endpoint.\n\n        Only enriches targets where uniprot_accessions or component_count are missing.\n        If data is already present from the main query (via serialize_target_arrays),\n        it will not be overwritten.\n        "
-    df =df .copy ()
-    if df .empty or "target_chembl_id"not in df .columns :
-        return df
-    needs_enrichment =df ["target_chembl_id"].notna ()
-    if "uniprot_accessions"in df .columns :
-        needs_enrichment =needs_enrichment &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
-    if "component_count"in df .columns :
-        needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["component_count"].isna ()|(df ["component_count"]==0 ))
-    target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
-    if not target_ids_to_enrich :
-        log .debug ("enrich_target_components_skipped",reason ="all_data_present")
-        return df
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =TargetSourceConfig .from_source_config (source_raw )
-    base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-    http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
-    chembl_client =ChemblClient (http_client )
-    if "target_chembl_id"not in df .columns :
-        return df
-    component_map :dict [str ,list [str ]]={}
-    target_ids_set :set [str ]=set (target_ids_to_enrich )
-    def _is_target (value :object )->bool :
-        if value is None or value is pd .NA :
-            return False
-        if isinstance (value ,Real )and math .isnan (float (value )):
-            return False
-        normalized =str (value ).strip ()
-        if not normalized :
-            return False
-        return normalized in target_ids_set
-    target_membership =df ["target_chembl_id"].map (_is_target )
-    log .info ("enrich_target_components_start",target_count =len (target_ids_to_enrich ))
-    for target_id in target_ids_to_enrich :
-        try :
-            components :list [str ]=[]
-            for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
-                accession =item .get ("accession")
-                if isinstance (accession ,str )and accession .strip ():
-                    components .append (accession .strip ())
-            if components :
-                component_map [target_id ]=components
-        except Exception as exc :
-            log .warning ("target_component_fetch_error",target_chembl_id =target_id ,error =str (exc ))
-    if "uniprot_accessions"in df .columns :
-        mask =target_membership &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
-        if mask .any ():
-            df .loc [mask ,"uniprot_accessions"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
-    if "component_count"in df .columns :
-        mask =target_membership &(df ["component_count"].isna ()|(df ["component_count"]==0 ))
-        if mask .any ():
-            df .loc [mask ,"component_count"]=df .loc [mask ,"target_chembl_id"].map (lambda x :len (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
-    log .info ("enrich_target_components_complete",enriched_count =len (component_map ))
-    return df
```

#### Горячий участок 6

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:272-289
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,14 +0,0 @@

-def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Harmonize identifier column names."
-    df =df .copy ()
-    actions :list [str ]=[]
-    if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-        df ["target_chembl_id"]=df ["target_id"]
-        actions .append ("target_id->target_chembl_id")
-    alias_columns =[column for column in ("target_id",)if column in df .columns ]
-    if alias_columns :
-        df =df .drop (columns =alias_columns )
-        actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-    if actions :
-        log .debug ("identifier_harmonization",actions =actions )
-    return df
```

#### Горячий участок 7

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:732-783
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,41 +0,0 @@

-def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any |None ,log :Any )->pd .DataFrame :
-    "Normalize data types to match schema expectations.\n\n        Overrides base implementation to handle component_count and species_group_flag specially.\n        "
-    df =super ()._normalize_data_types (df ,schema ,log )
-    def _coerce_nullable_int (value :object )->object :
-        if value is None or value is pd .NA :
-            return pd .NA
-        if isinstance (value ,bool ):
-            return int (value )
-        if isinstance (value ,Integral ):
-            return int (value )
-        if isinstance (value ,Decimal ):
-            if value .is_nan ()or value %1 !=0 :
-                return pd .NA
-            return int (value )
-        if isinstance (value ,Real ):
-            float_value =float (value )
-            if not math .isfinite (float_value )or not float_value .is_integer ():
-                return pd .NA
-            return int (float_value )
-        if isinstance (value ,str ):
-            stripped =value .strip ()
-            if not stripped :
-                return pd .NA
-            try :
-                decimal_value =Decimal (stripped )
-            except (InvalidOperation ,ValueError ):
-                return pd .NA
-            if decimal_value .is_nan ()or decimal_value %1 !=0 :
-                return pd .NA
-            return int (decimal_value )
-        return pd .NA
-    if "component_count"in df .columns :
-        coerced_component_count =df ["component_count"].map (_coerce_nullable_int )
-        df ["component_count"]=coerced_component_count .astype ("Int64")
-    if "species_group_flag"in df .columns :
-        try :
-            coerced_species_group_flag =df ["species_group_flag"].map (_coerce_nullable_int )
-            df ["species_group_flag"]=coerced_species_group_flag .astype ("Int64")
-        except (ValueError ,TypeError )as exc :
-            log .warning ("species_group_flag_conversion_failed",error =str (exc ))
-    return df
```

#### Горячий участок 8

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:291-310
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,8 +0,0 @@

-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize ChEMBL identifiers with regex validation."
-    rules =[IdentifierRule (name ="target_chembl",columns =["target_chembl_id"],pattern ="^CHEMBL\\d+$")]
-    normalized_df ,stats =normalize_identifier_columns (df ,rules )
-    invalid_info =stats .per_column .get ("target_chembl_id")
-    if invalid_info and invalid_info ["invalid"]>0 :
-        log .warning ("invalid_target_chembl_id",count =invalid_info ["invalid"])
-    return normalized_df
```

#### Горячий участок 9

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:710-730
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,8 +0,0 @@

-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields by trimming whitespace."
-    working_df =df .copy ()
-    rules ={"pref_name":StringRule (),"target_type":StringRule (),"organism":StringRule (),"tax_id":StringRule ()}
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    return normalized_df
```

#### Горячий участок 10

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:50-64
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,4 +0,0 @@

-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch target payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_target.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="target_chembl_id")
```

#### Горячий участок 11

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:66-70
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,4 +0,0 @@

-def extract_all (self )->pd .DataFrame :
-    "Extract all target records from ChEMBL using pagination."
-    descriptor =self ._build_target_descriptor ()
-    return self .run_extract_all (descriptor )
```

#### Горячий участок 12

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:143-222
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,26 +0,0 @@

-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract target records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of target_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted target records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =TargetSourceConfig .from_source_config (source_raw )
-    base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-    http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
-    chembl_client =ChemblClient (http_client )
-    self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-    if self .config .cli .dry_run :
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-        return pd .DataFrame ()
-    batch_size =source_config .batch_size
-    limit =self .config .cli .limit
-    select_fields =source_config .parameters .select_fields
-    target_client =ChemblTargetClient (chembl_client ,batch_size =min (batch_size ,25 ))
-    def fetch_targets (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        iterator =target_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield dict (item )
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="target_chembl_id",fetcher =fetch_targets ,select_fields =select_fields ,batch_size =batch_size ,chunk_size =min (batch_size ,100 ),max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release )
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_target.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-    return dataframe
```

#### Горячий участок 13

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\run.py:224-266
- testitem: нет в ветке

```diff
--- target:run.py

+++ testitem:run.py

@@ -1,22 +0,0 @@

-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw target data by normalizing fields and enriching with component/classification data."
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-    df =df .copy ()
-    df =self ._harmonize_identifier_columns (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =serialize_target_arrays (df ,self .config )
-    if not self .config .cli .dry_run :
-        df =self ._enrich_target_components (df ,log )
-    if not self .config .cli .dry_run :
-        df =self ._enrich_protein_classifications (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    df =self ._normalize_data_types (df ,TargetSchema ,log )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

#### Горячий участок 14

- target: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\run.py:52-807

```diff
--- target:run.py

+++ testitem:run.py

@@ -0,0 +1,296 @@

+class TestItemChemblPipeline (ChemblPipelineBase ):
+    "ETL pipeline extracting molecule records from the ChEMBL API."
+    actor ="testitem_chembl"
+    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+        super ().__init__ (config ,run_id )
+        self ._chembl_db_version :str |None =None
+        self ._api_version :str |None =None
+    @property
+    def chembl_db_version (self )->str |None :
+        "Return the cached ChEMBL DB version captured during extraction."
+        return self ._chembl_db_version
+    @property
+    def api_version (self )->str |None :
+        "Return the cached ChEMBL API version captured during extraction."
+        return self ._api_version
+    def _fetch_chembl_release (self ,client :UnifiedAPIClient |ChemblClient |Any ,log :BoundLogger |None =None )->str |None :
+        "Capture ChEMBL release and API version from status endpoint."
+        if log is None :
+            bound_log :BoundLogger =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
+        else :
+            bound_log =log
+        request_timestamp =datetime .now (timezone .utc )
+        release_value :str |None =None
+        api_version :str |None =None
+        try :
+            status_payload :dict [str ,Any ]={}
+            handshake_candidate =getattr (client ,"handshake",None )
+            if callable (handshake_candidate ):
+                status_payload =self ._coerce_mapping (handshake_candidate ("/status"))
+            else :
+                get_candidate =getattr (client ,"get",None )
+                if callable (get_candidate ):
+                    response =get_candidate ("/status.json")
+                    json_candidate =getattr (response ,"json",None )
+                    if callable (json_candidate ):
+                        status_payload =self ._coerce_mapping (json_candidate ())
+            if status_payload :
+                release_value =self ._extract_chembl_release (status_payload )
+                api_candidate =status_payload .get ("api_version")
+                if isinstance (api_candidate ,str )and api_candidate .strip ():
+                    api_version =api_candidate
+                bound_log .info ("chembl_testitem.status",chembl_db_version =release_value ,api_version =api_version )
+        except Exception as exc :
+            bound_log .warning ("chembl_testitem.status_failed",error =str (exc ))
+        finally :
+            self ._chembl_db_version =release_value
+            self ._api_version =api_version
+            self .record_extract_metadata (chembl_release =release_value ,requested_at_utc =request_timestamp )
+        return release_value
+    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
+        "Fetch molecule payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        return self ._dispatch_extract_mode (log ,event_name ="chembl_testitem.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="molecule_chembl_id")
+    def extract_all (self )->pd .DataFrame :
+        "Extract all molecule records from ChEMBL using pagination."
+        descriptor =self ._build_testitem_descriptor ()
+        return self .run_extract_all (descriptor )
+    def _build_testitem_descriptor (self :SelfTestitemChemblPipeline )->ChemblExtractionDescriptor [SelfTestitemChemblPipeline ]:
+        "Return the descriptor powering testitem extraction."
+        def build_context (pipeline :SelfTestitemChemblPipeline ,source_config :TestItemSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+            base_url =pipeline ._resolve_base_url (source_config .parameters )
+            http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+            chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
+            pipeline ._fetch_chembl_release (chembl_client ,log )
+            select_fields =source_config .parameters .select_fields
+            log .debug ("chembl_testitem.select_fields",fields =select_fields )
+            testitem_client =ChemblTestitemClient (chembl_client ,batch_size =min (source_config .page_size ,25 ))
+            return ChemblExtractionContext (source_config =source_config ,iterator =testitem_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,page_size =source_config .page_size ,chembl_release =pipeline ._chembl_db_version ,metadata ={"api_version":pipeline ._api_version })
+        def empty_frame (_ :SelfTestitemChemblPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
+            return pd .DataFrame ({"molecule_chembl_id":pd .Series (dtype ="string")})
+        def dry_run_handler (pipeline :SelfTestitemChemblPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =pipeline ._chembl_db_version ,api_version =pipeline ._api_version )
+            return pd .DataFrame ()
+        def summary_extra (pipeline :SelfTestitemChemblPipeline ,_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+            return {"chembl_db_version":pipeline ._chembl_db_version ,"api_version":pipeline ._api_version ,"limit":pipeline .config .cli .limit }
+        return ChemblExtractionDescriptor [SelfTestitemChemblPipeline ](name ="chembl_testitem",source_name ="chembl",source_config_factory =TestItemSourceConfig .from_source_config ,build_context =build_context ,id_column ="molecule_chembl_id",summary_event ="chembl_testitem.extract_summary",must_have_fields =tuple (MUST_HAVE_FIELDS ),default_select_fields =MUST_HAVE_FIELDS ,sort_by =("molecule_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra ,hard_page_size_cap =None )
+    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
+        "Extract molecule records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of molecule_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted molecule records.\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        stage_start =time .perf_counter ()
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =TestItemSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
+        self ._fetch_chembl_release (chembl_client ,log )
+        if self .config .cli .dry_run :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =self ._chembl_db_version ,api_version =self ._api_version )
+            return pd .DataFrame ()
+        page_size =min (source_config .page_size ,25 )
+        limit =self .config .cli .limit
+        resolved_select_fields =source_config .parameters .select_fields
+        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
+        log .debug ("chembl_testitem.select_fields",fields =merged_select_fields )
+        testitem_client =ChemblTestitemClient (chembl_client ,batch_size =page_size )
+        def fetch_testitems (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
+            iterator =testitem_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
+            for item in iterator :
+                yield dict (item )
+        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="molecule_chembl_id",fetcher =fetch_testitems ,select_fields =merged_select_fields or None ,batch_size =page_size ,chunk_size =min (page_size ,25 ),max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None },chembl_release =self ._chembl_release )
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_testitem.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_db_version =self ._chembl_db_version ,api_version =self ._api_version ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
+        return dataframe
+    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
+        "Transform raw molecule data by normalizing fields and extracting nested properties."
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
+        df =df .copy ()
+        if df .empty :
+            log .debug ("transform_empty_dataframe")
+            return df
+        log .info ("transform_started",rows =len (df ))
+        df =transform_testitem (df ,self .config )
+        df =self ._normalize_identifiers (df ,log )
+        df =self ._normalize_string_fields (df ,log )
+        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
+        df =self ._normalize_numeric_fields (df ,log )
+        self ._check_empty_columns (df ,log )
+        df =self ._remove_extra_columns (df ,log )
+        df ["_chembl_db_version"]=self ._chembl_db_version or ""
+        df ["_api_version"]=self ._api_version or ""
+        df =self ._deduplicate_molecules (df ,log )
+        if "molecule_chembl_id"in df .columns :
+            df =df .sort_values ("molecule_chembl_id").reset_index (drop =True )
+        df =self ._order_schema_columns (df ,COLUMN_ORDER )
+        log .info ("transform_completed",rows =len (df ))
+        return df
+    def augment_metadata (self ,metadata :Mapping [str ,object ],df :pd .DataFrame )->Mapping [str ,object ]:
+        "Enrich metadata with ChEMBL versions."
+        enriched =dict (super ().augment_metadata (metadata ,df ))
+        if self ._chembl_db_version :
+            enriched ["chembl_db_version"]=self ._chembl_db_version
+        if self ._api_version :
+            enriched ["api_version"]=self ._api_version
+        return enriched
+    def _flatten_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Flatten nested molecule_structures and molecule_properties into flat columns."
+        if df .empty :
+            return df
+        if "molecule_structures"in df .columns :
+            structures_df =pd .json_normalize (df ["molecule_structures"].tolist ())
+            if "canonical_smiles"in structures_df .columns :
+                df ["canonical_smiles"]=structures_df ["canonical_smiles"]
+            if "standard_inchi_key"in structures_df .columns :
+                df ["standard_inchi_key"]=structures_df ["standard_inchi_key"]
+        if "molecule_properties"in df .columns :
+            properties_df =pd .json_normalize (df ["molecule_properties"].tolist ())
+            property_columns =["full_mwt","mw_freebase","alogp","hbd","hba","psa","aromatic_rings","rtb","num_ro5_violations"]
+            for col in property_columns :
+                if col in properties_df .columns :
+                    df [col ]=properties_df [col ]
+        log .debug ("flatten_nested_structures_completed",columns =list (df .columns ))
+        return df
+    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize ChEMBL identifiers and InChI keys."
+        if df .empty :
+            return df
+        working_df =df .copy ()
+        inchi_key_col ="molecule_structures__standard_inchi_key"if "molecule_structures__standard_inchi_key"in working_df .columns else "standard_inchi_key"
+        rules :dict [str ,StringRule ]={}
+        if "molecule_chembl_id"in working_df .columns :
+            rules ["molecule_chembl_id"]=StringRule ()
+        if inchi_key_col in working_df .columns :
+            rules [inchi_key_col ]=StringRule (uppercase =True )
+        if rules :
+            normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        else :
+            normalized_df =working_df
+            stats =StringStats ()
+        if stats .has_changes :
+            log .debug ("normalize_identifiers_completed",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        else :
+            log .debug ("normalize_identifiers_completed")
+        return normalized_df
+    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize string fields: trim, replace empty strings with NaN."
+        if df .empty :
+            return df
+        working_df =df .copy ()
+        rules ={"pref_name":StringRule (),"molecule_type":StringRule (),"molecule_structures__canonical_smiles":StringRule (),"canonical_smiles":StringRule ()}
+        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        if stats .has_changes :
+            log .debug ("normalize_string_fields_completed",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        else :
+            log .debug ("normalize_string_fields_completed")
+        return normalized_df
+    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
+        specs =dict (super ()._schema_column_specs ())
+        int_columns =["max_phase","first_approval","first_in_class","availability_type","black_box_warning","chirality","dosed_ingredient","inorganic_flag","natural_product","prodrug","therapeutic_flag","molecule_properties__aromatic_rings","molecule_properties__hba","molecule_properties__hba_lipinski","molecule_properties__hbd","molecule_properties__hbd_lipinski","molecule_properties__heavy_atoms","molecule_properties__num_lipinski_ro5_violations","molecule_properties__num_ro5_violations","molecule_properties__ro3_pass","molecule_properties__rtb"]
+        float_columns =["molecule_properties__alogp","molecule_properties__cx_logd","molecule_properties__cx_logp","molecule_properties__cx_most_apka","molecule_properties__cx_most_bpka","molecule_properties__full_mwt","molecule_properties__mw_freebase","molecule_properties__mw_monoisotopic","molecule_properties__psa","molecule_properties__qed_weighted"]
+        for column in int_columns :
+            specs [column ]={"dtype":"Int64","default":pd .NA }
+        for column in float_columns :
+            specs [column ]={"dtype":"Float64","default":pd .NA }
+        for column in COLUMN_ORDER :
+            if column not in specs :
+                specs [column ]={"dtype":"string","default":pd .NA }
+        return specs
+    def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize numeric fields: convert types, replace negative values with None."
+        if df .empty :
+            return df
+        int_ge_zero_columns =["chirality","availability_type","hbd","hba","aromatic_rings","rtb","num_ro5_violations"]
+        int_columns =["max_phase","first_approval","black_box_warning"]
+        boolean_columns =["first_in_class","inorganic_flag","natural_product","prodrug","therapeutic_flag","dosed_ingredient"]
+        for col in int_ge_zero_columns +int_columns +boolean_columns :
+            if col in df .columns :
+                numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                if col in int_ge_zero_columns :
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                elif col in boolean_columns :
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                    numeric_series =numeric_series .where ((numeric_series ==0 )|(numeric_series ==1 )|numeric_series .isna ())
+                df [col ]=numeric_series .astype ("Int64")
+        for col in df .columns :
+            if col .startswith ("molecule_properties__"):
+                prop_name =col .replace ("molecule_properties__","")
+                if prop_name =="ro3_pass":
+                    mask =df [col ].notna ()
+                    if mask .any ():
+                        col_str =df .loc [mask ,col ].astype (str ).str .upper ().str .strip ()
+                        converted =df [col ].copy ()
+                        converted .loc [mask &(col_str =="Y")]=1
+                        converted .loc [mask &(col_str =="N")]=0
+                        other_mask =mask &~col_str .isin (["Y","N"])
+                        if other_mask .any ():
+                            numeric_vals =pd .to_numeric (df .loc [other_mask ,col ],errors ="coerce")
+                            valid_numeric =numeric_vals .notna ()&numeric_vals .isin ([0 ,1 ])
+                            converted .loc [other_mask ]=pd .NA
+                            if valid_numeric .any ():
+                                valid_indices =numeric_vals .index [valid_numeric ]
+                                converted .loc [valid_indices ]=numeric_vals .loc [valid_indices ]
+                        df [col ]=converted
+                    numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                    numeric_series =numeric_series .where ((numeric_series ==0 )|(numeric_series ==1 )|numeric_series .isna ())
+                    df [col ]=numeric_series .astype ("Int64")
+                elif prop_name in ["aromatic_rings","hba","hba_lipinski","hbd","hbd_lipinski","heavy_atoms","num_lipinski_ro5_violations","num_ro5_violations","rtb"]:
+                    numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                    df [col ]=numeric_series .astype ("Int64")
+        log .debug ("normalize_numeric_fields_completed")
+        return df
+    def _check_empty_columns (self ,df :pd .DataFrame ,log :Any )->None :
+        "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043f\u0443\u0441\u0442\u044b\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n        \u041b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u0435, \u0435\u0441\u043b\u0438 \u043f\u0440\u043e\u0446\u0435\u043d\u0442 \u043f\u0443\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u043e \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u043c \u043f\u043e\u043b\u044f\u043c > 95%.\n        \u042d\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443 \u0441 select_fields \u0438\u043b\u0438 flatten_objects.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        "
+        if df .empty :
+            return
+        key_fields =["molecule_chembl_id","pref_name","molecule_type","availability_type","chirality","first_approval","first_in_class","indication_class","helm_notation","molecule_properties__cx_logp","molecule_properties__cx_logd","molecule_properties__mw_monoisotopic","molecule_properties__hba_lipinski","molecule_properties__hbd_lipinski","molecule_properties__molecular_species","molecule_properties__num_lipinski_ro5_violations"]
+        available_key_fields =[col for col in key_fields if col in df .columns ]
+        if not available_key_fields :
+            return
+        empty_percentages :dict [str ,float ]={}
+        for col in available_key_fields :
+            if len (df )>0 :
+                empty_count =df [col ].isna ().sum ()
+                empty_percentage =empty_count /len (df )*100.0
+                empty_percentages [col ]=empty_percentage
+        highly_empty_fields ={col :pct for col ,pct in empty_percentages .items ()if pct >95.0 }
+        if highly_empty_fields :
+            log .warning ("highly_empty_columns_detected",empty_fields =highly_empty_fields ,message =f'Fields with >95% empty values detected: {highly_empty_fields }. This may indicate missing fields in select_fields or flatten_objects configuration.')
+    def _remove_extra_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Remove columns that are not in the schema."
+        if df .empty :
+            return df
+        from bioetl .schemas .testitem import COLUMN_ORDER
+        schema_columns =set (COLUMN_ORDER )
+        existing_columns =set (df .columns )
+        extra_columns =existing_columns -schema_columns
+        if extra_columns :
+            df =df .drop (columns =list (extra_columns ))
+            log .debug ("remove_extra_columns_completed",removed_columns =list (extra_columns ),remaining_columns =list (df .columns ))
+        return df
+    def _deduplicate_molecules (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Deduplicate molecules by standard_inchi_key (fallback to molecule_chembl_id)."
+        if df .empty :
+            return df
+        rows_before =len (df )
+        inchi_key_col ="molecule_structures__standard_inchi_key"if "molecule_structures__standard_inchi_key"in df .columns else "standard_inchi_key"
+        canonical_smiles_col ="molecule_structures__canonical_smiles"if "molecule_structures__canonical_smiles"in df .columns else "canonical_smiles"
+        full_mwt_col ="molecule_properties__full_mwt"if "molecule_properties__full_mwt"in df .columns else "full_mwt"
+        alogp_col ="molecule_properties__alogp"if "molecule_properties__alogp"in df .columns else "alogp"
+        if inchi_key_col in df .columns :
+            completeness_cols =[canonical_smiles_col ,full_mwt_col ,alogp_col ]
+            available_completeness =[col for col in completeness_cols if col in df .columns ]
+            if available_completeness :
+                df ["_completeness"]=df [available_completeness ].notna ().sum (axis =1 )
+                df =df .sort_values (["_completeness",canonical_smiles_col ],ascending =[False ,False ],na_position ="last")
+                df =df .drop (columns =["_completeness"])
+            df =df .drop_duplicates (subset =[inchi_key_col ],keep ="first")
+        else :
+            df =df .drop_duplicates (subset =["molecule_chembl_id"],keep ="first")
+        rows_after =len (df )
+        dropped =rows_before -rows_after
+        if dropped >0 :
+            log .info ("deduplicate_molecules_completed",rows_before =rows_before ,rows_after =rows_after ,dropped =dropped )
+        return df
```

#### Горячий участок 15

- target: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\run.py:57-60

```diff
--- target:run.py

+++ testitem:run.py

@@ -0,0 +1,4 @@

+def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+    super ().__init__ (config ,run_id )
+    self ._chembl_db_version :str |None =None
+    self ._api_version :str |None =None
```

#### Горячий участок 16

- target: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\run.py:151-234

```diff
--- target:run.py

+++ testitem:run.py

@@ -0,0 +1,20 @@

+def _build_testitem_descriptor (self :SelfTestitemChemblPipeline )->ChemblExtractionDescriptor [SelfTestitemChemblPipeline ]:
+    "Return the descriptor powering testitem extraction."
+    def build_context (pipeline :SelfTestitemChemblPipeline ,source_config :TestItemSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+        base_url =pipeline ._resolve_base_url (source_config .parameters )
+        http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+        chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
+        pipeline ._fetch_chembl_release (chembl_client ,log )
+        select_fields =source_config .parameters .select_fields
+        log .debug ("chembl_testitem.select_fields",fields =select_fields )
+        testitem_client =ChemblTestitemClient (chembl_client ,batch_size =min (source_config .page_size ,25 ))
+        return ChemblExtractionContext (source_config =source_config ,iterator =testitem_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,page_size =source_config .page_size ,chembl_release =pipeline ._chembl_db_version ,metadata ={"api_version":pipeline ._api_version })
+    def empty_frame (_ :SelfTestitemChemblPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
+        return pd .DataFrame ({"molecule_chembl_id":pd .Series (dtype ="string")})
+    def dry_run_handler (pipeline :SelfTestitemChemblPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =pipeline ._chembl_db_version ,api_version =pipeline ._api_version )
+        return pd .DataFrame ()
+    def summary_extra (pipeline :SelfTestitemChemblPipeline ,_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+        return {"chembl_db_version":pipeline ._chembl_db_version ,"api_version":pipeline ._api_version ,"limit":pipeline .config .cli .limit }
+    return ChemblExtractionDescriptor [SelfTestitemChemblPipeline ](name ="chembl_testitem",source_name ="chembl",source_config_factory =TestItemSourceConfig .from_source_config ,build_context =build_context ,id_column ="molecule_chembl_id",summary_event ="chembl_testitem.extract_summary",must_have_fields =tuple (MUST_HAVE_FIELDS ),default_select_fields =MUST_HAVE_FIELDS ,sort_by =("molecule_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra ,hard_page_size_cap =None )
```

#### Горячий участок 17

- target: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\run.py:664-724

```diff
--- target:run.py

+++ testitem:run.py

@@ -0,0 +1,17 @@

+def _check_empty_columns (self ,df :pd .DataFrame ,log :Any )->None :
+    "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043f\u0443\u0441\u0442\u044b\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n        \u041b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u0435, \u0435\u0441\u043b\u0438 \u043f\u0440\u043e\u0446\u0435\u043d\u0442 \u043f\u0443\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u043e \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u043c \u043f\u043e\u043b\u044f\u043c > 95%.\n        \u042d\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443 \u0441 select_fields \u0438\u043b\u0438 flatten_objects.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        "
+    if df .empty :
+        return
+    key_fields =["molecule_chembl_id","pref_name","molecule_type","availability_type","chirality","first_approval","first_in_class","indication_class","helm_notation","molecule_properties__cx_logp","molecule_properties__cx_logd","molecule_properties__mw_monoisotopic","molecule_properties__hba_lipinski","molecule_properties__hbd_lipinski","molecule_properties__molecular_species","molecule_properties__num_lipinski_ro5_violations"]
+    available_key_fields =[col for col in key_fields if col in df .columns ]
+    if not available_key_fields :
+        return
+    empty_percentages :dict [str ,float ]={}
+    for col in available_key_fields :
+        if len (df )>0 :
+            empty_count =df [col ].isna ().sum ()
+            empty_percentage =empty_count /len (df )*100.0
+            empty_percentages [col ]=empty_percentage
+    highly_empty_fields ={col :pct for col ,pct in empty_percentages .items ()if pct >95.0 }
+    if highly_empty_fields :
+        log .warning ("highly_empty_columns_detected",empty_fields =highly_empty_fields ,message =f'Fields with >95% empty values detected: {highly_empty_fields }. This may indicate missing fields in select_fields or flatten_objects configuration.')
```

#### Горячий участок 18

- target: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\run.py:748-807

```diff
--- target:run.py

+++ testitem:run.py

@@ -0,0 +1,24 @@

+def _deduplicate_molecules (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+    "Deduplicate molecules by standard_inchi_key (fallback to molecule_chembl_id)."
+    if df .empty :
+        return df
+    rows_before =len (df )
+    inchi_key_col ="molecule_structures__standard_inchi_key"if "molecule_structures__standard_inchi_key"in df .columns else "standard_inchi_key"
+    canonical_smiles_col ="molecule_structures__canonical_smiles"if "molecule_structures__canonical_smiles"in df .columns else "canonical_smiles"
+    full_mwt_col ="molecule_properties__full_mwt"if "molecule_properties__full_mwt"in df .columns else "full_mwt"
+    alogp_col ="molecule_properties__alogp"if "molecule_properties__alogp"in df .columns else "alogp"
+    if inchi_key_col in df .columns :
+        completeness_cols =[canonical_smiles_col ,full_mwt_col ,alogp_col ]
+        available_completeness =[col for col in completeness_cols if col in df .columns ]
+        if available_completeness :
+            df ["_completeness"]=df [available_completeness ].notna ().sum (axis =1 )
+            df =df .sort_values (["_completeness",canonical_smiles_col ],ascending =[False ,False ],na_position ="last")
+            df =df .drop (columns =["_completeness"])
+        df =df .drop_duplicates (subset =[inchi_key_col ],keep ="first")
+    else :
+        df =df .drop_duplicates (subset =["molecule_chembl_id"],keep ="first")
+    rows_after =len (df )
+    dropped =rows_before -rows_after
+    if dropped >0 :
+        log .info ("deduplicate_molecules_completed",rows_before =rows_before ,rows_after =rows_after ,dropped =dropped )
+    return df
```

#### Горячий участок 19

- target: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\run.py:72-123

```diff
--- target:run.py

+++ testitem:run.py

@@ -0,0 +1,34 @@

+def _fetch_chembl_release (self ,client :UnifiedAPIClient |ChemblClient |Any ,log :BoundLogger |None =None )->str |None :
+    "Capture ChEMBL release and API version from status endpoint."
+    if log is None :
+        bound_log :BoundLogger =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
+    else :
+        bound_log =log
+    request_timestamp =datetime .now (timezone .utc )
+    release_value :str |None =None
+    api_version :str |None =None
+    try :
+        status_payload :dict [str ,Any ]={}
+        handshake_candidate =getattr (client ,"handshake",None )
+        if callable (handshake_candidate ):
+            status_payload =self ._coerce_mapping (handshake_candidate ("/status"))
+        else :
+            get_candidate =getattr (client ,"get",None )
+            if callable (get_candidate ):
+                response =get_candidate ("/status.json")
+                json_candidate =getattr (response ,"json",None )
+                if callable (json_candidate ):
+                    status_payload =self ._coerce_mapping (json_candidate ())
+        if status_payload :
+            release_value =self ._extract_chembl_release (status_payload )
+            api_candidate =status_payload .get ("api_version")
+            if isinstance (api_candidate ,str )and api_candidate .strip ():
+                api_version =api_candidate
+            bound_log .info ("chembl_testitem.status",chembl_db_version =release_value ,api_version =api_version )
+    except Exception as exc :
+        bound_log .warning ("chembl_testitem.status_failed",error =str (exc ))
+    finally :
+        self ._chembl_db_version =release_value
+        self ._api_version =api_version
+        self .record_extract_metadata (chembl_release =release_value ,requested_at_utc =request_timestamp )
+    return release_value
```

#### Горячий участок 20

- target: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\run.py:397-434

```diff
--- target:run.py

+++ testitem:run.py

@@ -0,0 +1,18 @@

+def _flatten_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+    "Flatten nested molecule_structures and molecule_properties into flat columns."
+    if df .empty :
+        return df
+    if "molecule_structures"in df .columns :
+        structures_df =pd .json_normalize (df ["molecule_structures"].tolist ())
+        if "canonical_smiles"in structures_df .columns :
+            df ["canonical_smiles"]=structures_df ["canonical_smiles"]
+        if "standard_inchi_key"in structures_df .columns :
+            df ["standard_inchi_key"]=structures_df ["standard_inchi_key"]
+    if "molecule_properties"in df .columns :
+        properties_df =pd .json_normalize (df ["molecule_properties"].tolist ())
+        property_columns =["full_mwt","mw_freebase","alogp","hbd","hba","psa","aromatic_rings","rtb","num_ro5_violations"]
+        for col in property_columns :
+            if col in properties_df .columns :
+                df [col ]=properties_df [col ]
+    log .debug ("flatten_nested_structures_completed",columns =list (df .columns ))
+    return df
```

### Модуль transform.py

Определение                              | target сигнатура              | testitem сигнатура                                             | Побочные эффекты                                                      | Исключения              | Статус           
-----------------------------------------|-------------------------------|----------------------------------------------------------------|-----------------------------------------------------------------------|-------------------------|------------------
__module_block_0                         | —                             | —                                                              | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []} | target: []
testitem: [] | отличается       
__module_block_1                         | —                             | —                                                              | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []} | target: []
testitem: [] | совпадает        
__module_block_10                        | —                             | —                                                              | target: {'logging': [], 'io': []}
testitem: {}                        | target: []
testitem: [] | только в target  
__module_block_2                         | —                             | —                                                              | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []} | target: []
testitem: [] | отличается       
__module_block_3                         | —                             | —                                                              | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []} | target: []
testitem: [] | отличается       
__module_block_4                         | —                             | —                                                              | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []} | target: []
testitem: [] | отличается       
__module_block_5                         | —                             | —                                                              | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []} | target: []
testitem: [] | отличается       
__module_block_6                         | —                             | —                                                              | target: {'logging': [], 'io': []}
testitem: {'logging': [], 'io': []} | target: []
testitem: [] | отличается       
__module_block_7                         | —                             | —                                                              | target: {'logging': [], 'io': []}
testitem: {}                        | target: []
testitem: [] | только в target  
__module_block_8                         | —                             | —                                                              | target: {'logging': [], 'io': []}
testitem: {}                        | target: []
testitem: [] | только в target  
__module_block_9                         | —                             | —                                                              | target: {'logging': [], 'io': []}
testitem: {}                        | target: []
testitem: [] | только в target  
_collect_dicts                           | source: Any                   | —                                                              | target: {'logging': [], 'io': []}
testitem: {}                        | target: []
testitem: [] | только в target  
_is_iterable_of_objects                  | value: Any                    | —                                                              | target: {'logging': [], 'io': []}
testitem: {}                        | target: []
testitem: [] | только в target  
_is_json_dict                            | value: Any                    | —                                                              | target: {'logging': [], 'io': []}
testitem: {}                        | target: []
testitem: [] | только в target  
extract_and_serialize_component_synonyms | target_components: Any        | —                                                              | target: {'logging': [], 'io': []}
testitem: {}                        | target: []
testitem: [] | только в target  
flatten_object_col                       | —                             | df: pd.DataFrame, col: str, fields: Sequence[str], prefix: str | target: {}
testitem: {'logging': [], 'io': []}                        | target: []
testitem: [] | только в testitem
flatten_target_components                | rec: dict[str, Any]           | —                                                              | target: {'logging': [], 'io': ['json.dumps']}
testitem: {}            | target: []
testitem: [] | только в target  
serialize_target_arrays                  | df: pd.DataFrame, config: Any | —                                                              | target: {'logging': [], 'io': []}
testitem: {}                        | target: []
testitem: [] | только в target  
transform                                | —                             | df: pd.DataFrame, cfg: Any                                     | target: {}
testitem: {'logging': [], 'io': []}                        | target: []
testitem: [] | только в testitem

#### Горячий участок 1

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:1-1
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:1-1

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-"Transform utilities for ChEMBL target pipeline array serialization."
+"Transform utilities for ChEMBL testitem pipeline array serialization and flattening."
```

#### Горячий участок 2

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:22-22
- testitem: нет в ветке

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1 +0,0 @@

-JsonDict =dict [str ,Any ]
```

#### Горячий участок 3

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:13-13
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:10-10

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-from bioetl .core .serialization import header_rows_serialize
+from bioetl .core .serialization import serialize_objects ,serialize_simple_list
```

#### Горячий участок 4

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:6-6
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:5-5

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-from collections .abc import Iterable
+from collections .abc import Sequence
```

#### Горячий участок 5

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:7-7
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:6-6

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-from typing import Any ,TypeGuard ,cast
+from typing import Any
```

#### Горячий участок 6

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:5-5
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:8-8

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-import json
+import pandas as pd
```

#### Горячий участок 7

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:9-9
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:12-17

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1 +1 @@

-import numpy as np
+__all__ =["serialize_simple_list","serialize_objects","flatten_object_col","transform"]
```

#### Горячий участок 8

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:10-10
- testitem: нет в ветке

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1 +0,0 @@

-import numpy .typing as npt
```

#### Горячий участок 9

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:11-11
- testitem: нет в ветке

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1 +0,0 @@

-import pandas as pd
```

#### Горячий участок 10

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:15-19
- testitem: нет в ветке

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1 +0,0 @@

-__all__ =["serialize_target_arrays","extract_and_serialize_component_synonyms","flatten_target_components"]
```

#### Горячий участок 11

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:33-47
- testitem: нет в ветке

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1,12 +0,0 @@

-def _collect_dicts (source :Any )->list [JsonDict ]:
-    "Collect dictionary entries from arbitrary source keeping order."
-    result :list [JsonDict ]=[]
-    if _is_json_dict (source ):
-        result .append (source )
-        return result
-    if _is_iterable_of_objects (source ):
-        for element in source :
-            element_any :Any =element
-            if _is_json_dict (element_any ):
-                result .append (element_any )
-    return result
```

#### Горячий участок 12

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:29-30
- testitem: нет в ветке

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1,2 +0,0 @@

-def _is_iterable_of_objects (value :Any )->TypeGuard [Iterable [Any ]]:
-    return isinstance (value ,Iterable )and (not isinstance (value ,(str ,bytes )))
```

#### Горячий участок 13

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:25-26
- testitem: нет в ветке

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1,2 +0,0 @@

-def _is_json_dict (value :Any )->TypeGuard [JsonDict ]:
-    return isinstance (value ,dict )
```

#### Горячий участок 14

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:133-162
- testitem: нет в ветке

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1,15 +0,0 @@

-def extract_and_serialize_component_synonyms (target_components :Any )->str :
-    "Extract target_component_synonyms from target_components and serialize.\n\n    Parameters\n    ----------\n    target_components:\n        List of target component dicts, None, or empty list.\n\n    Returns\n    -------\n    str:\n        Serialized string in header+rows format, or empty string for None/empty.\n    "
-    if target_components is None :
-        return ""
-    components :list [dict [str ,Any ]]=_collect_dicts (target_components )
-    if not components :
-        return ""
-    all_synonyms :list [dict [str ,Any ]]=[]
-    for component in components :
-        syns_item :Any =component .get ("target_component_synonyms")
-        if syns_item :
-            all_synonyms .extend (_collect_dicts (syns_item ))
-    if not all_synonyms :
-        return ""
-    return header_rows_serialize (all_synonyms )
```

#### Горячий участок 15

- target: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:20-75

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -0,0 +1,16 @@

+def flatten_object_col (df :pd .DataFrame ,col :str ,fields :Sequence [str ],prefix :str )->pd .DataFrame :
+    "Flatten nested object column into flat columns with prefix.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    col:\n        Column name containing nested objects.\n    fields:\n        List of field names to extract from nested objects.\n    prefix:\n        Prefix to add to flattened column names (e.g., \"molecule_hierarchy__\").\n\n    Returns\n    -------\n    pd.DataFrame:\n        DataFrame with flattened columns added and original column removed.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame({\n    ...     \"molecule_chembl_id\": [\"CHEMBL1\"],\n    ...     \"molecule_hierarchy\": [{\"molecule_chembl_id\": \"CHEMBL1\", \"parent_chembl_id\": \"CHEMBL2\"}],\n    ... })\n    >>> result = flatten_object_col(df, \"molecule_hierarchy\", [\"molecule_chembl_id\", \"parent_chembl_id\"], \"molecule_hierarchy__\")\n    >>> \"molecule_hierarchy__molecule_chembl_id\" in result.columns\n    True\n    >>> \"molecule_hierarchy__parent_chembl_id\" in result.columns\n    True\n    >>> \"molecule_hierarchy\" not in result.columns\n    True\n    "
+    df =df .copy ()
+    if col not in df .columns :
+        for f in fields :
+            df [f'{prefix }{f }']=None
+        return df
+    def row_to_dict (obj :Any )->dict [str ,Any ]:
+        "Extract fields from nested object."
+        if not isinstance (obj ,dict ):
+            return dict .fromkeys (fields )
+        return {f :obj .get (f )for f in fields }
+    expanded =df [col ].map (row_to_dict ).apply (pd .Series )
+    expanded .columns =[f'{prefix }{c }'for c in expanded .columns ]
+    df =df .drop (columns =[col ])
+    return pd .concat ([df ,expanded ],axis =1 )
```

#### Горячий участок 16

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:50-130
- testitem: нет в ветке

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1,34 +0,0 @@

-def flatten_target_components (rec :dict [str ,Any ])->dict [str ,Any ]:
-    "Flatten nested target_components data into flat columns.\n\n    Extracts:\n    - uniprot_accessions from target_components[*].accession\n    - target_component_synonyms__flat from target_components[*].target_component_synonyms[*].component_synonym\n    - target_components__flat (serialized container)\n    - cross_references__flat (serialized from top-level)\n    - component_count (counted from accessions or from top-level)\n\n    Parameters\n    ----------\n    rec:\n        Target record dict from ChEMBL API.\n\n    Returns\n    -------\n    dict[str, Any]:\n        Dictionary with flattened fields:\n        - uniprot_accessions: sorted list of unique UniProt accessions (as JSON string)\n        - target_component_synonyms__flat: serialized synonyms\n        - target_components__flat: serialized components\n        - cross_references__flat: serialized cross-references\n        - component_count: count of unique accessions\n    "
-    result :dict [str ,Any ]={"uniprot_accessions":"","target_component_synonyms__flat":"","target_components__flat":"","cross_references__flat":"","component_count":None }
-    comps_raw :Any =rec .get ("target_components")or []
-    comps :list [dict [str ,Any ]]=_collect_dicts (comps_raw )
-    accessions :list [str ]=[]
-    all_synonyms :list [dict [str ,Any ]]=[]
-    for component in comps :
-        accession :Any =component .get ("accession")
-        if isinstance (accession ,str )and accession .strip ():
-            accessions .append (accession .strip ())
-        syns :Any =component .get ("target_component_synonyms")
-        if syns :
-            all_synonyms .extend (_collect_dicts (syns ))
-    unique_accessions =sorted (set (accessions ))
-    if unique_accessions :
-        result ["uniprot_accessions"]=json .dumps (unique_accessions ,ensure_ascii =False )
-        result ["component_count"]=len (unique_accessions )
-    else :
-        top_level_count =rec .get ("component_count")
-        if top_level_count is not None :
-            try :
-                result ["component_count"]=int (top_level_count )
-            except (ValueError ,TypeError ):
-                result ["component_count"]=None
-    if all_synonyms :
-        result ["target_component_synonyms__flat"]=header_rows_serialize (all_synonyms )
-    if comps :
-        result ["target_components__flat"]=header_rows_serialize (comps )
-    xrefs_raw :Any =rec .get ("cross_references")or []
-    xrefs :list [dict [str ,Any ]]=_collect_dicts (xrefs_raw )
-    if xrefs :
-        result ["cross_references__flat"]=header_rows_serialize (xrefs )
-    return result
```

#### Горячий участок 17

- target: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\target\transform.py:165-251
- testitem: нет в ветке

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -1,46 +0,0 @@

-def serialize_target_arrays (df :pd .DataFrame ,config :Any )->pd .DataFrame :
-    "Serialize array fields for target pipeline.\n\n    Uses flatten_target_components() to extract and serialize nested data\n    from target_components and cross_references.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    config:\n        Pipeline config with transform.arrays_to_header_rows.\n\n    Returns\n    -------\n    pd.DataFrame:\n        DataFrame with serialized array fields.\n    "
-    df =df .copy ()
-    arrays_to_serialize :list [str ]=[]
-    try :
-        if hasattr (config ,"transform")and config .transform is not None :
-            if hasattr (config .transform ,"arrays_to_header_rows"):
-                arrays_to_serialize =list (config .transform .arrays_to_header_rows )
-    except (AttributeError ,TypeError ):
-        pass
-    if not df .empty :
-        flattened_data :list [dict [str ,Any ]]=[]
-        for _ ,row in df .iterrows ():
-            row_dict :dict [str ,Any ]=row .to_dict ()
-            for key ,value in row_dict .items ():
-                if isinstance (value ,np .ndarray ):
-                    array_value =cast (npt .NDArray [Any ],value )
-                    if array_value .size ==0 :
-                        row_dict [key ]=None
-                    else :
-                        try :
-                            nan_mask =np .asarray (pd .isna (array_value ),dtype =bool )
-                            if bool (np .all (nan_mask )):
-                                row_dict [key ]=None
-                        except (TypeError ,ValueError ):
-                            pass
-                elif pd .api .types .is_scalar (value ):
-                    try :
-                        if pd .isna (value ):
-                            row_dict [key ]=None
-                    except (TypeError ,ValueError ):
-                        pass
-            flattened =flatten_target_components (row_dict )
-            row_dict .update (flattened )
-            flattened_data .append (row_dict )
-        df =pd .DataFrame (flattened_data )
-    else :
-        df ["cross_references__flat"]=""
-        df ["target_components__flat"]=""
-        df ["target_component_synonyms__flat"]=""
-        df ["uniprot_accessions"]=""
-        df ["component_count"]=None
-    for col in arrays_to_serialize :
-        if col in df .columns and f'{col }__flat'in df .columns :
-            df =df .drop (columns =[col ])
-    return df
```

#### Горячий участок 18

- target: нет в ветке
- testitem: E:\github\bioactivity_data_acquisition1\src\bioetl\pipelines\chembl\testitem\transform.py:78-138

```diff
--- target:transform.py

+++ testitem:transform.py

@@ -0,0 +1,25 @@

+def transform (df :pd .DataFrame ,cfg :Any )->pd .DataFrame :
+    "Transform testitem DataFrame by flattening nested objects and serializing arrays.\n\n    Parameters\n    ----------\n    df:\n        DataFrame to transform.\n    cfg:\n        Pipeline config with transform section (enable_flatten, enable_serialization,\n        arrays_simple_to_pipe, arrays_objects_to_header_rows, flatten_objects).\n\n    Returns\n    -------\n    pd.DataFrame:\n        Transformed DataFrame with flattened objects and serialized arrays.\n    "
+    df =df .copy ()
+    enable_flatten =getattr (cfg .transform ,"enable_flatten",True )if hasattr (cfg ,"transform")else True
+    enable_serialization =getattr (cfg .transform ,"enable_serialization",True )if hasattr (cfg ,"transform")else True
+    if enable_flatten and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"flatten_objects"):
+        flatten_objects =cfg .transform .flatten_objects
+        if isinstance (flatten_objects ,dict ):
+            for obj_col ,fields in flatten_objects .items ():
+                if isinstance (fields ,Sequence )and (not isinstance (fields ,(str ,bytes ))):
+                    df =flatten_object_col (df ,obj_col ,fields ,prefix =f'{obj_col }__')
+    if enable_serialization and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"arrays_simple_to_pipe"):
+        arrays_simple =cfg .transform .arrays_simple_to_pipe
+        if isinstance (arrays_simple ,Sequence )and (not isinstance (arrays_simple ,(str ,bytes ))):
+            for col in arrays_simple :
+                if col in df .columns :
+                    df [col ]=df [col ].map (serialize_simple_list )
+    if enable_serialization and hasattr (cfg ,"transform")and hasattr (cfg .transform ,"arrays_objects_to_header_rows"):
+        arrays_objects =cfg .transform .arrays_objects_to_header_rows
+        if isinstance (arrays_objects ,Sequence )and (not isinstance (arrays_objects ,(str ,bytes ))):
+            for col in arrays_objects :
+                if col in df .columns :
+                    df [f'{col }__flat']=df [col ].map (serialize_objects )
+                    df =df .drop (columns =[col ])
+    return df
```

---
