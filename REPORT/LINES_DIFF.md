# Comparative report for ChEMBL pipelines

## Pair: activity ↔ assay

- AST hash: 978d4152ad77000361c21b7f2051d3d1 ↔ 4e65bc8b94391cb8c868cf6ad530c2c6

- Jaccard over tokens: 0.212

### Module run.py

- File status: activity — present, assay — present
- AST hash: 13d39ff2ae0173b684048e972fd58d8d ↔ 6e346915a9c1b06ff585eb00b983cb26
- Jaccard over tokens: 0.212

Definition                                                       | activity signature                                                                                                                                                         | assay signature                                                   | Side effects                                                                                                                                                                                                                                                                                                                                                       | Exceptions                                            | Status          
-----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|-----------------
ChemblActivityPipeline                                           | —                                                                                                                                                                          | —                                                                 | activity: io=cache_file.read_text, json.dumps, json.loads, payload.get, tmp_path.write_text; logging=UnifiedLogger.bind, UnifiedLogger.get, bound_log.warning, errors_to_log.iterrows, log.debug, log.error, log.info, log.warning, pipeline._log_validity_comments_metrics, self._log_detailed_validation_errors, self._log_validity_comments_metrics<br>assay: ∅ | activity: TypeError(msg), ValueError(msg)<br>assay: ∅ | only in activity
ChemblActivityPipeline.__init__                                  | self, config: PipelineConfig, run_id: str                                                                                                                                  | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._add_row_metadata                         | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.debug<br>assay: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._build_activity_descriptor                | self                                                                                                                                                                       | —                                                                 | activity: io=∅; logging=pipeline._log_validity_comments_metrics<br>assay: ∅                                                                                                                                                                                                                                                                                        | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._cache_directory                          | self, release: str | None                                                                                                                                                  | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._cache_file_path                          | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._cache_key                                | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                 | activity: io=json.dumps; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                     | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._check_activity_id_uniqueness             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.debug, log.error, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                              | activity: ValueError(msg)<br>assay: ∅                 | only in activity
ChemblActivityPipeline._check_cache                              | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                 | activity: io=cache_file.read_text, json.loads; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._check_foreign_key_integrity              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.debug, log.error, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                              | activity: ValueError(msg)<br>assay: ∅                 | only in activity
ChemblActivityPipeline._coerce_activity_dataset                  | self, dataset: object                                                                                                                                                      | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: TypeError(msg)<br>assay: ∅                  | only in activity
ChemblActivityPipeline._coerce_mapping                           | payload: Any                                                                                                                                                               | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._collect_records_by_ids                   | self, normalized_ids: Sequence[tuple[int, str]], activity_iterator: ChemblActivityClient, *, select_fields: Sequence[str] | None                                           | —                                                                 | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.error, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                           | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._create_fallback_record                   | self, activity_id: int, error: Exception | None                                                                                                                            | —                                                                 | activity: io=json.dumps; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                     | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._deduplicate_activity_properties          | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                                 | activity: io=∅; logging=log.debug<br>assay: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._enrich_assay                             | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: io=∅; logging=UnifiedLogger.get, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                 | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._enrich_compound_record                   | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: io=∅; logging=UnifiedLogger.get, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                 | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._enrich_data_validity                     | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                       | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._enrich_molecule                          | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                       | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._ensure_comment_fields                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.debug<br>assay: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._extract_activity_properties_fields       | self, record: dict[str, Any]                                                                                                                                               | —                                                                 | activity: io=json.loads; logging=UnifiedLogger.get, log.debug, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                             | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._extract_assay_fields                     | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                                 | activity: io=∅; logging=log.debug, log.info, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._extract_chembl_release                   | payload: Mapping[str, Any]                                                                                                                                                 | —                                                                 | activity: io=payload.get; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._extract_data_validity_descriptions       | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                                 | activity: io=∅; logging=log.debug, log.info, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._extract_from_chembl                      | self, dataset: object, chembl_client: ChemblClient | Any, activity_iterator: ChemblActivityClient, *, limit: int | None = None, select_fields: Sequence[str] | None = None | —                                                                 | activity: io=∅; logging=UnifiedLogger.get, log.info, self._log_validity_comments_metrics<br>assay: ∅                                                                                                                                                                                                                                                               | activity: ValueError(msg)<br>assay: ∅                 | only in activity
ChemblActivityPipeline._extract_nested_fields                    | self, record: dict[str, Any]                                                                                                                                               | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._extract_page_items                       | payload: Mapping[str, Any], items_keys: Sequence[str] | None                                                                                                               | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._filter_invalid_required_fields           | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._finalize_identifier_columns              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._finalize_output_columns                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.debug, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._get_data_validity_comment_whitelist      | self                                                                                                                                                                       | —                                                                 | activity: io=∅; logging=UnifiedLogger.get<br>assay: ∅                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._harmonize_identifier_columns             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.debug<br>assay: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._log_detailed_validation_errors           | self, failure_cases: pd.DataFrame, payload: pd.DataFrame, log: BoundLogger                                                                                                 | —                                                                 | activity: io=∅; logging=errors_to_log.iterrows, log.error, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                 | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._log_validity_comments_metrics            | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.info, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                          | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._materialize_activity_record              | self, payload: Mapping[str, Any], *, activity_id: int | None = None                                                                                                        | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._next_link                                | payload: Mapping[str, Any], base_url: str                                                                                                                                  | —                                                                 | activity: io=payload.get; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._normalize_activity_ids                   | self, input_frame: pd.DataFrame, *, limit: int | None, log: BoundLogger                                                                                                    | —                                                                 | activity: io=∅; logging=log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._normalize_activity_properties_items      | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                                 | activity: io=json.loads; logging=log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                           | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._normalize_data_types                     | self, df: pd.DataFrame, schema: Any, log: BoundLogger                                                                                                                      | —                                                                 | activity: io=∅; logging=log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._normalize_identifiers                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.debug<br>assay: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._normalize_measurements                   | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.debug, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._normalize_nested_structures              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=json.dumps, json.loads; logging=log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._normalize_string_fields                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.debug, log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._prepare_activity_iteration               | self, *, client_name: str = 'chembl_activity_client'                                                                                                                       | —                                                                 | activity: io=∅; logging=UnifiedLogger.get<br>assay: ∅                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._sanitize_cache_component                 | value: str                                                                                                                                                                 | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._schema_column_specs                      | self                                                                                                                                                                       | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._serialize_activity_properties            | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                                 | activity: io=json.dumps; logging=log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                           | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_assay                      | self                                                                                                                                                                       | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_compound_record            | self                                                                                                                                                                       | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_data_validity              | self                                                                                                                                                                       | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_molecule                   | self                                                                                                                                                                       | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._store_cache                              | self, batch_ids: Sequence[str], batch_data: Mapping[str, Mapping[str, Any]], release: str | None                                                                           | —                                                                 | activity: io=json.dumps, tmp_path.write_text; logging=UnifiedLogger.get, log.debug<br>assay: ∅                                                                                                                                                                                                                                                                     | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._validate_activity_properties_truv        | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                                 | activity: io=∅; logging=log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._validate_data_validity_comment_soft_enum | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline._validate_foreign_keys                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                 | activity: io=∅; logging=log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline.build_quality_report                      | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline.chembl_release                            | self                                                                                                                                                                       | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline.extract                                   | self, *args, **kwargs                                                                                                                                                      | —                                                                 | activity: io=∅; logging=UnifiedLogger.get, bound_log.warning<br>assay: ∅                                                                                                                                                                                                                                                                                           | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline.extract_all                               | self                                                                                                                                                                       | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline.extract_by_ids                            | self, ids: Sequence[str]                                                                                                                                                   | —                                                                 | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning, self._log_validity_comments_metrics<br>assay: ∅                                                                                                                                                                                                                                                  | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline.transform                                 | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.info<br>assay: ∅                                                                                                                                                                                                                                                                                         | activity: ∅<br>assay: ∅                               | only in activity
ChemblActivityPipeline.validate                                  | self, df: pd.DataFrame                                                                                                                                                     | —                                                                 | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.error, log.info, self._log_detailed_validation_errors<br>assay: ∅                                                                                                                                                                                                                                        | activity: ValueError(msg)<br>assay: ∅                 | only in activity
ChemblActivityPipeline.write                                     | self, df: pd.DataFrame, output_path: Path, *, extended: bool = False, include_correlation: bool | None = None, include_qc_metrics: bool | None = None                      | —                                                                 | activity: io=∅; logging=UnifiedLogger.bind, UnifiedLogger.get, log.debug<br>assay: ∅                                                                                                                                                                                                                                                                               | activity: ∅<br>assay: ∅                               | only in activity
ChemblAssayPipeline                                              | —                                                                                                                                                                          | —                                                                 | activity: ∅<br>assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning                                                                                                                                                                                                                                                                            | activity: ∅<br>assay: TypeError(msg)                  | only in assay   
ChemblAssayPipeline.__init__                                     | —                                                                                                                                                                          | self, config: PipelineConfig, run_id: str                         | activity: ∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline._add_row_metadata                            | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: ∅<br>assay: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline._build_assay_descriptor                      | —                                                                                                                                                                          | self: SelfChemblAssayPipeline                                     | activity: ∅<br>assay: io=∅; logging=log.debug, log.info, log.warning                                                                                                                                                                                                                                                                                               | activity: ∅<br>assay: TypeError(msg)                  | only in assay   
ChemblAssayPipeline._check_missing_columns                       | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any, select_fields: list[str] | None | activity: ∅<br>assay: io=∅; logging=log.debug, log.warning                                                                                                                                                                                                                                                                                                         | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline._enrich_with_related_data                    | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: ∅<br>assay: io=∅; logging=log.debug, log.info, log.warning                                                                                                                                                                                                                                                                                               | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline._harmonize_identifier_columns                | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: ∅<br>assay: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline._normalize_data_types                        | —                                                                                                                                                                          | self, df: pd.DataFrame, schema: Any, log: Any                     | activity: ∅<br>assay: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline._normalize_identifiers                       | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: ∅<br>assay: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline._normalize_nested_structures                 | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: ∅<br>assay: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline._normalize_string_fields                     | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: ∅<br>assay: io=∅; logging=log.debug, log.warning                                                                                                                                                                                                                                                                                                         | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline._serialize_array_fields                      | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                  | activity: ∅<br>assay: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline.chembl_release                               | —                                                                                                                                                                          | self                                                              | activity: ∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline.extract                                      | —                                                                                                                                                                          | self, *args, **kwargs                                             | activity: ∅<br>assay: io=∅; logging=UnifiedLogger.get                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline.extract_all                                  | —                                                                                                                                                                          | self                                                              | activity: ∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline.extract_by_ids                               | —                                                                                                                                                                          | self, ids: Sequence[str]                                          | activity: ∅<br>assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning                                                                                                                                                                                                                                                                            | activity: ∅<br>assay: ∅                               | only in assay   
ChemblAssayPipeline.transform                                    | —                                                                                                                                                                          | self, df: pd.DataFrame                                            | activity: ∅<br>assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info                                                                                                                                                                                                                                                                                         | activity: ∅<br>assay: ∅                               | only in assay   
__module_block_0                                                 | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_1                                                 | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_10                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_11                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_12                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_13                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_14                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_15                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_16                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_17                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_18                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_19                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_2                                                 | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_20                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_21                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_22                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_23                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
__module_block_24                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
__module_block_25                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
__module_block_26                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_27                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
__module_block_28                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
__module_block_29                                                | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in activity
__module_block_3                                                 | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_4                                                 | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | identical       
__module_block_5                                                 | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_6                                                 | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | identical       
__module_block_7                                                 | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
__module_block_8                                                 | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | identical       
__module_block_9                                                 | —                                                                                                                                                                          | —                                                                 | activity: io=∅; logging=∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>assay: ∅                               | differs         
_extract_bao_ids_from_classifications                            | —                                                                                                                                                                          | node: Any                                                         | activity: ∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in assay   
_iter_classification_mappings                                    | —                                                                                                                                                                          | node: Any                                                         | activity: ∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in assay   
_normalize_bao_identifier                                        | —                                                                                                                                                                          | raw_value: Any                                                    | activity: ∅<br>assay: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>assay: ∅                               | only in assay   

#### Hotspot 1

- Definition: ChemblActivityPipeline#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:106-3476
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,1684 +0,0 @@
-class ChemblActivityPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting activity records from the ChEMBL API."
-    actor ="activity_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-        self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch activity payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        def legacy_activity_ids (bound_log :BoundLogger )->Sequence [str ]|None :
-            payload_activity_ids =kwargs .get ("activity_ids")
-            if payload_activity_ids is None :
-                return None
-            bound_log .warning ("chembl_activity.deprecated_kwargs",message ="Using activity_ids in kwargs is deprecated. Use --input-file instead.")
-            if isinstance (payload_activity_ids ,Sequence )and (not isinstance (payload_activity_ids ,(str ,bytes ))):
-                sequence_ids :Sequence [str |int ]=cast (Sequence [str |int ],payload_activity_ids )
-                return [str (id_val )for id_val in sequence_ids ]
-            return [str (payload_activity_ids )]
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_activity.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="activity_id",legacy_id_resolver =legacy_activity_ids ,legacy_source ="deprecated_kwargs")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all activity records from ChEMBL using the shared iterator."
-        return self .run_extract_all (self ._build_activity_descriptor ())
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract activity records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of activity_id values to extract (as strings or integers).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted activity records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_config ,chembl_client ,activity_iterator ,select_fields =self ._prepare_activity_iteration ()
-        limit =self .config .cli .limit
-        invalid_ids :list [Any ]=[]
-        def normalize_activity_id (raw :Any )->tuple [str |None ,Any ]:
-            if pd .isna (raw ):
-                return (None ,None )
-            try :
-                if isinstance (raw ,str ):
-                    candidate =raw .strip ()
-                    if not candidate :
-                        return (None ,None )
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw ,(int ,float )):
-                    numeric_id =int (raw )
-                else :
-                    numeric_id =int (raw )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw )
-                return (None ,None )
-            return (str (numeric_id ),int (numeric_id ))
-        def delegated_fetch (canonical_ids :Sequence [str ],context :BatchExtractionContext )->tuple [Sequence [Mapping [str ,Any ]],Mapping [str ,Any ]]:
-            numeric_map =context .metadata
-            normalized_ids :list [tuple [int ,str ]]=[]
-            for identifier in canonical_ids :
-                numeric_value =numeric_map .get (identifier )
-                if numeric_value is None :
-                    continue
-                normalized_ids .append ((int (numeric_value ),identifier ))
-            if not normalized_ids :
-                summary ={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-                context .extra ["delegated_summary"]=summary
-                return ([],summary )
-            records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =context .select_fields or None )
-            context .extra ["delegated_summary"]=summary
-            return (records ,summary )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            dataframe =self ._ensure_comment_fields (dataframe ,log )
-            dataframe =self ._extract_data_validity_descriptions (dataframe ,chembl_client ,log )
-            dataframe =self ._extract_assay_fields (dataframe ,chembl_client ,log )
-            self ._log_validity_comments_metrics (dataframe ,log )
-            return dataframe
-        def empty_activity_frame ()->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def finalize_context (context :BatchExtractionContext )->None :
-            summary =context .extra .get ("delegated_summary")
-            if isinstance (summary ,Mapping ):
-                summary_dict =dict (summary )
-                context .extra ["stats_attribute_override"]=summary_dict
-                self ._last_batch_extract_stats =summary_dict
-            else :
-                context .extra ["stats_attribute_override"]=context .stats .as_dict ()
-                self ._last_batch_extract_stats =context .stats .as_dict ()
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="activity_id",fetcher =delegated_fetch ,select_fields =select_fields ,batch_size =source_config .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release ,id_normalizer =normalize_activity_id ,sort_key =lambda pair :int (pair [0 ]),finalize =finalize_dataframe ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats",fetch_mode ="delegated",empty_frame_factory =empty_activity_frame )
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        batch_stats =self ._last_batch_extract_stats or {}
-        log .info ("chembl_activity.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =batch_stats .get ("batches"),api_calls =batch_stats .get ("api_calls"),cache_hits =batch_stats .get ("cache_hits"))
-        return dataframe
-    def _prepare_activity_iteration (self ,*,client_name :str ="chembl_activity_client")->tuple [ActivitySourceConfig ,ChemblClient ,ChemblActivityClient ,list [str ]]:
-        "Construct reusable ChEMBL clients and iterator for activity extraction."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name =client_name )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =self ._resolve_select_fields (source_raw ,default_fields =API_ACTIVITY_FIELDS )
-        return (source_config ,chembl_client ,activity_iterator ,select_fields )
-    def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-        "Construct the descriptor driving the shared extraction template."
-        def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-            http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-            chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-            typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-            activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-            select_fields =source_config .parameters .select_fields
-            return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-        def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            df =pipeline ._ensure_comment_fields (df ,log )
-            chembl_client =cast (ChemblClient ,context .chembl_client )
-            if chembl_client is not None :
-                df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-            pipeline ._log_validity_comments_metrics (df ,log )
-            return df
-        def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            return pipeline ._materialize_activity_record (payload )
-        return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
-    def _materialize_activity_record (self ,payload :Mapping [str ,Any ],*,activity_id :int |None =None )->dict [str ,Any ]:
-        "Normalize nested fields within an activity payload."
-        record =dict (payload )
-        record =self ._extract_nested_fields (record )
-        record =self ._extract_activity_properties_fields (record )
-        if activity_id is not None :
-            record .setdefault ("activity_id",activity_id )
-        return record
-    def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-        "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-        if isinstance (dataset ,pd .Series ):
-            return dataset .to_frame (name ="activity_id")
-        if isinstance (dataset ,pd .DataFrame ):
-            return dataset
-        if isinstance (dataset ,Mapping ):
-            mapping =cast (Mapping [str ,Any ],dataset )
-            return pd .DataFrame ([dict (mapping )])
-        if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-            dataset_list :list [Any ]=list (dataset )
-            return pd .DataFrame ({"activity_id":dataset_list })
-        msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-        raise TypeError (msg )
-    def _normalize_activity_ids (self ,input_frame :pd .DataFrame ,*,limit :int |None ,log :BoundLogger )->list [tuple [int ,str ]]:
-        "Normalize raw identifier values into deduplicated integer/string pairs."
-        normalized_ids :list [tuple [int ,str ]]=[]
-        invalid_ids :list [Any ]=[]
-        seen :set [str ]=set ()
-        for raw_id in input_frame ["activity_id"].tolist ():
-            if pd .isna (raw_id ):
-                continue
-            try :
-                if isinstance (raw_id ,str ):
-                    candidate =raw_id .strip ()
-                    if not candidate :
-                        continue
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw_id ,(int ,float )):
-                    numeric_id =int (raw_id )
-                else :
-                    numeric_id =int (raw_id )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw_id )
-                continue
-            key =str (numeric_id )
-            if key not in seen :
-                seen .add (key )
-                normalized_ids .append ((numeric_id ,key ))
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        if limit is not None :
-            normalized_ids =normalized_ids [:max (int (limit ),0 )]
-        return normalized_ids
-    def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-        "Iterate over IDs using the shared iterator while preserving cache semantics."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        records :list [dict [str ,Any ]]=[]
-        success_count =0
-        fallback_count =0
-        error_count =0
-        cache_hits =0
-        api_calls =0
-        total_batches =0
-        key_order =[key for _ ,key in normalized_ids ]
-        key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-        for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-            total_batches +=1
-            batch_start =time .perf_counter ()
-            from_cache =False
-            chunk_records :dict [str ,dict [str ,Any ]]={}
-            try :
-                cached_records =self ._check_cache (chunk ,self ._chembl_release )
-                if cached_records is not None :
-                    from_cache =True
-                    cache_hits +=len (chunk )
-                    chunk_records =cached_records
-                else :
-                    api_calls +=1
-                    fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                    for item in fetched_items :
-                        if not isinstance (item ,Mapping ):
-                            continue
-                        activity_value =item .get ("activity_id")
-                        if activity_value is None :
-                            continue
-                        chunk_records [str (activity_value )]=dict (item )
-                    self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-                success_in_batch =0
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    record =chunk_records .get (key )
-                    if record and (not record .get ("error")):
-                        materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                        records .append (materialized )
-                        success_count +=1
-                        success_in_batch +=1
-                    else :
-                        fallback_record =self ._create_fallback_record (numeric_id )
-                        records .append (fallback_record )
-                        fallback_count +=1
-                        error_count +=1
-                batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-                log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-            except CircuitBreakerOpenError as exc :
-                log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except RequestException as exc :
-                log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except Exception as exc :
-                log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-        total_records =len (normalized_ids )
-        success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-        summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-        return (records ,summary )
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw activity data by normalizing measurements, identifiers, and data types."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_measurements (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,ActivitySchema ,log )
-        if "curated"in df .columns or "curated_by"in df .columns :
-            if "curated"not in df .columns :
-                df ["curated"]=pd .NA
-            if "curated_by"in df .columns :
-                mask =df ["curated"].isna ()
-                df .loc [mask ,"curated"]=df .loc [mask ,"curated_by"].notna ()
-            df ["curated"]=df ["curated"].astype ("boolean")
-        df =self ._validate_foreign_keys (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if self ._should_enrich_compound_record ():
-            df =self ._enrich_compound_record (df )
-        if self ._should_enrich_assay ():
-            df =self ._enrich_assay (df )
-        if self ._should_enrich_molecule ():
-            df =self ._enrich_molecule (df )
-        if self ._should_enrich_data_validity ():
-            df =self ._enrich_data_validity (df )
-        df =self ._finalize_identifier_columns (df ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        df =self ._finalize_output_columns (df ,log )
-        df =self ._filter_invalid_required_fields (df ,log )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against ActivitySchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        if "target_tax_id"in df .columns :
-            dtype_name :str =str (df ["target_tax_id"].dtype .name )
-            if dtype_name !="Int64":
-                numeric_series :pd .Series [Any ]=pd .to_numeric (df ["target_tax_id"],errors ="coerce")
-                df ["target_tax_id"]=numeric_series .astype ("Int64")
-        self ._check_activity_id_uniqueness (df ,log )
-        self ._check_foreign_key_integrity (df ,log )
-        self ._validate_data_validity_comment_soft_enum (df ,log )
-        original_coerce =self .config .validation .coerce
-        try :
-            self .config .validation .coerce =False
-            validated =super ().validate (df )
-            log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce )
-            return validated
-        except pandera .errors .SchemaErrors as exc :
-            failure_cases_df :pd .DataFrame |None =None
-            if hasattr (exc ,"failure_cases"):
-                failure_cases_df =cast (pd .DataFrame ,exc .failure_cases )
-            error_count =len (failure_cases_df )if failure_cases_df is not None else 0
-            error_summary =summarize_schema_errors (exc )
-            log .error ("validation_failed",error_count =error_count ,schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce ,error_summary =error_summary ,exc_info =True )
-            if failure_cases_df is not None and (not failure_cases_df .empty ):
-                failure_cases_summary =format_failure_cases (failure_cases_df )
-                log .error ("validation_failure_cases",failure_cases =failure_cases_summary )
-                self ._log_detailed_validation_errors (failure_cases_df ,df ,log )
-            msg =f'Validation failed with {error_count } error(s) against schema {self .config .validation .schema_out }: {error_summary }'
-            raise ValueError (msg )from exc
-        except Exception as exc :
-            log .error ("validation_error",error =str (exc ),schema =self .config .validation .schema_out ,exc_info =True )
-            raise
-        finally :
-            self .config .validation .coerce =original_coerce
-    def _should_enrich_compound_record (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 compound_record \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            compound_record_section :Any =enrich_section .get ("compound_record")
-            if not isinstance (compound_record_section ,Mapping ):
-                return False
-            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-            enabled :Any =compound_record_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        compound_record_section :Any =enrich_section .get ("compound_record")
-                        if isinstance (compound_record_section ,Mapping ):
-                            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                            enrich_cfg =dict (compound_record_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_assay (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 assay \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            assay_section :Any =enrich_section .get ("assay")
-            if not isinstance (assay_section ,Mapping ):
-                return False
-            assay_section =cast (Mapping [str ,Any ],assay_section )
-            enabled :Any =assay_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        assay_section :Any =enrich_section .get ("assay")
-                        if isinstance (assay_section ,Mapping ):
-                            assay_section =cast (Mapping [str ,Any ],assay_section )
-                            enrich_cfg =dict (assay_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_assay (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_data_validity (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 data_validity \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            data_validity_section :Any =enrich_section .get ("data_validity")
-            if not isinstance (data_validity_section ,Mapping ):
-                return False
-            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-            enabled :Any =data_validity_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        data_validity_section :Any =enrich_section .get ("data_validity")
-                        if isinstance (data_validity_section ,Mapping ):
-                            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                            enrich_cfg =dict (data_validity_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        if "data_validity_description"in df .columns :
-            non_na_count =int (df ["data_validity_description"].notna ().sum ())
-            if non_na_count >0 :
-                log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_molecule (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 molecule \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            molecule_section :Any =enrich_section .get ("molecule")
-            if not isinstance (molecule_section ,Mapping ):
-                return False
-            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-            enabled :Any =molecule_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        molecule_section :Any =enrich_section .get ("molecule")
-                        if isinstance (molecule_section ,Mapping ):
-                            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                            enrich_cfg =dict (molecule_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-        if "molecule_name"in df_join .columns :
-            if "molecule_pref_name"not in df .columns :
-                df ["molecule_pref_name"]=pd .NA
-            mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-            if mask .any ():
-                df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-                df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-                log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-        return df
-    def _extract_from_chembl (self ,dataset :object ,chembl_client :ChemblClient |Any ,activity_iterator :ChemblActivityClient ,*,limit :int |None =None ,select_fields :Sequence [str ]|None =None )->pd .DataFrame :
-        "Extract activity records by delegating batching to ``ChemblActivityClient``."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        method_start =time .perf_counter ()
-        self ._last_batch_extract_stats =None
-        input_frame =self ._coerce_activity_dataset (dataset )
-        if "activity_id"not in input_frame .columns :
-            msg ="Input dataset must contain an 'activity_id' column"
-            raise ValueError (msg )
-        normalized_ids =self ._normalize_activity_ids (input_frame ,limit =limit ,log =log )
-        if not normalized_ids :
-            summary :dict [str ,Any ]={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-            self ._last_batch_extract_stats =summary
-            log .info ("chembl_activity.batch_summary",**summary )
-            empty_frame =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-            return self ._ensure_comment_fields (empty_frame ,log )
-        records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =select_fields )
-        duration_ms =(time .perf_counter ()-method_start )*1000.0
-        summary ["duration_ms"]=duration_ms
-        self ._last_batch_extract_stats =summary
-        log .info ("chembl_activity.batch_summary",**summary )
-        result_df :pd .DataFrame =pd .DataFrame .from_records (records )
-        if result_df .empty :
-            result_df =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        elif "activity_id"in result_df .columns :
-            result_df =result_df .sort_values ("activity_id").reset_index (drop =True )
-        result_df =self ._ensure_comment_fields (result_df ,log )
-        result_df =self ._extract_data_validity_descriptions (result_df ,chembl_client ,log )
-        result_df =self ._extract_assay_fields (result_df ,chembl_client ,log )
-        self ._log_validity_comments_metrics (result_df ,log )
-        return result_df
-    def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-        cache_config =self .config .cache
-        if not cache_config .enabled :
-            return None
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        if not cache_file .exists ():
-            return None
-        try :
-            stat =cache_file .stat ()
-        except OSError :
-            return None
-        ttl_seconds =int (cache_config .ttl )
-        if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        try :
-            payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-        except (OSError ,json .JSONDecodeError ):
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        if not isinstance (payload ,dict ):
-            return None
-        missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-        if missing :
-            return None
-        return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
-    def _store_cache (self ,batch_ids :Sequence [str ],batch_data :Mapping [str ,Mapping [str ,Any ]],release :str |None )->None :
-        cache_config =self .config .cache
-        if not cache_config .enabled or not batch_ids or (not batch_data ):
-            return
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        normalized_set =set (normalized_ids )
-        data_to_store ={key :batch_data [key ]for key in normalized_set if key in batch_data }
-        if not data_to_store :
-            return
-        try :
-            cache_file .parent .mkdir (parents =True ,exist_ok =True )
-            tmp_path =cache_file .with_suffix (cache_file .suffix +".tmp")
-            tmp_path .write_text (json .dumps (data_to_store ,sort_keys =True ,default =str ),encoding ="utf-8")
-            tmp_path .replace (cache_file )
-        except Exception as exc :
-            log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-            log .debug ("chembl_activity.cache_store_failed",error =str (exc ))
-    def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-        directory =self ._cache_directory (release )
-        cache_key =self ._cache_key (batch_ids ,release )
-        return directory /f'{cache_key }.json'
-    def _cache_directory (self ,release :str |None )->Path :
-        cache_root =Path (self .config .paths .cache_root )
-        directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-        release_component =self ._sanitize_cache_component (release or "unknown")
-        pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-        version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-        return cache_root /directory_name /pipeline_component /release_component /version_component
-    def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-        payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-        raw =json .dumps (payload ,sort_keys =True )
-        return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
-    @staticmethod
-    def _sanitize_cache_component (value :str )->str :
-        sanitized =re .sub ("[^0-9A-Za-z_.-]","_",value )
-        return sanitized or "default"
-    def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-        "Create fallback record enriched with error metadata."
-        base_message ="Fallback: ChEMBL activity unavailable"
-        message =f'{base_message } ({error })'if error else base_message
-        timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-        metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-        if isinstance (error ,RequestException ):
-            response =getattr (error ,"response",None )
-            status_code =getattr (response ,"status_code",None )
-            if status_code is not None :
-                metadata ["http_status"]=status_code
-            metadata ["error_message"]=str (error )
-        elif error is not None :
-            metadata ["error_message"]=str (error )
-        fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-        return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
-    def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-        if df .empty :
-            return df
-        required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-        if missing_fields :
-            for field in missing_fields :
-                df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            log .debug ("comment_fields_ensured",fields =missing_fields )
-        return df
-    def _extract_data_validity_descriptions (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c data_validity_description \u0438\u0437 DATA_VALIDITY_LOOKUP \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f data_validity_comment \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_data_validity_lookup() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c data_validity_comment.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 data_validity_description.\n        "
-        if df .empty :
-            return df
-        if "data_validity_comment"not in df .columns :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="data_validity_comment_column_missing")
-            return df
-        validity_comments :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            comment =row .get ("data_validity_comment")
-            if pd .isna (comment )or comment is None :
-                continue
-            comment_str =str (comment ).strip ()
-            if comment_str :
-                validity_comments .append (comment_str )
-        if not validity_comments :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="no_valid_comments")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        unique_comments =list (set (validity_comments ))
-        log .info ("extract_data_validity_descriptions_fetching",comments_count =len (unique_comments ))
-        try :
-            records_dict =client .fetch_data_validity_lookup (comments =unique_comments ,fields =["data_validity_comment","description"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_data_validity_descriptions_fetch_error",error =str (exc ),exc_info =True )
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for comment in unique_comments :
-            record =records_dict .get (comment )
-            if record :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":record .get ("description")})
-            else :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":None })
-        if not enrichment_data :
-            log .debug ("extract_data_validity_descriptions_no_records")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        original_index =df .index .copy ()
-        df_result =df .merge (df_enrich ,on =["data_validity_comment"],how ="left",suffixes =("","_enrich"))
-        if "data_validity_description"not in df_result .columns :
-            df_result ["data_validity_description"]=pd .Series ([pd .NA ]*len (df_result ),dtype ="string")
-        else :
-            df_result ["data_validity_description"]=df_result ["data_validity_description"].astype ("string")
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_data_validity_descriptions_complete",comments_requested =len (unique_comments ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _extract_assay_fields (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c assay_organism \u0438 assay_tax_id \u0438\u0437 ASSAYS \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f assay_chembl_id \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_assays_by_ids() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438 assay_organism \u0438 assay_tax_id.\n        "
-        if df .empty :
-            return df
-        if "assay_chembl_id"not in df .columns :
-            log .debug ("extract_assay_fields_skipped",reason ="assay_chembl_id_column_missing")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        assay_ids :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            assay_id =row .get ("assay_chembl_id")
-            if pd .isna (assay_id )or assay_id is None :
-                continue
-            assay_id_str =str (assay_id ).strip ().upper ()
-            if assay_id_str :
-                assay_ids .append (assay_id_str )
-        if not assay_ids :
-            log .debug ("extract_assay_fields_skipped",reason ="no_valid_assay_ids")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        unique_assay_ids =list (set (assay_ids ))
-        log .info ("extract_assay_fields_fetching",assay_ids_count =len (unique_assay_ids ))
-        try :
-            records_dict =client .fetch_assays_by_ids (ids =unique_assay_ids ,fields =["assay_chembl_id","assay_organism","assay_tax_id"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_assay_fields_fetch_error",error =str (exc ),exc_info =True )
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for assay_id in unique_assay_ids :
-            record =records_dict .get (assay_id )if records_dict else None
-            if record :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":record .get ("assay_organism"),"assay_tax_id":record .get ("assay_tax_id")})
-            else :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":None ,"assay_tax_id":None })
-        if not enrichment_data :
-            log .debug ("extract_assay_fields_no_records")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        df_enrich ["assay_chembl_id"]=df_enrich ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        original_index =df .index .copy ()
-        df_normalized =df .copy ()
-        df_normalized ["assay_chembl_id_normalized"]=df_normalized ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        df_result =df_normalized .merge (df_enrich ,left_on ="assay_chembl_id_normalized",right_on ="assay_chembl_id",how ="left",suffixes =("","_enrich"))
-        df_result =df_result .drop (columns =["assay_chembl_id_normalized"])
-        for col in ["assay_organism","assay_tax_id"]:
-            if f'{col }_enrich'in df_result .columns :
-                if col not in df_result .columns :
-                    df_result [col ]=df_result [f'{col }_enrich']
-                else :
-                    base_series :pd .Series [Any ]=df_result [col ]
-                    enrich_series :pd .Series [Any ]=df_result [f'{col }_enrich']
-                    missing_mask =base_series .isna ()
-                    if bool (missing_mask .any ()):
-                        df_result .loc [missing_mask ,col ]=enrich_series .loc [missing_mask ]
-                df_result =df_result .drop (columns =[f'{col }_enrich'])
-        for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-            if col not in df_result .columns :
-                df_result [col ]=pd .Series ([pd .NA ]*len (df_result ),dtype =dtype )
-        df_result ["assay_organism"]=df_result ["assay_organism"].astype ("string")
-        df_result ["assay_tax_id"]=pd .to_numeric (df_result ["assay_tax_id"],errors ="coerce").astype ("Int64")
-        mask_valid =df_result ["assay_tax_id"].notna ()
-        if mask_valid .any ():
-            invalid_mask =mask_valid &(df_result ["assay_tax_id"]<1 )
-            if invalid_mask .any ():
-                log .warning ("invalid_assay_tax_id_range",count =int (invalid_mask .sum ()))
-                df_result .loc [invalid_mask ,"assay_tax_id"]=pd .NA
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_assay_fields_complete",assay_ids_requested =len (unique_assay_ids ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _log_validity_comments_metrics (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "\u041b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442:\n        - \u0414\u043e\u043b\u044e NA \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0442\u0440\u0435\u0445 \u043f\u043e\u043b\u0435\u0439\n        - \u0422\u043e\u043f-10 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment\n        - \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment (\u043d\u0435 \u0432 whitelist)\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity.\n        log:\n            Logger instance.\n        "
-        if df .empty :
-            return
-        metrics :dict [str ,Any ]={}
-        comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        for field in comment_fields :
-            if field in df .columns :
-                na_count =int (df [field ].isna ().sum ())
-                total_count =len (df )
-                na_rate =float (na_count )/float (total_count )if total_count >0 else 0.0
-                metrics [f'{field }_na_rate']=na_rate
-                metrics [f'{field }_na_count']=na_count
-                metrics [f'{field }_total_count']=total_count
-        non_null_comments_series :pd .Series [str ]|None =None
-        if "data_validity_comment"in df .columns :
-            series_candidate =df ["data_validity_comment"].dropna ()
-            if len (series_candidate )>0 :
-                typed_series :pd .Series [str ]=series_candidate .astype ("string")
-                non_null_comments_series =typed_series
-                value_counts =typed_series .value_counts ().head (10 )
-                top_10 ={str (key ):int (value )for key ,value in value_counts .items ()}
-                metrics ["top_10_data_validity_comments"]=top_10
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if whitelist and non_null_comments_series is not None :
-            whitelist_set :set [str ]=set (whitelist )
-            def _is_unknown (value :str )->bool :
-                return value not in whitelist_set
-            unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-            unknown_count =int (unknown_mask .sum ())
-            if unknown_count >0 :
-                unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-                metrics ["unknown_data_validity_comments_count"]=unknown_count
-                metrics ["unknown_data_validity_comments_samples"]=unknown_values
-                log .warning ("unknown_data_validity_comments_detected",unknown_count =unknown_count ,samples =unknown_values ,whitelist =whitelist )
-        if metrics :
-            log .info ("validity_comments_metrics",**metrics )
-    def _get_data_validity_comment_whitelist (self )->list [str ]:
-        "\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c whitelist \u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0434\u043b\u044f data_validity_comment \u0438\u0437 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\n\n        Raises\n        ------\n        RuntimeError\n            \u0415\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d \u0438\u043b\u0438 \u043f\u0443\u0441\u0442.\n        "
-        try :
-            values =sorted (self ._required_vocab_ids ("data_validity_comment"))
-        except RuntimeError as exc :
-            UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validation',run_id =self .run_id ).error ("data_validity_comment_whitelist_unavailable",error =str (exc ))
-            raise
-        return values
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested assay and molecule objects."
-        if "assay"in record and isinstance (record ["assay"],Mapping ):
-            assay =cast (Mapping [str ,Any ],record ["assay"])
-            if "organism"in assay :
-                record .setdefault ("assay_organism",assay ["organism"])
-            if "tax_id"in assay :
-                record .setdefault ("assay_tax_id",assay ["tax_id"])
-        if "molecule"in record and isinstance (record ["molecule"],Mapping ):
-            molecule =cast (Mapping [str ,Any ],record ["molecule"])
-            if "pref_name"in molecule :
-                record .setdefault ("molecule_pref_name",molecule ["pref_name"])
-        if "curated_by"in record :
-            curated_by =record .get ("curated_by")
-            if curated_by is not None and (not pd .isna (curated_by )):
-                record .setdefault ("curated",True )
-            else :
-                record .setdefault ("curated",False )
-        elif "curated"not in record :
-            record .setdefault ("curated",None )
-        return record
-    def _extract_activity_properties_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract TRUV fields, standard_* fields, and comments from activity_properties array as fallback.\n\n        \u041f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0438\u0437 activity_properties \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442\n        \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043e\u0442\u0432\u0435\u0442\u0435 API (\u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 \u043f\u0440\u044f\u043c\u044b\u0445 \u043f\u043e\u043b\u0435\u0439 \u0438\u0437 ACTIVITIES).\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0435 TRUV-\u043f\u043e\u043b\u044f: value, text_value, relation, units.\n        \u0422\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f: standard_upper_value, standard_text_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b: upper_value, lower_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0438: activity_comment, data_validity_comment.\n\n        \u0422\u0430\u043a\u0436\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 activity_properties \u0432 \u0437\u0430\u043f\u0438\u0441\u0438 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438.\n        \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044e \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract_activity_properties')
-        activity_id =record .get ("activity_id")
-        if "activity_properties"not in record :
-            log .warning ("activity_properties_missing",activity_id =activity_id ,message ="activity_properties not found in API response (possible ChEMBL < v24)")
-            record ["activity_properties"]=None
-            return record
-        properties =record ["activity_properties"]
-        if properties is None :
-            log .debug ("activity_properties_null",activity_id =activity_id ,message ="activity_properties is None (possible ChEMBL < v24)")
-            return record
-        if isinstance (properties ,str ):
-            try :
-                properties =json .loads (properties )
-            except (TypeError ,ValueError ,json .JSONDecodeError )as exc :
-                log .debug ("activity_properties_parse_failed",error =str (exc ),activity_id =record .get ("activity_id"))
-                return record
-        if not isinstance (properties ,Sequence )or isinstance (properties ,(str ,bytes )):
-            return record
-        property_iterable :Iterable [Any ]=cast (Iterable [Any ],properties )
-        property_items :list [Any ]=list (property_iterable )
-        def _set_fallback (key :str ,value :Any )->None :
-            "Set fallback value only if key is missing in record and value is not None."
-            if value is not None and record .get (key )is None :
-                record [key ]=value
-        def _is_empty (value :Any )->bool :
-            "Check if value is empty (None, empty string, or whitespace)."
-            if value is None :
-                return True
-            if isinstance (value ,str ):
-                return not value .strip ()
-            return False
-        items :list [Mapping [str ,Any ]]=[]
-        for property_item in property_items :
-            if isinstance (property_item ,Mapping )and "type"in property_item and ("value"in property_item or "text_value"in property_item ):
-                items .append (cast (Mapping [str ,Any ],property_item ))
-        def _is_measured (p :Mapping [str ,Any ])->bool :
-            rf =p .get ("result_flag")
-            return rf is True or (isinstance (rf ,int )and rf ==1 )
-        items .sort (key =lambda p :not _is_measured (p ))
-        for prop in items :
-            val =prop .get ("value")
-            txt =prop .get ("text_value")
-            rel =prop .get ("relation")
-            unt =prop .get ("units")
-            prop_type =str (prop .get ("type","")).lower ()
-            will_set_value =val is not None and record .get ("value")is None
-            will_set_text_value =txt is not None and record .get ("text_value")is None
-            _set_fallback ("value",val )
-            _set_fallback ("text_value",txt )
-            if will_set_value or will_set_text_value :
-                _set_fallback ("relation",rel )
-                _set_fallback ("units",unt )
-            if unt is not None and record .get ("units")is None :
-                _set_fallback ("units",unt )
-            if record .get ("upper_value")is None and ("upper"in prop_type or prop_type in ("upper_value","upper limit")):
-                if val is not None :
-                    _set_fallback ("upper_value",val )
-            if record .get ("lower_value")is None and ("lower"in prop_type or prop_type in ("lower_value","lower limit")):
-                if val is not None :
-                    _set_fallback ("lower_value",val )
-            if record .get ("standard_upper_value")is None and ("standard_upper"in prop_type or prop_type in ("standard upper","standard upper value")):
-                if val is not None :
-                    _set_fallback ("standard_upper_value",val )
-            if record .get ("standard_text_value")is None and "standard"in prop_type and ("text"in prop_type ):
-                if txt is not None :
-                    _set_fallback ("standard_text_value",txt )
-                elif val is not None :
-                    _set_fallback ("standard_text_value",val )
-        current_comment =record .get ("data_validity_comment")
-        if _is_empty (current_comment ):
-            data_validity_items :list [Mapping [str ,Any ]]=[prop for prop in items if ("data_validity"in str (prop .get ("type","")).lower ()or "validity"in str (prop .get ("type","")).lower ())and (prop .get ("text_value")is not None or prop .get ("value")is not None )]
-            if data_validity_items :
-                measured_items =[p for p in data_validity_items if _is_measured (p )]
-                if measured_items :
-                    prop =measured_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="measured",comment_value =comment_value )
-                else :
-                    prop =data_validity_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="first",comment_value =comment_value )
-            else :
-                log .debug ("data_validity_comment_fallback_no_items",activity_id =record .get ("activity_id"),activity_properties_count =len (items ),has_activity_properties =True )
-        else :
-            log .debug ("data_validity_comment_from_api",activity_id =record .get ("activity_id"),comment_value =current_comment )
-        normalized_properties =self ._normalize_activity_properties_items (property_items ,log )
-        if normalized_properties is not None :
-            validated_properties ,validation_stats =self ._validate_activity_properties_truv (normalized_properties ,log ,activity_id )
-            deduplicated_properties ,dedup_stats =self ._deduplicate_activity_properties (validated_properties ,log ,activity_id )
-            record ["activity_properties"]=deduplicated_properties
-            log .debug ("activity_properties_processed",activity_id =activity_id ,original_count =len (property_items ),normalized_count =len (normalized_properties ),validated_count =len (validated_properties ),deduplicated_count =len (deduplicated_properties ),invalid_count =validation_stats .get ("invalid_count",0 ),duplicates_removed =dedup_stats .get ("duplicates_removed",0 ))
-        else :
-            record ["activity_properties"]=properties
-            log .debug ("activity_properties_normalization_failed",activity_id =activity_id ,message ="activity_properties normalization failed, keeping original")
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
-    @staticmethod
-    def _extract_chembl_release (payload :Mapping [str ,Any ])->str |None :
-        for key in ("chembl_release","chembl_db_version","release","version"):
-            value =payload .get (key )
-            if isinstance (value ,str )and value .strip ():
-                return value
-            if value is not None :
-                return str (value )
-        return None
-    @staticmethod
-    def _extract_page_items (payload :Mapping [str ,Any ],items_keys :Sequence [str ]|None =None )->list [dict [str ,Any ]]:
-        preferred_keys :tuple [str ,...]=("activities",)
-        if items_keys is None :
-            combined_keys =preferred_keys +("data","items","results")
-        else :
-            combined_keys =tuple (dict .fromkeys ((*preferred_keys ,*items_keys )))
-        return ChemblPipelineBase ._extract_page_items (payload ,combined_keys )
-    @staticmethod
-    def _next_link (payload :Mapping [str ,Any ],base_url :str )->str |None :
-        page_meta :Any =payload .get ("page_meta")
-        if isinstance (page_meta ,Mapping ):
-            next_link_raw :Any =page_meta .get ("next")
-            next_link :str |None =cast (str |None ,next_link_raw )if next_link_raw is not None else None
-            if isinstance (next_link ,str )and next_link :
-                base_url_str =str (base_url )
-                base_path_parse_result =urlparse (base_url_str )
-                base_path_raw =base_path_parse_result .path
-                base_path_str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                base_path :str =base_path_str .rstrip ("/")
-                if next_link .startswith ("http://")or next_link .startswith ("https://"):
-                    parsed =urlparse (next_link )
-                    base_parsed =urlparse (base_url_str )
-                    parsed_path_raw =parsed .path
-                    base_path_raw =base_parsed .path
-                    path :str =parsed_path_raw .decode ("utf-8","ignore")if isinstance (parsed_path_raw ,(bytes ,bytearray ))else parsed_path_raw
-                    base_path_from_url :str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                    path_normalized :str =path .rstrip ("/")
-                    base_path_normalized :str =base_path_from_url .rstrip ("/")
-                    if base_path_normalized and path_normalized .startswith (base_path_normalized ):
-                        relative_path =path_normalized [len (base_path_normalized ):]
-                        if not relative_path :
-                            return None
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    elif "/api/data/"in path :
-                        parts =path .split ("/api/data/",1 )
-                        if len (parts )>1 :
-                            relative_path ="/"+parts [1 ]
-                        else :
-                            relative_path =path
-                    else :
-                        relative_path =path
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    if parsed .query :
-                        relative_path =f'{relative_path }?{parsed .query }'
-                    return relative_path
-                if base_path :
-                    normalized_base =base_path .lstrip ("/")
-                    stripped_link =next_link .lstrip ("/")
-                    if stripped_link .startswith (normalized_base +"/"):
-                        stripped_link =stripped_link [len (normalized_base ):]
-                    elif stripped_link ==normalized_base :
-                        stripped_link =""
-                    next_link =stripped_link
-                next_link =next_link .lstrip ("/")
-                if next_link :
-                    next_link =f'/{next_link }'
-                else :
-                    next_link ="/"
-                return next_link
-        return None
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Ensure canonical identifier columns are present before normalization."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_chembl_id"not in df .columns and "assay_id"in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "testitem_chembl_id"not in df .columns :
-            if "testitem_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["testitem_id"]
-                actions .append ("testitem_id->testitem_chembl_id")
-            elif "molecule_chembl_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["molecule_chembl_id"]
-                actions .append ("molecule_chembl_id->testitem_chembl_id")
-        if "molecule_chembl_id"not in df .columns and "testitem_chembl_id"in df .columns :
-            df ["molecule_chembl_id"]=df ["testitem_chembl_id"]
-            actions .append ("testitem_chembl_id->molecule_chembl_id")
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_required =[column for column in required_columns if column not in df .columns ]
-        if missing_required :
-            for column in missing_required :
-                df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            actions .append (f"created_missing:{",".join (missing_required )}")
-        alias_columns =[column for column in ("assay_id","testitem_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        boolean_columns =("potential_duplicate","curated","removed")
-        for column in boolean_columns :
-            specs [column ]={"dtype":"boolean","default":pd .NA }
-        return specs
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize ChEMBL and BAO identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"],pattern ="^CHEMBL\\d+$"),IdentifierRule (name ="bao",columns =["bao_endpoint","bao_format"],pattern ="^BAO_\\d{7}$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _finalize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align identifier columns after normalization and drop aliases."
-        df =df .copy ()
-        if {"molecule_chembl_id","testitem_chembl_id"}.issubset (df .columns ):
-            mismatch_mask =df ["molecule_chembl_id"].notna ()&df ["testitem_chembl_id"].notna ()&(df ["molecule_chembl_id"]!=df ["testitem_chembl_id"])
-            if mismatch_mask .any ():
-                mismatch_count =int (mismatch_mask .sum ())
-                samples_raw =df .loc [mismatch_mask ,["molecule_chembl_id","testitem_chembl_id"]].drop_duplicates ().head (5 ).to_dict ("records")
-                samples :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],samples_raw )
-                log .warning ("identifier_mismatch",count =mismatch_count ,samples =samples )
-                df .loc [mismatch_mask ,"testitem_chembl_id"]=df .loc [mismatch_mask ,"molecule_chembl_id"]
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_columns =[column for column in required_columns if column not in df .columns ]
-        if missing_columns :
-            for column in missing_columns :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            log .warning ("identifier_columns_missing",columns =missing_columns )
-        return df
-    def _finalize_output_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align final column order with schema and drop unexpected fields."
-        df =df .copy ()
-        expected =list (COLUMN_ORDER )
-        extras =[column for column in df .columns if column not in expected ]
-        if extras :
-            df =df .drop (columns =extras )
-            log .debug ("output_columns_dropped",columns =extras )
-        missing =[column for column in expected if column not in df .columns ]
-        if missing :
-            for column in missing :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([pd .NA ]*len (df ),dtype ="object")
-            log .warning ("output_columns_missing",columns =missing )
-        if not expected :
-            return df
-        return df [expected ]
-    def _filter_invalid_required_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Filter out rows with NULL values in required identifier fields.\n\n        Removes rows where any of the required fields (assay_chembl_id,\n        testitem_chembl_id, molecule_chembl_id) are NULL, as these cannot\n        pass schema validation.\n\n        Parameters\n        ----------\n        df:\n            DataFrame to filter.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            Filtered DataFrame with only rows having all required fields populated.\n        "
-        df =df .copy ()
-        if df .empty :
-            return df
-        required_fields =["assay_chembl_id","molecule_chembl_id"]
-        missing_fields =[field for field in required_fields if field not in df .columns ]
-        if missing_fields :
-            log .warning ("filter_skipped_missing_columns",missing_columns =missing_fields ,message ="Cannot filter: required columns are missing")
-            return df
-        valid_mask =df ["assay_chembl_id"].notna ()&df ["molecule_chembl_id"].notna ()
-        invalid_count =int ((~valid_mask ).sum ())
-        if invalid_count >0 :
-            invalid_rows =df [~valid_mask ]
-            sample_size =min (5 ,len (invalid_rows ))
-            sample_activity_ids =invalid_rows ["activity_id"].head (sample_size ).tolist ()if "activity_id"in invalid_rows .columns else []
-            log .warning ("filtered_invalid_rows",filtered_count =invalid_count ,remaining_count =int (valid_mask .sum ()),sample_activity_ids =sample_activity_ids ,message ="Rows with NULL in required identifier fields were filtered out")
-            df =df [valid_mask ].reset_index (drop =True )
-        return df
-    def _normalize_measurements (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize standard_value, standard_units, standard_relation, and standard_type."
-        df =df .copy ()
-        normalized_count =0
-        if "standard_value"in df .columns :
-            mask =df ["standard_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_value"]=numeric_series_std
-                negative_mask =mask &(df ["standard_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["standard_relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"standard_relation"]=series
-                invalid_mask =mask &~df ["standard_relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_type"in df .columns :
-            mask =df ["standard_type"].notna ()
-            if mask .any ():
-                df .loc [mask ,"standard_type"]=df .loc [mask ,"standard_type"].astype (str ).str .strip ()
-                standard_types_set :set [str ]=STANDARD_TYPES
-                invalid_mask =mask &~df ["standard_type"].isin (standard_types_set )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_type",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_type"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_units"in df .columns :
-            unit_mapping ={"nanomolar":"nM","nmol":"nM","nm":"nM","NM":"nM","\u00b5M":"\u03bcM","uM":"\u03bcM","UM":"\u03bcM","micromolar":"\u03bcM","microM":"\u03bcM","umol":"\u03bcM","millimolar":"mM","milliM":"mM","mmol":"mM","MM":"mM","percent":"%","pct":"%","ratios":"ratio"}
-            mask =df ["standard_units"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_units"].astype (str ).str .strip ()
-                for old_unit ,new_unit in unit_mapping .items ():
-                    series =series .str .replace (old_unit ,new_unit ,regex =False ,case =False )
-                df .loc [mask ,"standard_units"]=series
-                normalized_count +=int (mask .sum ())
-        if "relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"relation"]=series
-                invalid_mask =mask &~df ["relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_upper_value"in df .columns :
-            mask =df ["standard_upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_upper_value"]=numeric_series_std_upper
-                negative_mask =mask &(df ["standard_upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "upper_value"in df .columns :
-            mask =df ["upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"upper_value"]=numeric_series_upper
-                negative_mask =mask &(df ["upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "lower_value"in df .columns :
-            mask =df ["lower_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"lower_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_lower :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"lower_value"]=numeric_series_lower
-                negative_mask =mask &(df ["lower_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_lower_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"lower_value"]=None
-                normalized_count +=int (mask .sum ())
-        if normalized_count >0 :
-            log .debug ("measurements_normalized",normalized_count =normalized_count )
-        return df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize string fields: trim, empty string to null, title-case for organism."
-        working_df =df .copy ()
-        if "data_validity_description"in working_df .columns and "data_validity_comment"in working_df .columns :
-            invalid_mask =working_df ["data_validity_description"].notna ()&working_df ["data_validity_comment"].isna ()
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                log .warning ("invariant_data_validity_description_without_comment",count =invalid_count ,message ="data_validity_description is filled while data_validity_comment is NA")
-        rules :dict [str ,StringRule ]={"canonical_smiles":StringRule (),"bao_label":StringRule (max_length =128 ),"target_organism":StringRule (title_case =True ),"assay_organism":StringRule (title_case =True ),"data_validity_comment":StringRule (),"data_validity_description":StringRule (),"activity_comment":StringRule (),"standard_text_value":StringRule (),"text_value":StringRule (),"type":StringRule (),"units":StringRule (),"assay_type":StringRule (),"assay_description":StringRule (),"molecule_pref_name":StringRule (),"target_pref_name":StringRule (),"uo_units":StringRule (),"qudt_units":StringRule ()}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Serialize nested structures (ligand_efficiency, activity_properties) to JSON strings."
-        df =df .copy ()
-        nested_fields =["ligand_efficiency","activity_properties"]
-        for field in nested_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                serialized :list [Any ]=[]
-                for idx ,value in df .loc [mask ,field ].items ():
-                    if field =="activity_properties":
-                        serialized_value =self ._serialize_activity_properties (value ,log )
-                        serialized .append (serialized_value )
-                        continue
-                    if isinstance (value ,(Mapping ,list )):
-                        try :
-                            serialized .append (json .dumps (value ,ensure_ascii =False ,sort_keys =True ))
-                        except (TypeError ,ValueError )as exc :
-                            log .warning ("nested_serialization_failed",field =field ,index =idx ,error =str (exc ))
-                            serialized .append (None )
-                    elif isinstance (value ,str ):
-                        try :
-                            json .loads (value )
-                            serialized .append (value )
-                        except (TypeError ,ValueError ):
-                            serialized .append (None )
-                    else :
-                        serialized .append (None )
-                df .loc [mask ,field ]=pd .Series (serialized ,dtype ="object",index =df .loc [mask ,field ].index )
-        if "standard_value"in df .columns and "ligand_efficiency"in df .columns :
-            mask =df ["standard_value"].notna ()&df ["ligand_efficiency"].isna ()
-            if mask .any ():
-                log .warning ("ligand_efficiency_missing_with_standard_value",count =int (mask .sum ()),message ="ligand_efficiency is empty while standard_value exists")
-        return df
-    def _serialize_activity_properties (self ,value :Any ,log :BoundLogger |None =None )->str |None :
-        "Return normalized JSON for activity_properties or None if not serializable."
-        normalized_items =self ._normalize_activity_properties_items (value ,log )
-        if normalized_items is None :
-            return None
-        try :
-            return json .dumps (normalized_items ,ensure_ascii =False ,sort_keys =True )
-        except (TypeError ,ValueError )as exc :
-            if log is not None :
-                log .warning ("activity_properties_serialization_failed",error =str (exc ))
-            return None
-    def _normalize_activity_properties_items (self ,value :Any ,log :BoundLogger |None =None )->list [dict [str ,Any ]]|None :
-        "Coerce activity_properties payloads into a list of constrained dictionaries."
-        if value is None :
-            return None
-        raw_value =value
-        if isinstance (value ,str ):
-            stripped =value .strip ()
-            if not stripped :
-                return []
-            try :
-                parsed =json .loads (stripped )
-            except (TypeError ,ValueError ):
-                fallback_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                fallback_base ["text_value"]=stripped
-                return [fallback_base ]
-            else :
-                value =parsed
-        if isinstance (value ,Mapping ):
-            items :list [Any ]=[value ]
-        elif isinstance (value ,Sequence )and (not isinstance (value ,(str ,bytes ))):
-            items =list (value )
-        else :
-            if log is not None :
-                log .warning ("activity_properties_unhandled_type",value_type =type (raw_value ).__name__ )
-            return None
-        normalized :list [dict [str ,Any ]]=[]
-        for item in items :
-            if item is None :
-                continue
-            if isinstance (item ,Mapping ):
-                item_mapping =cast (Mapping [str ,Any ],item )
-                normalized_item :dict [str ,Any |None ]={key :item_mapping .get (key )for key in ACTIVITY_PROPERTY_KEYS }
-                result_flag_value =normalized_item .get ("result_flag")
-                if isinstance (result_flag_value ,int )and result_flag_value in (0 ,1 ):
-                    normalized_item ["result_flag"]=bool (result_flag_value )
-                normalized .append (normalized_item )
-            elif isinstance (item ,str ):
-                str_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                str_base ["text_value"]=item
-                normalized .append (str_base )
-            elif log is not None :
-                log .warning ("activity_properties_item_unhandled",item_type =type (item ).__name__ )
-        return normalized
-    def _validate_activity_properties_truv (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0434\u043b\u044f activity_properties.\n\n        \u0412\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u0435\u0442 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n        - value IS NOT NULL \u21d2 text_value IS NULL (\u0438 \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442)\n        - relation IN ('=', '<', '\u2264', '>', '\u2265', '~') OR NULL\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        "
-        validated :list [dict [str ,Any ]]=[]
-        invalid_count =0
-        invalid_items :list [dict [str ,Any ]]=[]
-        for prop in properties :
-            is_valid =True
-            validation_errors :list [str ]=[]
-            value =prop .get ("value")
-            text_value =prop .get ("text_value")
-            relation =prop .get ("relation")
-            if value is not None and text_value is not None :
-                is_valid =False
-                validation_errors .append ("both value and text_value are not None")
-            elif value is None and text_value is None :
-                pass
-            if relation is not None :
-                if not isinstance (relation ,str ):
-                    is_valid =False
-                    validation_errors .append (f'relation is not a string: {type (relation ).__name__ }')
-                elif relation not in RELATIONS :
-                    is_valid =False
-                    validation_errors .append (f"relation '{relation }' not in allowed values: {RELATIONS }")
-            validated .append (prop )
-            if not is_valid :
-                invalid_count +=1
-                invalid_items .append (prop )
-                log .warning ("activity_property_truv_validation_failed",activity_id =activity_id ,property =prop ,errors =validation_errors ,message ="TRUV validation failed, but property is kept")
-        stats ={"invalid_count":invalid_count ,"valid_count":len (validated )}
-        return (validated ,stats )
-    def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-        seen :set [tuple [Any ,...]]=set ()
-        deduplicated :list [dict [str ,Any ]]=[]
-        duplicates_removed =0
-        for prop in properties :
-            dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-            if dedup_key not in seen :
-                seen .add (dedup_key )
-                deduplicated .append (prop )
-            else :
-                duplicates_removed +=1
-                log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-        stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-        return (deduplicated ,stats )
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_added",value ="activity")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_filled",value ="activity")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :BoundLogger )->pd .DataFrame :
-        "Convert data types according to the Pandera schema."
-        df =df .copy ()
-        non_nullable_int_fields ={"activity_id":"int64"}
-        nullable_int_fields ={"row_index":"Int64","target_tax_id":"int64","assay_tax_id":"int64","record_id":"int64","src_id":"int64"}
-        float_fields ={"standard_value":"float64","standard_upper_value":"float64","pchembl_value":"float64","upper_value":"float64","lower_value":"float64"}
-        bool_fields =["potential_duplicate","curated","removed"]
-        binary_flag_fields =["standard_flag"]
-        for field in non_nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_int :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_int .astype ("Int64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field =="row_index":
-                    numeric_series_row :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=numeric_series_row .astype ("Int64")
-                    if df [field ].isna ().any ():
-                        df [field ]=range (len (df ))
-                else :
-                    nullable_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=nullable_numeric_series .astype ("Int64")
-                    mask_valid =df [field ].notna ()
-                    if mask_valid .any ():
-                        invalid_mask =mask_valid &(df [field ]<1 )
-                        if invalid_mask .any ():
-                            log .warning ("invalid_positive_integer",field =field ,count =int (invalid_mask .sum ()))
-                            df .loc [invalid_mask ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in float_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_float :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_float .astype ("float64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in bool_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field in ("curated","removed")and df [field ].dtype =="boolean":
-                    continue
-                if field in ("curated","removed"):
-                    df [field ]=df [field ].astype ("boolean")
-                else :
-                    bool_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=(bool_numeric_series !=0 ).astype ("boolean")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("bool_conversion_failed",field =field ,error =str (exc ))
-        for field in binary_flag_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_flag :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_flag .astype ("Int64")
-                mask_valid =df [field ].notna ()
-                if mask_valid .any ():
-                    valid_values =df .loc [mask_valid ,field ]
-                    invalid_valid_mask =~valid_values .isin ([0 ,1 ])
-                    if invalid_valid_mask .any ():
-                        invalid_index =valid_values .index [invalid_valid_mask ]
-                        log .warning ("invalid_standard_flag",field =field ,count =int (invalid_valid_mask .sum ()))
-                        df .loc [invalid_index ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        object_fields =["value","activity_properties"]
-        for field in object_fields :
-            if field in df .columns :
-                if df [field ].dtype !="object":
-                    df [field ]=df [field ].astype ("object")
-        return df
-    def _validate_foreign_keys (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Validate foreign key integrity and format of ChEMBL IDs."
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        chembl_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        warnings :list [str ]=[]
-        for field in chembl_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-                if invalid_mask .any ():
-                    warning_msg :str =f'{field }: {int (invalid_mask .sum ())} invalid format(s)'
-                    warnings .append (warning_msg )
-        if warnings :
-            log .warning ("foreign_key_validation",warnings =warnings )
-        return df
-    def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check uniqueness of activity_id before validation."
-        if "activity_id"not in df .columns :
-            log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-            return
-        duplicates =df [df ["activity_id"].duplicated (keep =False )]
-        if not duplicates .empty :
-            duplicate_count =len (duplicates )
-            duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-            log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-            msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-            raise ValueError (msg )
-        log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
-    def _validate_data_validity_comment_soft_enum (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Soft enum \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0434\u043b\u044f data_validity_comment.\n\n        \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0442\u0438\u0432 whitelist \u0438\u0437 \u043a\u043e\u043d\u0444\u0438\u0433\u0430. \u041d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n        \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a warning, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e (soft enum).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        "
-        if df .empty or "data_validity_comment"not in df .columns :
-            return
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if not whitelist :
-            return
-        series_candidate =df ["data_validity_comment"].dropna ()
-        if len (series_candidate )==0 :
-            return
-        non_null_comments_series =series_candidate .astype ("string")
-        whitelist_set :set [str ]=set (whitelist )
-        def _is_unknown (value :str )->bool :
-            return value not in whitelist_set
-        unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-        unknown_count =int (unknown_mask .sum ())
-        if unknown_count >0 :
-            unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-            log .warning ("soft_enum_unknown_data_validity_comment",unknown_count =unknown_count ,total_count =len (non_null_comments_series ),samples =unknown_values ,whitelist =whitelist ,message ="Unknown data_validity_comment values detected (soft enum: not blocking)")
-    def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-        reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        errors :list [str ]=[]
-        for field in reference_fields :
-            if field not in df .columns :
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-                continue
-            mask =df [field ].notna ()
-            if not mask .any ():
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-                continue
-            invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-                errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-                log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-        if errors :
-            log .error ("foreign_key_integrity_check_failed",errors =errors )
-            msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-            raise ValueError (msg )
-        log .debug ("foreign_key_integrity_verified")
-    def _log_detailed_validation_errors (self ,failure_cases :pd .DataFrame ,payload :pd .DataFrame ,log :BoundLogger )->None :
-        "Log individual validation errors with row index and activity_id."
-        if failure_cases .empty or payload .empty :
-            return
-        activity_id_col ="activity_id"if "activity_id"in payload .columns else None
-        index_col ="index"if "index"in failure_cases .columns else None
-        if index_col is None :
-            return
-        max_errors =20
-        errors_to_log =failure_cases .head (max_errors )
-        for _ ,error_row in errors_to_log .iterrows ():
-            row_index =error_row .get (index_col )
-            if row_index is None :
-                continue
-            error_details :dict [str ,Any ]={"row_index":int (row_index )if isinstance (row_index ,(int ,float ))else str (row_index )}
-            if activity_id_col :
-                try :
-                    idx =int (row_index )if isinstance (row_index ,(int ,float ))else row_index
-                    activity_id_value :Any =payload .at [cast (int ,idx ),activity_id_col ]
-                    activity_id =activity_id_value
-                except (KeyError ,IndexError ):
-                    activity_id =None
-                if activity_id is not None and pd .notna (activity_id ):
-                    error_details ["activity_id"]=int (activity_id )if isinstance (activity_id ,(int ,float ))else str (activity_id )
-            if "column"in error_row and pd .notna (error_row ["column"]):
-                error_details ["column"]=str (error_row ["column"])
-            if "schema_context"in error_row and pd .notna (error_row ["schema_context"]):
-                error_details ["schema_context"]=str (error_row ["schema_context"])
-            if "failure_case"in error_row and pd .notna (error_row ["failure_case"]):
-                error_details ["failure_case"]=str (error_row ["failure_case"])
-            log .error ("validation_error_detail",**error_details )
-        if len (failure_cases )>max_errors :
-            log .warning ("validation_errors_truncated",total_errors =len (failure_cases ),logged_errors =max_errors )
-    def build_quality_report (self ,df :pd .DataFrame )->pd .DataFrame |dict [str ,object ]|None :
-        "Return QC report with activity-specific metrics including distributions."
-        business_key =["activity_id"]if "activity_id"in df .columns else None
-        base_report =build_default_quality_report (df ,business_key_fields =business_key )
-        rows :list [dict [str ,Any ]]=[]
-        if not base_report .empty :
-            records_raw =base_report .to_dict ("records")
-            records :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],records_raw )
-            for record in records :
-                rows .append ({str (k ):v for k ,v in record .items ()})
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        foreign_key_fields =["assay_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"]
-        for field in foreign_key_fields :
-            if field in df .columns :
-                mask =df [field ].notna ()
-                if mask .any ():
-                    string_series =df [field ].astype (str )
-                    valid_mask =mask &string_series .str .match (chembl_id_pattern .pattern ,na =False )
-                    invalid_count =int ((mask &~valid_mask ).astype (int ).sum ())
-                    valid_count =int (valid_mask .astype (int ).sum ())
-                    total_count =int (mask .astype (int ).sum ())
-                    integrity_ratio =float (valid_count /total_count )if total_count >0 else 0.0
-                    rows .append ({"section":"foreign_key","metric":"integrity_ratio","column":field ,"value":float (integrity_ratio ),"valid_count":int (valid_count ),"invalid_count":int (invalid_count ),"total_count":int (total_count )})
-        if "standard_type"in df .columns :
-            type_counts_series :pd .Series [Any ]=df ["standard_type"].value_counts ()
-            type_dist_raw =type_counts_series .to_dict ()
-            type_dist :dict [Any ,int ]=cast (dict [Any ,int ],type_dist_raw )
-            for type_value ,count in type_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_type_count","column":"standard_type","value":str (type_value )if type_value is not None else "null","count":int (count )})
-        if "standard_units"in df .columns :
-            unit_counts_series :pd .Series [Any ]=df ["standard_units"].value_counts ()
-            unit_dist_raw =unit_counts_series .to_dict ()
-            unit_dist :dict [Any ,int ]=cast (dict [Any ,int ],unit_dist_raw )
-            for unit_value ,count in unit_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_units_count","column":"standard_units","value":str (unit_value )if unit_value is not None else "null","count":int (count )})
-        return pd .DataFrame (rows )
-    def write (self ,df :pd .DataFrame ,output_path :Path ,*,extended :bool =False ,include_correlation :bool |None =None ,include_qc_metrics :bool |None =None )->RunResult :
-        "Override write() to bind actor and ensure deterministic sorting.\n\n        Parameters\n        ----------\n        df:\n            The DataFrame to write.\n        output_path:\n            The base output path for all artifacts.\n        extended:\n            Whether to include extended QC artifacts.\n\n        Returns\n        -------\n        RunResult:\n            All artifacts generated by the write operation.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.write')
-        UnifiedLogger .bind (actor =self .actor )
-        sort_keys =["assay_chembl_id","testitem_chembl_id","activity_id"]
-        if df .empty or not all ((key in df .columns for key in sort_keys )):
-            return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-        original_sort_by =self .config .determinism .sort .by
-        if not original_sort_by or original_sort_by !=sort_keys :
-            from copy import deepcopy
-            from bioetl .config .models .determinism import DeterminismSortingConfig
-            modified_config =deepcopy (self .config )
-            modified_config .determinism .sort =DeterminismSortingConfig (by =sort_keys ,ascending =[True ,True ,True ],na_position ="last")
-            log .debug ("write_sort_config_set",sort_keys =sort_keys ,original_sort_keys =list (original_sort_by )if original_sort_by else [])
-            original_config =self .config
-            self .config =modified_config
-            try :
-                result =super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-            finally :
-                self .config =original_config
-            return result
-        return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
```

#### Hotspot 2

- Definition: ChemblActivityPipeline.__init__#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:111-115
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,5 +0,0 @@
-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
```

#### Hotspot 3

- Definition: ChemblActivityPipeline._add_row_metadata#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2901-2924
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,18 +0,0 @@
-def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_added",value ="activity")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_filled",value ="activity")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Hotspot 4

- Definition: ChemblActivityPipeline._build_activity_descriptor#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:343-415
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,22 +0,0 @@
-def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-    "Construct the descriptor driving the shared extraction template."
-    def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-        http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-        chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-        typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =source_config .parameters .select_fields
-        return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-    def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-    def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        df =pipeline ._ensure_comment_fields (df ,log )
-        chembl_client =cast (ChemblClient ,context .chembl_client )
-        if chembl_client is not None :
-            df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-        pipeline ._log_validity_comments_metrics (df ,log )
-        return df
-    def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        return pipeline ._materialize_activity_record (payload )
-    return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
```

#### Hotspot 5

- Definition: ChemblActivityPipeline._cache_directory#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1257-1267
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,7 +0,0 @@
-def _cache_directory (self ,release :str |None )->Path :
-    cache_root =Path (self .config .paths .cache_root )
-    directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-    release_component =self ._sanitize_cache_component (release or "unknown")
-    pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-    version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-    return cache_root /directory_name /pipeline_component /release_component /version_component
```

#### Hotspot 6

- Definition: ChemblActivityPipeline._cache_file_path#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1252-1255
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,4 +0,0 @@
-def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-    directory =self ._cache_directory (release )
-    cache_key =self ._cache_key (batch_ids ,release )
-    return directory /f'{cache_key }.json'
```

#### Hotspot 7

- Definition: ChemblActivityPipeline._cache_key#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1269-1277
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,4 +0,0 @@
-def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-    payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-    raw =json .dumps (payload ,sort_keys =True )
-    return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
```

#### Hotspot 8

- Definition: ChemblActivityPipeline._check_activity_id_uniqueness#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:3108-3131
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,13 +0,0 @@
-def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check uniqueness of activity_id before validation."
-    if "activity_id"not in df .columns :
-        log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-        return
-    duplicates =df [df ["activity_id"].duplicated (keep =False )]
-    if not duplicates .empty :
-        duplicate_count =len (duplicates )
-        duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-        log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-        msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-        raise ValueError (msg )
-    log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
```

#### Hotspot 9

- Definition: ChemblActivityPipeline._check_cache#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1176-1221
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,33 +0,0 @@
-def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-    cache_config =self .config .cache
-    if not cache_config .enabled :
-        return None
-    normalized_ids =[str (identifier )for identifier in batch_ids ]
-    cache_file =self ._cache_file_path (normalized_ids ,release )
-    if not cache_file .exists ():
-        return None
-    try :
-        stat =cache_file .stat ()
-    except OSError :
-        return None
-    ttl_seconds =int (cache_config .ttl )
-    if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    try :
-        payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-    except (OSError ,json .JSONDecodeError ):
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    if not isinstance (payload ,dict ):
-        return None
-    missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-    if missing :
-        return None
-    return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
```

#### Hotspot 10

- Definition: ChemblActivityPipeline._check_foreign_key_integrity#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:3185-3232
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,24 +0,0 @@
-def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-    reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-    chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-    errors :list [str ]=[]
-    for field in reference_fields :
-        if field not in df .columns :
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-            continue
-        mask =df [field ].notna ()
-        if not mask .any ():
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-            continue
-        invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-        if invalid_mask .any ():
-            invalid_count =int (invalid_mask .sum ())
-            invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-            errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-            log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-    if errors :
-        log .error ("foreign_key_integrity_check_failed",errors =errors )
-        msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-        raise ValueError (msg )
-    log .debug ("foreign_key_integrity_verified")
```

#### Hotspot 11

- Definition: ChemblActivityPipeline._coerce_activity_dataset#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:432-450
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,14 +0,0 @@
-def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-    "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-    if isinstance (dataset ,pd .Series ):
-        return dataset .to_frame (name ="activity_id")
-    if isinstance (dataset ,pd .DataFrame ):
-        return dataset
-    if isinstance (dataset ,Mapping ):
-        mapping =cast (Mapping [str ,Any ],dataset )
-        return pd .DataFrame ([dict (mapping )])
-    if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-        dataset_list :list [Any ]=list (dataset )
-        return pd .DataFrame ({"activity_id":dataset_list })
-    msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-    raise TypeError (msg )
```

#### Hotspot 12

- Definition: ChemblActivityPipeline._coerce_mapping#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2065-2068
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,5 +0,0 @@
-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Hotspot 13

- Definition: ChemblActivityPipeline._collect_records_by_ids#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:498-637
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,83 +0,0 @@
-def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-    "Iterate over IDs using the shared iterator while preserving cache semantics."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    records :list [dict [str ,Any ]]=[]
-    success_count =0
-    fallback_count =0
-    error_count =0
-    cache_hits =0
-    api_calls =0
-    total_batches =0
-    key_order =[key for _ ,key in normalized_ids ]
-    key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-    for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-        total_batches +=1
-        batch_start =time .perf_counter ()
-        from_cache =False
-        chunk_records :dict [str ,dict [str ,Any ]]={}
-        try :
-            cached_records =self ._check_cache (chunk ,self ._chembl_release )
-            if cached_records is not None :
-                from_cache =True
-                cache_hits +=len (chunk )
-                chunk_records =cached_records
-            else :
-                api_calls +=1
-                fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                for item in fetched_items :
-                    if not isinstance (item ,Mapping ):
-                        continue
-                    activity_value =item .get ("activity_id")
-                    if activity_value is None :
-                        continue
-                    chunk_records [str (activity_value )]=dict (item )
-                self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-            success_in_batch =0
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                record =chunk_records .get (key )
-                if record and (not record .get ("error")):
-                    materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                    records .append (materialized )
-                    success_count +=1
-                    success_in_batch +=1
-                else :
-                    fallback_record =self ._create_fallback_record (numeric_id )
-                    records .append (fallback_record )
-                    fallback_count +=1
-                    error_count +=1
-            batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-            log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-        except CircuitBreakerOpenError as exc :
-            log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except RequestException as exc :
-            log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except Exception as exc :
-            log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-    total_records =len (normalized_ids )
-    success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-    summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-    return (records ,summary )
```

#### Hotspot 14

- Definition: ChemblActivityPipeline._create_fallback_record#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1284-1323
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,16 +0,0 @@
-def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-    "Create fallback record enriched with error metadata."
-    base_message ="Fallback: ChEMBL activity unavailable"
-    message =f'{base_message } ({error })'if error else base_message
-    timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-    metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-    if isinstance (error ,RequestException ):
-        response =getattr (error ,"response",None )
-        status_code =getattr (response ,"status_code",None )
-        if status_code is not None :
-            metadata ["http_status"]=status_code
-        metadata ["error_message"]=str (error )
-    elif error is not None :
-        metadata ["error_message"]=str (error )
-    fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-    return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
```

#### Hotspot 15

- Definition: ChemblActivityPipeline._deduplicate_activity_properties#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2843-2899
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,15 +0,0 @@
-def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-    "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-    seen :set [tuple [Any ,...]]=set ()
-    deduplicated :list [dict [str ,Any ]]=[]
-    duplicates_removed =0
-    for prop in properties :
-        dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-        if dedup_key not in seen :
-            seen .add (dedup_key )
-            deduplicated .append (prop )
-        else :
-            duplicates_removed +=1
-            log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-    stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-    return (deduplicated ,stats )
```

#### Hotspot 16

- Definition: ChemblActivityPipeline._enrich_assay#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:892-935
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,28 +0,0 @@
-def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    assay_section :Any =enrich_section .get ("assay")
-                    if isinstance (assay_section ,Mapping ):
-                        assay_section =cast (Mapping [str ,Any ],assay_section )
-                        enrich_cfg =dict (assay_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_assay (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 17

- Definition: ChemblActivityPipeline._enrich_compound_record#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:822-867
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,28 +0,0 @@
-def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    compound_record_section :Any =enrich_section .get ("compound_record")
-                    if isinstance (compound_record_section ,Mapping ):
-                        compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                        enrich_cfg =dict (compound_record_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 18

- Definition: ChemblActivityPipeline._enrich_data_validity#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:960-1014
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,32 +0,0 @@
-def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    data_validity_section :Any =enrich_section .get ("data_validity")
-                    if isinstance (data_validity_section ,Mapping ):
-                        data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                        enrich_cfg =dict (data_validity_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    if "data_validity_description"in df .columns :
-        non_na_count =int (df ["data_validity_description"].notna ().sum ())
-        if non_na_count >0 :
-            log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 19

- Definition: ChemblActivityPipeline._enrich_molecule#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1039-1108
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,37 +0,0 @@
-def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    molecule_section :Any =enrich_section .get ("molecule")
-                    if isinstance (molecule_section ,Mapping ):
-                        molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                        enrich_cfg =dict (molecule_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-    if "molecule_name"in df_join .columns :
-        if "molecule_pref_name"not in df .columns :
-            df ["molecule_pref_name"]=pd .NA
-        mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-        if mask .any ():
-            df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-            df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-            log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-    return df
```

#### Hotspot 20

- Definition: ChemblActivityPipeline._ensure_comment_fields#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1325-1358
- assay: absent

```diff
--- activity:run.py
+++ assay:run.py
@@ -1,11 +0,0 @@
-def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-    if df .empty :
-        return df
-    required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-    missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-    if missing_fields :
-        for field in missing_fields :
-            df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-        log .debug ("comment_fields_ensured",fields =missing_fields )
-    return df
```

_Only the first 20 hotspots of 110 are shown for module run.py._

_First 20 hotspots of 143 are shown for pair activity ↔ assay._

---

## Pair: activity ↔ document

- AST hash: 978d4152ad77000361c21b7f2051d3d1 ↔ 3774d930e14eb5befd7fdffc255cb346

- Jaccard over tokens: 0.231

### Module run.py

- File status: activity — present, document — present
- AST hash: 13d39ff2ae0173b684048e972fd58d8d ↔ 3fbedfe7f15a107384e19a0da1698398
- Jaccard over tokens: 0.224

Definition                                                       | activity signature                                                                                                                                                         | document signature                        | Side effects                                                                                                                                                                                                                                                                                                                                                          | Exceptions                                               | Status          
-----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|-----------------
ChemblActivityPipeline                                           | —                                                                                                                                                                          | —                                         | activity: io=cache_file.read_text, json.dumps, json.loads, payload.get, tmp_path.write_text; logging=UnifiedLogger.bind, UnifiedLogger.get, bound_log.warning, errors_to_log.iterrows, log.debug, log.error, log.info, log.warning, pipeline._log_validity_comments_metrics, self._log_detailed_validation_errors, self._log_validity_comments_metrics<br>document: ∅ | activity: TypeError(msg), ValueError(msg)<br>document: ∅ | only in activity
ChemblActivityPipeline.__init__                                  | self, config: PipelineConfig, run_id: str                                                                                                                                  | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._add_row_metadata                         | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.debug<br>document: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._build_activity_descriptor                | self                                                                                                                                                                       | —                                         | activity: io=∅; logging=pipeline._log_validity_comments_metrics<br>document: ∅                                                                                                                                                                                                                                                                                        | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._cache_directory                          | self, release: str | None                                                                                                                                                  | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._cache_file_path                          | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._cache_key                                | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                         | activity: io=json.dumps; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                     | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._check_activity_id_uniqueness             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.debug, log.error, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                              | activity: ValueError(msg)<br>document: ∅                 | only in activity
ChemblActivityPipeline._check_cache                              | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                         | activity: io=cache_file.read_text, json.loads; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._check_foreign_key_integrity              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.debug, log.error, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                              | activity: ValueError(msg)<br>document: ∅                 | only in activity
ChemblActivityPipeline._coerce_activity_dataset                  | self, dataset: object                                                                                                                                                      | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: TypeError(msg)<br>document: ∅                  | only in activity
ChemblActivityPipeline._coerce_mapping                           | payload: Any                                                                                                                                                               | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._collect_records_by_ids                   | self, normalized_ids: Sequence[tuple[int, str]], activity_iterator: ChemblActivityClient, *, select_fields: Sequence[str] | None                                           | —                                         | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.error, log.warning<br>document: ∅                                                                                                                                                                                                                                                                           | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._create_fallback_record                   | self, activity_id: int, error: Exception | None                                                                                                                            | —                                         | activity: io=json.dumps; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                     | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._deduplicate_activity_properties          | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                         | activity: io=∅; logging=log.debug<br>document: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._enrich_assay                             | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: io=∅; logging=UnifiedLogger.get, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                 | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._enrich_compound_record                   | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: io=∅; logging=UnifiedLogger.get, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                 | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._enrich_data_validity                     | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                       | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._enrich_molecule                          | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                       | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._ensure_comment_fields                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.debug<br>document: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._extract_activity_properties_fields       | self, record: dict[str, Any]                                                                                                                                               | —                                         | activity: io=json.loads; logging=UnifiedLogger.get, log.debug, log.warning<br>document: ∅                                                                                                                                                                                                                                                                             | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._extract_assay_fields                     | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                         | activity: io=∅; logging=log.debug, log.info, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._extract_chembl_release                   | payload: Mapping[str, Any]                                                                                                                                                 | —                                         | activity: io=payload.get; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._extract_data_validity_descriptions       | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                         | activity: io=∅; logging=log.debug, log.info, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._extract_from_chembl                      | self, dataset: object, chembl_client: ChemblClient | Any, activity_iterator: ChemblActivityClient, *, limit: int | None = None, select_fields: Sequence[str] | None = None | —                                         | activity: io=∅; logging=UnifiedLogger.get, log.info, self._log_validity_comments_metrics<br>document: ∅                                                                                                                                                                                                                                                               | activity: ValueError(msg)<br>document: ∅                 | only in activity
ChemblActivityPipeline._extract_nested_fields                    | self, record: dict[str, Any]                                                                                                                                               | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._extract_page_items                       | payload: Mapping[str, Any], items_keys: Sequence[str] | None                                                                                                               | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._filter_invalid_required_fields           | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._finalize_identifier_columns              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._finalize_output_columns                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.debug, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._get_data_validity_comment_whitelist      | self                                                                                                                                                                       | —                                         | activity: io=∅; logging=UnifiedLogger.get<br>document: ∅                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._harmonize_identifier_columns             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.debug<br>document: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._log_detailed_validation_errors           | self, failure_cases: pd.DataFrame, payload: pd.DataFrame, log: BoundLogger                                                                                                 | —                                         | activity: io=∅; logging=errors_to_log.iterrows, log.error, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                 | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._log_validity_comments_metrics            | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.info, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                          | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._materialize_activity_record              | self, payload: Mapping[str, Any], *, activity_id: int | None = None                                                                                                        | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._next_link                                | payload: Mapping[str, Any], base_url: str                                                                                                                                  | —                                         | activity: io=payload.get; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._normalize_activity_ids                   | self, input_frame: pd.DataFrame, *, limit: int | None, log: BoundLogger                                                                                                    | —                                         | activity: io=∅; logging=log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._normalize_activity_properties_items      | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                         | activity: io=json.loads; logging=log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                           | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._normalize_data_types                     | self, df: pd.DataFrame, schema: Any, log: BoundLogger                                                                                                                      | —                                         | activity: io=∅; logging=log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._normalize_identifiers                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.debug<br>document: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._normalize_measurements                   | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.debug, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._normalize_nested_structures              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=json.dumps, json.loads; logging=log.warning<br>document: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._normalize_string_fields                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.debug, log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._prepare_activity_iteration               | self, *, client_name: str = 'chembl_activity_client'                                                                                                                       | —                                         | activity: io=∅; logging=UnifiedLogger.get<br>document: ∅                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._sanitize_cache_component                 | value: str                                                                                                                                                                 | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._schema_column_specs                      | self                                                                                                                                                                       | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._serialize_activity_properties            | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                         | activity: io=json.dumps; logging=log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                           | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_assay                      | self                                                                                                                                                                       | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_compound_record            | self                                                                                                                                                                       | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_data_validity              | self                                                                                                                                                                       | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_molecule                   | self                                                                                                                                                                       | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._store_cache                              | self, batch_ids: Sequence[str], batch_data: Mapping[str, Mapping[str, Any]], release: str | None                                                                           | —                                         | activity: io=json.dumps, tmp_path.write_text; logging=UnifiedLogger.get, log.debug<br>document: ∅                                                                                                                                                                                                                                                                     | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._validate_activity_properties_truv        | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                         | activity: io=∅; logging=log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._validate_data_validity_comment_soft_enum | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline._validate_foreign_keys                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                         | activity: io=∅; logging=log.warning<br>document: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline.build_quality_report                      | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline.chembl_release                            | self                                                                                                                                                                       | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline.extract                                   | self, *args, **kwargs                                                                                                                                                      | —                                         | activity: io=∅; logging=UnifiedLogger.get, bound_log.warning<br>document: ∅                                                                                                                                                                                                                                                                                           | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline.extract_all                               | self                                                                                                                                                                       | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline.extract_by_ids                            | self, ids: Sequence[str]                                                                                                                                                   | —                                         | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning, self._log_validity_comments_metrics<br>document: ∅                                                                                                                                                                                                                                                  | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline.transform                                 | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.info<br>document: ∅                                                                                                                                                                                                                                                                                         | activity: ∅<br>document: ∅                               | only in activity
ChemblActivityPipeline.validate                                  | self, df: pd.DataFrame                                                                                                                                                     | —                                         | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.error, log.info, self._log_detailed_validation_errors<br>document: ∅                                                                                                                                                                                                                                        | activity: ValueError(msg)<br>document: ∅                 | only in activity
ChemblActivityPipeline.write                                     | self, df: pd.DataFrame, output_path: Path, *, extended: bool = False, include_correlation: bool | None = None, include_qc_metrics: bool | None = None                      | —                                         | activity: io=∅; logging=UnifiedLogger.bind, UnifiedLogger.get, log.debug<br>document: ∅                                                                                                                                                                                                                                                                               | activity: ∅<br>document: ∅                               | only in activity
ChemblDocumentPipeline                                           | —                                                                                                                                                                          | —                                         | activity: ∅<br>document: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning                                                                                                                                                                                                                                                                            | activity: ∅<br>document: TypeError(msg)                  | only in document
ChemblDocumentPipeline.__init__                                  | —                                                                                                                                                                          | self, config: PipelineConfig, run_id: str | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._add_system_fields                        | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any          | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._build_document_descriptor                | —                                                                                                                                                                          | self: SelfChemblDocumentPipeline          | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: TypeError(msg)                  | only in document
ChemblDocumentPipeline._check_document_id_uniqueness             | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any          | activity: ∅<br>document: io=∅; logging=log.warning                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._coerce_mapping                           | —                                                                                                                                                                          | payload: Any                              | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._enrich_document_terms                    | —                                                                                                                                                                          | self, df: pd.DataFrame                    | activity: ∅<br>document: io=∅; logging=UnifiedLogger.get, log.warning                                                                                                                                                                                                                                                                                                 | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._extract_nested_fields                    | —                                                                                                                                                                          | self, record: dict[str, Any]              | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._normalize_authors                        | —                                                                                                                                                                          | authors: Any, separator: str              | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._normalize_doi                            | —                                                                                                                                                                          | doi: str | None                           | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._normalize_identifiers                    | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any          | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._normalize_journal                        | —                                                                                                                                                                          | value: Any, max_len: int                  | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._normalize_numeric_fields                 | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any          | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._normalize_string_fields                  | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any          | activity: ∅<br>document: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._schema_column_specs                      | —                                                                                                                                                                          | self                                      | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline._should_enrich_document_terms             | —                                                                                                                                                                          | self                                      | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline.extract                                   | —                                                                                                                                                                          | self, *args, **kwargs                     | activity: ∅<br>document: io=∅; logging=UnifiedLogger.get                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline.extract_all                               | —                                                                                                                                                                          | self                                      | activity: ∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline.extract_by_ids                            | —                                                                                                                                                                          | self, ids: Sequence[str]                  | activity: ∅<br>document: io=∅; logging=UnifiedLogger.get, log.info                                                                                                                                                                                                                                                                                                    | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline.transform                                 | —                                                                                                                                                                          | self, df: pd.DataFrame                    | activity: ∅<br>document: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning                                                                                                                                                                                                                                                                            | activity: ∅<br>document: ∅                               | only in document
ChemblDocumentPipeline.validate                                  | —                                                                                                                                                                          | self, df: pd.DataFrame                    | activity: ∅<br>document: io=∅; logging=UnifiedLogger.get, log.debug, log.info                                                                                                                                                                                                                                                                                         | activity: ∅<br>document: ∅                               | only in document
__module_block_0                                                 | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_1                                                 | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_10                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_11                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_12                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_13                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_14                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_15                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_16                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_17                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_18                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_19                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_2                                                 | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_20                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_21                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
__module_block_22                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
__module_block_23                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
__module_block_24                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
__module_block_25                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
__module_block_26                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
__module_block_27                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
__module_block_28                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
__module_block_29                                                | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>document: ∅                               | only in activity
__module_block_3                                                 | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_4                                                 | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_5                                                 | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_6                                                 | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_7                                                 | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         
__module_block_8                                                 | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | identical       
__module_block_9                                                 | —                                                                                                                                                                          | —                                         | activity: io=∅; logging=∅<br>document: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>document: ∅                               | differs         

#### Hotspot 1

- Definition: ChemblActivityPipeline#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:106-3476
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,1684 +0,0 @@
-class ChemblActivityPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting activity records from the ChEMBL API."
-    actor ="activity_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-        self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch activity payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        def legacy_activity_ids (bound_log :BoundLogger )->Sequence [str ]|None :
-            payload_activity_ids =kwargs .get ("activity_ids")
-            if payload_activity_ids is None :
-                return None
-            bound_log .warning ("chembl_activity.deprecated_kwargs",message ="Using activity_ids in kwargs is deprecated. Use --input-file instead.")
-            if isinstance (payload_activity_ids ,Sequence )and (not isinstance (payload_activity_ids ,(str ,bytes ))):
-                sequence_ids :Sequence [str |int ]=cast (Sequence [str |int ],payload_activity_ids )
-                return [str (id_val )for id_val in sequence_ids ]
-            return [str (payload_activity_ids )]
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_activity.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="activity_id",legacy_id_resolver =legacy_activity_ids ,legacy_source ="deprecated_kwargs")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all activity records from ChEMBL using the shared iterator."
-        return self .run_extract_all (self ._build_activity_descriptor ())
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract activity records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of activity_id values to extract (as strings or integers).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted activity records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_config ,chembl_client ,activity_iterator ,select_fields =self ._prepare_activity_iteration ()
-        limit =self .config .cli .limit
-        invalid_ids :list [Any ]=[]
-        def normalize_activity_id (raw :Any )->tuple [str |None ,Any ]:
-            if pd .isna (raw ):
-                return (None ,None )
-            try :
-                if isinstance (raw ,str ):
-                    candidate =raw .strip ()
-                    if not candidate :
-                        return (None ,None )
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw ,(int ,float )):
-                    numeric_id =int (raw )
-                else :
-                    numeric_id =int (raw )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw )
-                return (None ,None )
-            return (str (numeric_id ),int (numeric_id ))
-        def delegated_fetch (canonical_ids :Sequence [str ],context :BatchExtractionContext )->tuple [Sequence [Mapping [str ,Any ]],Mapping [str ,Any ]]:
-            numeric_map =context .metadata
-            normalized_ids :list [tuple [int ,str ]]=[]
-            for identifier in canonical_ids :
-                numeric_value =numeric_map .get (identifier )
-                if numeric_value is None :
-                    continue
-                normalized_ids .append ((int (numeric_value ),identifier ))
-            if not normalized_ids :
-                summary ={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-                context .extra ["delegated_summary"]=summary
-                return ([],summary )
-            records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =context .select_fields or None )
-            context .extra ["delegated_summary"]=summary
-            return (records ,summary )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            dataframe =self ._ensure_comment_fields (dataframe ,log )
-            dataframe =self ._extract_data_validity_descriptions (dataframe ,chembl_client ,log )
-            dataframe =self ._extract_assay_fields (dataframe ,chembl_client ,log )
-            self ._log_validity_comments_metrics (dataframe ,log )
-            return dataframe
-        def empty_activity_frame ()->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def finalize_context (context :BatchExtractionContext )->None :
-            summary =context .extra .get ("delegated_summary")
-            if isinstance (summary ,Mapping ):
-                summary_dict =dict (summary )
-                context .extra ["stats_attribute_override"]=summary_dict
-                self ._last_batch_extract_stats =summary_dict
-            else :
-                context .extra ["stats_attribute_override"]=context .stats .as_dict ()
-                self ._last_batch_extract_stats =context .stats .as_dict ()
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="activity_id",fetcher =delegated_fetch ,select_fields =select_fields ,batch_size =source_config .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release ,id_normalizer =normalize_activity_id ,sort_key =lambda pair :int (pair [0 ]),finalize =finalize_dataframe ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats",fetch_mode ="delegated",empty_frame_factory =empty_activity_frame )
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        batch_stats =self ._last_batch_extract_stats or {}
-        log .info ("chembl_activity.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =batch_stats .get ("batches"),api_calls =batch_stats .get ("api_calls"),cache_hits =batch_stats .get ("cache_hits"))
-        return dataframe
-    def _prepare_activity_iteration (self ,*,client_name :str ="chembl_activity_client")->tuple [ActivitySourceConfig ,ChemblClient ,ChemblActivityClient ,list [str ]]:
-        "Construct reusable ChEMBL clients and iterator for activity extraction."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name =client_name )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =self ._resolve_select_fields (source_raw ,default_fields =API_ACTIVITY_FIELDS )
-        return (source_config ,chembl_client ,activity_iterator ,select_fields )
-    def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-        "Construct the descriptor driving the shared extraction template."
-        def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-            http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-            chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-            typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-            activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-            select_fields =source_config .parameters .select_fields
-            return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-        def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            df =pipeline ._ensure_comment_fields (df ,log )
-            chembl_client =cast (ChemblClient ,context .chembl_client )
-            if chembl_client is not None :
-                df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-            pipeline ._log_validity_comments_metrics (df ,log )
-            return df
-        def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            return pipeline ._materialize_activity_record (payload )
-        return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
-    def _materialize_activity_record (self ,payload :Mapping [str ,Any ],*,activity_id :int |None =None )->dict [str ,Any ]:
-        "Normalize nested fields within an activity payload."
-        record =dict (payload )
-        record =self ._extract_nested_fields (record )
-        record =self ._extract_activity_properties_fields (record )
-        if activity_id is not None :
-            record .setdefault ("activity_id",activity_id )
-        return record
-    def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-        "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-        if isinstance (dataset ,pd .Series ):
-            return dataset .to_frame (name ="activity_id")
-        if isinstance (dataset ,pd .DataFrame ):
-            return dataset
-        if isinstance (dataset ,Mapping ):
-            mapping =cast (Mapping [str ,Any ],dataset )
-            return pd .DataFrame ([dict (mapping )])
-        if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-            dataset_list :list [Any ]=list (dataset )
-            return pd .DataFrame ({"activity_id":dataset_list })
-        msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-        raise TypeError (msg )
-    def _normalize_activity_ids (self ,input_frame :pd .DataFrame ,*,limit :int |None ,log :BoundLogger )->list [tuple [int ,str ]]:
-        "Normalize raw identifier values into deduplicated integer/string pairs."
-        normalized_ids :list [tuple [int ,str ]]=[]
-        invalid_ids :list [Any ]=[]
-        seen :set [str ]=set ()
-        for raw_id in input_frame ["activity_id"].tolist ():
-            if pd .isna (raw_id ):
-                continue
-            try :
-                if isinstance (raw_id ,str ):
-                    candidate =raw_id .strip ()
-                    if not candidate :
-                        continue
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw_id ,(int ,float )):
-                    numeric_id =int (raw_id )
-                else :
-                    numeric_id =int (raw_id )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw_id )
-                continue
-            key =str (numeric_id )
-            if key not in seen :
-                seen .add (key )
-                normalized_ids .append ((numeric_id ,key ))
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        if limit is not None :
-            normalized_ids =normalized_ids [:max (int (limit ),0 )]
-        return normalized_ids
-    def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-        "Iterate over IDs using the shared iterator while preserving cache semantics."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        records :list [dict [str ,Any ]]=[]
-        success_count =0
-        fallback_count =0
-        error_count =0
-        cache_hits =0
-        api_calls =0
-        total_batches =0
-        key_order =[key for _ ,key in normalized_ids ]
-        key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-        for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-            total_batches +=1
-            batch_start =time .perf_counter ()
-            from_cache =False
-            chunk_records :dict [str ,dict [str ,Any ]]={}
-            try :
-                cached_records =self ._check_cache (chunk ,self ._chembl_release )
-                if cached_records is not None :
-                    from_cache =True
-                    cache_hits +=len (chunk )
-                    chunk_records =cached_records
-                else :
-                    api_calls +=1
-                    fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                    for item in fetched_items :
-                        if not isinstance (item ,Mapping ):
-                            continue
-                        activity_value =item .get ("activity_id")
-                        if activity_value is None :
-                            continue
-                        chunk_records [str (activity_value )]=dict (item )
-                    self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-                success_in_batch =0
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    record =chunk_records .get (key )
-                    if record and (not record .get ("error")):
-                        materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                        records .append (materialized )
-                        success_count +=1
-                        success_in_batch +=1
-                    else :
-                        fallback_record =self ._create_fallback_record (numeric_id )
-                        records .append (fallback_record )
-                        fallback_count +=1
-                        error_count +=1
-                batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-                log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-            except CircuitBreakerOpenError as exc :
-                log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except RequestException as exc :
-                log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except Exception as exc :
-                log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-        total_records =len (normalized_ids )
-        success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-        summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-        return (records ,summary )
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw activity data by normalizing measurements, identifiers, and data types."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_measurements (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,ActivitySchema ,log )
-        if "curated"in df .columns or "curated_by"in df .columns :
-            if "curated"not in df .columns :
-                df ["curated"]=pd .NA
-            if "curated_by"in df .columns :
-                mask =df ["curated"].isna ()
-                df .loc [mask ,"curated"]=df .loc [mask ,"curated_by"].notna ()
-            df ["curated"]=df ["curated"].astype ("boolean")
-        df =self ._validate_foreign_keys (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if self ._should_enrich_compound_record ():
-            df =self ._enrich_compound_record (df )
-        if self ._should_enrich_assay ():
-            df =self ._enrich_assay (df )
-        if self ._should_enrich_molecule ():
-            df =self ._enrich_molecule (df )
-        if self ._should_enrich_data_validity ():
-            df =self ._enrich_data_validity (df )
-        df =self ._finalize_identifier_columns (df ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        df =self ._finalize_output_columns (df ,log )
-        df =self ._filter_invalid_required_fields (df ,log )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against ActivitySchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        if "target_tax_id"in df .columns :
-            dtype_name :str =str (df ["target_tax_id"].dtype .name )
-            if dtype_name !="Int64":
-                numeric_series :pd .Series [Any ]=pd .to_numeric (df ["target_tax_id"],errors ="coerce")
-                df ["target_tax_id"]=numeric_series .astype ("Int64")
-        self ._check_activity_id_uniqueness (df ,log )
-        self ._check_foreign_key_integrity (df ,log )
-        self ._validate_data_validity_comment_soft_enum (df ,log )
-        original_coerce =self .config .validation .coerce
-        try :
-            self .config .validation .coerce =False
-            validated =super ().validate (df )
-            log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce )
-            return validated
-        except pandera .errors .SchemaErrors as exc :
-            failure_cases_df :pd .DataFrame |None =None
-            if hasattr (exc ,"failure_cases"):
-                failure_cases_df =cast (pd .DataFrame ,exc .failure_cases )
-            error_count =len (failure_cases_df )if failure_cases_df is not None else 0
-            error_summary =summarize_schema_errors (exc )
-            log .error ("validation_failed",error_count =error_count ,schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce ,error_summary =error_summary ,exc_info =True )
-            if failure_cases_df is not None and (not failure_cases_df .empty ):
-                failure_cases_summary =format_failure_cases (failure_cases_df )
-                log .error ("validation_failure_cases",failure_cases =failure_cases_summary )
-                self ._log_detailed_validation_errors (failure_cases_df ,df ,log )
-            msg =f'Validation failed with {error_count } error(s) against schema {self .config .validation .schema_out }: {error_summary }'
-            raise ValueError (msg )from exc
-        except Exception as exc :
-            log .error ("validation_error",error =str (exc ),schema =self .config .validation .schema_out ,exc_info =True )
-            raise
-        finally :
-            self .config .validation .coerce =original_coerce
-    def _should_enrich_compound_record (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 compound_record \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            compound_record_section :Any =enrich_section .get ("compound_record")
-            if not isinstance (compound_record_section ,Mapping ):
-                return False
-            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-            enabled :Any =compound_record_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        compound_record_section :Any =enrich_section .get ("compound_record")
-                        if isinstance (compound_record_section ,Mapping ):
-                            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                            enrich_cfg =dict (compound_record_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_assay (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 assay \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            assay_section :Any =enrich_section .get ("assay")
-            if not isinstance (assay_section ,Mapping ):
-                return False
-            assay_section =cast (Mapping [str ,Any ],assay_section )
-            enabled :Any =assay_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        assay_section :Any =enrich_section .get ("assay")
-                        if isinstance (assay_section ,Mapping ):
-                            assay_section =cast (Mapping [str ,Any ],assay_section )
-                            enrich_cfg =dict (assay_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_assay (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_data_validity (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 data_validity \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            data_validity_section :Any =enrich_section .get ("data_validity")
-            if not isinstance (data_validity_section ,Mapping ):
-                return False
-            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-            enabled :Any =data_validity_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        data_validity_section :Any =enrich_section .get ("data_validity")
-                        if isinstance (data_validity_section ,Mapping ):
-                            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                            enrich_cfg =dict (data_validity_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        if "data_validity_description"in df .columns :
-            non_na_count =int (df ["data_validity_description"].notna ().sum ())
-            if non_na_count >0 :
-                log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_molecule (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 molecule \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            molecule_section :Any =enrich_section .get ("molecule")
-            if not isinstance (molecule_section ,Mapping ):
-                return False
-            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-            enabled :Any =molecule_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        molecule_section :Any =enrich_section .get ("molecule")
-                        if isinstance (molecule_section ,Mapping ):
-                            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                            enrich_cfg =dict (molecule_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-        if "molecule_name"in df_join .columns :
-            if "molecule_pref_name"not in df .columns :
-                df ["molecule_pref_name"]=pd .NA
-            mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-            if mask .any ():
-                df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-                df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-                log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-        return df
-    def _extract_from_chembl (self ,dataset :object ,chembl_client :ChemblClient |Any ,activity_iterator :ChemblActivityClient ,*,limit :int |None =None ,select_fields :Sequence [str ]|None =None )->pd .DataFrame :
-        "Extract activity records by delegating batching to ``ChemblActivityClient``."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        method_start =time .perf_counter ()
-        self ._last_batch_extract_stats =None
-        input_frame =self ._coerce_activity_dataset (dataset )
-        if "activity_id"not in input_frame .columns :
-            msg ="Input dataset must contain an 'activity_id' column"
-            raise ValueError (msg )
-        normalized_ids =self ._normalize_activity_ids (input_frame ,limit =limit ,log =log )
-        if not normalized_ids :
-            summary :dict [str ,Any ]={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-            self ._last_batch_extract_stats =summary
-            log .info ("chembl_activity.batch_summary",**summary )
-            empty_frame =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-            return self ._ensure_comment_fields (empty_frame ,log )
-        records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =select_fields )
-        duration_ms =(time .perf_counter ()-method_start )*1000.0
-        summary ["duration_ms"]=duration_ms
-        self ._last_batch_extract_stats =summary
-        log .info ("chembl_activity.batch_summary",**summary )
-        result_df :pd .DataFrame =pd .DataFrame .from_records (records )
-        if result_df .empty :
-            result_df =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        elif "activity_id"in result_df .columns :
-            result_df =result_df .sort_values ("activity_id").reset_index (drop =True )
-        result_df =self ._ensure_comment_fields (result_df ,log )
-        result_df =self ._extract_data_validity_descriptions (result_df ,chembl_client ,log )
-        result_df =self ._extract_assay_fields (result_df ,chembl_client ,log )
-        self ._log_validity_comments_metrics (result_df ,log )
-        return result_df
-    def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-        cache_config =self .config .cache
-        if not cache_config .enabled :
-            return None
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        if not cache_file .exists ():
-            return None
-        try :
-            stat =cache_file .stat ()
-        except OSError :
-            return None
-        ttl_seconds =int (cache_config .ttl )
-        if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        try :
-            payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-        except (OSError ,json .JSONDecodeError ):
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        if not isinstance (payload ,dict ):
-            return None
-        missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-        if missing :
-            return None
-        return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
-    def _store_cache (self ,batch_ids :Sequence [str ],batch_data :Mapping [str ,Mapping [str ,Any ]],release :str |None )->None :
-        cache_config =self .config .cache
-        if not cache_config .enabled or not batch_ids or (not batch_data ):
-            return
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        normalized_set =set (normalized_ids )
-        data_to_store ={key :batch_data [key ]for key in normalized_set if key in batch_data }
-        if not data_to_store :
-            return
-        try :
-            cache_file .parent .mkdir (parents =True ,exist_ok =True )
-            tmp_path =cache_file .with_suffix (cache_file .suffix +".tmp")
-            tmp_path .write_text (json .dumps (data_to_store ,sort_keys =True ,default =str ),encoding ="utf-8")
-            tmp_path .replace (cache_file )
-        except Exception as exc :
-            log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-            log .debug ("chembl_activity.cache_store_failed",error =str (exc ))
-    def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-        directory =self ._cache_directory (release )
-        cache_key =self ._cache_key (batch_ids ,release )
-        return directory /f'{cache_key }.json'
-    def _cache_directory (self ,release :str |None )->Path :
-        cache_root =Path (self .config .paths .cache_root )
-        directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-        release_component =self ._sanitize_cache_component (release or "unknown")
-        pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-        version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-        return cache_root /directory_name /pipeline_component /release_component /version_component
-    def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-        payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-        raw =json .dumps (payload ,sort_keys =True )
-        return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
-    @staticmethod
-    def _sanitize_cache_component (value :str )->str :
-        sanitized =re .sub ("[^0-9A-Za-z_.-]","_",value )
-        return sanitized or "default"
-    def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-        "Create fallback record enriched with error metadata."
-        base_message ="Fallback: ChEMBL activity unavailable"
-        message =f'{base_message } ({error })'if error else base_message
-        timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-        metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-        if isinstance (error ,RequestException ):
-            response =getattr (error ,"response",None )
-            status_code =getattr (response ,"status_code",None )
-            if status_code is not None :
-                metadata ["http_status"]=status_code
-            metadata ["error_message"]=str (error )
-        elif error is not None :
-            metadata ["error_message"]=str (error )
-        fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-        return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
-    def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-        if df .empty :
-            return df
-        required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-        if missing_fields :
-            for field in missing_fields :
-                df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            log .debug ("comment_fields_ensured",fields =missing_fields )
-        return df
-    def _extract_data_validity_descriptions (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c data_validity_description \u0438\u0437 DATA_VALIDITY_LOOKUP \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f data_validity_comment \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_data_validity_lookup() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c data_validity_comment.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 data_validity_description.\n        "
-        if df .empty :
-            return df
-        if "data_validity_comment"not in df .columns :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="data_validity_comment_column_missing")
-            return df
-        validity_comments :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            comment =row .get ("data_validity_comment")
-            if pd .isna (comment )or comment is None :
-                continue
-            comment_str =str (comment ).strip ()
-            if comment_str :
-                validity_comments .append (comment_str )
-        if not validity_comments :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="no_valid_comments")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        unique_comments =list (set (validity_comments ))
-        log .info ("extract_data_validity_descriptions_fetching",comments_count =len (unique_comments ))
-        try :
-            records_dict =client .fetch_data_validity_lookup (comments =unique_comments ,fields =["data_validity_comment","description"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_data_validity_descriptions_fetch_error",error =str (exc ),exc_info =True )
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for comment in unique_comments :
-            record =records_dict .get (comment )
-            if record :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":record .get ("description")})
-            else :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":None })
-        if not enrichment_data :
-            log .debug ("extract_data_validity_descriptions_no_records")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        original_index =df .index .copy ()
-        df_result =df .merge (df_enrich ,on =["data_validity_comment"],how ="left",suffixes =("","_enrich"))
-        if "data_validity_description"not in df_result .columns :
-            df_result ["data_validity_description"]=pd .Series ([pd .NA ]*len (df_result ),dtype ="string")
-        else :
-            df_result ["data_validity_description"]=df_result ["data_validity_description"].astype ("string")
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_data_validity_descriptions_complete",comments_requested =len (unique_comments ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _extract_assay_fields (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c assay_organism \u0438 assay_tax_id \u0438\u0437 ASSAYS \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f assay_chembl_id \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_assays_by_ids() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438 assay_organism \u0438 assay_tax_id.\n        "
-        if df .empty :
-            return df
-        if "assay_chembl_id"not in df .columns :
-            log .debug ("extract_assay_fields_skipped",reason ="assay_chembl_id_column_missing")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        assay_ids :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            assay_id =row .get ("assay_chembl_id")
-            if pd .isna (assay_id )or assay_id is None :
-                continue
-            assay_id_str =str (assay_id ).strip ().upper ()
-            if assay_id_str :
-                assay_ids .append (assay_id_str )
-        if not assay_ids :
-            log .debug ("extract_assay_fields_skipped",reason ="no_valid_assay_ids")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        unique_assay_ids =list (set (assay_ids ))
-        log .info ("extract_assay_fields_fetching",assay_ids_count =len (unique_assay_ids ))
-        try :
-            records_dict =client .fetch_assays_by_ids (ids =unique_assay_ids ,fields =["assay_chembl_id","assay_organism","assay_tax_id"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_assay_fields_fetch_error",error =str (exc ),exc_info =True )
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for assay_id in unique_assay_ids :
-            record =records_dict .get (assay_id )if records_dict else None
-            if record :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":record .get ("assay_organism"),"assay_tax_id":record .get ("assay_tax_id")})
-            else :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":None ,"assay_tax_id":None })
-        if not enrichment_data :
-            log .debug ("extract_assay_fields_no_records")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        df_enrich ["assay_chembl_id"]=df_enrich ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        original_index =df .index .copy ()
-        df_normalized =df .copy ()
-        df_normalized ["assay_chembl_id_normalized"]=df_normalized ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        df_result =df_normalized .merge (df_enrich ,left_on ="assay_chembl_id_normalized",right_on ="assay_chembl_id",how ="left",suffixes =("","_enrich"))
-        df_result =df_result .drop (columns =["assay_chembl_id_normalized"])
-        for col in ["assay_organism","assay_tax_id"]:
-            if f'{col }_enrich'in df_result .columns :
-                if col not in df_result .columns :
-                    df_result [col ]=df_result [f'{col }_enrich']
-                else :
-                    base_series :pd .Series [Any ]=df_result [col ]
-                    enrich_series :pd .Series [Any ]=df_result [f'{col }_enrich']
-                    missing_mask =base_series .isna ()
-                    if bool (missing_mask .any ()):
-                        df_result .loc [missing_mask ,col ]=enrich_series .loc [missing_mask ]
-                df_result =df_result .drop (columns =[f'{col }_enrich'])
-        for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-            if col not in df_result .columns :
-                df_result [col ]=pd .Series ([pd .NA ]*len (df_result ),dtype =dtype )
-        df_result ["assay_organism"]=df_result ["assay_organism"].astype ("string")
-        df_result ["assay_tax_id"]=pd .to_numeric (df_result ["assay_tax_id"],errors ="coerce").astype ("Int64")
-        mask_valid =df_result ["assay_tax_id"].notna ()
-        if mask_valid .any ():
-            invalid_mask =mask_valid &(df_result ["assay_tax_id"]<1 )
-            if invalid_mask .any ():
-                log .warning ("invalid_assay_tax_id_range",count =int (invalid_mask .sum ()))
-                df_result .loc [invalid_mask ,"assay_tax_id"]=pd .NA
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_assay_fields_complete",assay_ids_requested =len (unique_assay_ids ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _log_validity_comments_metrics (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "\u041b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442:\n        - \u0414\u043e\u043b\u044e NA \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0442\u0440\u0435\u0445 \u043f\u043e\u043b\u0435\u0439\n        - \u0422\u043e\u043f-10 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment\n        - \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment (\u043d\u0435 \u0432 whitelist)\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity.\n        log:\n            Logger instance.\n        "
-        if df .empty :
-            return
-        metrics :dict [str ,Any ]={}
-        comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        for field in comment_fields :
-            if field in df .columns :
-                na_count =int (df [field ].isna ().sum ())
-                total_count =len (df )
-                na_rate =float (na_count )/float (total_count )if total_count >0 else 0.0
-                metrics [f'{field }_na_rate']=na_rate
-                metrics [f'{field }_na_count']=na_count
-                metrics [f'{field }_total_count']=total_count
-        non_null_comments_series :pd .Series [str ]|None =None
-        if "data_validity_comment"in df .columns :
-            series_candidate =df ["data_validity_comment"].dropna ()
-            if len (series_candidate )>0 :
-                typed_series :pd .Series [str ]=series_candidate .astype ("string")
-                non_null_comments_series =typed_series
-                value_counts =typed_series .value_counts ().head (10 )
-                top_10 ={str (key ):int (value )for key ,value in value_counts .items ()}
-                metrics ["top_10_data_validity_comments"]=top_10
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if whitelist and non_null_comments_series is not None :
-            whitelist_set :set [str ]=set (whitelist )
-            def _is_unknown (value :str )->bool :
-                return value not in whitelist_set
-            unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-            unknown_count =int (unknown_mask .sum ())
-            if unknown_count >0 :
-                unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-                metrics ["unknown_data_validity_comments_count"]=unknown_count
-                metrics ["unknown_data_validity_comments_samples"]=unknown_values
-                log .warning ("unknown_data_validity_comments_detected",unknown_count =unknown_count ,samples =unknown_values ,whitelist =whitelist )
-        if metrics :
-            log .info ("validity_comments_metrics",**metrics )
-    def _get_data_validity_comment_whitelist (self )->list [str ]:
-        "\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c whitelist \u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0434\u043b\u044f data_validity_comment \u0438\u0437 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\n\n        Raises\n        ------\n        RuntimeError\n            \u0415\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d \u0438\u043b\u0438 \u043f\u0443\u0441\u0442.\n        "
-        try :
-            values =sorted (self ._required_vocab_ids ("data_validity_comment"))
-        except RuntimeError as exc :
-            UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validation',run_id =self .run_id ).error ("data_validity_comment_whitelist_unavailable",error =str (exc ))
-            raise
-        return values
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested assay and molecule objects."
-        if "assay"in record and isinstance (record ["assay"],Mapping ):
-            assay =cast (Mapping [str ,Any ],record ["assay"])
-            if "organism"in assay :
-                record .setdefault ("assay_organism",assay ["organism"])
-            if "tax_id"in assay :
-                record .setdefault ("assay_tax_id",assay ["tax_id"])
-        if "molecule"in record and isinstance (record ["molecule"],Mapping ):
-            molecule =cast (Mapping [str ,Any ],record ["molecule"])
-            if "pref_name"in molecule :
-                record .setdefault ("molecule_pref_name",molecule ["pref_name"])
-        if "curated_by"in record :
-            curated_by =record .get ("curated_by")
-            if curated_by is not None and (not pd .isna (curated_by )):
-                record .setdefault ("curated",True )
-            else :
-                record .setdefault ("curated",False )
-        elif "curated"not in record :
-            record .setdefault ("curated",None )
-        return record
-    def _extract_activity_properties_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract TRUV fields, standard_* fields, and comments from activity_properties array as fallback.\n\n        \u041f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0438\u0437 activity_properties \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442\n        \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043e\u0442\u0432\u0435\u0442\u0435 API (\u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 \u043f\u0440\u044f\u043c\u044b\u0445 \u043f\u043e\u043b\u0435\u0439 \u0438\u0437 ACTIVITIES).\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0435 TRUV-\u043f\u043e\u043b\u044f: value, text_value, relation, units.\n        \u0422\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f: standard_upper_value, standard_text_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b: upper_value, lower_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0438: activity_comment, data_validity_comment.\n\n        \u0422\u0430\u043a\u0436\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 activity_properties \u0432 \u0437\u0430\u043f\u0438\u0441\u0438 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438.\n        \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044e \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract_activity_properties')
-        activity_id =record .get ("activity_id")
-        if "activity_properties"not in record :
-            log .warning ("activity_properties_missing",activity_id =activity_id ,message ="activity_properties not found in API response (possible ChEMBL < v24)")
-            record ["activity_properties"]=None
-            return record
-        properties =record ["activity_properties"]
-        if properties is None :
-            log .debug ("activity_properties_null",activity_id =activity_id ,message ="activity_properties is None (possible ChEMBL < v24)")
-            return record
-        if isinstance (properties ,str ):
-            try :
-                properties =json .loads (properties )
-            except (TypeError ,ValueError ,json .JSONDecodeError )as exc :
-                log .debug ("activity_properties_parse_failed",error =str (exc ),activity_id =record .get ("activity_id"))
-                return record
-        if not isinstance (properties ,Sequence )or isinstance (properties ,(str ,bytes )):
-            return record
-        property_iterable :Iterable [Any ]=cast (Iterable [Any ],properties )
-        property_items :list [Any ]=list (property_iterable )
-        def _set_fallback (key :str ,value :Any )->None :
-            "Set fallback value only if key is missing in record and value is not None."
-            if value is not None and record .get (key )is None :
-                record [key ]=value
-        def _is_empty (value :Any )->bool :
-            "Check if value is empty (None, empty string, or whitespace)."
-            if value is None :
-                return True
-            if isinstance (value ,str ):
-                return not value .strip ()
-            return False
-        items :list [Mapping [str ,Any ]]=[]
-        for property_item in property_items :
-            if isinstance (property_item ,Mapping )and "type"in property_item and ("value"in property_item or "text_value"in property_item ):
-                items .append (cast (Mapping [str ,Any ],property_item ))
-        def _is_measured (p :Mapping [str ,Any ])->bool :
-            rf =p .get ("result_flag")
-            return rf is True or (isinstance (rf ,int )and rf ==1 )
-        items .sort (key =lambda p :not _is_measured (p ))
-        for prop in items :
-            val =prop .get ("value")
-            txt =prop .get ("text_value")
-            rel =prop .get ("relation")
-            unt =prop .get ("units")
-            prop_type =str (prop .get ("type","")).lower ()
-            will_set_value =val is not None and record .get ("value")is None
-            will_set_text_value =txt is not None and record .get ("text_value")is None
-            _set_fallback ("value",val )
-            _set_fallback ("text_value",txt )
-            if will_set_value or will_set_text_value :
-                _set_fallback ("relation",rel )
-                _set_fallback ("units",unt )
-            if unt is not None and record .get ("units")is None :
-                _set_fallback ("units",unt )
-            if record .get ("upper_value")is None and ("upper"in prop_type or prop_type in ("upper_value","upper limit")):
-                if val is not None :
-                    _set_fallback ("upper_value",val )
-            if record .get ("lower_value")is None and ("lower"in prop_type or prop_type in ("lower_value","lower limit")):
-                if val is not None :
-                    _set_fallback ("lower_value",val )
-            if record .get ("standard_upper_value")is None and ("standard_upper"in prop_type or prop_type in ("standard upper","standard upper value")):
-                if val is not None :
-                    _set_fallback ("standard_upper_value",val )
-            if record .get ("standard_text_value")is None and "standard"in prop_type and ("text"in prop_type ):
-                if txt is not None :
-                    _set_fallback ("standard_text_value",txt )
-                elif val is not None :
-                    _set_fallback ("standard_text_value",val )
-        current_comment =record .get ("data_validity_comment")
-        if _is_empty (current_comment ):
-            data_validity_items :list [Mapping [str ,Any ]]=[prop for prop in items if ("data_validity"in str (prop .get ("type","")).lower ()or "validity"in str (prop .get ("type","")).lower ())and (prop .get ("text_value")is not None or prop .get ("value")is not None )]
-            if data_validity_items :
-                measured_items =[p for p in data_validity_items if _is_measured (p )]
-                if measured_items :
-                    prop =measured_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="measured",comment_value =comment_value )
-                else :
-                    prop =data_validity_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="first",comment_value =comment_value )
-            else :
-                log .debug ("data_validity_comment_fallback_no_items",activity_id =record .get ("activity_id"),activity_properties_count =len (items ),has_activity_properties =True )
-        else :
-            log .debug ("data_validity_comment_from_api",activity_id =record .get ("activity_id"),comment_value =current_comment )
-        normalized_properties =self ._normalize_activity_properties_items (property_items ,log )
-        if normalized_properties is not None :
-            validated_properties ,validation_stats =self ._validate_activity_properties_truv (normalized_properties ,log ,activity_id )
-            deduplicated_properties ,dedup_stats =self ._deduplicate_activity_properties (validated_properties ,log ,activity_id )
-            record ["activity_properties"]=deduplicated_properties
-            log .debug ("activity_properties_processed",activity_id =activity_id ,original_count =len (property_items ),normalized_count =len (normalized_properties ),validated_count =len (validated_properties ),deduplicated_count =len (deduplicated_properties ),invalid_count =validation_stats .get ("invalid_count",0 ),duplicates_removed =dedup_stats .get ("duplicates_removed",0 ))
-        else :
-            record ["activity_properties"]=properties
-            log .debug ("activity_properties_normalization_failed",activity_id =activity_id ,message ="activity_properties normalization failed, keeping original")
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
-    @staticmethod
-    def _extract_chembl_release (payload :Mapping [str ,Any ])->str |None :
-        for key in ("chembl_release","chembl_db_version","release","version"):
-            value =payload .get (key )
-            if isinstance (value ,str )and value .strip ():
-                return value
-            if value is not None :
-                return str (value )
-        return None
-    @staticmethod
-    def _extract_page_items (payload :Mapping [str ,Any ],items_keys :Sequence [str ]|None =None )->list [dict [str ,Any ]]:
-        preferred_keys :tuple [str ,...]=("activities",)
-        if items_keys is None :
-            combined_keys =preferred_keys +("data","items","results")
-        else :
-            combined_keys =tuple (dict .fromkeys ((*preferred_keys ,*items_keys )))
-        return ChemblPipelineBase ._extract_page_items (payload ,combined_keys )
-    @staticmethod
-    def _next_link (payload :Mapping [str ,Any ],base_url :str )->str |None :
-        page_meta :Any =payload .get ("page_meta")
-        if isinstance (page_meta ,Mapping ):
-            next_link_raw :Any =page_meta .get ("next")
-            next_link :str |None =cast (str |None ,next_link_raw )if next_link_raw is not None else None
-            if isinstance (next_link ,str )and next_link :
-                base_url_str =str (base_url )
-                base_path_parse_result =urlparse (base_url_str )
-                base_path_raw =base_path_parse_result .path
-                base_path_str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                base_path :str =base_path_str .rstrip ("/")
-                if next_link .startswith ("http://")or next_link .startswith ("https://"):
-                    parsed =urlparse (next_link )
-                    base_parsed =urlparse (base_url_str )
-                    parsed_path_raw =parsed .path
-                    base_path_raw =base_parsed .path
-                    path :str =parsed_path_raw .decode ("utf-8","ignore")if isinstance (parsed_path_raw ,(bytes ,bytearray ))else parsed_path_raw
-                    base_path_from_url :str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                    path_normalized :str =path .rstrip ("/")
-                    base_path_normalized :str =base_path_from_url .rstrip ("/")
-                    if base_path_normalized and path_normalized .startswith (base_path_normalized ):
-                        relative_path =path_normalized [len (base_path_normalized ):]
-                        if not relative_path :
-                            return None
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    elif "/api/data/"in path :
-                        parts =path .split ("/api/data/",1 )
-                        if len (parts )>1 :
-                            relative_path ="/"+parts [1 ]
-                        else :
-                            relative_path =path
-                    else :
-                        relative_path =path
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    if parsed .query :
-                        relative_path =f'{relative_path }?{parsed .query }'
-                    return relative_path
-                if base_path :
-                    normalized_base =base_path .lstrip ("/")
-                    stripped_link =next_link .lstrip ("/")
-                    if stripped_link .startswith (normalized_base +"/"):
-                        stripped_link =stripped_link [len (normalized_base ):]
-                    elif stripped_link ==normalized_base :
-                        stripped_link =""
-                    next_link =stripped_link
-                next_link =next_link .lstrip ("/")
-                if next_link :
-                    next_link =f'/{next_link }'
-                else :
-                    next_link ="/"
-                return next_link
-        return None
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Ensure canonical identifier columns are present before normalization."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_chembl_id"not in df .columns and "assay_id"in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "testitem_chembl_id"not in df .columns :
-            if "testitem_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["testitem_id"]
-                actions .append ("testitem_id->testitem_chembl_id")
-            elif "molecule_chembl_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["molecule_chembl_id"]
-                actions .append ("molecule_chembl_id->testitem_chembl_id")
-        if "molecule_chembl_id"not in df .columns and "testitem_chembl_id"in df .columns :
-            df ["molecule_chembl_id"]=df ["testitem_chembl_id"]
-            actions .append ("testitem_chembl_id->molecule_chembl_id")
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_required =[column for column in required_columns if column not in df .columns ]
-        if missing_required :
-            for column in missing_required :
-                df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            actions .append (f"created_missing:{",".join (missing_required )}")
-        alias_columns =[column for column in ("assay_id","testitem_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        boolean_columns =("potential_duplicate","curated","removed")
-        for column in boolean_columns :
-            specs [column ]={"dtype":"boolean","default":pd .NA }
-        return specs
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize ChEMBL and BAO identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"],pattern ="^CHEMBL\\d+$"),IdentifierRule (name ="bao",columns =["bao_endpoint","bao_format"],pattern ="^BAO_\\d{7}$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _finalize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align identifier columns after normalization and drop aliases."
-        df =df .copy ()
-        if {"molecule_chembl_id","testitem_chembl_id"}.issubset (df .columns ):
-            mismatch_mask =df ["molecule_chembl_id"].notna ()&df ["testitem_chembl_id"].notna ()&(df ["molecule_chembl_id"]!=df ["testitem_chembl_id"])
-            if mismatch_mask .any ():
-                mismatch_count =int (mismatch_mask .sum ())
-                samples_raw =df .loc [mismatch_mask ,["molecule_chembl_id","testitem_chembl_id"]].drop_duplicates ().head (5 ).to_dict ("records")
-                samples :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],samples_raw )
-                log .warning ("identifier_mismatch",count =mismatch_count ,samples =samples )
-                df .loc [mismatch_mask ,"testitem_chembl_id"]=df .loc [mismatch_mask ,"molecule_chembl_id"]
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_columns =[column for column in required_columns if column not in df .columns ]
-        if missing_columns :
-            for column in missing_columns :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            log .warning ("identifier_columns_missing",columns =missing_columns )
-        return df
-    def _finalize_output_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align final column order with schema and drop unexpected fields."
-        df =df .copy ()
-        expected =list (COLUMN_ORDER )
-        extras =[column for column in df .columns if column not in expected ]
-        if extras :
-            df =df .drop (columns =extras )
-            log .debug ("output_columns_dropped",columns =extras )
-        missing =[column for column in expected if column not in df .columns ]
-        if missing :
-            for column in missing :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([pd .NA ]*len (df ),dtype ="object")
-            log .warning ("output_columns_missing",columns =missing )
-        if not expected :
-            return df
-        return df [expected ]
-    def _filter_invalid_required_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Filter out rows with NULL values in required identifier fields.\n\n        Removes rows where any of the required fields (assay_chembl_id,\n        testitem_chembl_id, molecule_chembl_id) are NULL, as these cannot\n        pass schema validation.\n\n        Parameters\n        ----------\n        df:\n            DataFrame to filter.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            Filtered DataFrame with only rows having all required fields populated.\n        "
-        df =df .copy ()
-        if df .empty :
-            return df
-        required_fields =["assay_chembl_id","molecule_chembl_id"]
-        missing_fields =[field for field in required_fields if field not in df .columns ]
-        if missing_fields :
-            log .warning ("filter_skipped_missing_columns",missing_columns =missing_fields ,message ="Cannot filter: required columns are missing")
-            return df
-        valid_mask =df ["assay_chembl_id"].notna ()&df ["molecule_chembl_id"].notna ()
-        invalid_count =int ((~valid_mask ).sum ())
-        if invalid_count >0 :
-            invalid_rows =df [~valid_mask ]
-            sample_size =min (5 ,len (invalid_rows ))
-            sample_activity_ids =invalid_rows ["activity_id"].head (sample_size ).tolist ()if "activity_id"in invalid_rows .columns else []
-            log .warning ("filtered_invalid_rows",filtered_count =invalid_count ,remaining_count =int (valid_mask .sum ()),sample_activity_ids =sample_activity_ids ,message ="Rows with NULL in required identifier fields were filtered out")
-            df =df [valid_mask ].reset_index (drop =True )
-        return df
-    def _normalize_measurements (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize standard_value, standard_units, standard_relation, and standard_type."
-        df =df .copy ()
-        normalized_count =0
-        if "standard_value"in df .columns :
-            mask =df ["standard_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_value"]=numeric_series_std
-                negative_mask =mask &(df ["standard_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["standard_relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"standard_relation"]=series
-                invalid_mask =mask &~df ["standard_relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_type"in df .columns :
-            mask =df ["standard_type"].notna ()
-            if mask .any ():
-                df .loc [mask ,"standard_type"]=df .loc [mask ,"standard_type"].astype (str ).str .strip ()
-                standard_types_set :set [str ]=STANDARD_TYPES
-                invalid_mask =mask &~df ["standard_type"].isin (standard_types_set )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_type",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_type"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_units"in df .columns :
-            unit_mapping ={"nanomolar":"nM","nmol":"nM","nm":"nM","NM":"nM","\u00b5M":"\u03bcM","uM":"\u03bcM","UM":"\u03bcM","micromolar":"\u03bcM","microM":"\u03bcM","umol":"\u03bcM","millimolar":"mM","milliM":"mM","mmol":"mM","MM":"mM","percent":"%","pct":"%","ratios":"ratio"}
-            mask =df ["standard_units"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_units"].astype (str ).str .strip ()
-                for old_unit ,new_unit in unit_mapping .items ():
-                    series =series .str .replace (old_unit ,new_unit ,regex =False ,case =False )
-                df .loc [mask ,"standard_units"]=series
-                normalized_count +=int (mask .sum ())
-        if "relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"relation"]=series
-                invalid_mask =mask &~df ["relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_upper_value"in df .columns :
-            mask =df ["standard_upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_upper_value"]=numeric_series_std_upper
-                negative_mask =mask &(df ["standard_upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "upper_value"in df .columns :
-            mask =df ["upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"upper_value"]=numeric_series_upper
-                negative_mask =mask &(df ["upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "lower_value"in df .columns :
-            mask =df ["lower_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"lower_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_lower :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"lower_value"]=numeric_series_lower
-                negative_mask =mask &(df ["lower_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_lower_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"lower_value"]=None
-                normalized_count +=int (mask .sum ())
-        if normalized_count >0 :
-            log .debug ("measurements_normalized",normalized_count =normalized_count )
-        return df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize string fields: trim, empty string to null, title-case for organism."
-        working_df =df .copy ()
-        if "data_validity_description"in working_df .columns and "data_validity_comment"in working_df .columns :
-            invalid_mask =working_df ["data_validity_description"].notna ()&working_df ["data_validity_comment"].isna ()
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                log .warning ("invariant_data_validity_description_without_comment",count =invalid_count ,message ="data_validity_description is filled while data_validity_comment is NA")
-        rules :dict [str ,StringRule ]={"canonical_smiles":StringRule (),"bao_label":StringRule (max_length =128 ),"target_organism":StringRule (title_case =True ),"assay_organism":StringRule (title_case =True ),"data_validity_comment":StringRule (),"data_validity_description":StringRule (),"activity_comment":StringRule (),"standard_text_value":StringRule (),"text_value":StringRule (),"type":StringRule (),"units":StringRule (),"assay_type":StringRule (),"assay_description":StringRule (),"molecule_pref_name":StringRule (),"target_pref_name":StringRule (),"uo_units":StringRule (),"qudt_units":StringRule ()}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Serialize nested structures (ligand_efficiency, activity_properties) to JSON strings."
-        df =df .copy ()
-        nested_fields =["ligand_efficiency","activity_properties"]
-        for field in nested_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                serialized :list [Any ]=[]
-                for idx ,value in df .loc [mask ,field ].items ():
-                    if field =="activity_properties":
-                        serialized_value =self ._serialize_activity_properties (value ,log )
-                        serialized .append (serialized_value )
-                        continue
-                    if isinstance (value ,(Mapping ,list )):
-                        try :
-                            serialized .append (json .dumps (value ,ensure_ascii =False ,sort_keys =True ))
-                        except (TypeError ,ValueError )as exc :
-                            log .warning ("nested_serialization_failed",field =field ,index =idx ,error =str (exc ))
-                            serialized .append (None )
-                    elif isinstance (value ,str ):
-                        try :
-                            json .loads (value )
-                            serialized .append (value )
-                        except (TypeError ,ValueError ):
-                            serialized .append (None )
-                    else :
-                        serialized .append (None )
-                df .loc [mask ,field ]=pd .Series (serialized ,dtype ="object",index =df .loc [mask ,field ].index )
-        if "standard_value"in df .columns and "ligand_efficiency"in df .columns :
-            mask =df ["standard_value"].notna ()&df ["ligand_efficiency"].isna ()
-            if mask .any ():
-                log .warning ("ligand_efficiency_missing_with_standard_value",count =int (mask .sum ()),message ="ligand_efficiency is empty while standard_value exists")
-        return df
-    def _serialize_activity_properties (self ,value :Any ,log :BoundLogger |None =None )->str |None :
-        "Return normalized JSON for activity_properties or None if not serializable."
-        normalized_items =self ._normalize_activity_properties_items (value ,log )
-        if normalized_items is None :
-            return None
-        try :
-            return json .dumps (normalized_items ,ensure_ascii =False ,sort_keys =True )
-        except (TypeError ,ValueError )as exc :
-            if log is not None :
-                log .warning ("activity_properties_serialization_failed",error =str (exc ))
-            return None
-    def _normalize_activity_properties_items (self ,value :Any ,log :BoundLogger |None =None )->list [dict [str ,Any ]]|None :
-        "Coerce activity_properties payloads into a list of constrained dictionaries."
-        if value is None :
-            return None
-        raw_value =value
-        if isinstance (value ,str ):
-            stripped =value .strip ()
-            if not stripped :
-                return []
-            try :
-                parsed =json .loads (stripped )
-            except (TypeError ,ValueError ):
-                fallback_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                fallback_base ["text_value"]=stripped
-                return [fallback_base ]
-            else :
-                value =parsed
-        if isinstance (value ,Mapping ):
-            items :list [Any ]=[value ]
-        elif isinstance (value ,Sequence )and (not isinstance (value ,(str ,bytes ))):
-            items =list (value )
-        else :
-            if log is not None :
-                log .warning ("activity_properties_unhandled_type",value_type =type (raw_value ).__name__ )
-            return None
-        normalized :list [dict [str ,Any ]]=[]
-        for item in items :
-            if item is None :
-                continue
-            if isinstance (item ,Mapping ):
-                item_mapping =cast (Mapping [str ,Any ],item )
-                normalized_item :dict [str ,Any |None ]={key :item_mapping .get (key )for key in ACTIVITY_PROPERTY_KEYS }
-                result_flag_value =normalized_item .get ("result_flag")
-                if isinstance (result_flag_value ,int )and result_flag_value in (0 ,1 ):
-                    normalized_item ["result_flag"]=bool (result_flag_value )
-                normalized .append (normalized_item )
-            elif isinstance (item ,str ):
-                str_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                str_base ["text_value"]=item
-                normalized .append (str_base )
-            elif log is not None :
-                log .warning ("activity_properties_item_unhandled",item_type =type (item ).__name__ )
-        return normalized
-    def _validate_activity_properties_truv (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0434\u043b\u044f activity_properties.\n\n        \u0412\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u0435\u0442 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n        - value IS NOT NULL \u21d2 text_value IS NULL (\u0438 \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442)\n        - relation IN ('=', '<', '\u2264', '>', '\u2265', '~') OR NULL\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        "
-        validated :list [dict [str ,Any ]]=[]
-        invalid_count =0
-        invalid_items :list [dict [str ,Any ]]=[]
-        for prop in properties :
-            is_valid =True
-            validation_errors :list [str ]=[]
-            value =prop .get ("value")
-            text_value =prop .get ("text_value")
-            relation =prop .get ("relation")
-            if value is not None and text_value is not None :
-                is_valid =False
-                validation_errors .append ("both value and text_value are not None")
-            elif value is None and text_value is None :
-                pass
-            if relation is not None :
-                if not isinstance (relation ,str ):
-                    is_valid =False
-                    validation_errors .append (f'relation is not a string: {type (relation ).__name__ }')
-                elif relation not in RELATIONS :
-                    is_valid =False
-                    validation_errors .append (f"relation '{relation }' not in allowed values: {RELATIONS }")
-            validated .append (prop )
-            if not is_valid :
-                invalid_count +=1
-                invalid_items .append (prop )
-                log .warning ("activity_property_truv_validation_failed",activity_id =activity_id ,property =prop ,errors =validation_errors ,message ="TRUV validation failed, but property is kept")
-        stats ={"invalid_count":invalid_count ,"valid_count":len (validated )}
-        return (validated ,stats )
-    def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-        seen :set [tuple [Any ,...]]=set ()
-        deduplicated :list [dict [str ,Any ]]=[]
-        duplicates_removed =0
-        for prop in properties :
-            dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-            if dedup_key not in seen :
-                seen .add (dedup_key )
-                deduplicated .append (prop )
-            else :
-                duplicates_removed +=1
-                log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-        stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-        return (deduplicated ,stats )
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_added",value ="activity")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_filled",value ="activity")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :BoundLogger )->pd .DataFrame :
-        "Convert data types according to the Pandera schema."
-        df =df .copy ()
-        non_nullable_int_fields ={"activity_id":"int64"}
-        nullable_int_fields ={"row_index":"Int64","target_tax_id":"int64","assay_tax_id":"int64","record_id":"int64","src_id":"int64"}
-        float_fields ={"standard_value":"float64","standard_upper_value":"float64","pchembl_value":"float64","upper_value":"float64","lower_value":"float64"}
-        bool_fields =["potential_duplicate","curated","removed"]
-        binary_flag_fields =["standard_flag"]
-        for field in non_nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_int :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_int .astype ("Int64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field =="row_index":
-                    numeric_series_row :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=numeric_series_row .astype ("Int64")
-                    if df [field ].isna ().any ():
-                        df [field ]=range (len (df ))
-                else :
-                    nullable_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=nullable_numeric_series .astype ("Int64")
-                    mask_valid =df [field ].notna ()
-                    if mask_valid .any ():
-                        invalid_mask =mask_valid &(df [field ]<1 )
-                        if invalid_mask .any ():
-                            log .warning ("invalid_positive_integer",field =field ,count =int (invalid_mask .sum ()))
-                            df .loc [invalid_mask ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in float_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_float :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_float .astype ("float64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in bool_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field in ("curated","removed")and df [field ].dtype =="boolean":
-                    continue
-                if field in ("curated","removed"):
-                    df [field ]=df [field ].astype ("boolean")
-                else :
-                    bool_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=(bool_numeric_series !=0 ).astype ("boolean")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("bool_conversion_failed",field =field ,error =str (exc ))
-        for field in binary_flag_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_flag :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_flag .astype ("Int64")
-                mask_valid =df [field ].notna ()
-                if mask_valid .any ():
-                    valid_values =df .loc [mask_valid ,field ]
-                    invalid_valid_mask =~valid_values .isin ([0 ,1 ])
-                    if invalid_valid_mask .any ():
-                        invalid_index =valid_values .index [invalid_valid_mask ]
-                        log .warning ("invalid_standard_flag",field =field ,count =int (invalid_valid_mask .sum ()))
-                        df .loc [invalid_index ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        object_fields =["value","activity_properties"]
-        for field in object_fields :
-            if field in df .columns :
-                if df [field ].dtype !="object":
-                    df [field ]=df [field ].astype ("object")
-        return df
-    def _validate_foreign_keys (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Validate foreign key integrity and format of ChEMBL IDs."
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        chembl_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        warnings :list [str ]=[]
-        for field in chembl_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-                if invalid_mask .any ():
-                    warning_msg :str =f'{field }: {int (invalid_mask .sum ())} invalid format(s)'
-                    warnings .append (warning_msg )
-        if warnings :
-            log .warning ("foreign_key_validation",warnings =warnings )
-        return df
-    def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check uniqueness of activity_id before validation."
-        if "activity_id"not in df .columns :
-            log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-            return
-        duplicates =df [df ["activity_id"].duplicated (keep =False )]
-        if not duplicates .empty :
-            duplicate_count =len (duplicates )
-            duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-            log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-            msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-            raise ValueError (msg )
-        log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
-    def _validate_data_validity_comment_soft_enum (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Soft enum \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0434\u043b\u044f data_validity_comment.\n\n        \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0442\u0438\u0432 whitelist \u0438\u0437 \u043a\u043e\u043d\u0444\u0438\u0433\u0430. \u041d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n        \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a warning, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e (soft enum).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        "
-        if df .empty or "data_validity_comment"not in df .columns :
-            return
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if not whitelist :
-            return
-        series_candidate =df ["data_validity_comment"].dropna ()
-        if len (series_candidate )==0 :
-            return
-        non_null_comments_series =series_candidate .astype ("string")
-        whitelist_set :set [str ]=set (whitelist )
-        def _is_unknown (value :str )->bool :
-            return value not in whitelist_set
-        unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-        unknown_count =int (unknown_mask .sum ())
-        if unknown_count >0 :
-            unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-            log .warning ("soft_enum_unknown_data_validity_comment",unknown_count =unknown_count ,total_count =len (non_null_comments_series ),samples =unknown_values ,whitelist =whitelist ,message ="Unknown data_validity_comment values detected (soft enum: not blocking)")
-    def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-        reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        errors :list [str ]=[]
-        for field in reference_fields :
-            if field not in df .columns :
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-                continue
-            mask =df [field ].notna ()
-            if not mask .any ():
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-                continue
-            invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-                errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-                log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-        if errors :
-            log .error ("foreign_key_integrity_check_failed",errors =errors )
-            msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-            raise ValueError (msg )
-        log .debug ("foreign_key_integrity_verified")
-    def _log_detailed_validation_errors (self ,failure_cases :pd .DataFrame ,payload :pd .DataFrame ,log :BoundLogger )->None :
-        "Log individual validation errors with row index and activity_id."
-        if failure_cases .empty or payload .empty :
-            return
-        activity_id_col ="activity_id"if "activity_id"in payload .columns else None
-        index_col ="index"if "index"in failure_cases .columns else None
-        if index_col is None :
-            return
-        max_errors =20
-        errors_to_log =failure_cases .head (max_errors )
-        for _ ,error_row in errors_to_log .iterrows ():
-            row_index =error_row .get (index_col )
-            if row_index is None :
-                continue
-            error_details :dict [str ,Any ]={"row_index":int (row_index )if isinstance (row_index ,(int ,float ))else str (row_index )}
-            if activity_id_col :
-                try :
-                    idx =int (row_index )if isinstance (row_index ,(int ,float ))else row_index
-                    activity_id_value :Any =payload .at [cast (int ,idx ),activity_id_col ]
-                    activity_id =activity_id_value
-                except (KeyError ,IndexError ):
-                    activity_id =None
-                if activity_id is not None and pd .notna (activity_id ):
-                    error_details ["activity_id"]=int (activity_id )if isinstance (activity_id ,(int ,float ))else str (activity_id )
-            if "column"in error_row and pd .notna (error_row ["column"]):
-                error_details ["column"]=str (error_row ["column"])
-            if "schema_context"in error_row and pd .notna (error_row ["schema_context"]):
-                error_details ["schema_context"]=str (error_row ["schema_context"])
-            if "failure_case"in error_row and pd .notna (error_row ["failure_case"]):
-                error_details ["failure_case"]=str (error_row ["failure_case"])
-            log .error ("validation_error_detail",**error_details )
-        if len (failure_cases )>max_errors :
-            log .warning ("validation_errors_truncated",total_errors =len (failure_cases ),logged_errors =max_errors )
-    def build_quality_report (self ,df :pd .DataFrame )->pd .DataFrame |dict [str ,object ]|None :
-        "Return QC report with activity-specific metrics including distributions."
-        business_key =["activity_id"]if "activity_id"in df .columns else None
-        base_report =build_default_quality_report (df ,business_key_fields =business_key )
-        rows :list [dict [str ,Any ]]=[]
-        if not base_report .empty :
-            records_raw =base_report .to_dict ("records")
-            records :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],records_raw )
-            for record in records :
-                rows .append ({str (k ):v for k ,v in record .items ()})
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        foreign_key_fields =["assay_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"]
-        for field in foreign_key_fields :
-            if field in df .columns :
-                mask =df [field ].notna ()
-                if mask .any ():
-                    string_series =df [field ].astype (str )
-                    valid_mask =mask &string_series .str .match (chembl_id_pattern .pattern ,na =False )
-                    invalid_count =int ((mask &~valid_mask ).astype (int ).sum ())
-                    valid_count =int (valid_mask .astype (int ).sum ())
-                    total_count =int (mask .astype (int ).sum ())
-                    integrity_ratio =float (valid_count /total_count )if total_count >0 else 0.0
-                    rows .append ({"section":"foreign_key","metric":"integrity_ratio","column":field ,"value":float (integrity_ratio ),"valid_count":int (valid_count ),"invalid_count":int (invalid_count ),"total_count":int (total_count )})
-        if "standard_type"in df .columns :
-            type_counts_series :pd .Series [Any ]=df ["standard_type"].value_counts ()
-            type_dist_raw =type_counts_series .to_dict ()
-            type_dist :dict [Any ,int ]=cast (dict [Any ,int ],type_dist_raw )
-            for type_value ,count in type_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_type_count","column":"standard_type","value":str (type_value )if type_value is not None else "null","count":int (count )})
-        if "standard_units"in df .columns :
-            unit_counts_series :pd .Series [Any ]=df ["standard_units"].value_counts ()
-            unit_dist_raw =unit_counts_series .to_dict ()
-            unit_dist :dict [Any ,int ]=cast (dict [Any ,int ],unit_dist_raw )
-            for unit_value ,count in unit_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_units_count","column":"standard_units","value":str (unit_value )if unit_value is not None else "null","count":int (count )})
-        return pd .DataFrame (rows )
-    def write (self ,df :pd .DataFrame ,output_path :Path ,*,extended :bool =False ,include_correlation :bool |None =None ,include_qc_metrics :bool |None =None )->RunResult :
-        "Override write() to bind actor and ensure deterministic sorting.\n\n        Parameters\n        ----------\n        df:\n            The DataFrame to write.\n        output_path:\n            The base output path for all artifacts.\n        extended:\n            Whether to include extended QC artifacts.\n\n        Returns\n        -------\n        RunResult:\n            All artifacts generated by the write operation.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.write')
-        UnifiedLogger .bind (actor =self .actor )
-        sort_keys =["assay_chembl_id","testitem_chembl_id","activity_id"]
-        if df .empty or not all ((key in df .columns for key in sort_keys )):
-            return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-        original_sort_by =self .config .determinism .sort .by
-        if not original_sort_by or original_sort_by !=sort_keys :
-            from copy import deepcopy
-            from bioetl .config .models .determinism import DeterminismSortingConfig
-            modified_config =deepcopy (self .config )
-            modified_config .determinism .sort =DeterminismSortingConfig (by =sort_keys ,ascending =[True ,True ,True ],na_position ="last")
-            log .debug ("write_sort_config_set",sort_keys =sort_keys ,original_sort_keys =list (original_sort_by )if original_sort_by else [])
-            original_config =self .config
-            self .config =modified_config
-            try :
-                result =super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-            finally :
-                self .config =original_config
-            return result
-        return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
```

#### Hotspot 2

- Definition: ChemblActivityPipeline.__init__#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:111-115
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,5 +0,0 @@
-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
```

#### Hotspot 3

- Definition: ChemblActivityPipeline._add_row_metadata#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2901-2924
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,18 +0,0 @@
-def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_added",value ="activity")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_filled",value ="activity")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Hotspot 4

- Definition: ChemblActivityPipeline._build_activity_descriptor#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:343-415
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,22 +0,0 @@
-def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-    "Construct the descriptor driving the shared extraction template."
-    def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-        http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-        chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-        typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =source_config .parameters .select_fields
-        return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-    def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-    def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        df =pipeline ._ensure_comment_fields (df ,log )
-        chembl_client =cast (ChemblClient ,context .chembl_client )
-        if chembl_client is not None :
-            df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-        pipeline ._log_validity_comments_metrics (df ,log )
-        return df
-    def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        return pipeline ._materialize_activity_record (payload )
-    return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
```

#### Hotspot 5

- Definition: ChemblActivityPipeline._cache_directory#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1257-1267
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,7 +0,0 @@
-def _cache_directory (self ,release :str |None )->Path :
-    cache_root =Path (self .config .paths .cache_root )
-    directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-    release_component =self ._sanitize_cache_component (release or "unknown")
-    pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-    version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-    return cache_root /directory_name /pipeline_component /release_component /version_component
```

#### Hotspot 6

- Definition: ChemblActivityPipeline._cache_file_path#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1252-1255
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,4 +0,0 @@
-def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-    directory =self ._cache_directory (release )
-    cache_key =self ._cache_key (batch_ids ,release )
-    return directory /f'{cache_key }.json'
```

#### Hotspot 7

- Definition: ChemblActivityPipeline._cache_key#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1269-1277
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,4 +0,0 @@
-def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-    payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-    raw =json .dumps (payload ,sort_keys =True )
-    return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
```

#### Hotspot 8

- Definition: ChemblActivityPipeline._check_activity_id_uniqueness#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:3108-3131
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,13 +0,0 @@
-def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check uniqueness of activity_id before validation."
-    if "activity_id"not in df .columns :
-        log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-        return
-    duplicates =df [df ["activity_id"].duplicated (keep =False )]
-    if not duplicates .empty :
-        duplicate_count =len (duplicates )
-        duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-        log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-        msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-        raise ValueError (msg )
-    log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
```

#### Hotspot 9

- Definition: ChemblActivityPipeline._check_cache#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1176-1221
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,33 +0,0 @@
-def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-    cache_config =self .config .cache
-    if not cache_config .enabled :
-        return None
-    normalized_ids =[str (identifier )for identifier in batch_ids ]
-    cache_file =self ._cache_file_path (normalized_ids ,release )
-    if not cache_file .exists ():
-        return None
-    try :
-        stat =cache_file .stat ()
-    except OSError :
-        return None
-    ttl_seconds =int (cache_config .ttl )
-    if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    try :
-        payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-    except (OSError ,json .JSONDecodeError ):
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    if not isinstance (payload ,dict ):
-        return None
-    missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-    if missing :
-        return None
-    return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
```

#### Hotspot 10

- Definition: ChemblActivityPipeline._check_foreign_key_integrity#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:3185-3232
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,24 +0,0 @@
-def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-    reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-    chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-    errors :list [str ]=[]
-    for field in reference_fields :
-        if field not in df .columns :
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-            continue
-        mask =df [field ].notna ()
-        if not mask .any ():
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-            continue
-        invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-        if invalid_mask .any ():
-            invalid_count =int (invalid_mask .sum ())
-            invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-            errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-            log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-    if errors :
-        log .error ("foreign_key_integrity_check_failed",errors =errors )
-        msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-        raise ValueError (msg )
-    log .debug ("foreign_key_integrity_verified")
```

#### Hotspot 11

- Definition: ChemblActivityPipeline._coerce_activity_dataset#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:432-450
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,14 +0,0 @@
-def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-    "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-    if isinstance (dataset ,pd .Series ):
-        return dataset .to_frame (name ="activity_id")
-    if isinstance (dataset ,pd .DataFrame ):
-        return dataset
-    if isinstance (dataset ,Mapping ):
-        mapping =cast (Mapping [str ,Any ],dataset )
-        return pd .DataFrame ([dict (mapping )])
-    if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-        dataset_list :list [Any ]=list (dataset )
-        return pd .DataFrame ({"activity_id":dataset_list })
-    msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-    raise TypeError (msg )
```

#### Hotspot 12

- Definition: ChemblActivityPipeline._coerce_mapping#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2065-2068
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,5 +0,0 @@
-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Hotspot 13

- Definition: ChemblActivityPipeline._collect_records_by_ids#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:498-637
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,83 +0,0 @@
-def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-    "Iterate over IDs using the shared iterator while preserving cache semantics."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    records :list [dict [str ,Any ]]=[]
-    success_count =0
-    fallback_count =0
-    error_count =0
-    cache_hits =0
-    api_calls =0
-    total_batches =0
-    key_order =[key for _ ,key in normalized_ids ]
-    key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-    for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-        total_batches +=1
-        batch_start =time .perf_counter ()
-        from_cache =False
-        chunk_records :dict [str ,dict [str ,Any ]]={}
-        try :
-            cached_records =self ._check_cache (chunk ,self ._chembl_release )
-            if cached_records is not None :
-                from_cache =True
-                cache_hits +=len (chunk )
-                chunk_records =cached_records
-            else :
-                api_calls +=1
-                fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                for item in fetched_items :
-                    if not isinstance (item ,Mapping ):
-                        continue
-                    activity_value =item .get ("activity_id")
-                    if activity_value is None :
-                        continue
-                    chunk_records [str (activity_value )]=dict (item )
-                self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-            success_in_batch =0
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                record =chunk_records .get (key )
-                if record and (not record .get ("error")):
-                    materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                    records .append (materialized )
-                    success_count +=1
-                    success_in_batch +=1
-                else :
-                    fallback_record =self ._create_fallback_record (numeric_id )
-                    records .append (fallback_record )
-                    fallback_count +=1
-                    error_count +=1
-            batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-            log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-        except CircuitBreakerOpenError as exc :
-            log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except RequestException as exc :
-            log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except Exception as exc :
-            log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-    total_records =len (normalized_ids )
-    success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-    summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-    return (records ,summary )
```

#### Hotspot 14

- Definition: ChemblActivityPipeline._create_fallback_record#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1284-1323
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,16 +0,0 @@
-def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-    "Create fallback record enriched with error metadata."
-    base_message ="Fallback: ChEMBL activity unavailable"
-    message =f'{base_message } ({error })'if error else base_message
-    timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-    metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-    if isinstance (error ,RequestException ):
-        response =getattr (error ,"response",None )
-        status_code =getattr (response ,"status_code",None )
-        if status_code is not None :
-            metadata ["http_status"]=status_code
-        metadata ["error_message"]=str (error )
-    elif error is not None :
-        metadata ["error_message"]=str (error )
-    fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-    return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
```

#### Hotspot 15

- Definition: ChemblActivityPipeline._deduplicate_activity_properties#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2843-2899
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,15 +0,0 @@
-def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-    "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-    seen :set [tuple [Any ,...]]=set ()
-    deduplicated :list [dict [str ,Any ]]=[]
-    duplicates_removed =0
-    for prop in properties :
-        dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-        if dedup_key not in seen :
-            seen .add (dedup_key )
-            deduplicated .append (prop )
-        else :
-            duplicates_removed +=1
-            log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-    stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-    return (deduplicated ,stats )
```

#### Hotspot 16

- Definition: ChemblActivityPipeline._enrich_assay#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:892-935
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,28 +0,0 @@
-def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    assay_section :Any =enrich_section .get ("assay")
-                    if isinstance (assay_section ,Mapping ):
-                        assay_section =cast (Mapping [str ,Any ],assay_section )
-                        enrich_cfg =dict (assay_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_assay (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 17

- Definition: ChemblActivityPipeline._enrich_compound_record#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:822-867
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,28 +0,0 @@
-def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    compound_record_section :Any =enrich_section .get ("compound_record")
-                    if isinstance (compound_record_section ,Mapping ):
-                        compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                        enrich_cfg =dict (compound_record_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 18

- Definition: ChemblActivityPipeline._enrich_data_validity#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:960-1014
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,32 +0,0 @@
-def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    data_validity_section :Any =enrich_section .get ("data_validity")
-                    if isinstance (data_validity_section ,Mapping ):
-                        data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                        enrich_cfg =dict (data_validity_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    if "data_validity_description"in df .columns :
-        non_na_count =int (df ["data_validity_description"].notna ().sum ())
-        if non_na_count >0 :
-            log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 19

- Definition: ChemblActivityPipeline._enrich_molecule#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1039-1108
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,37 +0,0 @@
-def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    molecule_section :Any =enrich_section .get ("molecule")
-                    if isinstance (molecule_section ,Mapping ):
-                        molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                        enrich_cfg =dict (molecule_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-    if "molecule_name"in df_join .columns :
-        if "molecule_pref_name"not in df .columns :
-            df ["molecule_pref_name"]=pd .NA
-        mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-        if mask .any ():
-            df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-            df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-            log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-    return df
```

#### Hotspot 20

- Definition: ChemblActivityPipeline._ensure_comment_fields#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1325-1358
- document: absent

```diff
--- activity:run.py
+++ document:run.py
@@ -1,11 +0,0 @@
-def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-    if df .empty :
-        return df
-    required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-    missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-    if missing_fields :
-        for field in missing_fields :
-            df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-        log .debug ("comment_fields_ensured",fields =missing_fields )
-    return df
```

_Only the first 20 hotspots of 113 are shown for module run.py._

_First 20 hotspots of 135 are shown for pair activity ↔ document._

---

## Pair: activity ↔ target

- AST hash: 978d4152ad77000361c21b7f2051d3d1 ↔ 107171553e4f1c509bba122a3d0ccb96

- Jaccard over tokens: 0.176

### Module run.py

- File status: activity — present, target — present
- AST hash: 13d39ff2ae0173b684048e972fd58d8d ↔ 8689c415f6f3fc16a44fa879dc47ed50
- Jaccard over tokens: 0.183

Definition                                                       | activity signature                                                                                                                                                         | target signature                                     | Side effects                                                                                                                                                                                                                                                                                                                                                        | Exceptions                                             | Status          
-----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------|-----------------
ChemblActivityPipeline                                           | —                                                                                                                                                                          | —                                                    | activity: io=cache_file.read_text, json.dumps, json.loads, payload.get, tmp_path.write_text; logging=UnifiedLogger.bind, UnifiedLogger.get, bound_log.warning, errors_to_log.iterrows, log.debug, log.error, log.info, log.warning, pipeline._log_validity_comments_metrics, self._log_detailed_validation_errors, self._log_validity_comments_metrics<br>target: ∅ | activity: TypeError(msg), ValueError(msg)<br>target: ∅ | only in activity
ChemblActivityPipeline.__init__                                  | self, config: PipelineConfig, run_id: str                                                                                                                                  | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._add_row_metadata                         | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.debug<br>target: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._build_activity_descriptor                | self                                                                                                                                                                       | —                                                    | activity: io=∅; logging=pipeline._log_validity_comments_metrics<br>target: ∅                                                                                                                                                                                                                                                                                        | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._cache_directory                          | self, release: str | None                                                                                                                                                  | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._cache_file_path                          | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._cache_key                                | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                    | activity: io=json.dumps; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                     | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._check_activity_id_uniqueness             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.debug, log.error, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                              | activity: ValueError(msg)<br>target: ∅                 | only in activity
ChemblActivityPipeline._check_cache                              | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                    | activity: io=cache_file.read_text, json.loads; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._check_foreign_key_integrity              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.debug, log.error, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                              | activity: ValueError(msg)<br>target: ∅                 | only in activity
ChemblActivityPipeline._coerce_activity_dataset                  | self, dataset: object                                                                                                                                                      | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: TypeError(msg)<br>target: ∅                  | only in activity
ChemblActivityPipeline._coerce_mapping                           | payload: Any                                                                                                                                                               | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._collect_records_by_ids                   | self, normalized_ids: Sequence[tuple[int, str]], activity_iterator: ChemblActivityClient, *, select_fields: Sequence[str] | None                                           | —                                                    | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.error, log.warning<br>target: ∅                                                                                                                                                                                                                                                                           | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._create_fallback_record                   | self, activity_id: int, error: Exception | None                                                                                                                            | —                                                    | activity: io=json.dumps; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                     | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._deduplicate_activity_properties          | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                    | activity: io=∅; logging=log.debug<br>target: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._enrich_assay                             | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: io=∅; logging=UnifiedLogger.get, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                 | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._enrich_compound_record                   | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: io=∅; logging=UnifiedLogger.get, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                 | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._enrich_data_validity                     | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                       | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._enrich_molecule                          | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                       | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._ensure_comment_fields                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.debug<br>target: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._extract_activity_properties_fields       | self, record: dict[str, Any]                                                                                                                                               | —                                                    | activity: io=json.loads; logging=UnifiedLogger.get, log.debug, log.warning<br>target: ∅                                                                                                                                                                                                                                                                             | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._extract_assay_fields                     | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                    | activity: io=∅; logging=log.debug, log.info, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._extract_chembl_release                   | payload: Mapping[str, Any]                                                                                                                                                 | —                                                    | activity: io=payload.get; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._extract_data_validity_descriptions       | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                    | activity: io=∅; logging=log.debug, log.info, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._extract_from_chembl                      | self, dataset: object, chembl_client: ChemblClient | Any, activity_iterator: ChemblActivityClient, *, limit: int | None = None, select_fields: Sequence[str] | None = None | —                                                    | activity: io=∅; logging=UnifiedLogger.get, log.info, self._log_validity_comments_metrics<br>target: ∅                                                                                                                                                                                                                                                               | activity: ValueError(msg)<br>target: ∅                 | only in activity
ChemblActivityPipeline._extract_nested_fields                    | self, record: dict[str, Any]                                                                                                                                               | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._extract_page_items                       | payload: Mapping[str, Any], items_keys: Sequence[str] | None                                                                                                               | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._filter_invalid_required_fields           | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._finalize_identifier_columns              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._finalize_output_columns                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.debug, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._get_data_validity_comment_whitelist      | self                                                                                                                                                                       | —                                                    | activity: io=∅; logging=UnifiedLogger.get<br>target: ∅                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._harmonize_identifier_columns             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.debug<br>target: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._log_detailed_validation_errors           | self, failure_cases: pd.DataFrame, payload: pd.DataFrame, log: BoundLogger                                                                                                 | —                                                    | activity: io=∅; logging=errors_to_log.iterrows, log.error, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                 | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._log_validity_comments_metrics            | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.info, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                          | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._materialize_activity_record              | self, payload: Mapping[str, Any], *, activity_id: int | None = None                                                                                                        | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._next_link                                | payload: Mapping[str, Any], base_url: str                                                                                                                                  | —                                                    | activity: io=payload.get; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._normalize_activity_ids                   | self, input_frame: pd.DataFrame, *, limit: int | None, log: BoundLogger                                                                                                    | —                                                    | activity: io=∅; logging=log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._normalize_activity_properties_items      | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                    | activity: io=json.loads; logging=log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                           | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._normalize_data_types                     | self, df: pd.DataFrame, schema: Any, log: BoundLogger                                                                                                                      | —                                                    | activity: io=∅; logging=log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._normalize_identifiers                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.debug<br>target: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._normalize_measurements                   | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.debug, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._normalize_nested_structures              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=json.dumps, json.loads; logging=log.warning<br>target: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._normalize_string_fields                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.debug, log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._prepare_activity_iteration               | self, *, client_name: str = 'chembl_activity_client'                                                                                                                       | —                                                    | activity: io=∅; logging=UnifiedLogger.get<br>target: ∅                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._sanitize_cache_component                 | value: str                                                                                                                                                                 | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._schema_column_specs                      | self                                                                                                                                                                       | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._serialize_activity_properties            | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                    | activity: io=json.dumps; logging=log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                           | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_assay                      | self                                                                                                                                                                       | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_compound_record            | self                                                                                                                                                                       | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_data_validity              | self                                                                                                                                                                       | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_molecule                   | self                                                                                                                                                                       | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._store_cache                              | self, batch_ids: Sequence[str], batch_data: Mapping[str, Mapping[str, Any]], release: str | None                                                                           | —                                                    | activity: io=json.dumps, tmp_path.write_text; logging=UnifiedLogger.get, log.debug<br>target: ∅                                                                                                                                                                                                                                                                     | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._validate_activity_properties_truv        | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                    | activity: io=∅; logging=log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._validate_data_validity_comment_soft_enum | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline._validate_foreign_keys                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                    | activity: io=∅; logging=log.warning<br>target: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline.build_quality_report                      | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline.chembl_release                            | self                                                                                                                                                                       | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline.extract                                   | self, *args, **kwargs                                                                                                                                                      | —                                                    | activity: io=∅; logging=UnifiedLogger.get, bound_log.warning<br>target: ∅                                                                                                                                                                                                                                                                                           | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline.extract_all                               | self                                                                                                                                                                       | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline.extract_by_ids                            | self, ids: Sequence[str]                                                                                                                                                   | —                                                    | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning, self._log_validity_comments_metrics<br>target: ∅                                                                                                                                                                                                                                                  | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline.transform                                 | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.info<br>target: ∅                                                                                                                                                                                                                                                                                         | activity: ∅<br>target: ∅                               | only in activity
ChemblActivityPipeline.validate                                  | self, df: pd.DataFrame                                                                                                                                                     | —                                                    | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.error, log.info, self._log_detailed_validation_errors<br>target: ∅                                                                                                                                                                                                                                        | activity: ValueError(msg)<br>target: ∅                 | only in activity
ChemblActivityPipeline.write                                     | self, df: pd.DataFrame, output_path: Path, *, extended: bool = False, include_correlation: bool | None = None, include_qc_metrics: bool | None = None                      | —                                                    | activity: io=∅; logging=UnifiedLogger.bind, UnifiedLogger.get, log.debug<br>target: ∅                                                                                                                                                                                                                                                                               | activity: ∅<br>target: ∅                               | only in activity
ChemblTargetPipeline                                             | —                                                                                                                                                                          | —                                                    | activity: ∅<br>target: io=json.dumps; logging=UnifiedLogger.get, log.debug, log.info, log.warning                                                                                                                                                                                                                                                                   | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline.__init__                                    | —                                                                                                                                                                          | self, config: PipelineConfig, run_id: str            | activity: ∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline._build_target_descriptor                    | —                                                                                                                                                                          | self                                                 | activity: ∅<br>target: io=∅; logging=log.info                                                                                                                                                                                                                                                                                                                       | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline._enrich_protein_classifications             | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                     | activity: ∅<br>target: io=json.dumps; logging=log.debug, log.info, log.warning                                                                                                                                                                                                                                                                                      | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline._enrich_target_components                   | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                     | activity: ∅<br>target: io=json.dumps; logging=log.debug, log.info, log.warning                                                                                                                                                                                                                                                                                      | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline._harmonize_identifier_columns               | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                     | activity: ∅<br>target: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline._normalize_data_types                       | —                                                                                                                                                                          | self, df: pd.DataFrame, schema: Any | None, log: Any | activity: ∅<br>target: io=∅; logging=log.warning                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline._normalize_identifiers                      | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                     | activity: ∅<br>target: io=∅; logging=log.warning                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline._normalize_string_fields                    | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                     | activity: ∅<br>target: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline.extract                                     | —                                                                                                                                                                          | self, *args, **kwargs                                | activity: ∅<br>target: io=∅; logging=UnifiedLogger.get                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline.extract_all                                 | —                                                                                                                                                                          | self                                                 | activity: ∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline.extract_by_ids                              | —                                                                                                                                                                          | self, ids: Sequence[str]                             | activity: ∅<br>target: io=∅; logging=UnifiedLogger.get, log.info                                                                                                                                                                                                                                                                                                    | activity: ∅<br>target: ∅                               | only in target  
ChemblTargetPipeline.transform                                   | —                                                                                                                                                                          | self, df: pd.DataFrame                               | activity: ∅<br>target: io=∅; logging=UnifiedLogger.get, log.debug, log.info                                                                                                                                                                                                                                                                                         | activity: ∅<br>target: ∅                               | only in target  
__module_block_0                                                 | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_1                                                 | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_10                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_11                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_12                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_13                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_14                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_15                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_16                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_17                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_18                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_19                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_2                                                 | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_20                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
__module_block_21                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
__module_block_22                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
__module_block_23                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
__module_block_24                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
__module_block_25                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
__module_block_26                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
__module_block_27                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
__module_block_28                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
__module_block_29                                                | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>target: ∅                               | only in activity
__module_block_3                                                 | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_4                                                 | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_5                                                 | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_6                                                 | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_7                                                 | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_8                                                 | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         
__module_block_9                                                 | —                                                                                                                                                                          | —                                                    | activity: io=∅; logging=∅<br>target: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>target: ∅                               | differs         

#### Hotspot 1

- Definition: ChemblActivityPipeline#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:106-3476
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,1684 +0,0 @@
-class ChemblActivityPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting activity records from the ChEMBL API."
-    actor ="activity_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-        self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch activity payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        def legacy_activity_ids (bound_log :BoundLogger )->Sequence [str ]|None :
-            payload_activity_ids =kwargs .get ("activity_ids")
-            if payload_activity_ids is None :
-                return None
-            bound_log .warning ("chembl_activity.deprecated_kwargs",message ="Using activity_ids in kwargs is deprecated. Use --input-file instead.")
-            if isinstance (payload_activity_ids ,Sequence )and (not isinstance (payload_activity_ids ,(str ,bytes ))):
-                sequence_ids :Sequence [str |int ]=cast (Sequence [str |int ],payload_activity_ids )
-                return [str (id_val )for id_val in sequence_ids ]
-            return [str (payload_activity_ids )]
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_activity.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="activity_id",legacy_id_resolver =legacy_activity_ids ,legacy_source ="deprecated_kwargs")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all activity records from ChEMBL using the shared iterator."
-        return self .run_extract_all (self ._build_activity_descriptor ())
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract activity records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of activity_id values to extract (as strings or integers).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted activity records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_config ,chembl_client ,activity_iterator ,select_fields =self ._prepare_activity_iteration ()
-        limit =self .config .cli .limit
-        invalid_ids :list [Any ]=[]
-        def normalize_activity_id (raw :Any )->tuple [str |None ,Any ]:
-            if pd .isna (raw ):
-                return (None ,None )
-            try :
-                if isinstance (raw ,str ):
-                    candidate =raw .strip ()
-                    if not candidate :
-                        return (None ,None )
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw ,(int ,float )):
-                    numeric_id =int (raw )
-                else :
-                    numeric_id =int (raw )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw )
-                return (None ,None )
-            return (str (numeric_id ),int (numeric_id ))
-        def delegated_fetch (canonical_ids :Sequence [str ],context :BatchExtractionContext )->tuple [Sequence [Mapping [str ,Any ]],Mapping [str ,Any ]]:
-            numeric_map =context .metadata
-            normalized_ids :list [tuple [int ,str ]]=[]
-            for identifier in canonical_ids :
-                numeric_value =numeric_map .get (identifier )
-                if numeric_value is None :
-                    continue
-                normalized_ids .append ((int (numeric_value ),identifier ))
-            if not normalized_ids :
-                summary ={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-                context .extra ["delegated_summary"]=summary
-                return ([],summary )
-            records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =context .select_fields or None )
-            context .extra ["delegated_summary"]=summary
-            return (records ,summary )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            dataframe =self ._ensure_comment_fields (dataframe ,log )
-            dataframe =self ._extract_data_validity_descriptions (dataframe ,chembl_client ,log )
-            dataframe =self ._extract_assay_fields (dataframe ,chembl_client ,log )
-            self ._log_validity_comments_metrics (dataframe ,log )
-            return dataframe
-        def empty_activity_frame ()->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def finalize_context (context :BatchExtractionContext )->None :
-            summary =context .extra .get ("delegated_summary")
-            if isinstance (summary ,Mapping ):
-                summary_dict =dict (summary )
-                context .extra ["stats_attribute_override"]=summary_dict
-                self ._last_batch_extract_stats =summary_dict
-            else :
-                context .extra ["stats_attribute_override"]=context .stats .as_dict ()
-                self ._last_batch_extract_stats =context .stats .as_dict ()
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="activity_id",fetcher =delegated_fetch ,select_fields =select_fields ,batch_size =source_config .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release ,id_normalizer =normalize_activity_id ,sort_key =lambda pair :int (pair [0 ]),finalize =finalize_dataframe ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats",fetch_mode ="delegated",empty_frame_factory =empty_activity_frame )
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        batch_stats =self ._last_batch_extract_stats or {}
-        log .info ("chembl_activity.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =batch_stats .get ("batches"),api_calls =batch_stats .get ("api_calls"),cache_hits =batch_stats .get ("cache_hits"))
-        return dataframe
-    def _prepare_activity_iteration (self ,*,client_name :str ="chembl_activity_client")->tuple [ActivitySourceConfig ,ChemblClient ,ChemblActivityClient ,list [str ]]:
-        "Construct reusable ChEMBL clients and iterator for activity extraction."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name =client_name )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =self ._resolve_select_fields (source_raw ,default_fields =API_ACTIVITY_FIELDS )
-        return (source_config ,chembl_client ,activity_iterator ,select_fields )
-    def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-        "Construct the descriptor driving the shared extraction template."
-        def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-            http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-            chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-            typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-            activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-            select_fields =source_config .parameters .select_fields
-            return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-        def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            df =pipeline ._ensure_comment_fields (df ,log )
-            chembl_client =cast (ChemblClient ,context .chembl_client )
-            if chembl_client is not None :
-                df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-            pipeline ._log_validity_comments_metrics (df ,log )
-            return df
-        def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            return pipeline ._materialize_activity_record (payload )
-        return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
-    def _materialize_activity_record (self ,payload :Mapping [str ,Any ],*,activity_id :int |None =None )->dict [str ,Any ]:
-        "Normalize nested fields within an activity payload."
-        record =dict (payload )
-        record =self ._extract_nested_fields (record )
-        record =self ._extract_activity_properties_fields (record )
-        if activity_id is not None :
-            record .setdefault ("activity_id",activity_id )
-        return record
-    def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-        "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-        if isinstance (dataset ,pd .Series ):
-            return dataset .to_frame (name ="activity_id")
-        if isinstance (dataset ,pd .DataFrame ):
-            return dataset
-        if isinstance (dataset ,Mapping ):
-            mapping =cast (Mapping [str ,Any ],dataset )
-            return pd .DataFrame ([dict (mapping )])
-        if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-            dataset_list :list [Any ]=list (dataset )
-            return pd .DataFrame ({"activity_id":dataset_list })
-        msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-        raise TypeError (msg )
-    def _normalize_activity_ids (self ,input_frame :pd .DataFrame ,*,limit :int |None ,log :BoundLogger )->list [tuple [int ,str ]]:
-        "Normalize raw identifier values into deduplicated integer/string pairs."
-        normalized_ids :list [tuple [int ,str ]]=[]
-        invalid_ids :list [Any ]=[]
-        seen :set [str ]=set ()
-        for raw_id in input_frame ["activity_id"].tolist ():
-            if pd .isna (raw_id ):
-                continue
-            try :
-                if isinstance (raw_id ,str ):
-                    candidate =raw_id .strip ()
-                    if not candidate :
-                        continue
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw_id ,(int ,float )):
-                    numeric_id =int (raw_id )
-                else :
-                    numeric_id =int (raw_id )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw_id )
-                continue
-            key =str (numeric_id )
-            if key not in seen :
-                seen .add (key )
-                normalized_ids .append ((numeric_id ,key ))
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        if limit is not None :
-            normalized_ids =normalized_ids [:max (int (limit ),0 )]
-        return normalized_ids
-    def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-        "Iterate over IDs using the shared iterator while preserving cache semantics."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        records :list [dict [str ,Any ]]=[]
-        success_count =0
-        fallback_count =0
-        error_count =0
-        cache_hits =0
-        api_calls =0
-        total_batches =0
-        key_order =[key for _ ,key in normalized_ids ]
-        key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-        for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-            total_batches +=1
-            batch_start =time .perf_counter ()
-            from_cache =False
-            chunk_records :dict [str ,dict [str ,Any ]]={}
-            try :
-                cached_records =self ._check_cache (chunk ,self ._chembl_release )
-                if cached_records is not None :
-                    from_cache =True
-                    cache_hits +=len (chunk )
-                    chunk_records =cached_records
-                else :
-                    api_calls +=1
-                    fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                    for item in fetched_items :
-                        if not isinstance (item ,Mapping ):
-                            continue
-                        activity_value =item .get ("activity_id")
-                        if activity_value is None :
-                            continue
-                        chunk_records [str (activity_value )]=dict (item )
-                    self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-                success_in_batch =0
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    record =chunk_records .get (key )
-                    if record and (not record .get ("error")):
-                        materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                        records .append (materialized )
-                        success_count +=1
-                        success_in_batch +=1
-                    else :
-                        fallback_record =self ._create_fallback_record (numeric_id )
-                        records .append (fallback_record )
-                        fallback_count +=1
-                        error_count +=1
-                batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-                log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-            except CircuitBreakerOpenError as exc :
-                log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except RequestException as exc :
-                log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except Exception as exc :
-                log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-        total_records =len (normalized_ids )
-        success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-        summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-        return (records ,summary )
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw activity data by normalizing measurements, identifiers, and data types."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_measurements (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,ActivitySchema ,log )
-        if "curated"in df .columns or "curated_by"in df .columns :
-            if "curated"not in df .columns :
-                df ["curated"]=pd .NA
-            if "curated_by"in df .columns :
-                mask =df ["curated"].isna ()
-                df .loc [mask ,"curated"]=df .loc [mask ,"curated_by"].notna ()
-            df ["curated"]=df ["curated"].astype ("boolean")
-        df =self ._validate_foreign_keys (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if self ._should_enrich_compound_record ():
-            df =self ._enrich_compound_record (df )
-        if self ._should_enrich_assay ():
-            df =self ._enrich_assay (df )
-        if self ._should_enrich_molecule ():
-            df =self ._enrich_molecule (df )
-        if self ._should_enrich_data_validity ():
-            df =self ._enrich_data_validity (df )
-        df =self ._finalize_identifier_columns (df ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        df =self ._finalize_output_columns (df ,log )
-        df =self ._filter_invalid_required_fields (df ,log )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against ActivitySchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        if "target_tax_id"in df .columns :
-            dtype_name :str =str (df ["target_tax_id"].dtype .name )
-            if dtype_name !="Int64":
-                numeric_series :pd .Series [Any ]=pd .to_numeric (df ["target_tax_id"],errors ="coerce")
-                df ["target_tax_id"]=numeric_series .astype ("Int64")
-        self ._check_activity_id_uniqueness (df ,log )
-        self ._check_foreign_key_integrity (df ,log )
-        self ._validate_data_validity_comment_soft_enum (df ,log )
-        original_coerce =self .config .validation .coerce
-        try :
-            self .config .validation .coerce =False
-            validated =super ().validate (df )
-            log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce )
-            return validated
-        except pandera .errors .SchemaErrors as exc :
-            failure_cases_df :pd .DataFrame |None =None
-            if hasattr (exc ,"failure_cases"):
-                failure_cases_df =cast (pd .DataFrame ,exc .failure_cases )
-            error_count =len (failure_cases_df )if failure_cases_df is not None else 0
-            error_summary =summarize_schema_errors (exc )
-            log .error ("validation_failed",error_count =error_count ,schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce ,error_summary =error_summary ,exc_info =True )
-            if failure_cases_df is not None and (not failure_cases_df .empty ):
-                failure_cases_summary =format_failure_cases (failure_cases_df )
-                log .error ("validation_failure_cases",failure_cases =failure_cases_summary )
-                self ._log_detailed_validation_errors (failure_cases_df ,df ,log )
-            msg =f'Validation failed with {error_count } error(s) against schema {self .config .validation .schema_out }: {error_summary }'
-            raise ValueError (msg )from exc
-        except Exception as exc :
-            log .error ("validation_error",error =str (exc ),schema =self .config .validation .schema_out ,exc_info =True )
-            raise
-        finally :
-            self .config .validation .coerce =original_coerce
-    def _should_enrich_compound_record (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 compound_record \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            compound_record_section :Any =enrich_section .get ("compound_record")
-            if not isinstance (compound_record_section ,Mapping ):
-                return False
-            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-            enabled :Any =compound_record_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        compound_record_section :Any =enrich_section .get ("compound_record")
-                        if isinstance (compound_record_section ,Mapping ):
-                            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                            enrich_cfg =dict (compound_record_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_assay (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 assay \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            assay_section :Any =enrich_section .get ("assay")
-            if not isinstance (assay_section ,Mapping ):
-                return False
-            assay_section =cast (Mapping [str ,Any ],assay_section )
-            enabled :Any =assay_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        assay_section :Any =enrich_section .get ("assay")
-                        if isinstance (assay_section ,Mapping ):
-                            assay_section =cast (Mapping [str ,Any ],assay_section )
-                            enrich_cfg =dict (assay_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_assay (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_data_validity (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 data_validity \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            data_validity_section :Any =enrich_section .get ("data_validity")
-            if not isinstance (data_validity_section ,Mapping ):
-                return False
-            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-            enabled :Any =data_validity_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        data_validity_section :Any =enrich_section .get ("data_validity")
-                        if isinstance (data_validity_section ,Mapping ):
-                            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                            enrich_cfg =dict (data_validity_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        if "data_validity_description"in df .columns :
-            non_na_count =int (df ["data_validity_description"].notna ().sum ())
-            if non_na_count >0 :
-                log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_molecule (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 molecule \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            molecule_section :Any =enrich_section .get ("molecule")
-            if not isinstance (molecule_section ,Mapping ):
-                return False
-            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-            enabled :Any =molecule_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        molecule_section :Any =enrich_section .get ("molecule")
-                        if isinstance (molecule_section ,Mapping ):
-                            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                            enrich_cfg =dict (molecule_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-        if "molecule_name"in df_join .columns :
-            if "molecule_pref_name"not in df .columns :
-                df ["molecule_pref_name"]=pd .NA
-            mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-            if mask .any ():
-                df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-                df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-                log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-        return df
-    def _extract_from_chembl (self ,dataset :object ,chembl_client :ChemblClient |Any ,activity_iterator :ChemblActivityClient ,*,limit :int |None =None ,select_fields :Sequence [str ]|None =None )->pd .DataFrame :
-        "Extract activity records by delegating batching to ``ChemblActivityClient``."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        method_start =time .perf_counter ()
-        self ._last_batch_extract_stats =None
-        input_frame =self ._coerce_activity_dataset (dataset )
-        if "activity_id"not in input_frame .columns :
-            msg ="Input dataset must contain an 'activity_id' column"
-            raise ValueError (msg )
-        normalized_ids =self ._normalize_activity_ids (input_frame ,limit =limit ,log =log )
-        if not normalized_ids :
-            summary :dict [str ,Any ]={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-            self ._last_batch_extract_stats =summary
-            log .info ("chembl_activity.batch_summary",**summary )
-            empty_frame =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-            return self ._ensure_comment_fields (empty_frame ,log )
-        records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =select_fields )
-        duration_ms =(time .perf_counter ()-method_start )*1000.0
-        summary ["duration_ms"]=duration_ms
-        self ._last_batch_extract_stats =summary
-        log .info ("chembl_activity.batch_summary",**summary )
-        result_df :pd .DataFrame =pd .DataFrame .from_records (records )
-        if result_df .empty :
-            result_df =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        elif "activity_id"in result_df .columns :
-            result_df =result_df .sort_values ("activity_id").reset_index (drop =True )
-        result_df =self ._ensure_comment_fields (result_df ,log )
-        result_df =self ._extract_data_validity_descriptions (result_df ,chembl_client ,log )
-        result_df =self ._extract_assay_fields (result_df ,chembl_client ,log )
-        self ._log_validity_comments_metrics (result_df ,log )
-        return result_df
-    def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-        cache_config =self .config .cache
-        if not cache_config .enabled :
-            return None
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        if not cache_file .exists ():
-            return None
-        try :
-            stat =cache_file .stat ()
-        except OSError :
-            return None
-        ttl_seconds =int (cache_config .ttl )
-        if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        try :
-            payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-        except (OSError ,json .JSONDecodeError ):
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        if not isinstance (payload ,dict ):
-            return None
-        missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-        if missing :
-            return None
-        return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
-    def _store_cache (self ,batch_ids :Sequence [str ],batch_data :Mapping [str ,Mapping [str ,Any ]],release :str |None )->None :
-        cache_config =self .config .cache
-        if not cache_config .enabled or not batch_ids or (not batch_data ):
-            return
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        normalized_set =set (normalized_ids )
-        data_to_store ={key :batch_data [key ]for key in normalized_set if key in batch_data }
-        if not data_to_store :
-            return
-        try :
-            cache_file .parent .mkdir (parents =True ,exist_ok =True )
-            tmp_path =cache_file .with_suffix (cache_file .suffix +".tmp")
-            tmp_path .write_text (json .dumps (data_to_store ,sort_keys =True ,default =str ),encoding ="utf-8")
-            tmp_path .replace (cache_file )
-        except Exception as exc :
-            log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-            log .debug ("chembl_activity.cache_store_failed",error =str (exc ))
-    def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-        directory =self ._cache_directory (release )
-        cache_key =self ._cache_key (batch_ids ,release )
-        return directory /f'{cache_key }.json'
-    def _cache_directory (self ,release :str |None )->Path :
-        cache_root =Path (self .config .paths .cache_root )
-        directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-        release_component =self ._sanitize_cache_component (release or "unknown")
-        pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-        version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-        return cache_root /directory_name /pipeline_component /release_component /version_component
-    def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-        payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-        raw =json .dumps (payload ,sort_keys =True )
-        return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
-    @staticmethod
-    def _sanitize_cache_component (value :str )->str :
-        sanitized =re .sub ("[^0-9A-Za-z_.-]","_",value )
-        return sanitized or "default"
-    def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-        "Create fallback record enriched with error metadata."
-        base_message ="Fallback: ChEMBL activity unavailable"
-        message =f'{base_message } ({error })'if error else base_message
-        timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-        metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-        if isinstance (error ,RequestException ):
-            response =getattr (error ,"response",None )
-            status_code =getattr (response ,"status_code",None )
-            if status_code is not None :
-                metadata ["http_status"]=status_code
-            metadata ["error_message"]=str (error )
-        elif error is not None :
-            metadata ["error_message"]=str (error )
-        fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-        return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
-    def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-        if df .empty :
-            return df
-        required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-        if missing_fields :
-            for field in missing_fields :
-                df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            log .debug ("comment_fields_ensured",fields =missing_fields )
-        return df
-    def _extract_data_validity_descriptions (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c data_validity_description \u0438\u0437 DATA_VALIDITY_LOOKUP \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f data_validity_comment \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_data_validity_lookup() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c data_validity_comment.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 data_validity_description.\n        "
-        if df .empty :
-            return df
-        if "data_validity_comment"not in df .columns :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="data_validity_comment_column_missing")
-            return df
-        validity_comments :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            comment =row .get ("data_validity_comment")
-            if pd .isna (comment )or comment is None :
-                continue
-            comment_str =str (comment ).strip ()
-            if comment_str :
-                validity_comments .append (comment_str )
-        if not validity_comments :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="no_valid_comments")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        unique_comments =list (set (validity_comments ))
-        log .info ("extract_data_validity_descriptions_fetching",comments_count =len (unique_comments ))
-        try :
-            records_dict =client .fetch_data_validity_lookup (comments =unique_comments ,fields =["data_validity_comment","description"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_data_validity_descriptions_fetch_error",error =str (exc ),exc_info =True )
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for comment in unique_comments :
-            record =records_dict .get (comment )
-            if record :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":record .get ("description")})
-            else :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":None })
-        if not enrichment_data :
-            log .debug ("extract_data_validity_descriptions_no_records")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        original_index =df .index .copy ()
-        df_result =df .merge (df_enrich ,on =["data_validity_comment"],how ="left",suffixes =("","_enrich"))
-        if "data_validity_description"not in df_result .columns :
-            df_result ["data_validity_description"]=pd .Series ([pd .NA ]*len (df_result ),dtype ="string")
-        else :
-            df_result ["data_validity_description"]=df_result ["data_validity_description"].astype ("string")
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_data_validity_descriptions_complete",comments_requested =len (unique_comments ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _extract_assay_fields (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c assay_organism \u0438 assay_tax_id \u0438\u0437 ASSAYS \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f assay_chembl_id \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_assays_by_ids() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438 assay_organism \u0438 assay_tax_id.\n        "
-        if df .empty :
-            return df
-        if "assay_chembl_id"not in df .columns :
-            log .debug ("extract_assay_fields_skipped",reason ="assay_chembl_id_column_missing")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        assay_ids :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            assay_id =row .get ("assay_chembl_id")
-            if pd .isna (assay_id )or assay_id is None :
-                continue
-            assay_id_str =str (assay_id ).strip ().upper ()
-            if assay_id_str :
-                assay_ids .append (assay_id_str )
-        if not assay_ids :
-            log .debug ("extract_assay_fields_skipped",reason ="no_valid_assay_ids")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        unique_assay_ids =list (set (assay_ids ))
-        log .info ("extract_assay_fields_fetching",assay_ids_count =len (unique_assay_ids ))
-        try :
-            records_dict =client .fetch_assays_by_ids (ids =unique_assay_ids ,fields =["assay_chembl_id","assay_organism","assay_tax_id"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_assay_fields_fetch_error",error =str (exc ),exc_info =True )
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for assay_id in unique_assay_ids :
-            record =records_dict .get (assay_id )if records_dict else None
-            if record :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":record .get ("assay_organism"),"assay_tax_id":record .get ("assay_tax_id")})
-            else :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":None ,"assay_tax_id":None })
-        if not enrichment_data :
-            log .debug ("extract_assay_fields_no_records")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        df_enrich ["assay_chembl_id"]=df_enrich ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        original_index =df .index .copy ()
-        df_normalized =df .copy ()
-        df_normalized ["assay_chembl_id_normalized"]=df_normalized ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        df_result =df_normalized .merge (df_enrich ,left_on ="assay_chembl_id_normalized",right_on ="assay_chembl_id",how ="left",suffixes =("","_enrich"))
-        df_result =df_result .drop (columns =["assay_chembl_id_normalized"])
-        for col in ["assay_organism","assay_tax_id"]:
-            if f'{col }_enrich'in df_result .columns :
-                if col not in df_result .columns :
-                    df_result [col ]=df_result [f'{col }_enrich']
-                else :
-                    base_series :pd .Series [Any ]=df_result [col ]
-                    enrich_series :pd .Series [Any ]=df_result [f'{col }_enrich']
-                    missing_mask =base_series .isna ()
-                    if bool (missing_mask .any ()):
-                        df_result .loc [missing_mask ,col ]=enrich_series .loc [missing_mask ]
-                df_result =df_result .drop (columns =[f'{col }_enrich'])
-        for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-            if col not in df_result .columns :
-                df_result [col ]=pd .Series ([pd .NA ]*len (df_result ),dtype =dtype )
-        df_result ["assay_organism"]=df_result ["assay_organism"].astype ("string")
-        df_result ["assay_tax_id"]=pd .to_numeric (df_result ["assay_tax_id"],errors ="coerce").astype ("Int64")
-        mask_valid =df_result ["assay_tax_id"].notna ()
-        if mask_valid .any ():
-            invalid_mask =mask_valid &(df_result ["assay_tax_id"]<1 )
-            if invalid_mask .any ():
-                log .warning ("invalid_assay_tax_id_range",count =int (invalid_mask .sum ()))
-                df_result .loc [invalid_mask ,"assay_tax_id"]=pd .NA
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_assay_fields_complete",assay_ids_requested =len (unique_assay_ids ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _log_validity_comments_metrics (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "\u041b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442:\n        - \u0414\u043e\u043b\u044e NA \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0442\u0440\u0435\u0445 \u043f\u043e\u043b\u0435\u0439\n        - \u0422\u043e\u043f-10 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment\n        - \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment (\u043d\u0435 \u0432 whitelist)\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity.\n        log:\n            Logger instance.\n        "
-        if df .empty :
-            return
-        metrics :dict [str ,Any ]={}
-        comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        for field in comment_fields :
-            if field in df .columns :
-                na_count =int (df [field ].isna ().sum ())
-                total_count =len (df )
-                na_rate =float (na_count )/float (total_count )if total_count >0 else 0.0
-                metrics [f'{field }_na_rate']=na_rate
-                metrics [f'{field }_na_count']=na_count
-                metrics [f'{field }_total_count']=total_count
-        non_null_comments_series :pd .Series [str ]|None =None
-        if "data_validity_comment"in df .columns :
-            series_candidate =df ["data_validity_comment"].dropna ()
-            if len (series_candidate )>0 :
-                typed_series :pd .Series [str ]=series_candidate .astype ("string")
-                non_null_comments_series =typed_series
-                value_counts =typed_series .value_counts ().head (10 )
-                top_10 ={str (key ):int (value )for key ,value in value_counts .items ()}
-                metrics ["top_10_data_validity_comments"]=top_10
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if whitelist and non_null_comments_series is not None :
-            whitelist_set :set [str ]=set (whitelist )
-            def _is_unknown (value :str )->bool :
-                return value not in whitelist_set
-            unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-            unknown_count =int (unknown_mask .sum ())
-            if unknown_count >0 :
-                unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-                metrics ["unknown_data_validity_comments_count"]=unknown_count
-                metrics ["unknown_data_validity_comments_samples"]=unknown_values
-                log .warning ("unknown_data_validity_comments_detected",unknown_count =unknown_count ,samples =unknown_values ,whitelist =whitelist )
-        if metrics :
-            log .info ("validity_comments_metrics",**metrics )
-    def _get_data_validity_comment_whitelist (self )->list [str ]:
-        "\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c whitelist \u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0434\u043b\u044f data_validity_comment \u0438\u0437 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\n\n        Raises\n        ------\n        RuntimeError\n            \u0415\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d \u0438\u043b\u0438 \u043f\u0443\u0441\u0442.\n        "
-        try :
-            values =sorted (self ._required_vocab_ids ("data_validity_comment"))
-        except RuntimeError as exc :
-            UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validation',run_id =self .run_id ).error ("data_validity_comment_whitelist_unavailable",error =str (exc ))
-            raise
-        return values
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested assay and molecule objects."
-        if "assay"in record and isinstance (record ["assay"],Mapping ):
-            assay =cast (Mapping [str ,Any ],record ["assay"])
-            if "organism"in assay :
-                record .setdefault ("assay_organism",assay ["organism"])
-            if "tax_id"in assay :
-                record .setdefault ("assay_tax_id",assay ["tax_id"])
-        if "molecule"in record and isinstance (record ["molecule"],Mapping ):
-            molecule =cast (Mapping [str ,Any ],record ["molecule"])
-            if "pref_name"in molecule :
-                record .setdefault ("molecule_pref_name",molecule ["pref_name"])
-        if "curated_by"in record :
-            curated_by =record .get ("curated_by")
-            if curated_by is not None and (not pd .isna (curated_by )):
-                record .setdefault ("curated",True )
-            else :
-                record .setdefault ("curated",False )
-        elif "curated"not in record :
-            record .setdefault ("curated",None )
-        return record
-    def _extract_activity_properties_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract TRUV fields, standard_* fields, and comments from activity_properties array as fallback.\n\n        \u041f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0438\u0437 activity_properties \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442\n        \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043e\u0442\u0432\u0435\u0442\u0435 API (\u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 \u043f\u0440\u044f\u043c\u044b\u0445 \u043f\u043e\u043b\u0435\u0439 \u0438\u0437 ACTIVITIES).\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0435 TRUV-\u043f\u043e\u043b\u044f: value, text_value, relation, units.\n        \u0422\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f: standard_upper_value, standard_text_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b: upper_value, lower_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0438: activity_comment, data_validity_comment.\n\n        \u0422\u0430\u043a\u0436\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 activity_properties \u0432 \u0437\u0430\u043f\u0438\u0441\u0438 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438.\n        \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044e \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract_activity_properties')
-        activity_id =record .get ("activity_id")
-        if "activity_properties"not in record :
-            log .warning ("activity_properties_missing",activity_id =activity_id ,message ="activity_properties not found in API response (possible ChEMBL < v24)")
-            record ["activity_properties"]=None
-            return record
-        properties =record ["activity_properties"]
-        if properties is None :
-            log .debug ("activity_properties_null",activity_id =activity_id ,message ="activity_properties is None (possible ChEMBL < v24)")
-            return record
-        if isinstance (properties ,str ):
-            try :
-                properties =json .loads (properties )
-            except (TypeError ,ValueError ,json .JSONDecodeError )as exc :
-                log .debug ("activity_properties_parse_failed",error =str (exc ),activity_id =record .get ("activity_id"))
-                return record
-        if not isinstance (properties ,Sequence )or isinstance (properties ,(str ,bytes )):
-            return record
-        property_iterable :Iterable [Any ]=cast (Iterable [Any ],properties )
-        property_items :list [Any ]=list (property_iterable )
-        def _set_fallback (key :str ,value :Any )->None :
-            "Set fallback value only if key is missing in record and value is not None."
-            if value is not None and record .get (key )is None :
-                record [key ]=value
-        def _is_empty (value :Any )->bool :
-            "Check if value is empty (None, empty string, or whitespace)."
-            if value is None :
-                return True
-            if isinstance (value ,str ):
-                return not value .strip ()
-            return False
-        items :list [Mapping [str ,Any ]]=[]
-        for property_item in property_items :
-            if isinstance (property_item ,Mapping )and "type"in property_item and ("value"in property_item or "text_value"in property_item ):
-                items .append (cast (Mapping [str ,Any ],property_item ))
-        def _is_measured (p :Mapping [str ,Any ])->bool :
-            rf =p .get ("result_flag")
-            return rf is True or (isinstance (rf ,int )and rf ==1 )
-        items .sort (key =lambda p :not _is_measured (p ))
-        for prop in items :
-            val =prop .get ("value")
-            txt =prop .get ("text_value")
-            rel =prop .get ("relation")
-            unt =prop .get ("units")
-            prop_type =str (prop .get ("type","")).lower ()
-            will_set_value =val is not None and record .get ("value")is None
-            will_set_text_value =txt is not None and record .get ("text_value")is None
-            _set_fallback ("value",val )
-            _set_fallback ("text_value",txt )
-            if will_set_value or will_set_text_value :
-                _set_fallback ("relation",rel )
-                _set_fallback ("units",unt )
-            if unt is not None and record .get ("units")is None :
-                _set_fallback ("units",unt )
-            if record .get ("upper_value")is None and ("upper"in prop_type or prop_type in ("upper_value","upper limit")):
-                if val is not None :
-                    _set_fallback ("upper_value",val )
-            if record .get ("lower_value")is None and ("lower"in prop_type or prop_type in ("lower_value","lower limit")):
-                if val is not None :
-                    _set_fallback ("lower_value",val )
-            if record .get ("standard_upper_value")is None and ("standard_upper"in prop_type or prop_type in ("standard upper","standard upper value")):
-                if val is not None :
-                    _set_fallback ("standard_upper_value",val )
-            if record .get ("standard_text_value")is None and "standard"in prop_type and ("text"in prop_type ):
-                if txt is not None :
-                    _set_fallback ("standard_text_value",txt )
-                elif val is not None :
-                    _set_fallback ("standard_text_value",val )
-        current_comment =record .get ("data_validity_comment")
-        if _is_empty (current_comment ):
-            data_validity_items :list [Mapping [str ,Any ]]=[prop for prop in items if ("data_validity"in str (prop .get ("type","")).lower ()or "validity"in str (prop .get ("type","")).lower ())and (prop .get ("text_value")is not None or prop .get ("value")is not None )]
-            if data_validity_items :
-                measured_items =[p for p in data_validity_items if _is_measured (p )]
-                if measured_items :
-                    prop =measured_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="measured",comment_value =comment_value )
-                else :
-                    prop =data_validity_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="first",comment_value =comment_value )
-            else :
-                log .debug ("data_validity_comment_fallback_no_items",activity_id =record .get ("activity_id"),activity_properties_count =len (items ),has_activity_properties =True )
-        else :
-            log .debug ("data_validity_comment_from_api",activity_id =record .get ("activity_id"),comment_value =current_comment )
-        normalized_properties =self ._normalize_activity_properties_items (property_items ,log )
-        if normalized_properties is not None :
-            validated_properties ,validation_stats =self ._validate_activity_properties_truv (normalized_properties ,log ,activity_id )
-            deduplicated_properties ,dedup_stats =self ._deduplicate_activity_properties (validated_properties ,log ,activity_id )
-            record ["activity_properties"]=deduplicated_properties
-            log .debug ("activity_properties_processed",activity_id =activity_id ,original_count =len (property_items ),normalized_count =len (normalized_properties ),validated_count =len (validated_properties ),deduplicated_count =len (deduplicated_properties ),invalid_count =validation_stats .get ("invalid_count",0 ),duplicates_removed =dedup_stats .get ("duplicates_removed",0 ))
-        else :
-            record ["activity_properties"]=properties
-            log .debug ("activity_properties_normalization_failed",activity_id =activity_id ,message ="activity_properties normalization failed, keeping original")
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
-    @staticmethod
-    def _extract_chembl_release (payload :Mapping [str ,Any ])->str |None :
-        for key in ("chembl_release","chembl_db_version","release","version"):
-            value =payload .get (key )
-            if isinstance (value ,str )and value .strip ():
-                return value
-            if value is not None :
-                return str (value )
-        return None
-    @staticmethod
-    def _extract_page_items (payload :Mapping [str ,Any ],items_keys :Sequence [str ]|None =None )->list [dict [str ,Any ]]:
-        preferred_keys :tuple [str ,...]=("activities",)
-        if items_keys is None :
-            combined_keys =preferred_keys +("data","items","results")
-        else :
-            combined_keys =tuple (dict .fromkeys ((*preferred_keys ,*items_keys )))
-        return ChemblPipelineBase ._extract_page_items (payload ,combined_keys )
-    @staticmethod
-    def _next_link (payload :Mapping [str ,Any ],base_url :str )->str |None :
-        page_meta :Any =payload .get ("page_meta")
-        if isinstance (page_meta ,Mapping ):
-            next_link_raw :Any =page_meta .get ("next")
-            next_link :str |None =cast (str |None ,next_link_raw )if next_link_raw is not None else None
-            if isinstance (next_link ,str )and next_link :
-                base_url_str =str (base_url )
-                base_path_parse_result =urlparse (base_url_str )
-                base_path_raw =base_path_parse_result .path
-                base_path_str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                base_path :str =base_path_str .rstrip ("/")
-                if next_link .startswith ("http://")or next_link .startswith ("https://"):
-                    parsed =urlparse (next_link )
-                    base_parsed =urlparse (base_url_str )
-                    parsed_path_raw =parsed .path
-                    base_path_raw =base_parsed .path
-                    path :str =parsed_path_raw .decode ("utf-8","ignore")if isinstance (parsed_path_raw ,(bytes ,bytearray ))else parsed_path_raw
-                    base_path_from_url :str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                    path_normalized :str =path .rstrip ("/")
-                    base_path_normalized :str =base_path_from_url .rstrip ("/")
-                    if base_path_normalized and path_normalized .startswith (base_path_normalized ):
-                        relative_path =path_normalized [len (base_path_normalized ):]
-                        if not relative_path :
-                            return None
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    elif "/api/data/"in path :
-                        parts =path .split ("/api/data/",1 )
-                        if len (parts )>1 :
-                            relative_path ="/"+parts [1 ]
-                        else :
-                            relative_path =path
-                    else :
-                        relative_path =path
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    if parsed .query :
-                        relative_path =f'{relative_path }?{parsed .query }'
-                    return relative_path
-                if base_path :
-                    normalized_base =base_path .lstrip ("/")
-                    stripped_link =next_link .lstrip ("/")
-                    if stripped_link .startswith (normalized_base +"/"):
-                        stripped_link =stripped_link [len (normalized_base ):]
-                    elif stripped_link ==normalized_base :
-                        stripped_link =""
-                    next_link =stripped_link
-                next_link =next_link .lstrip ("/")
-                if next_link :
-                    next_link =f'/{next_link }'
-                else :
-                    next_link ="/"
-                return next_link
-        return None
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Ensure canonical identifier columns are present before normalization."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_chembl_id"not in df .columns and "assay_id"in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "testitem_chembl_id"not in df .columns :
-            if "testitem_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["testitem_id"]
-                actions .append ("testitem_id->testitem_chembl_id")
-            elif "molecule_chembl_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["molecule_chembl_id"]
-                actions .append ("molecule_chembl_id->testitem_chembl_id")
-        if "molecule_chembl_id"not in df .columns and "testitem_chembl_id"in df .columns :
-            df ["molecule_chembl_id"]=df ["testitem_chembl_id"]
-            actions .append ("testitem_chembl_id->molecule_chembl_id")
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_required =[column for column in required_columns if column not in df .columns ]
-        if missing_required :
-            for column in missing_required :
-                df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            actions .append (f"created_missing:{",".join (missing_required )}")
-        alias_columns =[column for column in ("assay_id","testitem_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        boolean_columns =("potential_duplicate","curated","removed")
-        for column in boolean_columns :
-            specs [column ]={"dtype":"boolean","default":pd .NA }
-        return specs
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize ChEMBL and BAO identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"],pattern ="^CHEMBL\\d+$"),IdentifierRule (name ="bao",columns =["bao_endpoint","bao_format"],pattern ="^BAO_\\d{7}$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _finalize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align identifier columns after normalization and drop aliases."
-        df =df .copy ()
-        if {"molecule_chembl_id","testitem_chembl_id"}.issubset (df .columns ):
-            mismatch_mask =df ["molecule_chembl_id"].notna ()&df ["testitem_chembl_id"].notna ()&(df ["molecule_chembl_id"]!=df ["testitem_chembl_id"])
-            if mismatch_mask .any ():
-                mismatch_count =int (mismatch_mask .sum ())
-                samples_raw =df .loc [mismatch_mask ,["molecule_chembl_id","testitem_chembl_id"]].drop_duplicates ().head (5 ).to_dict ("records")
-                samples :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],samples_raw )
-                log .warning ("identifier_mismatch",count =mismatch_count ,samples =samples )
-                df .loc [mismatch_mask ,"testitem_chembl_id"]=df .loc [mismatch_mask ,"molecule_chembl_id"]
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_columns =[column for column in required_columns if column not in df .columns ]
-        if missing_columns :
-            for column in missing_columns :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            log .warning ("identifier_columns_missing",columns =missing_columns )
-        return df
-    def _finalize_output_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align final column order with schema and drop unexpected fields."
-        df =df .copy ()
-        expected =list (COLUMN_ORDER )
-        extras =[column for column in df .columns if column not in expected ]
-        if extras :
-            df =df .drop (columns =extras )
-            log .debug ("output_columns_dropped",columns =extras )
-        missing =[column for column in expected if column not in df .columns ]
-        if missing :
-            for column in missing :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([pd .NA ]*len (df ),dtype ="object")
-            log .warning ("output_columns_missing",columns =missing )
-        if not expected :
-            return df
-        return df [expected ]
-    def _filter_invalid_required_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Filter out rows with NULL values in required identifier fields.\n\n        Removes rows where any of the required fields (assay_chembl_id,\n        testitem_chembl_id, molecule_chembl_id) are NULL, as these cannot\n        pass schema validation.\n\n        Parameters\n        ----------\n        df:\n            DataFrame to filter.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            Filtered DataFrame with only rows having all required fields populated.\n        "
-        df =df .copy ()
-        if df .empty :
-            return df
-        required_fields =["assay_chembl_id","molecule_chembl_id"]
-        missing_fields =[field for field in required_fields if field not in df .columns ]
-        if missing_fields :
-            log .warning ("filter_skipped_missing_columns",missing_columns =missing_fields ,message ="Cannot filter: required columns are missing")
-            return df
-        valid_mask =df ["assay_chembl_id"].notna ()&df ["molecule_chembl_id"].notna ()
-        invalid_count =int ((~valid_mask ).sum ())
-        if invalid_count >0 :
-            invalid_rows =df [~valid_mask ]
-            sample_size =min (5 ,len (invalid_rows ))
-            sample_activity_ids =invalid_rows ["activity_id"].head (sample_size ).tolist ()if "activity_id"in invalid_rows .columns else []
-            log .warning ("filtered_invalid_rows",filtered_count =invalid_count ,remaining_count =int (valid_mask .sum ()),sample_activity_ids =sample_activity_ids ,message ="Rows with NULL in required identifier fields were filtered out")
-            df =df [valid_mask ].reset_index (drop =True )
-        return df
-    def _normalize_measurements (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize standard_value, standard_units, standard_relation, and standard_type."
-        df =df .copy ()
-        normalized_count =0
-        if "standard_value"in df .columns :
-            mask =df ["standard_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_value"]=numeric_series_std
-                negative_mask =mask &(df ["standard_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["standard_relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"standard_relation"]=series
-                invalid_mask =mask &~df ["standard_relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_type"in df .columns :
-            mask =df ["standard_type"].notna ()
-            if mask .any ():
-                df .loc [mask ,"standard_type"]=df .loc [mask ,"standard_type"].astype (str ).str .strip ()
-                standard_types_set :set [str ]=STANDARD_TYPES
-                invalid_mask =mask &~df ["standard_type"].isin (standard_types_set )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_type",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_type"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_units"in df .columns :
-            unit_mapping ={"nanomolar":"nM","nmol":"nM","nm":"nM","NM":"nM","\u00b5M":"\u03bcM","uM":"\u03bcM","UM":"\u03bcM","micromolar":"\u03bcM","microM":"\u03bcM","umol":"\u03bcM","millimolar":"mM","milliM":"mM","mmol":"mM","MM":"mM","percent":"%","pct":"%","ratios":"ratio"}
-            mask =df ["standard_units"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_units"].astype (str ).str .strip ()
-                for old_unit ,new_unit in unit_mapping .items ():
-                    series =series .str .replace (old_unit ,new_unit ,regex =False ,case =False )
-                df .loc [mask ,"standard_units"]=series
-                normalized_count +=int (mask .sum ())
-        if "relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"relation"]=series
-                invalid_mask =mask &~df ["relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_upper_value"in df .columns :
-            mask =df ["standard_upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_upper_value"]=numeric_series_std_upper
-                negative_mask =mask &(df ["standard_upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "upper_value"in df .columns :
-            mask =df ["upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"upper_value"]=numeric_series_upper
-                negative_mask =mask &(df ["upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "lower_value"in df .columns :
-            mask =df ["lower_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"lower_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_lower :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"lower_value"]=numeric_series_lower
-                negative_mask =mask &(df ["lower_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_lower_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"lower_value"]=None
-                normalized_count +=int (mask .sum ())
-        if normalized_count >0 :
-            log .debug ("measurements_normalized",normalized_count =normalized_count )
-        return df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize string fields: trim, empty string to null, title-case for organism."
-        working_df =df .copy ()
-        if "data_validity_description"in working_df .columns and "data_validity_comment"in working_df .columns :
-            invalid_mask =working_df ["data_validity_description"].notna ()&working_df ["data_validity_comment"].isna ()
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                log .warning ("invariant_data_validity_description_without_comment",count =invalid_count ,message ="data_validity_description is filled while data_validity_comment is NA")
-        rules :dict [str ,StringRule ]={"canonical_smiles":StringRule (),"bao_label":StringRule (max_length =128 ),"target_organism":StringRule (title_case =True ),"assay_organism":StringRule (title_case =True ),"data_validity_comment":StringRule (),"data_validity_description":StringRule (),"activity_comment":StringRule (),"standard_text_value":StringRule (),"text_value":StringRule (),"type":StringRule (),"units":StringRule (),"assay_type":StringRule (),"assay_description":StringRule (),"molecule_pref_name":StringRule (),"target_pref_name":StringRule (),"uo_units":StringRule (),"qudt_units":StringRule ()}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Serialize nested structures (ligand_efficiency, activity_properties) to JSON strings."
-        df =df .copy ()
-        nested_fields =["ligand_efficiency","activity_properties"]
-        for field in nested_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                serialized :list [Any ]=[]
-                for idx ,value in df .loc [mask ,field ].items ():
-                    if field =="activity_properties":
-                        serialized_value =self ._serialize_activity_properties (value ,log )
-                        serialized .append (serialized_value )
-                        continue
-                    if isinstance (value ,(Mapping ,list )):
-                        try :
-                            serialized .append (json .dumps (value ,ensure_ascii =False ,sort_keys =True ))
-                        except (TypeError ,ValueError )as exc :
-                            log .warning ("nested_serialization_failed",field =field ,index =idx ,error =str (exc ))
-                            serialized .append (None )
-                    elif isinstance (value ,str ):
-                        try :
-                            json .loads (value )
-                            serialized .append (value )
-                        except (TypeError ,ValueError ):
-                            serialized .append (None )
-                    else :
-                        serialized .append (None )
-                df .loc [mask ,field ]=pd .Series (serialized ,dtype ="object",index =df .loc [mask ,field ].index )
-        if "standard_value"in df .columns and "ligand_efficiency"in df .columns :
-            mask =df ["standard_value"].notna ()&df ["ligand_efficiency"].isna ()
-            if mask .any ():
-                log .warning ("ligand_efficiency_missing_with_standard_value",count =int (mask .sum ()),message ="ligand_efficiency is empty while standard_value exists")
-        return df
-    def _serialize_activity_properties (self ,value :Any ,log :BoundLogger |None =None )->str |None :
-        "Return normalized JSON for activity_properties or None if not serializable."
-        normalized_items =self ._normalize_activity_properties_items (value ,log )
-        if normalized_items is None :
-            return None
-        try :
-            return json .dumps (normalized_items ,ensure_ascii =False ,sort_keys =True )
-        except (TypeError ,ValueError )as exc :
-            if log is not None :
-                log .warning ("activity_properties_serialization_failed",error =str (exc ))
-            return None
-    def _normalize_activity_properties_items (self ,value :Any ,log :BoundLogger |None =None )->list [dict [str ,Any ]]|None :
-        "Coerce activity_properties payloads into a list of constrained dictionaries."
-        if value is None :
-            return None
-        raw_value =value
-        if isinstance (value ,str ):
-            stripped =value .strip ()
-            if not stripped :
-                return []
-            try :
-                parsed =json .loads (stripped )
-            except (TypeError ,ValueError ):
-                fallback_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                fallback_base ["text_value"]=stripped
-                return [fallback_base ]
-            else :
-                value =parsed
-        if isinstance (value ,Mapping ):
-            items :list [Any ]=[value ]
-        elif isinstance (value ,Sequence )and (not isinstance (value ,(str ,bytes ))):
-            items =list (value )
-        else :
-            if log is not None :
-                log .warning ("activity_properties_unhandled_type",value_type =type (raw_value ).__name__ )
-            return None
-        normalized :list [dict [str ,Any ]]=[]
-        for item in items :
-            if item is None :
-                continue
-            if isinstance (item ,Mapping ):
-                item_mapping =cast (Mapping [str ,Any ],item )
-                normalized_item :dict [str ,Any |None ]={key :item_mapping .get (key )for key in ACTIVITY_PROPERTY_KEYS }
-                result_flag_value =normalized_item .get ("result_flag")
-                if isinstance (result_flag_value ,int )and result_flag_value in (0 ,1 ):
-                    normalized_item ["result_flag"]=bool (result_flag_value )
-                normalized .append (normalized_item )
-            elif isinstance (item ,str ):
-                str_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                str_base ["text_value"]=item
-                normalized .append (str_base )
-            elif log is not None :
-                log .warning ("activity_properties_item_unhandled",item_type =type (item ).__name__ )
-        return normalized
-    def _validate_activity_properties_truv (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0434\u043b\u044f activity_properties.\n\n        \u0412\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u0435\u0442 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n        - value IS NOT NULL \u21d2 text_value IS NULL (\u0438 \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442)\n        - relation IN ('=', '<', '\u2264', '>', '\u2265', '~') OR NULL\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        "
-        validated :list [dict [str ,Any ]]=[]
-        invalid_count =0
-        invalid_items :list [dict [str ,Any ]]=[]
-        for prop in properties :
-            is_valid =True
-            validation_errors :list [str ]=[]
-            value =prop .get ("value")
-            text_value =prop .get ("text_value")
-            relation =prop .get ("relation")
-            if value is not None and text_value is not None :
-                is_valid =False
-                validation_errors .append ("both value and text_value are not None")
-            elif value is None and text_value is None :
-                pass
-            if relation is not None :
-                if not isinstance (relation ,str ):
-                    is_valid =False
-                    validation_errors .append (f'relation is not a string: {type (relation ).__name__ }')
-                elif relation not in RELATIONS :
-                    is_valid =False
-                    validation_errors .append (f"relation '{relation }' not in allowed values: {RELATIONS }")
-            validated .append (prop )
-            if not is_valid :
-                invalid_count +=1
-                invalid_items .append (prop )
-                log .warning ("activity_property_truv_validation_failed",activity_id =activity_id ,property =prop ,errors =validation_errors ,message ="TRUV validation failed, but property is kept")
-        stats ={"invalid_count":invalid_count ,"valid_count":len (validated )}
-        return (validated ,stats )
-    def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-        seen :set [tuple [Any ,...]]=set ()
-        deduplicated :list [dict [str ,Any ]]=[]
-        duplicates_removed =0
-        for prop in properties :
-            dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-            if dedup_key not in seen :
-                seen .add (dedup_key )
-                deduplicated .append (prop )
-            else :
-                duplicates_removed +=1
-                log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-        stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-        return (deduplicated ,stats )
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_added",value ="activity")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_filled",value ="activity")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :BoundLogger )->pd .DataFrame :
-        "Convert data types according to the Pandera schema."
-        df =df .copy ()
-        non_nullable_int_fields ={"activity_id":"int64"}
-        nullable_int_fields ={"row_index":"Int64","target_tax_id":"int64","assay_tax_id":"int64","record_id":"int64","src_id":"int64"}
-        float_fields ={"standard_value":"float64","standard_upper_value":"float64","pchembl_value":"float64","upper_value":"float64","lower_value":"float64"}
-        bool_fields =["potential_duplicate","curated","removed"]
-        binary_flag_fields =["standard_flag"]
-        for field in non_nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_int :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_int .astype ("Int64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field =="row_index":
-                    numeric_series_row :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=numeric_series_row .astype ("Int64")
-                    if df [field ].isna ().any ():
-                        df [field ]=range (len (df ))
-                else :
-                    nullable_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=nullable_numeric_series .astype ("Int64")
-                    mask_valid =df [field ].notna ()
-                    if mask_valid .any ():
-                        invalid_mask =mask_valid &(df [field ]<1 )
-                        if invalid_mask .any ():
-                            log .warning ("invalid_positive_integer",field =field ,count =int (invalid_mask .sum ()))
-                            df .loc [invalid_mask ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in float_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_float :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_float .astype ("float64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in bool_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field in ("curated","removed")and df [field ].dtype =="boolean":
-                    continue
-                if field in ("curated","removed"):
-                    df [field ]=df [field ].astype ("boolean")
-                else :
-                    bool_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=(bool_numeric_series !=0 ).astype ("boolean")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("bool_conversion_failed",field =field ,error =str (exc ))
-        for field in binary_flag_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_flag :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_flag .astype ("Int64")
-                mask_valid =df [field ].notna ()
-                if mask_valid .any ():
-                    valid_values =df .loc [mask_valid ,field ]
-                    invalid_valid_mask =~valid_values .isin ([0 ,1 ])
-                    if invalid_valid_mask .any ():
-                        invalid_index =valid_values .index [invalid_valid_mask ]
-                        log .warning ("invalid_standard_flag",field =field ,count =int (invalid_valid_mask .sum ()))
-                        df .loc [invalid_index ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        object_fields =["value","activity_properties"]
-        for field in object_fields :
-            if field in df .columns :
-                if df [field ].dtype !="object":
-                    df [field ]=df [field ].astype ("object")
-        return df
-    def _validate_foreign_keys (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Validate foreign key integrity and format of ChEMBL IDs."
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        chembl_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        warnings :list [str ]=[]
-        for field in chembl_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-                if invalid_mask .any ():
-                    warning_msg :str =f'{field }: {int (invalid_mask .sum ())} invalid format(s)'
-                    warnings .append (warning_msg )
-        if warnings :
-            log .warning ("foreign_key_validation",warnings =warnings )
-        return df
-    def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check uniqueness of activity_id before validation."
-        if "activity_id"not in df .columns :
-            log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-            return
-        duplicates =df [df ["activity_id"].duplicated (keep =False )]
-        if not duplicates .empty :
-            duplicate_count =len (duplicates )
-            duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-            log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-            msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-            raise ValueError (msg )
-        log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
-    def _validate_data_validity_comment_soft_enum (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Soft enum \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0434\u043b\u044f data_validity_comment.\n\n        \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0442\u0438\u0432 whitelist \u0438\u0437 \u043a\u043e\u043d\u0444\u0438\u0433\u0430. \u041d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n        \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a warning, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e (soft enum).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        "
-        if df .empty or "data_validity_comment"not in df .columns :
-            return
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if not whitelist :
-            return
-        series_candidate =df ["data_validity_comment"].dropna ()
-        if len (series_candidate )==0 :
-            return
-        non_null_comments_series =series_candidate .astype ("string")
-        whitelist_set :set [str ]=set (whitelist )
-        def _is_unknown (value :str )->bool :
-            return value not in whitelist_set
-        unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-        unknown_count =int (unknown_mask .sum ())
-        if unknown_count >0 :
-            unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-            log .warning ("soft_enum_unknown_data_validity_comment",unknown_count =unknown_count ,total_count =len (non_null_comments_series ),samples =unknown_values ,whitelist =whitelist ,message ="Unknown data_validity_comment values detected (soft enum: not blocking)")
-    def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-        reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        errors :list [str ]=[]
-        for field in reference_fields :
-            if field not in df .columns :
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-                continue
-            mask =df [field ].notna ()
-            if not mask .any ():
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-                continue
-            invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-                errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-                log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-        if errors :
-            log .error ("foreign_key_integrity_check_failed",errors =errors )
-            msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-            raise ValueError (msg )
-        log .debug ("foreign_key_integrity_verified")
-    def _log_detailed_validation_errors (self ,failure_cases :pd .DataFrame ,payload :pd .DataFrame ,log :BoundLogger )->None :
-        "Log individual validation errors with row index and activity_id."
-        if failure_cases .empty or payload .empty :
-            return
-        activity_id_col ="activity_id"if "activity_id"in payload .columns else None
-        index_col ="index"if "index"in failure_cases .columns else None
-        if index_col is None :
-            return
-        max_errors =20
-        errors_to_log =failure_cases .head (max_errors )
-        for _ ,error_row in errors_to_log .iterrows ():
-            row_index =error_row .get (index_col )
-            if row_index is None :
-                continue
-            error_details :dict [str ,Any ]={"row_index":int (row_index )if isinstance (row_index ,(int ,float ))else str (row_index )}
-            if activity_id_col :
-                try :
-                    idx =int (row_index )if isinstance (row_index ,(int ,float ))else row_index
-                    activity_id_value :Any =payload .at [cast (int ,idx ),activity_id_col ]
-                    activity_id =activity_id_value
-                except (KeyError ,IndexError ):
-                    activity_id =None
-                if activity_id is not None and pd .notna (activity_id ):
-                    error_details ["activity_id"]=int (activity_id )if isinstance (activity_id ,(int ,float ))else str (activity_id )
-            if "column"in error_row and pd .notna (error_row ["column"]):
-                error_details ["column"]=str (error_row ["column"])
-            if "schema_context"in error_row and pd .notna (error_row ["schema_context"]):
-                error_details ["schema_context"]=str (error_row ["schema_context"])
-            if "failure_case"in error_row and pd .notna (error_row ["failure_case"]):
-                error_details ["failure_case"]=str (error_row ["failure_case"])
-            log .error ("validation_error_detail",**error_details )
-        if len (failure_cases )>max_errors :
-            log .warning ("validation_errors_truncated",total_errors =len (failure_cases ),logged_errors =max_errors )
-    def build_quality_report (self ,df :pd .DataFrame )->pd .DataFrame |dict [str ,object ]|None :
-        "Return QC report with activity-specific metrics including distributions."
-        business_key =["activity_id"]if "activity_id"in df .columns else None
-        base_report =build_default_quality_report (df ,business_key_fields =business_key )
-        rows :list [dict [str ,Any ]]=[]
-        if not base_report .empty :
-            records_raw =base_report .to_dict ("records")
-            records :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],records_raw )
-            for record in records :
-                rows .append ({str (k ):v for k ,v in record .items ()})
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        foreign_key_fields =["assay_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"]
-        for field in foreign_key_fields :
-            if field in df .columns :
-                mask =df [field ].notna ()
-                if mask .any ():
-                    string_series =df [field ].astype (str )
-                    valid_mask =mask &string_series .str .match (chembl_id_pattern .pattern ,na =False )
-                    invalid_count =int ((mask &~valid_mask ).astype (int ).sum ())
-                    valid_count =int (valid_mask .astype (int ).sum ())
-                    total_count =int (mask .astype (int ).sum ())
-                    integrity_ratio =float (valid_count /total_count )if total_count >0 else 0.0
-                    rows .append ({"section":"foreign_key","metric":"integrity_ratio","column":field ,"value":float (integrity_ratio ),"valid_count":int (valid_count ),"invalid_count":int (invalid_count ),"total_count":int (total_count )})
-        if "standard_type"in df .columns :
-            type_counts_series :pd .Series [Any ]=df ["standard_type"].value_counts ()
-            type_dist_raw =type_counts_series .to_dict ()
-            type_dist :dict [Any ,int ]=cast (dict [Any ,int ],type_dist_raw )
-            for type_value ,count in type_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_type_count","column":"standard_type","value":str (type_value )if type_value is not None else "null","count":int (count )})
-        if "standard_units"in df .columns :
-            unit_counts_series :pd .Series [Any ]=df ["standard_units"].value_counts ()
-            unit_dist_raw =unit_counts_series .to_dict ()
-            unit_dist :dict [Any ,int ]=cast (dict [Any ,int ],unit_dist_raw )
-            for unit_value ,count in unit_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_units_count","column":"standard_units","value":str (unit_value )if unit_value is not None else "null","count":int (count )})
-        return pd .DataFrame (rows )
-    def write (self ,df :pd .DataFrame ,output_path :Path ,*,extended :bool =False ,include_correlation :bool |None =None ,include_qc_metrics :bool |None =None )->RunResult :
-        "Override write() to bind actor and ensure deterministic sorting.\n\n        Parameters\n        ----------\n        df:\n            The DataFrame to write.\n        output_path:\n            The base output path for all artifacts.\n        extended:\n            Whether to include extended QC artifacts.\n\n        Returns\n        -------\n        RunResult:\n            All artifacts generated by the write operation.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.write')
-        UnifiedLogger .bind (actor =self .actor )
-        sort_keys =["assay_chembl_id","testitem_chembl_id","activity_id"]
-        if df .empty or not all ((key in df .columns for key in sort_keys )):
-            return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-        original_sort_by =self .config .determinism .sort .by
-        if not original_sort_by or original_sort_by !=sort_keys :
-            from copy import deepcopy
-            from bioetl .config .models .determinism import DeterminismSortingConfig
-            modified_config =deepcopy (self .config )
-            modified_config .determinism .sort =DeterminismSortingConfig (by =sort_keys ,ascending =[True ,True ,True ],na_position ="last")
-            log .debug ("write_sort_config_set",sort_keys =sort_keys ,original_sort_keys =list (original_sort_by )if original_sort_by else [])
-            original_config =self .config
-            self .config =modified_config
-            try :
-                result =super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-            finally :
-                self .config =original_config
-            return result
-        return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
```

#### Hotspot 2

- Definition: ChemblActivityPipeline.__init__#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:111-115
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,5 +0,0 @@
-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
```

#### Hotspot 3

- Definition: ChemblActivityPipeline._add_row_metadata#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2901-2924
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,18 +0,0 @@
-def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_added",value ="activity")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_filled",value ="activity")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Hotspot 4

- Definition: ChemblActivityPipeline._build_activity_descriptor#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:343-415
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,22 +0,0 @@
-def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-    "Construct the descriptor driving the shared extraction template."
-    def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-        http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-        chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-        typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =source_config .parameters .select_fields
-        return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-    def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-    def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        df =pipeline ._ensure_comment_fields (df ,log )
-        chembl_client =cast (ChemblClient ,context .chembl_client )
-        if chembl_client is not None :
-            df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-        pipeline ._log_validity_comments_metrics (df ,log )
-        return df
-    def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        return pipeline ._materialize_activity_record (payload )
-    return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
```

#### Hotspot 5

- Definition: ChemblActivityPipeline._cache_directory#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1257-1267
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,7 +0,0 @@
-def _cache_directory (self ,release :str |None )->Path :
-    cache_root =Path (self .config .paths .cache_root )
-    directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-    release_component =self ._sanitize_cache_component (release or "unknown")
-    pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-    version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-    return cache_root /directory_name /pipeline_component /release_component /version_component
```

#### Hotspot 6

- Definition: ChemblActivityPipeline._cache_file_path#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1252-1255
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,4 +0,0 @@
-def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-    directory =self ._cache_directory (release )
-    cache_key =self ._cache_key (batch_ids ,release )
-    return directory /f'{cache_key }.json'
```

#### Hotspot 7

- Definition: ChemblActivityPipeline._cache_key#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1269-1277
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,4 +0,0 @@
-def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-    payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-    raw =json .dumps (payload ,sort_keys =True )
-    return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
```

#### Hotspot 8

- Definition: ChemblActivityPipeline._check_activity_id_uniqueness#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:3108-3131
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,13 +0,0 @@
-def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check uniqueness of activity_id before validation."
-    if "activity_id"not in df .columns :
-        log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-        return
-    duplicates =df [df ["activity_id"].duplicated (keep =False )]
-    if not duplicates .empty :
-        duplicate_count =len (duplicates )
-        duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-        log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-        msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-        raise ValueError (msg )
-    log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
```

#### Hotspot 9

- Definition: ChemblActivityPipeline._check_cache#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1176-1221
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,33 +0,0 @@
-def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-    cache_config =self .config .cache
-    if not cache_config .enabled :
-        return None
-    normalized_ids =[str (identifier )for identifier in batch_ids ]
-    cache_file =self ._cache_file_path (normalized_ids ,release )
-    if not cache_file .exists ():
-        return None
-    try :
-        stat =cache_file .stat ()
-    except OSError :
-        return None
-    ttl_seconds =int (cache_config .ttl )
-    if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    try :
-        payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-    except (OSError ,json .JSONDecodeError ):
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    if not isinstance (payload ,dict ):
-        return None
-    missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-    if missing :
-        return None
-    return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
```

#### Hotspot 10

- Definition: ChemblActivityPipeline._check_foreign_key_integrity#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:3185-3232
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,24 +0,0 @@
-def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-    reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-    chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-    errors :list [str ]=[]
-    for field in reference_fields :
-        if field not in df .columns :
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-            continue
-        mask =df [field ].notna ()
-        if not mask .any ():
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-            continue
-        invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-        if invalid_mask .any ():
-            invalid_count =int (invalid_mask .sum ())
-            invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-            errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-            log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-    if errors :
-        log .error ("foreign_key_integrity_check_failed",errors =errors )
-        msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-        raise ValueError (msg )
-    log .debug ("foreign_key_integrity_verified")
```

#### Hotspot 11

- Definition: ChemblActivityPipeline._coerce_activity_dataset#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:432-450
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,14 +0,0 @@
-def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-    "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-    if isinstance (dataset ,pd .Series ):
-        return dataset .to_frame (name ="activity_id")
-    if isinstance (dataset ,pd .DataFrame ):
-        return dataset
-    if isinstance (dataset ,Mapping ):
-        mapping =cast (Mapping [str ,Any ],dataset )
-        return pd .DataFrame ([dict (mapping )])
-    if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-        dataset_list :list [Any ]=list (dataset )
-        return pd .DataFrame ({"activity_id":dataset_list })
-    msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-    raise TypeError (msg )
```

#### Hotspot 12

- Definition: ChemblActivityPipeline._coerce_mapping#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2065-2068
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,5 +0,0 @@
-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Hotspot 13

- Definition: ChemblActivityPipeline._collect_records_by_ids#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:498-637
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,83 +0,0 @@
-def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-    "Iterate over IDs using the shared iterator while preserving cache semantics."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    records :list [dict [str ,Any ]]=[]
-    success_count =0
-    fallback_count =0
-    error_count =0
-    cache_hits =0
-    api_calls =0
-    total_batches =0
-    key_order =[key for _ ,key in normalized_ids ]
-    key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-    for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-        total_batches +=1
-        batch_start =time .perf_counter ()
-        from_cache =False
-        chunk_records :dict [str ,dict [str ,Any ]]={}
-        try :
-            cached_records =self ._check_cache (chunk ,self ._chembl_release )
-            if cached_records is not None :
-                from_cache =True
-                cache_hits +=len (chunk )
-                chunk_records =cached_records
-            else :
-                api_calls +=1
-                fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                for item in fetched_items :
-                    if not isinstance (item ,Mapping ):
-                        continue
-                    activity_value =item .get ("activity_id")
-                    if activity_value is None :
-                        continue
-                    chunk_records [str (activity_value )]=dict (item )
-                self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-            success_in_batch =0
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                record =chunk_records .get (key )
-                if record and (not record .get ("error")):
-                    materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                    records .append (materialized )
-                    success_count +=1
-                    success_in_batch +=1
-                else :
-                    fallback_record =self ._create_fallback_record (numeric_id )
-                    records .append (fallback_record )
-                    fallback_count +=1
-                    error_count +=1
-            batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-            log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-        except CircuitBreakerOpenError as exc :
-            log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except RequestException as exc :
-            log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except Exception as exc :
-            log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-    total_records =len (normalized_ids )
-    success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-    summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-    return (records ,summary )
```

#### Hotspot 14

- Definition: ChemblActivityPipeline._create_fallback_record#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1284-1323
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,16 +0,0 @@
-def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-    "Create fallback record enriched with error metadata."
-    base_message ="Fallback: ChEMBL activity unavailable"
-    message =f'{base_message } ({error })'if error else base_message
-    timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-    metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-    if isinstance (error ,RequestException ):
-        response =getattr (error ,"response",None )
-        status_code =getattr (response ,"status_code",None )
-        if status_code is not None :
-            metadata ["http_status"]=status_code
-        metadata ["error_message"]=str (error )
-    elif error is not None :
-        metadata ["error_message"]=str (error )
-    fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-    return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
```

#### Hotspot 15

- Definition: ChemblActivityPipeline._deduplicate_activity_properties#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2843-2899
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,15 +0,0 @@
-def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-    "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-    seen :set [tuple [Any ,...]]=set ()
-    deduplicated :list [dict [str ,Any ]]=[]
-    duplicates_removed =0
-    for prop in properties :
-        dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-        if dedup_key not in seen :
-            seen .add (dedup_key )
-            deduplicated .append (prop )
-        else :
-            duplicates_removed +=1
-            log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-    stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-    return (deduplicated ,stats )
```

#### Hotspot 16

- Definition: ChemblActivityPipeline._enrich_assay#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:892-935
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,28 +0,0 @@
-def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    assay_section :Any =enrich_section .get ("assay")
-                    if isinstance (assay_section ,Mapping ):
-                        assay_section =cast (Mapping [str ,Any ],assay_section )
-                        enrich_cfg =dict (assay_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_assay (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 17

- Definition: ChemblActivityPipeline._enrich_compound_record#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:822-867
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,28 +0,0 @@
-def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    compound_record_section :Any =enrich_section .get ("compound_record")
-                    if isinstance (compound_record_section ,Mapping ):
-                        compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                        enrich_cfg =dict (compound_record_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 18

- Definition: ChemblActivityPipeline._enrich_data_validity#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:960-1014
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,32 +0,0 @@
-def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    data_validity_section :Any =enrich_section .get ("data_validity")
-                    if isinstance (data_validity_section ,Mapping ):
-                        data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                        enrich_cfg =dict (data_validity_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    if "data_validity_description"in df .columns :
-        non_na_count =int (df ["data_validity_description"].notna ().sum ())
-        if non_na_count >0 :
-            log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 19

- Definition: ChemblActivityPipeline._enrich_molecule#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1039-1108
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,37 +0,0 @@
-def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    molecule_section :Any =enrich_section .get ("molecule")
-                    if isinstance (molecule_section ,Mapping ):
-                        molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                        enrich_cfg =dict (molecule_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-    if "molecule_name"in df_join .columns :
-        if "molecule_pref_name"not in df .columns :
-            df ["molecule_pref_name"]=pd .NA
-        mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-        if mask .any ():
-            df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-            df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-            log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-    return df
```

#### Hotspot 20

- Definition: ChemblActivityPipeline._ensure_comment_fields#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1325-1358
- target: absent

```diff
--- activity:run.py
+++ target:run.py
@@ -1,11 +0,0 @@
-def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-    if df .empty :
-        return df
-    required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-    missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-    if missing_fields :
-        for field in missing_fields :
-            df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-        log .debug ("comment_fields_ensured",fields =missing_fields )
-    return df
```

_Only the first 20 hotspots of 106 are shown for module run.py._

_First 20 hotspots of 146 are shown for pair activity ↔ target._

---

## Pair: activity ↔ testitem

- AST hash: 978d4152ad77000361c21b7f2051d3d1 ↔ 729263c98934cfcb08473bcacfb20e9e

- Jaccard over tokens: 0.171

### Module run.py

- File status: activity — present, testitem — present
- AST hash: 13d39ff2ae0173b684048e972fd58d8d ↔ 4380f065a3fdcc94928fad4ab20f1d2b
- Jaccard over tokens: 0.185

Definition                                                       | activity signature                                                                                                                                                         | testitem signature                                                           | Side effects                                                                                                                                                                                                                                                                                                                                                          | Exceptions                                               | Status          
-----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|-----------------
ChemblActivityPipeline                                           | —                                                                                                                                                                          | —                                                                            | activity: io=cache_file.read_text, json.dumps, json.loads, payload.get, tmp_path.write_text; logging=UnifiedLogger.bind, UnifiedLogger.get, bound_log.warning, errors_to_log.iterrows, log.debug, log.error, log.info, log.warning, pipeline._log_validity_comments_metrics, self._log_detailed_validation_errors, self._log_validity_comments_metrics<br>testitem: ∅ | activity: TypeError(msg), ValueError(msg)<br>testitem: ∅ | only in activity
ChemblActivityPipeline.__init__                                  | self, config: PipelineConfig, run_id: str                                                                                                                                  | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._add_row_metadata                         | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.debug<br>testitem: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._build_activity_descriptor                | self                                                                                                                                                                       | —                                                                            | activity: io=∅; logging=pipeline._log_validity_comments_metrics<br>testitem: ∅                                                                                                                                                                                                                                                                                        | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._cache_directory                          | self, release: str | None                                                                                                                                                  | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._cache_file_path                          | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._cache_key                                | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                            | activity: io=json.dumps; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                     | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._check_activity_id_uniqueness             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.debug, log.error, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                              | activity: ValueError(msg)<br>testitem: ∅                 | only in activity
ChemblActivityPipeline._check_cache                              | self, batch_ids: Sequence[str], release: str | None                                                                                                                        | —                                                                            | activity: io=cache_file.read_text, json.loads; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._check_foreign_key_integrity              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.debug, log.error, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                              | activity: ValueError(msg)<br>testitem: ∅                 | only in activity
ChemblActivityPipeline._coerce_activity_dataset                  | self, dataset: object                                                                                                                                                      | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: TypeError(msg)<br>testitem: ∅                  | only in activity
ChemblActivityPipeline._coerce_mapping                           | payload: Any                                                                                                                                                               | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._collect_records_by_ids                   | self, normalized_ids: Sequence[tuple[int, str]], activity_iterator: ChemblActivityClient, *, select_fields: Sequence[str] | None                                           | —                                                                            | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.error, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                           | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._create_fallback_record                   | self, activity_id: int, error: Exception | None                                                                                                                            | —                                                                            | activity: io=json.dumps; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                     | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._deduplicate_activity_properties          | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                                            | activity: io=∅; logging=log.debug<br>testitem: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._enrich_assay                             | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: io=∅; logging=UnifiedLogger.get, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                 | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._enrich_compound_record                   | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: io=∅; logging=UnifiedLogger.get, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                 | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._enrich_data_validity                     | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                       | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._enrich_molecule                          | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                       | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._ensure_comment_fields                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.debug<br>testitem: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._extract_activity_properties_fields       | self, record: dict[str, Any]                                                                                                                                               | —                                                                            | activity: io=json.loads; logging=UnifiedLogger.get, log.debug, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                             | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._extract_assay_fields                     | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                                            | activity: io=∅; logging=log.debug, log.info, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._extract_chembl_release                   | payload: Mapping[str, Any]                                                                                                                                                 | —                                                                            | activity: io=payload.get; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._extract_data_validity_descriptions       | self, df: pd.DataFrame, client: ChemblClient, log: BoundLogger                                                                                                             | —                                                                            | activity: io=∅; logging=log.debug, log.info, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._extract_from_chembl                      | self, dataset: object, chembl_client: ChemblClient | Any, activity_iterator: ChemblActivityClient, *, limit: int | None = None, select_fields: Sequence[str] | None = None | —                                                                            | activity: io=∅; logging=UnifiedLogger.get, log.info, self._log_validity_comments_metrics<br>testitem: ∅                                                                                                                                                                                                                                                               | activity: ValueError(msg)<br>testitem: ∅                 | only in activity
ChemblActivityPipeline._extract_nested_fields                    | self, record: dict[str, Any]                                                                                                                                               | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._extract_page_items                       | payload: Mapping[str, Any], items_keys: Sequence[str] | None                                                                                                               | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._filter_invalid_required_fields           | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._finalize_identifier_columns              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._finalize_output_columns                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.debug, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._get_data_validity_comment_whitelist      | self                                                                                                                                                                       | —                                                                            | activity: io=∅; logging=UnifiedLogger.get<br>testitem: ∅                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._harmonize_identifier_columns             | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.debug<br>testitem: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._log_detailed_validation_errors           | self, failure_cases: pd.DataFrame, payload: pd.DataFrame, log: BoundLogger                                                                                                 | —                                                                            | activity: io=∅; logging=errors_to_log.iterrows, log.error, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                 | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._log_validity_comments_metrics            | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.info, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                          | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._materialize_activity_record              | self, payload: Mapping[str, Any], *, activity_id: int | None = None                                                                                                        | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._next_link                                | payload: Mapping[str, Any], base_url: str                                                                                                                                  | —                                                                            | activity: io=payload.get; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._normalize_activity_ids                   | self, input_frame: pd.DataFrame, *, limit: int | None, log: BoundLogger                                                                                                    | —                                                                            | activity: io=∅; logging=log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._normalize_activity_properties_items      | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                                            | activity: io=json.loads; logging=log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                           | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._normalize_data_types                     | self, df: pd.DataFrame, schema: Any, log: BoundLogger                                                                                                                      | —                                                                            | activity: io=∅; logging=log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._normalize_identifiers                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.debug<br>testitem: ∅                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._normalize_measurements                   | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.debug, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._normalize_nested_structures              | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=json.dumps, json.loads; logging=log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                               | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._normalize_string_fields                  | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.debug, log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                         | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._prepare_activity_iteration               | self, *, client_name: str = 'chembl_activity_client'                                                                                                                       | —                                                                            | activity: io=∅; logging=UnifiedLogger.get<br>testitem: ∅                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._sanitize_cache_component                 | value: str                                                                                                                                                                 | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._schema_column_specs                      | self                                                                                                                                                                       | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._serialize_activity_properties            | self, value: Any, log: BoundLogger | None                                                                                                                                  | —                                                                            | activity: io=json.dumps; logging=log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                           | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_assay                      | self                                                                                                                                                                       | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_compound_record            | self                                                                                                                                                                       | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_data_validity              | self                                                                                                                                                                       | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._should_enrich_molecule                   | self                                                                                                                                                                       | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._store_cache                              | self, batch_ids: Sequence[str], batch_data: Mapping[str, Mapping[str, Any]], release: str | None                                                                           | —                                                                            | activity: io=json.dumps, tmp_path.write_text; logging=UnifiedLogger.get, log.debug<br>testitem: ∅                                                                                                                                                                                                                                                                     | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._validate_activity_properties_truv        | self, properties: list[dict[str, Any]], log: BoundLogger, activity_id: Any | None                                                                                          | —                                                                            | activity: io=∅; logging=log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._validate_data_validity_comment_soft_enum | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline._validate_foreign_keys                    | self, df: pd.DataFrame, log: BoundLogger                                                                                                                                   | —                                                                            | activity: io=∅; logging=log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline.build_quality_report                      | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline.chembl_release                            | self                                                                                                                                                                       | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline.extract                                   | self, *args, **kwargs                                                                                                                                                      | —                                                                            | activity: io=∅; logging=UnifiedLogger.get, bound_log.warning<br>testitem: ∅                                                                                                                                                                                                                                                                                           | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline.extract_all                               | self                                                                                                                                                                       | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline.extract_by_ids                            | self, ids: Sequence[str]                                                                                                                                                   | —                                                                            | activity: io=∅; logging=UnifiedLogger.get, log.info, log.warning, self._log_validity_comments_metrics<br>testitem: ∅                                                                                                                                                                                                                                                  | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline.transform                                 | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.info<br>testitem: ∅                                                                                                                                                                                                                                                                                         | activity: ∅<br>testitem: ∅                               | only in activity
ChemblActivityPipeline.validate                                  | self, df: pd.DataFrame                                                                                                                                                     | —                                                                            | activity: io=∅; logging=UnifiedLogger.get, log.debug, log.error, log.info, self._log_detailed_validation_errors<br>testitem: ∅                                                                                                                                                                                                                                        | activity: ValueError(msg)<br>testitem: ∅                 | only in activity
ChemblActivityPipeline.write                                     | self, df: pd.DataFrame, output_path: Path, *, extended: bool = False, include_correlation: bool | None = None, include_qc_metrics: bool | None = None                      | —                                                                            | activity: io=∅; logging=UnifiedLogger.bind, UnifiedLogger.get, log.debug<br>testitem: ∅                                                                                                                                                                                                                                                                               | activity: ∅<br>testitem: ∅                               | only in activity
TestItemChemblPipeline                                           | —                                                                                                                                                                          | —                                                                            | activity: ∅<br>testitem: io=status_payload.get; logging=UnifiedLogger.get, bound_log.info, bound_log.warning, log.debug, log.info, log.warning                                                                                                                                                                                                                        | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline.__init__                                  | —                                                                                                                                                                          | self, config: PipelineConfig, run_id: str                                    | activity: ∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline._build_testitem_descriptor                | —                                                                                                                                                                          | self: SelfTestitemChemblPipeline                                             | activity: ∅<br>testitem: io=∅; logging=log.debug, log.info                                                                                                                                                                                                                                                                                                            | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline._check_empty_columns                      | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: ∅<br>testitem: io=∅; logging=log.warning                                                                                                                                                                                                                                                                                                                    | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline._deduplicate_molecules                    | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: ∅<br>testitem: io=∅; logging=log.info                                                                                                                                                                                                                                                                                                                       | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline._fetch_chembl_release                     | —                                                                                                                                                                          | self, client: UnifiedAPIClient | ChemblClient | Any, log: BoundLogger | None | activity: ∅<br>testitem: io=status_payload.get; logging=UnifiedLogger.get, bound_log.info, bound_log.warning                                                                                                                                                                                                                                                          | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline._flatten_nested_structures                | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: ∅<br>testitem: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline._normalize_identifiers                    | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: ∅<br>testitem: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline._normalize_numeric_fields                 | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: ∅<br>testitem: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline._normalize_string_fields                  | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: ∅<br>testitem: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline._remove_extra_columns                     | —                                                                                                                                                                          | self, df: pd.DataFrame, log: Any                                             | activity: ∅<br>testitem: io=∅; logging=log.debug                                                                                                                                                                                                                                                                                                                      | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline._schema_column_specs                      | —                                                                                                                                                                          | self                                                                         | activity: ∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline.api_version                               | —                                                                                                                                                                          | self                                                                         | activity: ∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline.augment_metadata                          | —                                                                                                                                                                          | self, metadata: Mapping[str, object], df: pd.DataFrame                       | activity: ∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline.chembl_db_version                         | —                                                                                                                                                                          | self                                                                         | activity: ∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline.extract                                   | —                                                                                                                                                                          | self, *args, **kwargs                                                        | activity: ∅<br>testitem: io=∅; logging=UnifiedLogger.get                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline.extract_all                               | —                                                                                                                                                                          | self                                                                         | activity: ∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline.extract_by_ids                            | —                                                                                                                                                                          | self, ids: Sequence[str]                                                     | activity: ∅<br>testitem: io=∅; logging=UnifiedLogger.get, log.debug, log.info                                                                                                                                                                                                                                                                                         | activity: ∅<br>testitem: ∅                               | only in testitem
TestItemChemblPipeline.transform                                 | —                                                                                                                                                                          | self, df: pd.DataFrame                                                       | activity: ∅<br>testitem: io=∅; logging=UnifiedLogger.get, log.debug, log.info                                                                                                                                                                                                                                                                                         | activity: ∅<br>testitem: ∅                               | only in testitem
__module_block_0                                                 | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_1                                                 | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_10                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_11                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_12                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_13                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_14                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_15                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_16                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_17                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_18                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_19                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
__module_block_2                                                 | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_20                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
__module_block_21                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
__module_block_22                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
__module_block_23                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
__module_block_24                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
__module_block_25                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
__module_block_26                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
__module_block_27                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
__module_block_28                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
__module_block_29                                                | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: ∅                                                                                                                                                                                                                                                                                                                              | activity: ∅<br>testitem: ∅                               | only in activity
__module_block_3                                                 | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_4                                                 | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_5                                                 | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_6                                                 | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_7                                                 | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_8                                                 | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         
__module_block_9                                                 | —                                                                                                                                                                          | —                                                                            | activity: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                                                                                                                                                                                                                                                | activity: ∅<br>testitem: ∅                               | differs         

#### Hotspot 1

- Definition: ChemblActivityPipeline#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:106-3476
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,1684 +0,0 @@
-class ChemblActivityPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting activity records from the ChEMBL API."
-    actor ="activity_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-        self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch activity payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        def legacy_activity_ids (bound_log :BoundLogger )->Sequence [str ]|None :
-            payload_activity_ids =kwargs .get ("activity_ids")
-            if payload_activity_ids is None :
-                return None
-            bound_log .warning ("chembl_activity.deprecated_kwargs",message ="Using activity_ids in kwargs is deprecated. Use --input-file instead.")
-            if isinstance (payload_activity_ids ,Sequence )and (not isinstance (payload_activity_ids ,(str ,bytes ))):
-                sequence_ids :Sequence [str |int ]=cast (Sequence [str |int ],payload_activity_ids )
-                return [str (id_val )for id_val in sequence_ids ]
-            return [str (payload_activity_ids )]
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_activity.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="activity_id",legacy_id_resolver =legacy_activity_ids ,legacy_source ="deprecated_kwargs")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all activity records from ChEMBL using the shared iterator."
-        return self .run_extract_all (self ._build_activity_descriptor ())
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract activity records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of activity_id values to extract (as strings or integers).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted activity records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_config ,chembl_client ,activity_iterator ,select_fields =self ._prepare_activity_iteration ()
-        limit =self .config .cli .limit
-        invalid_ids :list [Any ]=[]
-        def normalize_activity_id (raw :Any )->tuple [str |None ,Any ]:
-            if pd .isna (raw ):
-                return (None ,None )
-            try :
-                if isinstance (raw ,str ):
-                    candidate =raw .strip ()
-                    if not candidate :
-                        return (None ,None )
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw ,(int ,float )):
-                    numeric_id =int (raw )
-                else :
-                    numeric_id =int (raw )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw )
-                return (None ,None )
-            return (str (numeric_id ),int (numeric_id ))
-        def delegated_fetch (canonical_ids :Sequence [str ],context :BatchExtractionContext )->tuple [Sequence [Mapping [str ,Any ]],Mapping [str ,Any ]]:
-            numeric_map =context .metadata
-            normalized_ids :list [tuple [int ,str ]]=[]
-            for identifier in canonical_ids :
-                numeric_value =numeric_map .get (identifier )
-                if numeric_value is None :
-                    continue
-                normalized_ids .append ((int (numeric_value ),identifier ))
-            if not normalized_ids :
-                summary ={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-                context .extra ["delegated_summary"]=summary
-                return ([],summary )
-            records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =context .select_fields or None )
-            context .extra ["delegated_summary"]=summary
-            return (records ,summary )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            dataframe =self ._ensure_comment_fields (dataframe ,log )
-            dataframe =self ._extract_data_validity_descriptions (dataframe ,chembl_client ,log )
-            dataframe =self ._extract_assay_fields (dataframe ,chembl_client ,log )
-            self ._log_validity_comments_metrics (dataframe ,log )
-            return dataframe
-        def empty_activity_frame ()->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def finalize_context (context :BatchExtractionContext )->None :
-            summary =context .extra .get ("delegated_summary")
-            if isinstance (summary ,Mapping ):
-                summary_dict =dict (summary )
-                context .extra ["stats_attribute_override"]=summary_dict
-                self ._last_batch_extract_stats =summary_dict
-            else :
-                context .extra ["stats_attribute_override"]=context .stats .as_dict ()
-                self ._last_batch_extract_stats =context .stats .as_dict ()
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="activity_id",fetcher =delegated_fetch ,select_fields =select_fields ,batch_size =source_config .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release ,id_normalizer =normalize_activity_id ,sort_key =lambda pair :int (pair [0 ]),finalize =finalize_dataframe ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats",fetch_mode ="delegated",empty_frame_factory =empty_activity_frame )
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        batch_stats =self ._last_batch_extract_stats or {}
-        log .info ("chembl_activity.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =batch_stats .get ("batches"),api_calls =batch_stats .get ("api_calls"),cache_hits =batch_stats .get ("cache_hits"))
-        return dataframe
-    def _prepare_activity_iteration (self ,*,client_name :str ="chembl_activity_client")->tuple [ActivitySourceConfig ,ChemblClient ,ChemblActivityClient ,list [str ]]:
-        "Construct reusable ChEMBL clients and iterator for activity extraction."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name =client_name )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =self ._resolve_select_fields (source_raw ,default_fields =API_ACTIVITY_FIELDS )
-        return (source_config ,chembl_client ,activity_iterator ,select_fields )
-    def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-        "Construct the descriptor driving the shared extraction template."
-        def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-            http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-            chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-            typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-            activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-            select_fields =source_config .parameters .select_fields
-            return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-        def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            df =pipeline ._ensure_comment_fields (df ,log )
-            chembl_client =cast (ChemblClient ,context .chembl_client )
-            if chembl_client is not None :
-                df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-            pipeline ._log_validity_comments_metrics (df ,log )
-            return df
-        def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            return pipeline ._materialize_activity_record (payload )
-        return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
-    def _materialize_activity_record (self ,payload :Mapping [str ,Any ],*,activity_id :int |None =None )->dict [str ,Any ]:
-        "Normalize nested fields within an activity payload."
-        record =dict (payload )
-        record =self ._extract_nested_fields (record )
-        record =self ._extract_activity_properties_fields (record )
-        if activity_id is not None :
-            record .setdefault ("activity_id",activity_id )
-        return record
-    def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-        "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-        if isinstance (dataset ,pd .Series ):
-            return dataset .to_frame (name ="activity_id")
-        if isinstance (dataset ,pd .DataFrame ):
-            return dataset
-        if isinstance (dataset ,Mapping ):
-            mapping =cast (Mapping [str ,Any ],dataset )
-            return pd .DataFrame ([dict (mapping )])
-        if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-            dataset_list :list [Any ]=list (dataset )
-            return pd .DataFrame ({"activity_id":dataset_list })
-        msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-        raise TypeError (msg )
-    def _normalize_activity_ids (self ,input_frame :pd .DataFrame ,*,limit :int |None ,log :BoundLogger )->list [tuple [int ,str ]]:
-        "Normalize raw identifier values into deduplicated integer/string pairs."
-        normalized_ids :list [tuple [int ,str ]]=[]
-        invalid_ids :list [Any ]=[]
-        seen :set [str ]=set ()
-        for raw_id in input_frame ["activity_id"].tolist ():
-            if pd .isna (raw_id ):
-                continue
-            try :
-                if isinstance (raw_id ,str ):
-                    candidate =raw_id .strip ()
-                    if not candidate :
-                        continue
-                    numeric_id =int (float (candidate ))if "."in candidate else int (candidate )
-                elif isinstance (raw_id ,(int ,float )):
-                    numeric_id =int (raw_id )
-                else :
-                    numeric_id =int (raw_id )
-            except (TypeError ,ValueError ):
-                invalid_ids .append (raw_id )
-                continue
-            key =str (numeric_id )
-            if key not in seen :
-                seen .add (key )
-                normalized_ids .append ((numeric_id ,key ))
-        if invalid_ids :
-            log .warning ("chembl_activity.invalid_activity_ids",invalid_count =len (invalid_ids ))
-        if limit is not None :
-            normalized_ids =normalized_ids [:max (int (limit ),0 )]
-        return normalized_ids
-    def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-        "Iterate over IDs using the shared iterator while preserving cache semantics."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        records :list [dict [str ,Any ]]=[]
-        success_count =0
-        fallback_count =0
-        error_count =0
-        cache_hits =0
-        api_calls =0
-        total_batches =0
-        key_order =[key for _ ,key in normalized_ids ]
-        key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-        for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-            total_batches +=1
-            batch_start =time .perf_counter ()
-            from_cache =False
-            chunk_records :dict [str ,dict [str ,Any ]]={}
-            try :
-                cached_records =self ._check_cache (chunk ,self ._chembl_release )
-                if cached_records is not None :
-                    from_cache =True
-                    cache_hits +=len (chunk )
-                    chunk_records =cached_records
-                else :
-                    api_calls +=1
-                    fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                    for item in fetched_items :
-                        if not isinstance (item ,Mapping ):
-                            continue
-                        activity_value =item .get ("activity_id")
-                        if activity_value is None :
-                            continue
-                        chunk_records [str (activity_value )]=dict (item )
-                    self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-                success_in_batch =0
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    record =chunk_records .get (key )
-                    if record and (not record .get ("error")):
-                        materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                        records .append (materialized )
-                        success_count +=1
-                        success_in_batch +=1
-                    else :
-                        fallback_record =self ._create_fallback_record (numeric_id )
-                        records .append (fallback_record )
-                        fallback_count +=1
-                        error_count +=1
-                batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-                log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-            except CircuitBreakerOpenError as exc :
-                log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except RequestException as exc :
-                log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-            except Exception as exc :
-                log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-                for key in chunk :
-                    numeric_id =key_to_numeric .get (key )
-                    if numeric_id is None :
-                        continue
-                    records .append (self ._create_fallback_record (numeric_id ,exc ))
-                    fallback_count +=1
-                    error_count +=1
-        total_records =len (normalized_ids )
-        success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-        summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-        return (records ,summary )
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw activity data by normalizing measurements, identifiers, and data types."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_measurements (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,ActivitySchema ,log )
-        if "curated"in df .columns or "curated_by"in df .columns :
-            if "curated"not in df .columns :
-                df ["curated"]=pd .NA
-            if "curated_by"in df .columns :
-                mask =df ["curated"].isna ()
-                df .loc [mask ,"curated"]=df .loc [mask ,"curated_by"].notna ()
-            df ["curated"]=df ["curated"].astype ("boolean")
-        df =self ._validate_foreign_keys (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if self ._should_enrich_compound_record ():
-            df =self ._enrich_compound_record (df )
-        if self ._should_enrich_assay ():
-            df =self ._enrich_assay (df )
-        if self ._should_enrich_molecule ():
-            df =self ._enrich_molecule (df )
-        if self ._should_enrich_data_validity ():
-            df =self ._enrich_data_validity (df )
-        df =self ._finalize_identifier_columns (df ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        df =self ._finalize_output_columns (df ,log )
-        df =self ._filter_invalid_required_fields (df ,log )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against ActivitySchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        if "target_tax_id"in df .columns :
-            dtype_name :str =str (df ["target_tax_id"].dtype .name )
-            if dtype_name !="Int64":
-                numeric_series :pd .Series [Any ]=pd .to_numeric (df ["target_tax_id"],errors ="coerce")
-                df ["target_tax_id"]=numeric_series .astype ("Int64")
-        self ._check_activity_id_uniqueness (df ,log )
-        self ._check_foreign_key_integrity (df ,log )
-        self ._validate_data_validity_comment_soft_enum (df ,log )
-        original_coerce =self .config .validation .coerce
-        try :
-            self .config .validation .coerce =False
-            validated =super ().validate (df )
-            log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce )
-            return validated
-        except pandera .errors .SchemaErrors as exc :
-            failure_cases_df :pd .DataFrame |None =None
-            if hasattr (exc ,"failure_cases"):
-                failure_cases_df =cast (pd .DataFrame ,exc .failure_cases )
-            error_count =len (failure_cases_df )if failure_cases_df is not None else 0
-            error_summary =summarize_schema_errors (exc )
-            log .error ("validation_failed",error_count =error_count ,schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =original_coerce ,error_summary =error_summary ,exc_info =True )
-            if failure_cases_df is not None and (not failure_cases_df .empty ):
-                failure_cases_summary =format_failure_cases (failure_cases_df )
-                log .error ("validation_failure_cases",failure_cases =failure_cases_summary )
-                self ._log_detailed_validation_errors (failure_cases_df ,df ,log )
-            msg =f'Validation failed with {error_count } error(s) against schema {self .config .validation .schema_out }: {error_summary }'
-            raise ValueError (msg )from exc
-        except Exception as exc :
-            log .error ("validation_error",error =str (exc ),schema =self .config .validation .schema_out ,exc_info =True )
-            raise
-        finally :
-            self .config .validation .coerce =original_coerce
-    def _should_enrich_compound_record (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 compound_record \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            compound_record_section :Any =enrich_section .get ("compound_record")
-            if not isinstance (compound_record_section ,Mapping ):
-                return False
-            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-            enabled :Any =compound_record_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        compound_record_section :Any =enrich_section .get ("compound_record")
-                        if isinstance (compound_record_section ,Mapping ):
-                            compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                            enrich_cfg =dict (compound_record_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_assay (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 assay \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            assay_section :Any =enrich_section .get ("assay")
-            if not isinstance (assay_section ,Mapping ):
-                return False
-            assay_section =cast (Mapping [str ,Any ],assay_section )
-            enabled :Any =assay_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        assay_section :Any =enrich_section .get ("assay")
-                        if isinstance (assay_section ,Mapping ):
-                            assay_section =cast (Mapping [str ,Any ],assay_section )
-                            enrich_cfg =dict (assay_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_assay (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_data_validity (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 data_validity \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            data_validity_section :Any =enrich_section .get ("data_validity")
-            if not isinstance (data_validity_section ,Mapping ):
-                return False
-            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-            enabled :Any =data_validity_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        data_validity_section :Any =enrich_section .get ("data_validity")
-                        if isinstance (data_validity_section ,Mapping ):
-                            data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                            enrich_cfg =dict (data_validity_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        if "data_validity_description"in df .columns :
-            non_na_count =int (df ["data_validity_description"].notna ().sum ())
-            if non_na_count >0 :
-                log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
-    def _should_enrich_molecule (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 molecule \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if not isinstance (activity_section ,Mapping ):
-                return False
-            activity_section =cast (Mapping [str ,Any ],activity_section )
-            enrich_section :Any =activity_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            molecule_section :Any =enrich_section .get ("molecule")
-            if not isinstance (molecule_section ,Mapping ):
-                return False
-            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-            enabled :Any =molecule_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                activity_section :Any =chembl_section .get ("activity")
-                if isinstance (activity_section ,Mapping ):
-                    activity_section =cast (Mapping [str ,Any ],activity_section )
-                    enrich_section :Any =activity_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        molecule_section :Any =enrich_section .get ("molecule")
-                        if isinstance (molecule_section ,Mapping ):
-                            molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                            enrich_cfg =dict (molecule_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =ActivitySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        if "chembl_enrichment_client"not in self ._registered_clients :
-            self .register_client ("chembl_enrichment_client",api_client )
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-        if "molecule_name"in df_join .columns :
-            if "molecule_pref_name"not in df .columns :
-                df ["molecule_pref_name"]=pd .NA
-            mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-            if mask .any ():
-                df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-                df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-                log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-        return df
-    def _extract_from_chembl (self ,dataset :object ,chembl_client :ChemblClient |Any ,activity_iterator :ChemblActivityClient ,*,limit :int |None =None ,select_fields :Sequence [str ]|None =None )->pd .DataFrame :
-        "Extract activity records by delegating batching to ``ChemblActivityClient``."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        method_start =time .perf_counter ()
-        self ._last_batch_extract_stats =None
-        input_frame =self ._coerce_activity_dataset (dataset )
-        if "activity_id"not in input_frame .columns :
-            msg ="Input dataset must contain an 'activity_id' column"
-            raise ValueError (msg )
-        normalized_ids =self ._normalize_activity_ids (input_frame ,limit =limit ,log =log )
-        if not normalized_ids :
-            summary :dict [str ,Any ]={"total_activities":0 ,"success":0 ,"fallback":0 ,"errors":0 ,"api_calls":0 ,"cache_hits":0 ,"batches":0 ,"duration_ms":0.0 ,"success_rate":0.0 }
-            self ._last_batch_extract_stats =summary
-            log .info ("chembl_activity.batch_summary",**summary )
-            empty_frame =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-            return self ._ensure_comment_fields (empty_frame ,log )
-        records ,summary =self ._collect_records_by_ids (normalized_ids ,activity_iterator ,select_fields =select_fields )
-        duration_ms =(time .perf_counter ()-method_start )*1000.0
-        summary ["duration_ms"]=duration_ms
-        self ._last_batch_extract_stats =summary
-        log .info ("chembl_activity.batch_summary",**summary )
-        result_df :pd .DataFrame =pd .DataFrame .from_records (records )
-        if result_df .empty :
-            result_df =pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-        elif "activity_id"in result_df .columns :
-            result_df =result_df .sort_values ("activity_id").reset_index (drop =True )
-        result_df =self ._ensure_comment_fields (result_df ,log )
-        result_df =self ._extract_data_validity_descriptions (result_df ,chembl_client ,log )
-        result_df =self ._extract_assay_fields (result_df ,chembl_client ,log )
-        self ._log_validity_comments_metrics (result_df ,log )
-        return result_df
-    def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-        cache_config =self .config .cache
-        if not cache_config .enabled :
-            return None
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        if not cache_file .exists ():
-            return None
-        try :
-            stat =cache_file .stat ()
-        except OSError :
-            return None
-        ttl_seconds =int (cache_config .ttl )
-        if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        try :
-            payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-        except (OSError ,json .JSONDecodeError ):
-            try :
-                cache_file .unlink (missing_ok =True )
-            except OSError :
-                pass
-            return None
-        if not isinstance (payload ,dict ):
-            return None
-        missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-        if missing :
-            return None
-        return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
-    def _store_cache (self ,batch_ids :Sequence [str ],batch_data :Mapping [str ,Mapping [str ,Any ]],release :str |None )->None :
-        cache_config =self .config .cache
-        if not cache_config .enabled or not batch_ids or (not batch_data ):
-            return
-        normalized_ids =[str (identifier )for identifier in batch_ids ]
-        cache_file =self ._cache_file_path (normalized_ids ,release )
-        normalized_set =set (normalized_ids )
-        data_to_store ={key :batch_data [key ]for key in normalized_set if key in batch_data }
-        if not data_to_store :
-            return
-        try :
-            cache_file .parent .mkdir (parents =True ,exist_ok =True )
-            tmp_path =cache_file .with_suffix (cache_file .suffix +".tmp")
-            tmp_path .write_text (json .dumps (data_to_store ,sort_keys =True ,default =str ),encoding ="utf-8")
-            tmp_path .replace (cache_file )
-        except Exception as exc :
-            log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-            log .debug ("chembl_activity.cache_store_failed",error =str (exc ))
-    def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-        directory =self ._cache_directory (release )
-        cache_key =self ._cache_key (batch_ids ,release )
-        return directory /f'{cache_key }.json'
-    def _cache_directory (self ,release :str |None )->Path :
-        cache_root =Path (self .config .paths .cache_root )
-        directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-        release_component =self ._sanitize_cache_component (release or "unknown")
-        pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-        version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-        return cache_root /directory_name /pipeline_component /release_component /version_component
-    def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-        payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-        raw =json .dumps (payload ,sort_keys =True )
-        return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
-    @staticmethod
-    def _sanitize_cache_component (value :str )->str :
-        sanitized =re .sub ("[^0-9A-Za-z_.-]","_",value )
-        return sanitized or "default"
-    def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-        "Create fallback record enriched with error metadata."
-        base_message ="Fallback: ChEMBL activity unavailable"
-        message =f'{base_message } ({error })'if error else base_message
-        timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-        metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-        if isinstance (error ,RequestException ):
-            response =getattr (error ,"response",None )
-            status_code =getattr (response ,"status_code",None )
-            if status_code is not None :
-                metadata ["http_status"]=status_code
-            metadata ["error_message"]=str (error )
-        elif error is not None :
-            metadata ["error_message"]=str (error )
-        fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-        return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
-    def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-        if df .empty :
-            return df
-        required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-        if missing_fields :
-            for field in missing_fields :
-                df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            log .debug ("comment_fields_ensured",fields =missing_fields )
-        return df
-    def _extract_data_validity_descriptions (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c data_validity_description \u0438\u0437 DATA_VALIDITY_LOOKUP \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f data_validity_comment \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_data_validity_lookup() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c data_validity_comment.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u043e\u0439 data_validity_description.\n        "
-        if df .empty :
-            return df
-        if "data_validity_comment"not in df .columns :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="data_validity_comment_column_missing")
-            return df
-        validity_comments :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            comment =row .get ("data_validity_comment")
-            if pd .isna (comment )or comment is None :
-                continue
-            comment_str =str (comment ).strip ()
-            if comment_str :
-                validity_comments .append (comment_str )
-        if not validity_comments :
-            log .debug ("extract_data_validity_descriptions_skipped",reason ="no_valid_comments")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        unique_comments =list (set (validity_comments ))
-        log .info ("extract_data_validity_descriptions_fetching",comments_count =len (unique_comments ))
-        try :
-            records_dict =client .fetch_data_validity_lookup (comments =unique_comments ,fields =["data_validity_comment","description"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_data_validity_descriptions_fetch_error",error =str (exc ),exc_info =True )
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for comment in unique_comments :
-            record =records_dict .get (comment )
-            if record :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":record .get ("description")})
-            else :
-                enrichment_data .append ({"data_validity_comment":comment ,"data_validity_description":None })
-        if not enrichment_data :
-            log .debug ("extract_data_validity_descriptions_no_records")
-            if "data_validity_description"not in df .columns :
-                df ["data_validity_description"]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        original_index =df .index .copy ()
-        df_result =df .merge (df_enrich ,on =["data_validity_comment"],how ="left",suffixes =("","_enrich"))
-        if "data_validity_description"not in df_result .columns :
-            df_result ["data_validity_description"]=pd .Series ([pd .NA ]*len (df_result ),dtype ="string")
-        else :
-            df_result ["data_validity_description"]=df_result ["data_validity_description"].astype ("string")
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_data_validity_descriptions_complete",comments_requested =len (unique_comments ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _extract_assay_fields (self ,df :pd .DataFrame ,client :ChemblClient ,log :BoundLogger )->pd .DataFrame :
-        "\u0418\u0437\u0432\u043b\u0435\u0447\u044c assay_organism \u0438 assay_tax_id \u0438\u0437 ASSAYS \u0447\u0435\u0440\u0435\u0437 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441.\n\n        \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0435\u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f assay_chembl_id \u0438\u0437 DataFrame,\n        \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442 fetch_assays_by_ids() \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 LEFT JOIN \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043a DataFrame.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity, \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c assay_chembl_id.\n        client:\n            ChemblClient \u0434\u043b\u044f \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a ChEMBL API.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u044b\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438 assay_organism \u0438 assay_tax_id.\n        "
-        if df .empty :
-            return df
-        if "assay_chembl_id"not in df .columns :
-            log .debug ("extract_assay_fields_skipped",reason ="assay_chembl_id_column_missing")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        assay_ids :list [str ]=[]
-        for _ ,row in df .iterrows ():
-            assay_id =row .get ("assay_chembl_id")
-            if pd .isna (assay_id )or assay_id is None :
-                continue
-            assay_id_str =str (assay_id ).strip ().upper ()
-            if assay_id_str :
-                assay_ids .append (assay_id_str )
-        if not assay_ids :
-            log .debug ("extract_assay_fields_skipped",reason ="no_valid_assay_ids")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        unique_assay_ids =list (set (assay_ids ))
-        log .info ("extract_assay_fields_fetching",assay_ids_count =len (unique_assay_ids ))
-        try :
-            records_dict =client .fetch_assays_by_ids (ids =unique_assay_ids ,fields =["assay_chembl_id","assay_organism","assay_tax_id"],page_limit =1000 )
-        except Exception as exc :
-            log .warning ("extract_assay_fields_fetch_error",error =str (exc ),exc_info =True )
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        enrichment_data :list [dict [str ,Any ]]=[]
-        for assay_id in unique_assay_ids :
-            record =records_dict .get (assay_id )if records_dict else None
-            if record :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":record .get ("assay_organism"),"assay_tax_id":record .get ("assay_tax_id")})
-            else :
-                enrichment_data .append ({"assay_chembl_id":assay_id ,"assay_organism":None ,"assay_tax_id":None })
-        if not enrichment_data :
-            log .debug ("extract_assay_fields_no_records")
-            for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-                if col not in df .columns :
-                    df [col ]=pd .Series ([pd .NA ]*len (df ),dtype =dtype )
-            return df
-        df_enrich =pd .DataFrame (enrichment_data )
-        df_enrich ["assay_chembl_id"]=df_enrich ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        original_index =df .index .copy ()
-        df_normalized =df .copy ()
-        df_normalized ["assay_chembl_id_normalized"]=df_normalized ["assay_chembl_id"].astype ("string").str .strip ().str .upper ()
-        df_result =df_normalized .merge (df_enrich ,left_on ="assay_chembl_id_normalized",right_on ="assay_chembl_id",how ="left",suffixes =("","_enrich"))
-        df_result =df_result .drop (columns =["assay_chembl_id_normalized"])
-        for col in ["assay_organism","assay_tax_id"]:
-            if f'{col }_enrich'in df_result .columns :
-                if col not in df_result .columns :
-                    df_result [col ]=df_result [f'{col }_enrich']
-                else :
-                    base_series :pd .Series [Any ]=df_result [col ]
-                    enrich_series :pd .Series [Any ]=df_result [f'{col }_enrich']
-                    missing_mask =base_series .isna ()
-                    if bool (missing_mask .any ()):
-                        df_result .loc [missing_mask ,col ]=enrich_series .loc [missing_mask ]
-                df_result =df_result .drop (columns =[f'{col }_enrich'])
-        for col ,dtype in (("assay_organism","string"),("assay_tax_id","Int64")):
-            if col not in df_result .columns :
-                df_result [col ]=pd .Series ([pd .NA ]*len (df_result ),dtype =dtype )
-        df_result ["assay_organism"]=df_result ["assay_organism"].astype ("string")
-        df_result ["assay_tax_id"]=pd .to_numeric (df_result ["assay_tax_id"],errors ="coerce").astype ("Int64")
-        mask_valid =df_result ["assay_tax_id"].notna ()
-        if mask_valid .any ():
-            invalid_mask =mask_valid &(df_result ["assay_tax_id"]<1 )
-            if invalid_mask .any ():
-                log .warning ("invalid_assay_tax_id_range",count =int (invalid_mask .sum ()))
-                df_result .loc [invalid_mask ,"assay_tax_id"]=pd .NA
-        df_result =df_result .reindex (original_index )
-        log .info ("extract_assay_fields_complete",assay_ids_requested =len (unique_assay_ids ),records_fetched =len (enrichment_data ),rows_enriched =len (df_result ))
-        return df_result
-    def _log_validity_comments_metrics (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "\u041b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442:\n        - \u0414\u043e\u043b\u044e NA \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0442\u0440\u0435\u0445 \u043f\u043e\u043b\u0435\u0439\n        - \u0422\u043e\u043f-10 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment\n        - \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 data_validity_comment (\u043d\u0435 \u0432 whitelist)\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 activity.\n        log:\n            Logger instance.\n        "
-        if df .empty :
-            return
-        metrics :dict [str ,Any ]={}
-        comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-        for field in comment_fields :
-            if field in df .columns :
-                na_count =int (df [field ].isna ().sum ())
-                total_count =len (df )
-                na_rate =float (na_count )/float (total_count )if total_count >0 else 0.0
-                metrics [f'{field }_na_rate']=na_rate
-                metrics [f'{field }_na_count']=na_count
-                metrics [f'{field }_total_count']=total_count
-        non_null_comments_series :pd .Series [str ]|None =None
-        if "data_validity_comment"in df .columns :
-            series_candidate =df ["data_validity_comment"].dropna ()
-            if len (series_candidate )>0 :
-                typed_series :pd .Series [str ]=series_candidate .astype ("string")
-                non_null_comments_series =typed_series
-                value_counts =typed_series .value_counts ().head (10 )
-                top_10 ={str (key ):int (value )for key ,value in value_counts .items ()}
-                metrics ["top_10_data_validity_comments"]=top_10
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if whitelist and non_null_comments_series is not None :
-            whitelist_set :set [str ]=set (whitelist )
-            def _is_unknown (value :str )->bool :
-                return value not in whitelist_set
-            unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-            unknown_count =int (unknown_mask .sum ())
-            if unknown_count >0 :
-                unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-                metrics ["unknown_data_validity_comments_count"]=unknown_count
-                metrics ["unknown_data_validity_comments_samples"]=unknown_values
-                log .warning ("unknown_data_validity_comments_detected",unknown_count =unknown_count ,samples =unknown_values ,whitelist =whitelist )
-        if metrics :
-            log .info ("validity_comments_metrics",**metrics )
-    def _get_data_validity_comment_whitelist (self )->list [str ]:
-        "\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c whitelist \u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0434\u043b\u044f data_validity_comment \u0438\u0437 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\n\n        Raises\n        ------\n        RuntimeError\n            \u0415\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d \u0438\u043b\u0438 \u043f\u0443\u0441\u0442.\n        "
-        try :
-            values =sorted (self ._required_vocab_ids ("data_validity_comment"))
-        except RuntimeError as exc :
-            UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validation',run_id =self .run_id ).error ("data_validity_comment_whitelist_unavailable",error =str (exc ))
-            raise
-        return values
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested assay and molecule objects."
-        if "assay"in record and isinstance (record ["assay"],Mapping ):
-            assay =cast (Mapping [str ,Any ],record ["assay"])
-            if "organism"in assay :
-                record .setdefault ("assay_organism",assay ["organism"])
-            if "tax_id"in assay :
-                record .setdefault ("assay_tax_id",assay ["tax_id"])
-        if "molecule"in record and isinstance (record ["molecule"],Mapping ):
-            molecule =cast (Mapping [str ,Any ],record ["molecule"])
-            if "pref_name"in molecule :
-                record .setdefault ("molecule_pref_name",molecule ["pref_name"])
-        if "curated_by"in record :
-            curated_by =record .get ("curated_by")
-            if curated_by is not None and (not pd .isna (curated_by )):
-                record .setdefault ("curated",True )
-            else :
-                record .setdefault ("curated",False )
-        elif "curated"not in record :
-            record .setdefault ("curated",None )
-        return record
-    def _extract_activity_properties_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract TRUV fields, standard_* fields, and comments from activity_properties array as fallback.\n\n        \u041f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0438\u0437 activity_properties \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442\n        \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043e\u0442\u0432\u0435\u0442\u0435 API (\u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 \u043f\u0440\u044f\u043c\u044b\u0445 \u043f\u043e\u043b\u0435\u0439 \u0438\u0437 ACTIVITIES).\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0435 TRUV-\u043f\u043e\u043b\u044f: value, text_value, relation, units.\n        \u0422\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044f: standard_upper_value, standard_text_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b: upper_value, lower_value.\n        \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0438: activity_comment, data_validity_comment.\n\n        \u0422\u0430\u043a\u0436\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 activity_properties \u0432 \u0437\u0430\u043f\u0438\u0441\u0438 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438.\n        \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044e \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract_activity_properties')
-        activity_id =record .get ("activity_id")
-        if "activity_properties"not in record :
-            log .warning ("activity_properties_missing",activity_id =activity_id ,message ="activity_properties not found in API response (possible ChEMBL < v24)")
-            record ["activity_properties"]=None
-            return record
-        properties =record ["activity_properties"]
-        if properties is None :
-            log .debug ("activity_properties_null",activity_id =activity_id ,message ="activity_properties is None (possible ChEMBL < v24)")
-            return record
-        if isinstance (properties ,str ):
-            try :
-                properties =json .loads (properties )
-            except (TypeError ,ValueError ,json .JSONDecodeError )as exc :
-                log .debug ("activity_properties_parse_failed",error =str (exc ),activity_id =record .get ("activity_id"))
-                return record
-        if not isinstance (properties ,Sequence )or isinstance (properties ,(str ,bytes )):
-            return record
-        property_iterable :Iterable [Any ]=cast (Iterable [Any ],properties )
-        property_items :list [Any ]=list (property_iterable )
-        def _set_fallback (key :str ,value :Any )->None :
-            "Set fallback value only if key is missing in record and value is not None."
-            if value is not None and record .get (key )is None :
-                record [key ]=value
-        def _is_empty (value :Any )->bool :
-            "Check if value is empty (None, empty string, or whitespace)."
-            if value is None :
-                return True
-            if isinstance (value ,str ):
-                return not value .strip ()
-            return False
-        items :list [Mapping [str ,Any ]]=[]
-        for property_item in property_items :
-            if isinstance (property_item ,Mapping )and "type"in property_item and ("value"in property_item or "text_value"in property_item ):
-                items .append (cast (Mapping [str ,Any ],property_item ))
-        def _is_measured (p :Mapping [str ,Any ])->bool :
-            rf =p .get ("result_flag")
-            return rf is True or (isinstance (rf ,int )and rf ==1 )
-        items .sort (key =lambda p :not _is_measured (p ))
-        for prop in items :
-            val =prop .get ("value")
-            txt =prop .get ("text_value")
-            rel =prop .get ("relation")
-            unt =prop .get ("units")
-            prop_type =str (prop .get ("type","")).lower ()
-            will_set_value =val is not None and record .get ("value")is None
-            will_set_text_value =txt is not None and record .get ("text_value")is None
-            _set_fallback ("value",val )
-            _set_fallback ("text_value",txt )
-            if will_set_value or will_set_text_value :
-                _set_fallback ("relation",rel )
-                _set_fallback ("units",unt )
-            if unt is not None and record .get ("units")is None :
-                _set_fallback ("units",unt )
-            if record .get ("upper_value")is None and ("upper"in prop_type or prop_type in ("upper_value","upper limit")):
-                if val is not None :
-                    _set_fallback ("upper_value",val )
-            if record .get ("lower_value")is None and ("lower"in prop_type or prop_type in ("lower_value","lower limit")):
-                if val is not None :
-                    _set_fallback ("lower_value",val )
-            if record .get ("standard_upper_value")is None and ("standard_upper"in prop_type or prop_type in ("standard upper","standard upper value")):
-                if val is not None :
-                    _set_fallback ("standard_upper_value",val )
-            if record .get ("standard_text_value")is None and "standard"in prop_type and ("text"in prop_type ):
-                if txt is not None :
-                    _set_fallback ("standard_text_value",txt )
-                elif val is not None :
-                    _set_fallback ("standard_text_value",val )
-        current_comment =record .get ("data_validity_comment")
-        if _is_empty (current_comment ):
-            data_validity_items :list [Mapping [str ,Any ]]=[prop for prop in items if ("data_validity"in str (prop .get ("type","")).lower ()or "validity"in str (prop .get ("type","")).lower ())and (prop .get ("text_value")is not None or prop .get ("value")is not None )]
-            if data_validity_items :
-                measured_items =[p for p in data_validity_items if _is_measured (p )]
-                if measured_items :
-                    prop =measured_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="measured",comment_value =comment_value )
-                else :
-                    prop =data_validity_items [0 ]
-                    text_value =prop .get ("text_value")
-                    value =prop .get ("value")
-                    comment_value =None
-                    if text_value is not None and (not _is_empty (text_value )):
-                        comment_value =str (text_value ).strip ()
-                    elif value is not None and (not _is_empty (value )):
-                        comment_value =str (value ).strip ()
-                    if comment_value :
-                        record ["data_validity_comment"]=comment_value
-                        log .debug ("data_validity_comment_fallback_applied",activity_id =record .get ("activity_id"),source ="activity_properties",priority ="first",comment_value =comment_value )
-            else :
-                log .debug ("data_validity_comment_fallback_no_items",activity_id =record .get ("activity_id"),activity_properties_count =len (items ),has_activity_properties =True )
-        else :
-            log .debug ("data_validity_comment_from_api",activity_id =record .get ("activity_id"),comment_value =current_comment )
-        normalized_properties =self ._normalize_activity_properties_items (property_items ,log )
-        if normalized_properties is not None :
-            validated_properties ,validation_stats =self ._validate_activity_properties_truv (normalized_properties ,log ,activity_id )
-            deduplicated_properties ,dedup_stats =self ._deduplicate_activity_properties (validated_properties ,log ,activity_id )
-            record ["activity_properties"]=deduplicated_properties
-            log .debug ("activity_properties_processed",activity_id =activity_id ,original_count =len (property_items ),normalized_count =len (normalized_properties ),validated_count =len (validated_properties ),deduplicated_count =len (deduplicated_properties ),invalid_count =validation_stats .get ("invalid_count",0 ),duplicates_removed =dedup_stats .get ("duplicates_removed",0 ))
-        else :
-            record ["activity_properties"]=properties
-            log .debug ("activity_properties_normalization_failed",activity_id =activity_id ,message ="activity_properties normalization failed, keeping original")
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
-    @staticmethod
-    def _extract_chembl_release (payload :Mapping [str ,Any ])->str |None :
-        for key in ("chembl_release","chembl_db_version","release","version"):
-            value =payload .get (key )
-            if isinstance (value ,str )and value .strip ():
-                return value
-            if value is not None :
-                return str (value )
-        return None
-    @staticmethod
-    def _extract_page_items (payload :Mapping [str ,Any ],items_keys :Sequence [str ]|None =None )->list [dict [str ,Any ]]:
-        preferred_keys :tuple [str ,...]=("activities",)
-        if items_keys is None :
-            combined_keys =preferred_keys +("data","items","results")
-        else :
-            combined_keys =tuple (dict .fromkeys ((*preferred_keys ,*items_keys )))
-        return ChemblPipelineBase ._extract_page_items (payload ,combined_keys )
-    @staticmethod
-    def _next_link (payload :Mapping [str ,Any ],base_url :str )->str |None :
-        page_meta :Any =payload .get ("page_meta")
-        if isinstance (page_meta ,Mapping ):
-            next_link_raw :Any =page_meta .get ("next")
-            next_link :str |None =cast (str |None ,next_link_raw )if next_link_raw is not None else None
-            if isinstance (next_link ,str )and next_link :
-                base_url_str =str (base_url )
-                base_path_parse_result =urlparse (base_url_str )
-                base_path_raw =base_path_parse_result .path
-                base_path_str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                base_path :str =base_path_str .rstrip ("/")
-                if next_link .startswith ("http://")or next_link .startswith ("https://"):
-                    parsed =urlparse (next_link )
-                    base_parsed =urlparse (base_url_str )
-                    parsed_path_raw =parsed .path
-                    base_path_raw =base_parsed .path
-                    path :str =parsed_path_raw .decode ("utf-8","ignore")if isinstance (parsed_path_raw ,(bytes ,bytearray ))else parsed_path_raw
-                    base_path_from_url :str =base_path_raw .decode ("utf-8","ignore")if isinstance (base_path_raw ,(bytes ,bytearray ))else base_path_raw
-                    path_normalized :str =path .rstrip ("/")
-                    base_path_normalized :str =base_path_from_url .rstrip ("/")
-                    if base_path_normalized and path_normalized .startswith (base_path_normalized ):
-                        relative_path =path_normalized [len (base_path_normalized ):]
-                        if not relative_path :
-                            return None
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    elif "/api/data/"in path :
-                        parts =path .split ("/api/data/",1 )
-                        if len (parts )>1 :
-                            relative_path ="/"+parts [1 ]
-                        else :
-                            relative_path =path
-                    else :
-                        relative_path =path
-                        if not relative_path .startswith ("/"):
-                            relative_path =f'/{relative_path }'
-                    if parsed .query :
-                        relative_path =f'{relative_path }?{parsed .query }'
-                    return relative_path
-                if base_path :
-                    normalized_base =base_path .lstrip ("/")
-                    stripped_link =next_link .lstrip ("/")
-                    if stripped_link .startswith (normalized_base +"/"):
-                        stripped_link =stripped_link [len (normalized_base ):]
-                    elif stripped_link ==normalized_base :
-                        stripped_link =""
-                    next_link =stripped_link
-                next_link =next_link .lstrip ("/")
-                if next_link :
-                    next_link =f'/{next_link }'
-                else :
-                    next_link ="/"
-                return next_link
-        return None
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Ensure canonical identifier columns are present before normalization."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_chembl_id"not in df .columns and "assay_id"in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "testitem_chembl_id"not in df .columns :
-            if "testitem_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["testitem_id"]
-                actions .append ("testitem_id->testitem_chembl_id")
-            elif "molecule_chembl_id"in df .columns :
-                df ["testitem_chembl_id"]=df ["molecule_chembl_id"]
-                actions .append ("molecule_chembl_id->testitem_chembl_id")
-        if "molecule_chembl_id"not in df .columns and "testitem_chembl_id"in df .columns :
-            df ["molecule_chembl_id"]=df ["testitem_chembl_id"]
-            actions .append ("testitem_chembl_id->molecule_chembl_id")
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_required =[column for column in required_columns if column not in df .columns ]
-        if missing_required :
-            for column in missing_required :
-                df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            actions .append (f"created_missing:{",".join (missing_required )}")
-        alias_columns =[column for column in ("assay_id","testitem_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        boolean_columns =("potential_duplicate","curated","removed")
-        for column in boolean_columns :
-            specs [column ]={"dtype":"boolean","default":pd .NA }
-        return specs
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize ChEMBL and BAO identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"],pattern ="^CHEMBL\\d+$"),IdentifierRule (name ="bao",columns =["bao_endpoint","bao_format"],pattern ="^BAO_\\d{7}$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _finalize_identifier_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align identifier columns after normalization and drop aliases."
-        df =df .copy ()
-        if {"molecule_chembl_id","testitem_chembl_id"}.issubset (df .columns ):
-            mismatch_mask =df ["molecule_chembl_id"].notna ()&df ["testitem_chembl_id"].notna ()&(df ["molecule_chembl_id"]!=df ["testitem_chembl_id"])
-            if mismatch_mask .any ():
-                mismatch_count =int (mismatch_mask .sum ())
-                samples_raw =df .loc [mismatch_mask ,["molecule_chembl_id","testitem_chembl_id"]].drop_duplicates ().head (5 ).to_dict ("records")
-                samples :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],samples_raw )
-                log .warning ("identifier_mismatch",count =mismatch_count ,samples =samples )
-                df .loc [mismatch_mask ,"testitem_chembl_id"]=df .loc [mismatch_mask ,"molecule_chembl_id"]
-        required_columns =["activity_id","assay_chembl_id","testitem_chembl_id","molecule_chembl_id"]
-        missing_columns =[column for column in required_columns if column not in df .columns ]
-        if missing_columns :
-            for column in missing_columns :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([None ]*len (df ),dtype ="object")
-            log .warning ("identifier_columns_missing",columns =missing_columns )
-        return df
-    def _finalize_output_columns (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Align final column order with schema and drop unexpected fields."
-        df =df .copy ()
-        expected =list (COLUMN_ORDER )
-        extras =[column for column in df .columns if column not in expected ]
-        if extras :
-            df =df .drop (columns =extras )
-            log .debug ("output_columns_dropped",columns =extras )
-        missing =[column for column in expected if column not in df .columns ]
-        if missing :
-            for column in missing :
-                if column =="testitem_chembl_id"and "molecule_chembl_id"in df .columns :
-                    df [column ]=df ["molecule_chembl_id"].copy ()
-                else :
-                    df [column ]=pd .Series ([pd .NA ]*len (df ),dtype ="object")
-            log .warning ("output_columns_missing",columns =missing )
-        if not expected :
-            return df
-        return df [expected ]
-    def _filter_invalid_required_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Filter out rows with NULL values in required identifier fields.\n\n        Removes rows where any of the required fields (assay_chembl_id,\n        testitem_chembl_id, molecule_chembl_id) are NULL, as these cannot\n        pass schema validation.\n\n        Parameters\n        ----------\n        df:\n            DataFrame to filter.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            Filtered DataFrame with only rows having all required fields populated.\n        "
-        df =df .copy ()
-        if df .empty :
-            return df
-        required_fields =["assay_chembl_id","molecule_chembl_id"]
-        missing_fields =[field for field in required_fields if field not in df .columns ]
-        if missing_fields :
-            log .warning ("filter_skipped_missing_columns",missing_columns =missing_fields ,message ="Cannot filter: required columns are missing")
-            return df
-        valid_mask =df ["assay_chembl_id"].notna ()&df ["molecule_chembl_id"].notna ()
-        invalid_count =int ((~valid_mask ).sum ())
-        if invalid_count >0 :
-            invalid_rows =df [~valid_mask ]
-            sample_size =min (5 ,len (invalid_rows ))
-            sample_activity_ids =invalid_rows ["activity_id"].head (sample_size ).tolist ()if "activity_id"in invalid_rows .columns else []
-            log .warning ("filtered_invalid_rows",filtered_count =invalid_count ,remaining_count =int (valid_mask .sum ()),sample_activity_ids =sample_activity_ids ,message ="Rows with NULL in required identifier fields were filtered out")
-            df =df [valid_mask ].reset_index (drop =True )
-        return df
-    def _normalize_measurements (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize standard_value, standard_units, standard_relation, and standard_type."
-        df =df .copy ()
-        normalized_count =0
-        if "standard_value"in df .columns :
-            mask =df ["standard_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_value"]=numeric_series_std
-                negative_mask =mask &(df ["standard_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["standard_relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"standard_relation"]=series
-                invalid_mask =mask &~df ["standard_relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_type"in df .columns :
-            mask =df ["standard_type"].notna ()
-            if mask .any ():
-                df .loc [mask ,"standard_type"]=df .loc [mask ,"standard_type"].astype (str ).str .strip ()
-                standard_types_set :set [str ]=STANDARD_TYPES
-                invalid_mask =mask &~df ["standard_type"].isin (standard_types_set )
-                if invalid_mask .any ():
-                    log .warning ("invalid_standard_type",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"standard_type"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_units"in df .columns :
-            unit_mapping ={"nanomolar":"nM","nmol":"nM","nm":"nM","NM":"nM","\u00b5M":"\u03bcM","uM":"\u03bcM","UM":"\u03bcM","micromolar":"\u03bcM","microM":"\u03bcM","umol":"\u03bcM","millimolar":"mM","milliM":"mM","mmol":"mM","MM":"mM","percent":"%","pct":"%","ratios":"ratio"}
-            mask =df ["standard_units"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_units"].astype (str ).str .strip ()
-                for old_unit ,new_unit in unit_mapping .items ():
-                    series =series .str .replace (old_unit ,new_unit ,regex =False ,case =False )
-                df .loc [mask ,"standard_units"]=series
-                normalized_count +=int (mask .sum ())
-        if "relation"in df .columns :
-            unicode_to_ascii ={"\u2264":"<=","\u2265":">=","\u2260":"~"}
-            mask =df ["relation"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"relation"].astype (str ).str .strip ()
-                for unicode_char ,ascii_repl in unicode_to_ascii .items ():
-                    series =series .str .replace (unicode_char ,ascii_repl ,regex =False )
-                df .loc [mask ,"relation"]=series
-                invalid_mask =mask &~df ["relation"].isin (RELATIONS )
-                if invalid_mask .any ():
-                    log .warning ("invalid_relation",count =int (invalid_mask .sum ()))
-                    df .loc [invalid_mask ,"relation"]=None
-                normalized_count +=int (mask .sum ())
-        if "standard_upper_value"in df .columns :
-            mask =df ["standard_upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"standard_upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_std_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"standard_upper_value"]=numeric_series_std_upper
-                negative_mask =mask &(df ["standard_upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_standard_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"standard_upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "upper_value"in df .columns :
-            mask =df ["upper_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"upper_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_upper :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"upper_value"]=numeric_series_upper
-                negative_mask =mask &(df ["upper_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_upper_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"upper_value"]=None
-                normalized_count +=int (mask .sum ())
-        if "lower_value"in df .columns :
-            mask =df ["lower_value"].notna ()
-            if mask .any ():
-                series =df .loc [mask ,"lower_value"].astype (str ).str .strip ()
-                series =series .str .replace ("[,\\s]","",regex =True )
-                series =series .str .extract ("([+-]?\\d*\\.?\\d+)",expand =False )
-                numeric_series_lower :pd .Series [Any ]=pd .to_numeric (series ,errors ="coerce")
-                df .loc [mask ,"lower_value"]=numeric_series_lower
-                negative_mask =mask &(df ["lower_value"]<0 )
-                if negative_mask .any ():
-                    log .warning ("negative_lower_value",count =int (negative_mask .sum ()))
-                    df .loc [negative_mask ,"lower_value"]=None
-                normalized_count +=int (mask .sum ())
-        if normalized_count >0 :
-            log .debug ("measurements_normalized",normalized_count =normalized_count )
-        return df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Normalize string fields: trim, empty string to null, title-case for organism."
-        working_df =df .copy ()
-        if "data_validity_description"in working_df .columns and "data_validity_comment"in working_df .columns :
-            invalid_mask =working_df ["data_validity_description"].notna ()&working_df ["data_validity_comment"].isna ()
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                log .warning ("invariant_data_validity_description_without_comment",count =invalid_count ,message ="data_validity_description is filled while data_validity_comment is NA")
-        rules :dict [str ,StringRule ]={"canonical_smiles":StringRule (),"bao_label":StringRule (max_length =128 ),"target_organism":StringRule (title_case =True ),"assay_organism":StringRule (title_case =True ),"data_validity_comment":StringRule (),"data_validity_description":StringRule (),"activity_comment":StringRule (),"standard_text_value":StringRule (),"text_value":StringRule (),"type":StringRule (),"units":StringRule (),"assay_type":StringRule (),"assay_description":StringRule (),"molecule_pref_name":StringRule (),"target_pref_name":StringRule (),"uo_units":StringRule (),"qudt_units":StringRule ()}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Serialize nested structures (ligand_efficiency, activity_properties) to JSON strings."
-        df =df .copy ()
-        nested_fields =["ligand_efficiency","activity_properties"]
-        for field in nested_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                serialized :list [Any ]=[]
-                for idx ,value in df .loc [mask ,field ].items ():
-                    if field =="activity_properties":
-                        serialized_value =self ._serialize_activity_properties (value ,log )
-                        serialized .append (serialized_value )
-                        continue
-                    if isinstance (value ,(Mapping ,list )):
-                        try :
-                            serialized .append (json .dumps (value ,ensure_ascii =False ,sort_keys =True ))
-                        except (TypeError ,ValueError )as exc :
-                            log .warning ("nested_serialization_failed",field =field ,index =idx ,error =str (exc ))
-                            serialized .append (None )
-                    elif isinstance (value ,str ):
-                        try :
-                            json .loads (value )
-                            serialized .append (value )
-                        except (TypeError ,ValueError ):
-                            serialized .append (None )
-                    else :
-                        serialized .append (None )
-                df .loc [mask ,field ]=pd .Series (serialized ,dtype ="object",index =df .loc [mask ,field ].index )
-        if "standard_value"in df .columns and "ligand_efficiency"in df .columns :
-            mask =df ["standard_value"].notna ()&df ["ligand_efficiency"].isna ()
-            if mask .any ():
-                log .warning ("ligand_efficiency_missing_with_standard_value",count =int (mask .sum ()),message ="ligand_efficiency is empty while standard_value exists")
-        return df
-    def _serialize_activity_properties (self ,value :Any ,log :BoundLogger |None =None )->str |None :
-        "Return normalized JSON for activity_properties or None if not serializable."
-        normalized_items =self ._normalize_activity_properties_items (value ,log )
-        if normalized_items is None :
-            return None
-        try :
-            return json .dumps (normalized_items ,ensure_ascii =False ,sort_keys =True )
-        except (TypeError ,ValueError )as exc :
-            if log is not None :
-                log .warning ("activity_properties_serialization_failed",error =str (exc ))
-            return None
-    def _normalize_activity_properties_items (self ,value :Any ,log :BoundLogger |None =None )->list [dict [str ,Any ]]|None :
-        "Coerce activity_properties payloads into a list of constrained dictionaries."
-        if value is None :
-            return None
-        raw_value =value
-        if isinstance (value ,str ):
-            stripped =value .strip ()
-            if not stripped :
-                return []
-            try :
-                parsed =json .loads (stripped )
-            except (TypeError ,ValueError ):
-                fallback_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                fallback_base ["text_value"]=stripped
-                return [fallback_base ]
-            else :
-                value =parsed
-        if isinstance (value ,Mapping ):
-            items :list [Any ]=[value ]
-        elif isinstance (value ,Sequence )and (not isinstance (value ,(str ,bytes ))):
-            items =list (value )
-        else :
-            if log is not None :
-                log .warning ("activity_properties_unhandled_type",value_type =type (raw_value ).__name__ )
-            return None
-        normalized :list [dict [str ,Any ]]=[]
-        for item in items :
-            if item is None :
-                continue
-            if isinstance (item ,Mapping ):
-                item_mapping =cast (Mapping [str ,Any ],item )
-                normalized_item :dict [str ,Any |None ]={key :item_mapping .get (key )for key in ACTIVITY_PROPERTY_KEYS }
-                result_flag_value =normalized_item .get ("result_flag")
-                if isinstance (result_flag_value ,int )and result_flag_value in (0 ,1 ):
-                    normalized_item ["result_flag"]=bool (result_flag_value )
-                normalized .append (normalized_item )
-            elif isinstance (item ,str ):
-                str_base :dict [str ,Any |None ]=dict .fromkeys (ACTIVITY_PROPERTY_KEYS ,None )
-                str_base ["text_value"]=item
-                normalized .append (str_base )
-            elif log is not None :
-                log .warning ("activity_properties_item_unhandled",item_type =type (item ).__name__ )
-        return normalized
-    def _validate_activity_properties_truv (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f TRUV-\u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0434\u043b\u044f activity_properties.\n\n        \u0412\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u0435\u0442 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:\n        - value IS NOT NULL \u21d2 text_value IS NULL (\u0438 \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442)\n        - relation IN ('=', '<', '\u2264', '>', '\u2265', '~') OR NULL\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        "
-        validated :list [dict [str ,Any ]]=[]
-        invalid_count =0
-        invalid_items :list [dict [str ,Any ]]=[]
-        for prop in properties :
-            is_valid =True
-            validation_errors :list [str ]=[]
-            value =prop .get ("value")
-            text_value =prop .get ("text_value")
-            relation =prop .get ("relation")
-            if value is not None and text_value is not None :
-                is_valid =False
-                validation_errors .append ("both value and text_value are not None")
-            elif value is None and text_value is None :
-                pass
-            if relation is not None :
-                if not isinstance (relation ,str ):
-                    is_valid =False
-                    validation_errors .append (f'relation is not a string: {type (relation ).__name__ }')
-                elif relation not in RELATIONS :
-                    is_valid =False
-                    validation_errors .append (f"relation '{relation }' not in allowed values: {RELATIONS }")
-            validated .append (prop )
-            if not is_valid :
-                invalid_count +=1
-                invalid_items .append (prop )
-                log .warning ("activity_property_truv_validation_failed",activity_id =activity_id ,property =prop ,errors =validation_errors ,message ="TRUV validation failed, but property is kept")
-        stats ={"invalid_count":invalid_count ,"valid_count":len (validated )}
-        return (validated ,stats )
-    def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-        "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-        seen :set [tuple [Any ,...]]=set ()
-        deduplicated :list [dict [str ,Any ]]=[]
-        duplicates_removed =0
-        for prop in properties :
-            dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-            if dedup_key not in seen :
-                seen .add (dedup_key )
-                deduplicated .append (prop )
-            else :
-                duplicates_removed +=1
-                log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-        stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-        return (deduplicated ,stats )
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_added",value ="activity")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="activity"
-            log .debug ("row_subtype_filled",value ="activity")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :BoundLogger )->pd .DataFrame :
-        "Convert data types according to the Pandera schema."
-        df =df .copy ()
-        non_nullable_int_fields ={"activity_id":"int64"}
-        nullable_int_fields ={"row_index":"Int64","target_tax_id":"int64","assay_tax_id":"int64","record_id":"int64","src_id":"int64"}
-        float_fields ={"standard_value":"float64","standard_upper_value":"float64","pchembl_value":"float64","upper_value":"float64","lower_value":"float64"}
-        bool_fields =["potential_duplicate","curated","removed"]
-        binary_flag_fields =["standard_flag"]
-        for field in non_nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_int :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_int .astype ("Int64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in nullable_int_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field =="row_index":
-                    numeric_series_row :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=numeric_series_row .astype ("Int64")
-                    if df [field ].isna ().any ():
-                        df [field ]=range (len (df ))
-                else :
-                    nullable_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=nullable_numeric_series .astype ("Int64")
-                    mask_valid =df [field ].notna ()
-                    if mask_valid .any ():
-                        invalid_mask =mask_valid &(df [field ]<1 )
-                        if invalid_mask .any ():
-                            log .warning ("invalid_positive_integer",field =field ,count =int (invalid_mask .sum ()))
-                            df .loc [invalid_mask ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in float_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_float :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_float .astype ("float64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        for field in bool_fields :
-            if field not in df .columns :
-                continue
-            try :
-                if field in ("curated","removed")and df [field ].dtype =="boolean":
-                    continue
-                if field in ("curated","removed"):
-                    df [field ]=df [field ].astype ("boolean")
-                else :
-                    bool_numeric_series :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                    df [field ]=(bool_numeric_series !=0 ).astype ("boolean")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("bool_conversion_failed",field =field ,error =str (exc ))
-        for field in binary_flag_fields :
-            if field not in df .columns :
-                continue
-            try :
-                numeric_series_flag :pd .Series [Any ]=pd .to_numeric (df [field ],errors ="coerce")
-                df [field ]=numeric_series_flag .astype ("Int64")
-                mask_valid =df [field ].notna ()
-                if mask_valid .any ():
-                    valid_values =df .loc [mask_valid ,field ]
-                    invalid_valid_mask =~valid_values .isin ([0 ,1 ])
-                    if invalid_valid_mask .any ():
-                        invalid_index =valid_values .index [invalid_valid_mask ]
-                        log .warning ("invalid_standard_flag",field =field ,count =int (invalid_valid_mask .sum ()))
-                        df .loc [invalid_index ,field ]=pd .NA
-            except (ValueError ,TypeError )as exc :
-                log .warning ("type_conversion_failed",field =field ,error =str (exc ))
-        object_fields =["value","activity_properties"]
-        for field in object_fields :
-            if field in df .columns :
-                if df [field ].dtype !="object":
-                    df [field ]=df [field ].astype ("object")
-        return df
-    def _validate_foreign_keys (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-        "Validate foreign key integrity and format of ChEMBL IDs."
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        chembl_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        warnings :list [str ]=[]
-        for field in chembl_fields :
-            if field not in df .columns :
-                continue
-            mask =df [field ].notna ()
-            if mask .any ():
-                invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-                if invalid_mask .any ():
-                    warning_msg :str =f'{field }: {int (invalid_mask .sum ())} invalid format(s)'
-                    warnings .append (warning_msg )
-        if warnings :
-            log .warning ("foreign_key_validation",warnings =warnings )
-        return df
-    def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check uniqueness of activity_id before validation."
-        if "activity_id"not in df .columns :
-            log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-            return
-        duplicates =df [df ["activity_id"].duplicated (keep =False )]
-        if not duplicates .empty :
-            duplicate_count =len (duplicates )
-            duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-            log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-            msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-            raise ValueError (msg )
-        log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
-    def _validate_data_validity_comment_soft_enum (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Soft enum \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0434\u043b\u044f data_validity_comment.\n\n        \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0442\u0438\u0432 whitelist \u0438\u0437 \u043a\u043e\u043d\u0444\u0438\u0433\u0430. \u041d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n        \u043b\u043e\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a warning, \u043d\u043e \u043d\u0435 \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e (soft enum).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        "
-        if df .empty or "data_validity_comment"not in df .columns :
-            return
-        whitelist =self ._get_data_validity_comment_whitelist ()
-        if not whitelist :
-            return
-        series_candidate =df ["data_validity_comment"].dropna ()
-        if len (series_candidate )==0 :
-            return
-        non_null_comments_series =series_candidate .astype ("string")
-        whitelist_set :set [str ]=set (whitelist )
-        def _is_unknown (value :str )->bool :
-            return value not in whitelist_set
-        unknown_mask =non_null_comments_series .map (_is_unknown ).astype (bool )
-        unknown_count =int (unknown_mask .sum ())
-        if unknown_count >0 :
-            unknown_values ={str (key ):int (value )for key ,value in non_null_comments_series [unknown_mask ].value_counts ().head (10 ).items ()}
-            log .warning ("soft_enum_unknown_data_validity_comment",unknown_count =unknown_count ,total_count =len (non_null_comments_series ),samples =unknown_values ,whitelist =whitelist ,message ="Unknown data_validity_comment values detected (soft enum: not blocking)")
-    def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-        "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-        reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        errors :list [str ]=[]
-        for field in reference_fields :
-            if field not in df .columns :
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-                continue
-            mask =df [field ].notna ()
-            if not mask .any ():
-                log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-                continue
-            invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-            if invalid_mask .any ():
-                invalid_count =int (invalid_mask .sum ())
-                invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-                errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-                log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-        if errors :
-            log .error ("foreign_key_integrity_check_failed",errors =errors )
-            msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-            raise ValueError (msg )
-        log .debug ("foreign_key_integrity_verified")
-    def _log_detailed_validation_errors (self ,failure_cases :pd .DataFrame ,payload :pd .DataFrame ,log :BoundLogger )->None :
-        "Log individual validation errors with row index and activity_id."
-        if failure_cases .empty or payload .empty :
-            return
-        activity_id_col ="activity_id"if "activity_id"in payload .columns else None
-        index_col ="index"if "index"in failure_cases .columns else None
-        if index_col is None :
-            return
-        max_errors =20
-        errors_to_log =failure_cases .head (max_errors )
-        for _ ,error_row in errors_to_log .iterrows ():
-            row_index =error_row .get (index_col )
-            if row_index is None :
-                continue
-            error_details :dict [str ,Any ]={"row_index":int (row_index )if isinstance (row_index ,(int ,float ))else str (row_index )}
-            if activity_id_col :
-                try :
-                    idx =int (row_index )if isinstance (row_index ,(int ,float ))else row_index
-                    activity_id_value :Any =payload .at [cast (int ,idx ),activity_id_col ]
-                    activity_id =activity_id_value
-                except (KeyError ,IndexError ):
-                    activity_id =None
-                if activity_id is not None and pd .notna (activity_id ):
-                    error_details ["activity_id"]=int (activity_id )if isinstance (activity_id ,(int ,float ))else str (activity_id )
-            if "column"in error_row and pd .notna (error_row ["column"]):
-                error_details ["column"]=str (error_row ["column"])
-            if "schema_context"in error_row and pd .notna (error_row ["schema_context"]):
-                error_details ["schema_context"]=str (error_row ["schema_context"])
-            if "failure_case"in error_row and pd .notna (error_row ["failure_case"]):
-                error_details ["failure_case"]=str (error_row ["failure_case"])
-            log .error ("validation_error_detail",**error_details )
-        if len (failure_cases )>max_errors :
-            log .warning ("validation_errors_truncated",total_errors =len (failure_cases ),logged_errors =max_errors )
-    def build_quality_report (self ,df :pd .DataFrame )->pd .DataFrame |dict [str ,object ]|None :
-        "Return QC report with activity-specific metrics including distributions."
-        business_key =["activity_id"]if "activity_id"in df .columns else None
-        base_report =build_default_quality_report (df ,business_key_fields =business_key )
-        rows :list [dict [str ,Any ]]=[]
-        if not base_report .empty :
-            records_raw =base_report .to_dict ("records")
-            records :list [dict [str ,Any ]]=cast (list [dict [str ,Any ]],records_raw )
-            for record in records :
-                rows .append ({str (k ):v for k ,v in record .items ()})
-        chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-        foreign_key_fields =["assay_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id"]
-        for field in foreign_key_fields :
-            if field in df .columns :
-                mask =df [field ].notna ()
-                if mask .any ():
-                    string_series =df [field ].astype (str )
-                    valid_mask =mask &string_series .str .match (chembl_id_pattern .pattern ,na =False )
-                    invalid_count =int ((mask &~valid_mask ).astype (int ).sum ())
-                    valid_count =int (valid_mask .astype (int ).sum ())
-                    total_count =int (mask .astype (int ).sum ())
-                    integrity_ratio =float (valid_count /total_count )if total_count >0 else 0.0
-                    rows .append ({"section":"foreign_key","metric":"integrity_ratio","column":field ,"value":float (integrity_ratio ),"valid_count":int (valid_count ),"invalid_count":int (invalid_count ),"total_count":int (total_count )})
-        if "standard_type"in df .columns :
-            type_counts_series :pd .Series [Any ]=df ["standard_type"].value_counts ()
-            type_dist_raw =type_counts_series .to_dict ()
-            type_dist :dict [Any ,int ]=cast (dict [Any ,int ],type_dist_raw )
-            for type_value ,count in type_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_type_count","column":"standard_type","value":str (type_value )if type_value is not None else "null","count":int (count )})
-        if "standard_units"in df .columns :
-            unit_counts_series :pd .Series [Any ]=df ["standard_units"].value_counts ()
-            unit_dist_raw =unit_counts_series .to_dict ()
-            unit_dist :dict [Any ,int ]=cast (dict [Any ,int ],unit_dist_raw )
-            for unit_value ,count in unit_dist .items ():
-                rows .append ({"section":"distribution","metric":"standard_units_count","column":"standard_units","value":str (unit_value )if unit_value is not None else "null","count":int (count )})
-        return pd .DataFrame (rows )
-    def write (self ,df :pd .DataFrame ,output_path :Path ,*,extended :bool =False ,include_correlation :bool |None =None ,include_qc_metrics :bool |None =None )->RunResult :
-        "Override write() to bind actor and ensure deterministic sorting.\n\n        Parameters\n        ----------\n        df:\n            The DataFrame to write.\n        output_path:\n            The base output path for all artifacts.\n        extended:\n            Whether to include extended QC artifacts.\n\n        Returns\n        -------\n        RunResult:\n            All artifacts generated by the write operation.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.write')
-        UnifiedLogger .bind (actor =self .actor )
-        sort_keys =["assay_chembl_id","testitem_chembl_id","activity_id"]
-        if df .empty or not all ((key in df .columns for key in sort_keys )):
-            return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-        original_sort_by =self .config .determinism .sort .by
-        if not original_sort_by or original_sort_by !=sort_keys :
-            from copy import deepcopy
-            from bioetl .config .models .determinism import DeterminismSortingConfig
-            modified_config =deepcopy (self .config )
-            modified_config .determinism .sort =DeterminismSortingConfig (by =sort_keys ,ascending =[True ,True ,True ],na_position ="last")
-            log .debug ("write_sort_config_set",sort_keys =sort_keys ,original_sort_keys =list (original_sort_by )if original_sort_by else [])
-            original_config =self .config
-            self .config =modified_config
-            try :
-                result =super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
-            finally :
-                self .config =original_config
-            return result
-        return super ().write (df ,output_path ,extended =extended ,include_correlation =include_correlation ,include_qc_metrics =include_qc_metrics )
```

#### Hotspot 2

- Definition: ChemblActivityPipeline.__init__#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:111-115
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,5 +0,0 @@
-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    self ._required_vocab_ids :Callable [[str ],Iterable [str ]]=required_vocab_ids
```

#### Hotspot 3

- Definition: ChemblActivityPipeline._add_row_metadata#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2901-2924
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,18 +0,0 @@
-def _add_row_metadata (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_added",value ="activity")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="activity"
-        log .debug ("row_subtype_filled",value ="activity")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Hotspot 4

- Definition: ChemblActivityPipeline._build_activity_descriptor#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:343-415
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,22 +0,0 @@
-def _build_activity_descriptor (self )->ChemblExtractionDescriptor [ChemblActivityPipeline ]:
-    "Construct the descriptor driving the shared extraction template."
-    def build_context (pipeline :ChemblPipelineBase ,source_config :ActivitySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        typed_pipeline =cast ("ChemblActivityPipeline",pipeline )
-        http_client ,_ =pipeline .prepare_chembl_client ("chembl",client_name ="chembl_activity_client")
-        chembl_client =ChemblClient (http_client ,load_meta_store =typed_pipeline .load_meta_store ,job_id =typed_pipeline .run_id ,operator =typed_pipeline .pipeline_code )
-        typed_pipeline ._chembl_release =typed_pipeline .fetch_chembl_release (chembl_client ,log )
-        activity_iterator =ChemblActivityClient (chembl_client ,batch_size =source_config .batch_size )
-        select_fields =source_config .parameters .select_fields
-        return ChemblExtractionContext (source_config =source_config ,iterator =activity_iterator ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =typed_pipeline ._chembl_release )
-    def empty_frame (pipeline :ChemblPipelineBase ,_ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"activity_id":pd .Series (dtype ="Int64")})
-    def post_process (pipeline :ChemblActivityPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        df =pipeline ._ensure_comment_fields (df ,log )
-        chembl_client =cast (ChemblClient ,context .chembl_client )
-        if chembl_client is not None :
-            df =pipeline ._extract_data_validity_descriptions (df ,chembl_client ,log )
-        pipeline ._log_validity_comments_metrics (df ,log )
-        return df
-    def record_transform (pipeline :ChemblActivityPipeline ,payload :Mapping [str ,Any ],context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        return pipeline ._materialize_activity_record (payload )
-    return ChemblExtractionDescriptor [ChemblActivityPipeline ](name ="chembl_activity",source_name ="chembl",source_config_factory =ActivitySourceConfig .from_source_config ,build_context =build_context ,id_column ="activity_id",summary_event ="chembl_activity.extract_summary",must_have_fields =("activity_id",),default_select_fields =API_ACTIVITY_FIELDS ,record_transform =record_transform ,post_processors =(post_process ,),sort_by =("activity_id",),empty_frame_factory =empty_frame )
```

#### Hotspot 5

- Definition: ChemblActivityPipeline._cache_directory#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1257-1267
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,7 +0,0 @@
-def _cache_directory (self ,release :str |None )->Path :
-    cache_root =Path (self .config .paths .cache_root )
-    directory_name =(self .config .cache .directory or "http_cache").strip ()or "http_cache"
-    release_component =self ._sanitize_cache_component (release or "unknown")
-    pipeline_component =self ._sanitize_cache_component (self .pipeline_code )
-    version_component =self ._sanitize_cache_component (self .config .pipeline .version or "unknown")
-    return cache_root /directory_name /pipeline_component /release_component /version_component
```

#### Hotspot 6

- Definition: ChemblActivityPipeline._cache_file_path#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1252-1255
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,4 +0,0 @@
-def _cache_file_path (self ,batch_ids :Sequence [str ],release :str |None )->Path :
-    directory =self ._cache_directory (release )
-    cache_key =self ._cache_key (batch_ids ,release )
-    return directory /f'{cache_key }.json'
```

#### Hotspot 7

- Definition: ChemblActivityPipeline._cache_key#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1269-1277
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,4 +0,0 @@
-def _cache_key (self ,batch_ids :Sequence [str ],release :str |None )->str :
-    payload ={"ids":list (batch_ids ),"release":release or "unknown","pipeline":self .pipeline_code ,"pipeline_version":self .config .pipeline .version or "unknown"}
-    raw =json .dumps (payload ,sort_keys =True )
-    return hashlib .sha256 (raw .encode ("utf-8")).hexdigest ()
```

#### Hotspot 8

- Definition: ChemblActivityPipeline._check_activity_id_uniqueness#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:3108-3131
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,13 +0,0 @@
-def _check_activity_id_uniqueness (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check uniqueness of activity_id before validation."
-    if "activity_id"not in df .columns :
-        log .warning ("activity_id_uniqueness_check_skipped",reason ="column_not_found")
-        return
-    duplicates =df [df ["activity_id"].duplicated (keep =False )]
-    if not duplicates .empty :
-        duplicate_count =len (duplicates )
-        duplicate_ids =duplicates ["activity_id"].unique ().tolist ()
-        log .error ("activity_id_duplicates_found",duplicate_count =duplicate_count ,duplicate_ids =duplicate_ids [:10 ],total_duplicate_ids =len (duplicate_ids ))
-        msg =f"Found {duplicate_count } duplicate activity_id value(s): {duplicate_ids [:5 ]}{("..."if len (duplicate_ids )>5 else "")}"
-        raise ValueError (msg )
-    log .debug ("activity_id_uniqueness_verified",unique_count =df ["activity_id"].nunique ())
```

#### Hotspot 9

- Definition: ChemblActivityPipeline._check_cache#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1176-1221
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,33 +0,0 @@
-def _check_cache (self ,batch_ids :Sequence [str ],release :str |None )->dict [str ,dict [str ,Any ]]|None :
-    cache_config =self .config .cache
-    if not cache_config .enabled :
-        return None
-    normalized_ids =[str (identifier )for identifier in batch_ids ]
-    cache_file =self ._cache_file_path (normalized_ids ,release )
-    if not cache_file .exists ():
-        return None
-    try :
-        stat =cache_file .stat ()
-    except OSError :
-        return None
-    ttl_seconds =int (cache_config .ttl )
-    if ttl_seconds >0 and time .time ()-stat .st_mtime >ttl_seconds :
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    try :
-        payload =json .loads (cache_file .read_text (encoding ="utf-8"))
-    except (OSError ,json .JSONDecodeError ):
-        try :
-            cache_file .unlink (missing_ok =True )
-        except OSError :
-            pass
-        return None
-    if not isinstance (payload ,dict ):
-        return None
-    missing =[identifier for identifier in normalized_ids if identifier not in payload ]
-    if missing :
-        return None
-    return {identifier :cast (dict [str ,Any ],payload [identifier ])for identifier in normalized_ids }
```

#### Hotspot 10

- Definition: ChemblActivityPipeline._check_foreign_key_integrity#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:3185-3232
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,24 +0,0 @@
-def _check_foreign_key_integrity (self ,df :pd .DataFrame ,log :BoundLogger )->None :
-    "Check foreign key integrity for ChEMBL IDs (format validation for non-null values)."
-    reference_fields =["assay_chembl_id","testitem_chembl_id","molecule_chembl_id","target_chembl_id","document_chembl_id","parent_molecule_chembl_id"]
-    chembl_id_pattern =re .compile ("^CHEMBL\\d+$")
-    errors :list [str ]=[]
-    for field in reference_fields :
-        if field not in df .columns :
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="column_not_found")
-            continue
-        mask =df [field ].notna ()
-        if not mask .any ():
-            log .debug ("foreign_key_integrity_check_skipped",field =field ,reason ="all_null")
-            continue
-        invalid_mask =mask &~df [field ].astype (str ).str .match (chembl_id_pattern .pattern ,na =False )
-        if invalid_mask .any ():
-            invalid_count =int (invalid_mask .sum ())
-            invalid_samples =df .loc [invalid_mask ,field ].unique ().tolist ()[:5 ]
-            errors .append (f'{field }: {invalid_count } invalid format(s), samples: {invalid_samples }')
-            log .warning ("foreign_key_integrity_invalid",field =field ,invalid_count =invalid_count ,samples =invalid_samples )
-    if errors :
-        log .error ("foreign_key_integrity_check_failed",errors =errors )
-        msg =f"Foreign key integrity check failed: {"; ".join (errors )}"
-        raise ValueError (msg )
-    log .debug ("foreign_key_integrity_verified")
```

#### Hotspot 11

- Definition: ChemblActivityPipeline._coerce_activity_dataset#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:432-450
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,14 +0,0 @@
-def _coerce_activity_dataset (self ,dataset :object )->pd .DataFrame :
-    "Convert arbitrary dataset inputs to a DataFrame of activity identifiers."
-    if isinstance (dataset ,pd .Series ):
-        return dataset .to_frame (name ="activity_id")
-    if isinstance (dataset ,pd .DataFrame ):
-        return dataset
-    if isinstance (dataset ,Mapping ):
-        mapping =cast (Mapping [str ,Any ],dataset )
-        return pd .DataFrame ([dict (mapping )])
-    if isinstance (dataset ,Sequence )and (not isinstance (dataset ,(str ,bytes ))):
-        dataset_list :list [Any ]=list (dataset )
-        return pd .DataFrame ({"activity_id":dataset_list })
-    msg ="ChemblActivityPipeline._extract_from_chembl expects a DataFrame, Series, mapping, or sequence of activity_id values"
-    raise TypeError (msg )
```

#### Hotspot 12

- Definition: ChemblActivityPipeline._coerce_mapping#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2065-2068
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,5 +0,0 @@
-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Hotspot 13

- Definition: ChemblActivityPipeline._collect_records_by_ids#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:498-637
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,83 +0,0 @@
-def _collect_records_by_ids (self ,normalized_ids :Sequence [tuple [int ,str ]],activity_iterator :ChemblActivityClient ,*,select_fields :Sequence [str ]|None )->tuple [list [dict [str ,Any ]],dict [str ,Any ]]:
-    "Iterate over IDs using the shared iterator while preserving cache semantics."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    records :list [dict [str ,Any ]]=[]
-    success_count =0
-    fallback_count =0
-    error_count =0
-    cache_hits =0
-    api_calls =0
-    total_batches =0
-    key_order =[key for _ ,key in normalized_ids ]
-    key_to_numeric ={key :numeric_id for numeric_id ,key in normalized_ids }
-    for chunk in activity_iterator ._chunk_identifiers (key_order ,select_fields =select_fields ):
-        total_batches +=1
-        batch_start =time .perf_counter ()
-        from_cache =False
-        chunk_records :dict [str ,dict [str ,Any ]]={}
-        try :
-            cached_records =self ._check_cache (chunk ,self ._chembl_release )
-            if cached_records is not None :
-                from_cache =True
-                cache_hits +=len (chunk )
-                chunk_records =cached_records
-            else :
-                api_calls +=1
-                fetched_items =list (activity_iterator .iterate_by_ids (chunk ,select_fields =select_fields ))
-                for item in fetched_items :
-                    if not isinstance (item ,Mapping ):
-                        continue
-                    activity_value =item .get ("activity_id")
-                    if activity_value is None :
-                        continue
-                    chunk_records [str (activity_value )]=dict (item )
-                self ._store_cache (chunk ,chunk_records ,self ._chembl_release )
-            success_in_batch =0
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                record =chunk_records .get (key )
-                if record and (not record .get ("error")):
-                    materialized =self ._materialize_activity_record (record ,activity_id =numeric_id )
-                    records .append (materialized )
-                    success_count +=1
-                    success_in_batch +=1
-                else :
-                    fallback_record =self ._create_fallback_record (numeric_id )
-                    records .append (fallback_record )
-                    fallback_count +=1
-                    error_count +=1
-            batch_duration_ms =(time .perf_counter ()-batch_start )*1000.0
-            log .debug ("chembl_activity.batch_processed",batch_size =len (chunk ),from_cache =from_cache ,success_in_batch =success_in_batch ,fallback_in_batch =len (chunk )-success_in_batch ,duration_ms =batch_duration_ms )
-        except CircuitBreakerOpenError as exc :
-            log .warning ("chembl_activity.batch_circuit_breaker",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except RequestException as exc :
-            log .error ("chembl_activity.batch_request_error",batch_size =len (chunk ),error =str (exc ))
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-        except Exception as exc :
-            log .error ("chembl_activity.batch_unhandled_error",batch_size =len (chunk ),error =str (exc ),exc_info =True )
-            for key in chunk :
-                numeric_id =key_to_numeric .get (key )
-                if numeric_id is None :
-                    continue
-                records .append (self ._create_fallback_record (numeric_id ,exc ))
-                fallback_count +=1
-                error_count +=1
-    total_records =len (normalized_ids )
-    success_rate =float (success_count +fallback_count )/float (total_records )if total_records else 0.0
-    summary ={"total_activities":total_records ,"success":success_count ,"fallback":fallback_count ,"errors":error_count ,"api_calls":api_calls ,"cache_hits":cache_hits ,"batches":total_batches ,"duration_ms":0.0 ,"success_rate":success_rate }
-    return (records ,summary )
```

#### Hotspot 14

- Definition: ChemblActivityPipeline._create_fallback_record#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1284-1323
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,16 +0,0 @@
-def _create_fallback_record (self ,activity_id :int ,error :Exception |None =None )->dict [str ,Any ]:
-    "Create fallback record enriched with error metadata."
-    base_message ="Fallback: ChEMBL activity unavailable"
-    message =f'{base_message } ({error })'if error else base_message
-    timestamp =datetime .now (timezone .utc ).isoformat ().replace ("+00:00","Z")
-    metadata :dict [str ,Any ]={"source_system":"ChEMBL_FALLBACK","error_type":error .__class__ .__name__ if error else None ,"chembl_release":self ._chembl_release ,"run_id":self .run_id ,"timestamp":timestamp }
-    if isinstance (error ,RequestException ):
-        response =getattr (error ,"response",None )
-        status_code =getattr (response ,"status_code",None )
-        if status_code is not None :
-            metadata ["http_status"]=status_code
-        metadata ["error_message"]=str (error )
-    elif error is not None :
-        metadata ["error_message"]=str (error )
-    fallback_properties =[{"type":"fallback_metadata","relation":None ,"units":None ,"value":None ,"text_value":json .dumps (metadata ,sort_keys =True ,default =str ),"result_flag":None }]
-    return {"activity_id":activity_id ,"data_validity_comment":message ,"activity_properties":self ._serialize_activity_properties (fallback_properties )}
```

#### Hotspot 15

- Definition: ChemblActivityPipeline._deduplicate_activity_properties#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:2843-2899
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,15 +0,0 @@
-def _deduplicate_activity_properties (self ,properties :list [dict [str ,Any ]],log :BoundLogger ,activity_id :Any |None =None )->tuple [list [dict [str ,Any ]],dict [str ,int ]]:
-    "\u0414\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 activity_properties.\n\n        \u0423\u0434\u0430\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u043f\u043e \u0432\u0441\u0435\u043c \u043f\u043e\u043b\u044f\u043c: type, relation, units, value, text_value.\n        \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0432\u043e\u0439\u0441\u0442\u0432 (\u043f\u0435\u0440\u0432\u043e\u0435 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438).\n\n        Parameters\n        ----------\n        properties:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0434\u043b\u044f \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        log:\n            Logger instance.\n        activity_id:\n            ID \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (\u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e).\n\n        Returns\n        -------\n        tuple[list[dict[str, Any]], dict[str, int]]:\n            \u041a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0434\u0443\u043f\u043b\u0438\u043a\u0430\u0446\u0438\u0438.\n        "
-    seen :set [tuple [Any ,...]]=set ()
-    deduplicated :list [dict [str ,Any ]]=[]
-    duplicates_removed =0
-    for prop in properties :
-        dedup_key =(prop .get ("type"),prop .get ("relation"),prop .get ("units"),prop .get ("value"),prop .get ("text_value"))
-        if dedup_key not in seen :
-            seen .add (dedup_key )
-            deduplicated .append (prop )
-        else :
-            duplicates_removed +=1
-            log .debug ("activity_property_duplicate_removed",activity_id =activity_id ,property =prop ,message ="Exact duplicate removed")
-    stats ={"duplicates_removed":duplicates_removed ,"deduplicated_count":len (deduplicated )}
-    return (deduplicated ,stats )
```

#### Hotspot 16

- Definition: ChemblActivityPipeline._enrich_assay#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:892-935
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,28 +0,0 @@
-def _enrich_assay (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 assay."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    assay_section :Any =enrich_section .get ("assay")
-                    if isinstance (assay_section ,Mapping ):
-                        assay_section =cast (Mapping [str ,Any ],assay_section )
-                        enrich_cfg =dict (assay_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_assay (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 17

- Definition: ChemblActivityPipeline._enrich_compound_record#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:822-867
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,28 +0,0 @@
-def _enrich_compound_record (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 compound_record."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    compound_record_section :Any =enrich_section .get ("compound_record")
-                    if isinstance (compound_record_section ,Mapping ):
-                        compound_record_section =cast (Mapping [str ,Any ],compound_record_section )
-                        enrich_cfg =dict (compound_record_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_compound_record (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 18

- Definition: ChemblActivityPipeline._enrich_data_validity#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:960-1014
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,32 +0,0 @@
-def _enrich_data_validity (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 data_validity_lookup."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    data_validity_section :Any =enrich_section .get ("data_validity")
-                    if isinstance (data_validity_section ,Mapping ):
-                        data_validity_section =cast (Mapping [str ,Any ],data_validity_section )
-                        enrich_cfg =dict (data_validity_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    if "data_validity_description"in df .columns :
-        non_na_count =int (df ["data_validity_description"].notna ().sum ())
-        if non_na_count >0 :
-            log .info ("enrichment_data_validity_description_already_filled",non_na_count =non_na_count ,total_count =len (df ),message ="data_validity_description already populated from extract, enrichment will update/overwrite")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_data_validity (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 19

- Definition: ChemblActivityPipeline._enrich_molecule#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1039-1108
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,37 +0,0 @@
-def _enrich_molecule (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 molecule \u0447\u0435\u0440\u0435\u0437 join."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            activity_section :Any =chembl_section .get ("activity")
-            if isinstance (activity_section ,Mapping ):
-                activity_section =cast (Mapping [str ,Any ],activity_section )
-                enrich_section :Any =activity_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    molecule_section :Any =enrich_section .get ("molecule")
-                    if isinstance (molecule_section ,Mapping ):
-                        molecule_section =cast (Mapping [str ,Any ],molecule_section )
-                        enrich_cfg =dict (molecule_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =ActivitySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    api_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    if "chembl_enrichment_client"not in self ._registered_clients :
-        self .register_client ("chembl_enrichment_client",api_client )
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    df_join =join_activity_with_molecule (df ,chembl_client ,enrich_cfg )
-    if "molecule_name"in df_join .columns :
-        if "molecule_pref_name"not in df .columns :
-            df ["molecule_pref_name"]=pd .NA
-        mask =df ["molecule_pref_name"].isna ()|(df ["molecule_pref_name"].astype ("string").str .strip ()=="")
-        if mask .any ():
-            df_merged =df .merge (df_join [["activity_id","molecule_name"]],on ="activity_id",how ="left",suffixes =("","_join"))
-            df .loc [mask ,"molecule_pref_name"]=df_merged .loc [mask ,"molecule_name"]
-            log .info ("molecule_pref_name_enriched",rows_enriched =int (mask .sum ()),total_rows =len (df ))
-    return df
```

#### Hotspot 20

- Definition: ChemblActivityPipeline._ensure_comment_fields#1
- activity: src/bioetl/pipelines/chembl/activity/run.py:1325-1358
- testitem: absent

```diff
--- activity:run.py
+++ testitem:run.py
@@ -1,11 +0,0 @@
-def _ensure_comment_fields (self ,df :pd .DataFrame ,log :BoundLogger )->pd .DataFrame :
-    "\u0413\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u0432 DataFrame.\n\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044f \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u0445 \u0441 pd.NA (dtype=\"string\").\n        data_validity_description \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0432 extract \u0447\u0435\u0440\u0435\u0437 _extract_data_validity_descriptions().\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\n        log:\n            Logger instance.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u043c \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432.\n        "
-    if df .empty :
-        return df
-    required_comment_fields =["activity_comment","data_validity_comment","data_validity_description"]
-    missing_fields =[field for field in required_comment_fields if field not in df .columns ]
-    if missing_fields :
-        for field in missing_fields :
-            df [field ]=pd .Series ([pd .NA ]*len (df ),dtype ="string")
-        log .debug ("comment_fields_ensured",fields =missing_fields )
-    return df
```

_Only the first 20 hotspots of 112 are shown for module run.py._

_First 20 hotspots of 144 are shown for pair activity ↔ testitem._

---

## Pair: assay ↔ document

- AST hash: 4e65bc8b94391cb8c868cf6ad530c2c6 ↔ 3774d930e14eb5befd7fdffc255cb346

- Jaccard over tokens: 0.288

### Module run.py

- File status: assay — present, document — present
- AST hash: 6e346915a9c1b06ff585eb00b983cb26 ↔ 3fbedfe7f15a107384e19a0da1698398
- Jaccard over tokens: 0.330

Definition                                           | assay signature                                                   | document signature                        | Side effects                                                                            | Exceptions                           | Status          
-----------------------------------------------------|-------------------------------------------------------------------|-------------------------------------------|-----------------------------------------------------------------------------------------|--------------------------------------|-----------------
ChemblAssayPipeline                                  | —                                                                 | —                                         | assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning<br>document: ∅ | assay: TypeError(msg)<br>document: ∅ | only in assay   
ChemblAssayPipeline.__init__                         | self, config: PipelineConfig, run_id: str                         | —                                         | assay: io=∅; logging=∅<br>document: ∅                                                   | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline._add_row_metadata                | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: io=∅; logging=log.debug<br>document: ∅                                           | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline._build_assay_descriptor          | self: SelfChemblAssayPipeline                                     | —                                         | assay: io=∅; logging=log.debug, log.info, log.warning<br>document: ∅                    | assay: TypeError(msg)<br>document: ∅ | only in assay   
ChemblAssayPipeline._check_missing_columns           | self, df: pd.DataFrame, log: Any, select_fields: list[str] | None | —                                         | assay: io=∅; logging=log.debug, log.warning<br>document: ∅                              | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline._enrich_with_related_data        | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: io=∅; logging=log.debug, log.info, log.warning<br>document: ∅                    | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline._harmonize_identifier_columns    | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: io=∅; logging=log.debug<br>document: ∅                                           | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline._normalize_data_types            | self, df: pd.DataFrame, schema: Any, log: Any                     | —                                         | assay: io=∅; logging=log.debug<br>document: ∅                                           | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline._normalize_identifiers           | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: io=∅; logging=log.debug<br>document: ∅                                           | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline._normalize_nested_structures     | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: io=∅; logging=log.debug<br>document: ∅                                           | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline._normalize_string_fields         | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: io=∅; logging=log.debug, log.warning<br>document: ∅                              | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline._serialize_array_fields          | self, df: pd.DataFrame, log: Any                                  | —                                         | assay: io=∅; logging=log.debug<br>document: ∅                                           | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline.chembl_release                   | self                                                              | —                                         | assay: io=∅; logging=∅<br>document: ∅                                                   | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline.extract                          | self, *args, **kwargs                                             | —                                         | assay: io=∅; logging=UnifiedLogger.get<br>document: ∅                                   | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline.extract_all                      | self                                                              | —                                         | assay: io=∅; logging=∅<br>document: ∅                                                   | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline.extract_by_ids                   | self, ids: Sequence[str]                                          | —                                         | assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning<br>document: ∅ | assay: ∅<br>document: ∅              | only in assay   
ChemblAssayPipeline.transform                        | self, df: pd.DataFrame                                            | —                                         | assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info<br>document: ∅              | assay: ∅<br>document: ∅              | only in assay   
ChemblDocumentPipeline                               | —                                                                 | —                                         | assay: ∅<br>document: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning | assay: ∅<br>document: TypeError(msg) | only in document
ChemblDocumentPipeline.__init__                      | —                                                                 | self, config: PipelineConfig, run_id: str | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._add_system_fields            | —                                                                 | self, df: pd.DataFrame, log: Any          | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._build_document_descriptor    | —                                                                 | self: SelfChemblDocumentPipeline          | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: TypeError(msg) | only in document
ChemblDocumentPipeline._check_document_id_uniqueness | —                                                                 | self, df: pd.DataFrame, log: Any          | assay: ∅<br>document: io=∅; logging=log.warning                                         | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._coerce_mapping               | —                                                                 | payload: Any                              | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._enrich_document_terms        | —                                                                 | self, df: pd.DataFrame                    | assay: ∅<br>document: io=∅; logging=UnifiedLogger.get, log.warning                      | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._extract_nested_fields        | —                                                                 | self, record: dict[str, Any]              | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._normalize_authors            | —                                                                 | authors: Any, separator: str              | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._normalize_doi                | —                                                                 | doi: str | None                           | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._normalize_identifiers        | —                                                                 | self, df: pd.DataFrame, log: Any          | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._normalize_journal            | —                                                                 | value: Any, max_len: int                  | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._normalize_numeric_fields     | —                                                                 | self, df: pd.DataFrame, log: Any          | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._normalize_string_fields      | —                                                                 | self, df: pd.DataFrame, log: Any          | assay: ∅<br>document: io=∅; logging=log.debug                                           | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._schema_column_specs          | —                                                                 | self                                      | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline._should_enrich_document_terms | —                                                                 | self                                      | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline.extract                       | —                                                                 | self, *args, **kwargs                     | assay: ∅<br>document: io=∅; logging=UnifiedLogger.get                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline.extract_all                   | —                                                                 | self                                      | assay: ∅<br>document: io=∅; logging=∅                                                   | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline.extract_by_ids                | —                                                                 | self, ids: Sequence[str]                  | assay: ∅<br>document: io=∅; logging=UnifiedLogger.get, log.info                         | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline.transform                     | —                                                                 | self, df: pd.DataFrame                    | assay: ∅<br>document: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning | assay: ∅<br>document: ∅              | only in document
ChemblDocumentPipeline.validate                      | —                                                                 | self, df: pd.DataFrame                    | assay: ∅<br>document: io=∅; logging=UnifiedLogger.get, log.debug, log.info              | assay: ∅<br>document: ∅              | only in document
__module_block_0                                     | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_1                                     | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | identical       
__module_block_10                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_11                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_12                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_13                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | identical       
__module_block_14                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | identical       
__module_block_15                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | identical       
__module_block_16                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | identical       
__module_block_17                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | identical       
__module_block_18                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_19                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_2                                     | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_20                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_21                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: ∅                                                   | assay: ∅<br>document: ∅              | only in assay   
__module_block_22                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: ∅                                                   | assay: ∅<br>document: ∅              | only in assay   
__module_block_26                                    | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: ∅                                                   | assay: ∅<br>document: ∅              | only in assay   
__module_block_3                                     | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_4                                     | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_5                                     | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_6                                     | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_7                                     | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
__module_block_8                                     | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | identical       
__module_block_9                                     | —                                                                 | —                                         | assay: io=∅; logging=∅<br>document: io=∅; logging=∅                                     | assay: ∅<br>document: ∅              | differs         
_extract_bao_ids_from_classifications                | node: Any                                                         | —                                         | assay: io=∅; logging=∅<br>document: ∅                                                   | assay: ∅<br>document: ∅              | only in assay   
_iter_classification_mappings                        | node: Any                                                         | —                                         | assay: io=∅; logging=∅<br>document: ∅                                                   | assay: ∅<br>document: ∅              | only in assay   
_normalize_bao_identifier                            | raw_value: Any                                                    | —                                         | assay: io=∅; logging=∅<br>document: ∅                                                   | assay: ∅<br>document: ∅              | only in assay   

#### Hotspot 1

- Definition: ChemblAssayPipeline#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:126-952
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,324 +0,0 @@
-class ChemblAssayPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting assay records from the ChEMBL API."
-    actor ="assay_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all assay records from ChEMBL using pagination."
-        descriptor =self ._build_assay_descriptor ()
-        return self .run_extract_all (descriptor )
-    def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-        "Return the descriptor powering the shared extraction template."
-        def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-            if isinstance (pipeline ,ChemblAssayPipeline ):
-                return pipeline
-            msg ="ChemblAssayPipeline instance required"
-            raise TypeError (msg )
-        def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-            chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-            assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-            assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-            assay_pipeline ._chembl_release =assay_client .chembl_release
-            log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-            raw_source =assay_pipeline ._resolve_source_config ("chembl")
-            select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-            log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-            context =ChemblExtractionContext (source_config ,assay_client )
-            context .chembl_client =chembl_client
-            context .select_fields =tuple (select_fields )if select_fields else None
-            context .chembl_release =assay_pipeline ._chembl_release
-            context .extra_filters ={"max_url_length":source_config .max_url_length }
-            return context
-        def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-        def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            if df .empty :
-                return df
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in df .columns or df [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            select_fields =context .select_fields
-            if select_fields :
-                expected_fields =set (select_fields )
-                actual_fields =set (df .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-            return df
-        def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-            return pd .DataFrame ()
-        def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-        return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        self ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        if self .config .cli .dry_run :
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-            return pd .DataFrame ()
-        limit =self .config .cli .limit
-        resolved_select_fields =self ._resolve_select_fields (source_raw )
-        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-        log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-        def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield dict (item )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            if dataframe .empty :
-                return dataframe
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            if context .select_fields :
-                expected_fields =set (context .select_fields )
-                actual_fields =set (dataframe .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-            return dataframe
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw assay data by normalizing identifiers, types, and nested structures."
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._enrich_with_related_data (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._serialize_array_fields (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,AssaySchema ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Serialize array-of-object fields to header+rows format."
-        df =df .copy ()
-        arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-        if arrays_to_serialize :
-            df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-            for column in arrays_to_serialize :
-                if column in df_result .columns :
-                    column_as_string :Series =df_result [column ].astype ("string")
-                    filled_column :Series =column_as_string .copy ()
-                    filled_column [column_as_string .isna ()]=""
-                    empty_mask :Series =filled_column .eq ("")
-                    if bool (empty_mask .any ()):
-                        df_result .loc [empty_mask ,column ]=pd .NA
-            df =df_result
-            log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-        return df
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-            df ["target_chembl_id"]=df ["target_id"]
-            actions .append ("target_id->target_chembl_id")
-        alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize ChEMBL identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-        working_df =df .copy ()
-        string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-        rules ={column :StringRule ()for column in string_fields }
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if "curation_level"not in normalized_df .columns :
-            normalized_df ["curation_level"]=pd .NA
-            log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-        df =df .copy ()
-        if "assay_parameters"in df .columns :
-            log .debug ("validating_assay_parameters_truv")
-            df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-        if "assay_classifications"in df .columns :
-            if "assay_class_id"not in df .columns :
-                df ["assay_class_id"]=pd .NA
-            updated_rows =0
-            classifications_series =df ["assay_classifications"]
-            for row_index ,value in classifications_series .items ():
-                if value is None or value is pd .NA :
-                    continue
-                if isinstance (value ,float )and pd .isna (value ):
-                    continue
-                if isinstance (value ,str ):
-                    continue
-                extracted_ids =_extract_bao_ids_from_classifications (value )
-                if not extracted_ids :
-                    continue
-                joined_ids =";".join (extracted_ids )
-                row_label =cast (Any ,row_index )
-                current_value =df .at [row_label ,"assay_class_id"]
-                if pd .isna (current_value )or current_value !=joined_ids :
-                    df .at [row_label ,"assay_class_id"]=joined_ids
-                    updated_rows +=1
-            if updated_rows >0 :
-                log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-        return df
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_added",value ="assay")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_filled",value ="assay")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-        "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-        df =super ()._normalize_data_types (df ,schema ,log )
-        if "row_index"in df .columns and df ["row_index"].isna ().any ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-        df =df .copy ()
-        optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-        expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-        select_fields_set :set [str ]=set ()
-        if select_fields is not None :
-            select_fields_set =set (select_fields )
-        missing_in_response :list [str ]=[]
-        missing_in_select_fields :list [str ]=[]
-        for column in expected_api_fields :
-            if column not in df .columns :
-                missing_in_response .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-                else :
-                    log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-        missing_columns :list [str ]=[]
-        for column ,version in optional_columns .items ():
-            if column not in df .columns :
-                df [column ]=pd .NA
-                missing_columns .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-                else :
-                    log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-        if missing_in_response or missing_columns :
-            log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-        return df
-    def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-        if df .empty :
-            return df
-        try :
-            source_raw =self ._resolve_source_config ("chembl")
-        except KeyError as exc :
-            log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-            return df
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        chembl_config =getattr (self .config ,"chembl",None )
-        if chembl_config is None :
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-            return df
-        if not isinstance (chembl_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-            return df
-        assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-        if not isinstance (assay_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-            return df
-        enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-        if not isinstance (enrich_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-            return df
-        classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-        if classifications_cfg is not None :
-            log .info ("enrichment_classifications_started")
-            df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-            df =df_with_classifications
-            log .info ("enrichment_classifications_completed")
-            if "assay_class_id"in df .columns :
-                filled_count =int (df ["assay_class_id"].notna ().sum ())
-                total_count =len (df )
-                if filled_count ==0 :
-                    log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-                else :
-                    log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-            else :
-                log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-        else :
-            log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-        parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-        if parameters_cfg is not None :
-            log .info ("enrichment_parameters_started")
-            df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-            df =df_with_parameters
-            log .info ("enrichment_parameters_completed")
-        return df
```

#### Hotspot 2

- Definition: ChemblAssayPipeline.__init__#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:131-133
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,3 +0,0 @@
-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
```

#### Hotspot 3

- Definition: ChemblAssayPipeline._add_row_metadata#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:681-704
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,18 +0,0 @@
-def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_added",value ="assay")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_filled",value ="assay")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Hotspot 4

- Definition: ChemblAssayPipeline._build_assay_descriptor#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:167-323
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,51 +0,0 @@
-def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-    "Return the descriptor powering the shared extraction template."
-    def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-        if isinstance (pipeline ,ChemblAssayPipeline ):
-            return pipeline
-        msg ="ChemblAssayPipeline instance required"
-        raise TypeError (msg )
-    def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        assay_pipeline ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        raw_source =assay_pipeline ._resolve_source_config ("chembl")
-        select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-        log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-        context =ChemblExtractionContext (source_config ,assay_client )
-        context .chembl_client =chembl_client
-        context .select_fields =tuple (select_fields )if select_fields else None
-        context .chembl_release =assay_pipeline ._chembl_release
-        context .extra_filters ={"max_url_length":source_config .max_url_length }
-        return context
-    def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-    def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        if df .empty :
-            return df
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in df .columns or df [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        select_fields =context .select_fields
-        if select_fields :
-            expected_fields =set (select_fields )
-            actual_fields =set (df .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-        return df
-    def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-        return pd .DataFrame ()
-    def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-    return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
```

#### Hotspot 5

- Definition: ChemblAssayPipeline._check_missing_columns#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:720-835
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,31 +0,0 @@
-def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-    "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-    df =df .copy ()
-    optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-    expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-    select_fields_set :set [str ]=set ()
-    if select_fields is not None :
-        select_fields_set =set (select_fields )
-    missing_in_response :list [str ]=[]
-    missing_in_select_fields :list [str ]=[]
-    for column in expected_api_fields :
-        if column not in df .columns :
-            missing_in_response .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-            else :
-                log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-    missing_columns :list [str ]=[]
-    for column ,version in optional_columns .items ():
-        if column not in df .columns :
-            df [column ]=pd .NA
-            missing_columns .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-            else :
-                log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-    if missing_in_response or missing_columns :
-        log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-    return df
```

#### Hotspot 6

- Definition: ChemblAssayPipeline._enrich_with_related_data#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:837-952
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,53 +0,0 @@
-def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-    if df .empty :
-        return df
-    try :
-        source_raw =self ._resolve_source_config ("chembl")
-    except KeyError as exc :
-        log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-        return df
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    chembl_config =getattr (self .config ,"chembl",None )
-    if chembl_config is None :
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-        return df
-    if not isinstance (chembl_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-        return df
-    assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-    if not isinstance (assay_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-        return df
-    enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-    if not isinstance (enrich_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-        return df
-    classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-    if classifications_cfg is not None :
-        log .info ("enrichment_classifications_started")
-        df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-        df =df_with_classifications
-        log .info ("enrichment_classifications_completed")
-        if "assay_class_id"in df .columns :
-            filled_count =int (df ["assay_class_id"].notna ().sum ())
-            total_count =len (df )
-            if filled_count ==0 :
-                log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-            else :
-                log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-        else :
-            log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-    else :
-        log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-    parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-    if parameters_cfg is not None :
-        log .info ("enrichment_parameters_started")
-        df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-        df =df_with_parameters
-        log .info ("enrichment_parameters_completed")
-    return df
```

#### Hotspot 7

- Definition: ChemblAssayPipeline._harmonize_identifier_columns#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:526-548
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,17 +0,0 @@
-def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-    df =df .copy ()
-    actions :list [str ]=[]
-    if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-        df ["assay_chembl_id"]=df ["assay_id"]
-        actions .append ("assay_id->assay_chembl_id")
-    if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-        df ["target_chembl_id"]=df ["target_id"]
-        actions .append ("target_id->target_chembl_id")
-    alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-    if alias_columns :
-        df =df .drop (columns =alias_columns )
-        actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-    if actions :
-        log .debug ("identifier_harmonization",actions =actions )
-    return df
```

#### Hotspot 8

- Definition: ChemblAssayPipeline._normalize_data_types#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:706-718
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,7 +0,0 @@
-def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-    "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-    df =super ()._normalize_data_types (df ,schema ,log )
-    if "row_index"in df .columns and df ["row_index"].isna ().any ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Hotspot 9

- Definition: ChemblAssayPipeline._normalize_identifiers#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:550-577
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,7 +0,0 @@
-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize ChEMBL identifiers with regex validation."
-    rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-    normalized_df ,stats =normalize_identifier_columns (df ,rules )
-    if stats .has_changes :
-        log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-    return normalized_df
```

#### Hotspot 10

- Definition: ChemblAssayPipeline._normalize_nested_structures#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:623-679
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,30 +0,0 @@
-def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-    df =df .copy ()
-    if "assay_parameters"in df .columns :
-        log .debug ("validating_assay_parameters_truv")
-        df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-    if "assay_classifications"in df .columns :
-        if "assay_class_id"not in df .columns :
-            df ["assay_class_id"]=pd .NA
-        updated_rows =0
-        classifications_series =df ["assay_classifications"]
-        for row_index ,value in classifications_series .items ():
-            if value is None or value is pd .NA :
-                continue
-            if isinstance (value ,float )and pd .isna (value ):
-                continue
-            if isinstance (value ,str ):
-                continue
-            extracted_ids =_extract_bao_ids_from_classifications (value )
-            if not extracted_ids :
-                continue
-            joined_ids =";".join (extracted_ids )
-            row_label =cast (Any ,row_index )
-            current_value =df .at [row_label ,"assay_class_id"]
-            if pd .isna (current_value )or current_value !=joined_ids :
-                df .at [row_label ,"assay_class_id"]=joined_ids
-                updated_rows +=1
-        if updated_rows >0 :
-            log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-    return df
```

#### Hotspot 11

- Definition: ChemblAssayPipeline._normalize_string_fields#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:579-621
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,12 +0,0 @@
-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-    working_df =df .copy ()
-    string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-    rules ={column :StringRule ()for column in string_fields }
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if "curation_level"not in normalized_df .columns :
-        normalized_df ["curation_level"]=pd .NA
-        log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    return normalized_df
```

#### Hotspot 12

- Definition: ChemblAssayPipeline._serialize_array_fields#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:504-524
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,17 +0,0 @@
-def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Serialize array-of-object fields to header+rows format."
-    df =df .copy ()
-    arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-    if arrays_to_serialize :
-        df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-        for column in arrays_to_serialize :
-            if column in df_result .columns :
-                column_as_string :Series =df_result [column ].astype ("string")
-                filled_column :Series =column_as_string .copy ()
-                filled_column [column_as_string .isna ()]=""
-                empty_mask :Series =filled_column .eq ("")
-                if bool (empty_mask .any ()):
-                    df_result .loc [empty_mask ,column ]=pd .NA
-        df =df_result
-        log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-    return df
```

#### Hotspot 13

- Definition: ChemblAssayPipeline.chembl_release#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:136-139
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,4 +0,0 @@
-@property
-def chembl_release (self )->str |None :
-    "Return the cached ChEMBL release captured during extraction."
-    return self ._chembl_release
```

#### Hotspot 14

- Definition: ChemblAssayPipeline.extract#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:145-159
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,4 +0,0 @@
-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
```

#### Hotspot 15

- Definition: ChemblAssayPipeline.extract_all#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:161-165
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,4 +0,0 @@
-def extract_all (self )->pd .DataFrame :
-    "Extract all assay records from ChEMBL using pagination."
-    descriptor =self ._build_assay_descriptor ()
-    return self .run_extract_all (descriptor )
```

#### Hotspot 16

- Definition: ChemblAssayPipeline.extract_by_ids#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:325-469
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,42 +0,0 @@
-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-    assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-    self ._chembl_release =assay_client .chembl_release
-    log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-    if self .config .cli .dry_run :
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-        return pd .DataFrame ()
-    limit =self .config .cli .limit
-    resolved_select_fields =self ._resolve_select_fields (source_raw )
-    merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-    log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-    def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield dict (item )
-    def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-        if dataframe .empty :
-            return dataframe
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        if context .select_fields :
-            expected_fields =set (context .select_fields )
-            actual_fields =set (dataframe .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-        return dataframe
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-    return dataframe
```

#### Hotspot 17

- Definition: ChemblAssayPipeline.transform#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:471-498
- document: absent

```diff
--- assay:run.py
+++ document:run.py
@@ -1,21 +0,0 @@
-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw assay data by normalizing identifiers, types, and nested structures."
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-    df =df .copy ()
-    df =self ._harmonize_identifier_columns (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._enrich_with_related_data (df ,log )
-    df =self ._normalize_nested_structures (df ,log )
-    df =self ._serialize_array_fields (df ,log )
-    df =self ._add_row_metadata (df ,log )
-    df =self ._normalize_data_types (df ,AssaySchema ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

#### Hotspot 18

- Definition: ChemblDocumentPipeline#1
- assay: absent
- document: src/bioetl/pipelines/chembl/document/run.py:60-654

```diff
--- assay:run.py
+++ document:run.py
@@ -0,0 +1,324 @@
+class ChemblDocumentPipeline (ChemblPipelineBase ):
+    "ETL pipeline extracting document records from the ChEMBL API."
+    actor ="document_chembl"
+    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+        super ().__init__ (config ,run_id )
+        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
+    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
+        "Fetch document payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
+        return self ._dispatch_extract_mode (log ,event_name ="chembl_document.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="document_chembl_id")
+    def extract_all (self )->pd .DataFrame :
+        "Extract all document records from ChEMBL using pagination."
+        return self .run_extract_all (self ._build_document_descriptor ())
+    def _build_document_descriptor (self :SelfChemblDocumentPipeline )->ChemblExtractionDescriptor [SelfChemblDocumentPipeline ]:
+        "Return the descriptor powering the shared extraction routine."
+        def _require_document_pipeline (pipeline :ChemblPipelineBase )->ChemblDocumentPipeline :
+            if isinstance (pipeline ,ChemblDocumentPipeline ):
+                return pipeline
+            msg ="ChemblDocumentPipeline instance required"
+            raise TypeError (msg )
+        def build_context (pipeline :SelfChemblDocumentPipeline ,source_config :Any ,log :BoundLogger )->ChemblExtractionContext :
+            document_pipeline =_require_document_pipeline (pipeline )
+            typed_source_config =source_config if isinstance (source_config ,DocumentSourceConfig )else DocumentSourceConfig .from_source_config (cast (Any ,source_config ))
+            base_url =document_pipeline ._resolve_base_url (typed_source_config .parameters )
+            http_client ,_ =document_pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
+            chembl_client =ChemblClient (http_client )
+            document_client =ChemblDocumentClient (chembl_client ,batch_size =min (typed_source_config .batch_size ,25 ))
+            document_pipeline ._chembl_release =document_pipeline .fetch_chembl_release (chembl_client ,log )
+            select_fields =document_pipeline ._resolve_select_fields (cast (SourceConfig ,cast (Any ,typed_source_config )),default_fields =API_DOCUMENT_FIELDS )
+            context =ChemblExtractionContext (typed_source_config ,document_client )
+            context .chembl_client =chembl_client
+            context .select_fields =tuple (select_fields )if select_fields else None
+            context .chembl_release =document_pipeline ._chembl_release
+            return context
+        def empty_frame (_ :SelfChemblDocumentPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
+            return pd .DataFrame ({"document_chembl_id":pd .Series (dtype ="string")})
+        def record_transform (pipeline :SelfChemblDocumentPipeline ,payload :Mapping [str ,Any ],_ :ChemblExtractionContext )->Mapping [str ,Any ]:
+            document_pipeline =_require_document_pipeline (pipeline )
+            return document_pipeline ._extract_nested_fields (dict (payload ))
+        def summary_extra (pipeline :SelfChemblDocumentPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
+            _require_document_pipeline (pipeline )
+            page_size =context .page_size or 0
+            pages =0
+            if page_size >0 :
+                total_rows =int (df .shape [0 ])
+                pages =(total_rows +page_size -1 )//page_size
+            return {"pages":pages }
+        return ChemblExtractionDescriptor [SelfChemblDocumentPipeline ](name ="chembl_document",source_name ="chembl",source_config_factory =DocumentSourceConfig .from_source_config ,build_context =build_context ,id_column ="document_chembl_id",summary_event ="chembl_document.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =API_DOCUMENT_FIELDS ,record_transform =record_transform ,sort_by =("document_chembl_id",),empty_frame_factory =empty_frame ,summary_extra =summary_extra )
+    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
+        "Extract document records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of document_chembl_id values to extract (as strings).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted document records.\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
+        stage_start =time .perf_counter ()
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =DocumentSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
+        chembl_client =ChemblClient (http_client )
+        document_client =ChemblDocumentClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
+        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
+        resolved_select_fields =self ._resolve_select_fields (source_raw ,default_fields =list (API_DOCUMENT_FIELDS ))
+        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
+        limit =self .config .cli .limit
+        def fetch_documents (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
+            if "original_paginate"not in context .extra :
+                original_paginate =chembl_client .paginate
+                def counted_paginate (*args :Any ,**kwargs :Any )->Any :
+                    context .increment_api_calls ()
+                    return original_paginate (*args ,**kwargs )
+                chembl_client .paginate =counted_paginate
+                context .extra ["original_paginate"]=original_paginate
+            iterator =document_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
+            for item in iterator :
+                yield self ._extract_nested_fields (dict (item ))
+        def finalize_context (context :BatchExtractionContext )->None :
+            original =context .extra .pop ("original_paginate",None )
+            if original is not None :
+                chembl_client .paginate =original
+            api_calls_value =context .stats .api_calls if context .stats .api_calls is not None else 0
+            override ={"batches":context .stats .batches ,"api_calls":api_calls_value ,"cache_hits":context .stats .cache_hits }
+            context .extra ["stats_attribute_override"]=override
+        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="document_chembl_id",fetcher =fetch_documents ,select_fields =merged_select_fields or None ,batch_size =document_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":merged_select_fields }if merged_select_fields else None ,chembl_release =self ._chembl_release ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats")
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_document.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =stats .batches ,api_calls =stats .api_calls ,cache_hits =stats .cache_hits )
+        return dataframe
+    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
+        "Transform raw document data by normalizing fields and identifiers."
+        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
+        df =df .copy ()
+        if df .empty :
+            log .debug ("transform_empty_dataframe")
+            return df
+        log .info ("transform_started",rows =len (df ))
+        df =self ._normalize_identifiers (df ,log )
+        df =self ._normalize_string_fields (df ,log )
+        df =self ._normalize_numeric_fields (df ,log )
+        if self ._should_enrich_document_terms ():
+            df =self ._enrich_document_terms (df )
+        df =self ._add_system_fields (df ,log )
+        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
+        if "document_chembl_id"in df .columns and df ["document_chembl_id"].duplicated ().any ():
+            initial_count =len (df )
+            df =df .sort_values (by =list (df .columns )).drop_duplicates (subset =["document_chembl_id"],keep ="first")
+            deduped_count =len (df )
+            if deduped_count <initial_count :
+                log .warning ("document_deduplication_applied",initial_count =initial_count ,deduped_count =deduped_count ,removed_count =initial_count -deduped_count )
+        df =self ._order_schema_columns (df ,COLUMN_ORDER )
+        log .info ("transform_completed",rows =len (df ))
+        return df
+    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
+        "Validate payload against DocumentSchema with detailed error handling."
+        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
+        if df .empty :
+            log .debug ("validate_empty_dataframe")
+            return df
+        if self .config .validation .strict :
+            allowed_columns =set (COLUMN_ORDER )
+            extra_columns =[column for column in df .columns if column not in allowed_columns ]
+            if extra_columns :
+                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
+                df =df .drop (columns =extra_columns )
+        log .info ("validate_started",rows =len (df ))
+        self ._check_document_id_uniqueness (df ,log )
+        validated =super ().validate (df )
+        log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =self .config .validation .coerce )
+        return validated
+    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize identifier fields (DOI, PMID)."
+        df =df .copy ()
+        if "doi"in df .columns :
+            df ["doi_clean"]=df ["doi"].apply (self ._normalize_doi )
+        if "pubmed_id"in df .columns :
+            df ["pubmed_id"]=pd .to_numeric (df ["pubmed_id"],errors ="coerce").astype ("Int64")
+        return df
+    @staticmethod
+    def _normalize_doi (doi :str |None )->str :
+        "Normalize DOI by removing prefixes and validating format."
+        if not doi :
+            return ""
+        if not isinstance (doi ,str ):
+            return ""
+        doi =doi .strip ().lower ()
+        for prefix in ["doi:","https://doi.org/","http://dx.doi.org/","http://doi.org/"]:
+            if doi .startswith (prefix ):
+                doi =doi [len (prefix ):]
+        doi =doi .strip ()
+        doi_pattern =re .compile ("^10\\.\\d{4,9}/\\S+$")
+        if doi_pattern .match (doi ):
+            return doi
+        return ""
+    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize string fields (title, abstract, journal, authors)."
+        working_df =df .copy ()
+        rules ={"title":StringRule (max_length =1000 ),"abstract":StringRule (max_length =5000 )}
+        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        if stats .has_changes :
+            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        if "journal"in normalized_df .columns :
+            journal_series :pd .Series [Any ]=normalized_df ["journal"]
+            normalized_df ["journal"]=journal_series .map (lambda value :self ._normalize_journal (value ))
+        if "authors"in normalized_df .columns :
+            def _to_author_tuple (item :object )->tuple [str ,int ]|None :
+                if not isinstance (item ,tuple ):
+                    return None
+                tuple_item =cast (tuple [object ,...],item )
+                if len (tuple_item )!=2 :
+                    return None
+                name_raw ,count_raw =tuple_item
+                if not isinstance (name_raw ,str ):
+                    return None
+                name_value :str =name_raw
+                if isinstance (count_raw ,Integral ):
+                    count_value =int (count_raw )
+                elif isinstance (count_raw ,Real ):
+                    float_value =float (count_raw )
+                    if not float_value .is_integer ():
+                        return None
+                    count_value =int (float_value )
+                else :
+                    return None
+                if count_value <0 :
+                    return None
+                return (name_value ,count_value )
+            def _author_name_from_tuple (data :tuple [str ,int ]|None )->str :
+                return data [0 ]if data is not None else ""
+            def _author_count_from_tuple (data :tuple [str ,int ]|None )->int :
+                return data [1 ]if data is not None else 0
+            authors_series :pd .Series [Any ]=normalized_df ["authors"]
+            normalized_result =authors_series .apply (self ._normalize_authors )
+            normalized_tuples =normalized_result .apply (_to_author_tuple )
+            normalized_df ["authors"]=normalized_tuples .apply (_author_name_from_tuple )
+            normalized_df ["authors_count"]=normalized_tuples .apply (_author_count_from_tuple )
+        return normalized_df
+    @staticmethod
+    def _normalize_journal (value :Any ,max_len :int =255 )->str :
+        "Trim and collapse whitespace for journal name."
+        if pd .isna (value ):
+            return ""
+        text =str (value )
+        text =re .sub ("\\s+"," ",text ).strip ()
+        return text [:max_len ]if len (text )>max_len else text
+    @staticmethod
+    def _normalize_authors (authors :Any ,separator :str =", ")->tuple [str ,int ]:
+        "Normalize author separators and count."
+        if pd .isna (authors ):
+            return ("",0 )
+        text =str (authors ).strip ()
+        text =re .sub (";",",",text )
+        text =re .sub ("\\s+"," ",text )
+        if not text :
+            return ("",0 )
+        parts =text .split (",")
+        parts =[p .strip ()for p in parts if p .strip ()]
+        return (separator .join (parts ),len (parts ))
+    def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize numeric fields (year)."
+        df =df .copy ()
+        if "year"in df .columns :
+            def _coerce_year (value :object )->int |None :
+                if value is None or value is pd .NA :
+                    return None
+                if isinstance (value ,Integral ):
+                    year_int =int (value )
+                elif isinstance (value ,Real ):
+                    float_value =float (value )
+                    if not float_value .is_integer ():
+                        return None
+                    year_int =int (float_value )
+                elif isinstance (value ,str ):
+                    stripped =value .strip ()
+                    if not stripped :
+                        return None
+                    if not stripped .isdigit ():
+                        return None
+                    year_int =int (stripped )
+                else :
+                    return None
+                if 1500 <=year_int <=2100 :
+                    return year_int
+                return None
+            normalized_year =df ["year"].apply (_coerce_year )
+            df ["year"]=normalized_year .astype ("Int64")
+        return df
+    def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Add document-specific system fields (source)."
+        df =df .copy ()
+        df ["source"]="ChEMBL"
+        return df
+    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
+        specs =dict (super ()._schema_column_specs ())
+        specs ["source"]={"default":"ChEMBL"}
+        specs ["authors_count"]={"default":0 ,"dtype":"Int64"}
+        hashing_config =self .config .determinism .hashing
+        business_key_column =hashing_config .business_key_column
+        row_hash_column =hashing_config .row_hash_column
+        if business_key_column :
+            specs [business_key_column ]={"default":""}
+        if row_hash_column :
+            specs [row_hash_column ]={"default":""}
+        return specs
+    def _check_document_id_uniqueness (self ,df :pd .DataFrame ,log :Any )->None :
+        "Check that document_chembl_id is unique."
+        if df .empty :
+            return
+        if "document_chembl_id"not in df .columns :
+            return
+        duplicates =df ["document_chembl_id"].duplicated ()
+        if duplicates .any ():
+            duplicate_ids =df [df ["document_chembl_id"].duplicated ()]["document_chembl_id"].unique ().tolist ()
+            log .warning ("document_id_duplicates",duplicate_count =duplicates .sum (),duplicate_ids =duplicate_ids [:10 ])
+    def _should_enrich_document_terms (self )->bool :
+        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 document_term \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
+        if not self .config .chembl :
+            return False
+        try :
+            chembl_section =self .config .chembl
+            document_section :Any =chembl_section .get ("document")
+            if not isinstance (document_section ,Mapping ):
+                return False
+            document_section =cast (Mapping [str ,Any ],document_section )
+            enrich_section :Any =document_section .get ("enrich")
+            if not isinstance (enrich_section ,Mapping ):
+                return False
+            enrich_section =cast (Mapping [str ,Any ],enrich_section )
+            document_term_section :Any =enrich_section .get ("document_term")
+            if not isinstance (document_term_section ,Mapping ):
+                return False
+            document_term_section =cast (Mapping [str ,Any ],document_term_section )
+            enabled :Any =document_term_section .get ("enabled")
+            return bool (enabled )if enabled is not None else False
+        except (AttributeError ,KeyError ,TypeError ):
+            return False
+    def _enrich_document_terms (self ,df :pd .DataFrame )->pd .DataFrame :
+        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term."
+        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
+        enrich_cfg :dict [str ,Any ]={}
+        try :
+            if self .config .chembl :
+                chembl_section =self .config .chembl
+                document_section :Any =chembl_section .get ("document")
+                if isinstance (document_section ,Mapping ):
+                    document_section =cast (Mapping [str ,Any ],document_section )
+                    enrich_section :Any =document_section .get ("enrich")
+                    if isinstance (enrich_section ,Mapping ):
+                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
+                        document_term_section :Any =enrich_section .get ("document_term")
+                        if isinstance (document_term_section ,Mapping ):
+                            document_term_section =cast (Mapping [str ,Any ],document_term_section )
+                            enrich_cfg =dict (document_term_section )
+        except (AttributeError ,KeyError ,TypeError )as exc :
+            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =DocumentSourceConfig .from_source_config (source_raw )
+        api_client ,_ =self .prepare_chembl_client ("chembl",base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters ))),client_name ="chembl_enrichment_client")
+        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
+        return enrich_with_document_terms (df ,chembl_client ,enrich_cfg )
+    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
+        "Extract fields from nested objects in document records."
+        return record
+    @staticmethod
+    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
+        "Coerce payload to dictionary mapping."
+        if isinstance (payload ,Mapping ):
+            return cast (dict [str ,Any ],payload )
+        return {}
```

#### Hotspot 19

- Definition: ChemblDocumentPipeline.__init__#1
- assay: absent
- document: src/bioetl/pipelines/chembl/document/run.py:65-67

```diff
--- assay:run.py
+++ document:run.py
@@ -0,0 +1,3 @@
+def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+    super ().__init__ (config ,run_id )
+    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
```

#### Hotspot 20

- Definition: ChemblDocumentPipeline._add_system_fields#1
- assay: absent
- document: src/bioetl/pipelines/chembl/document/run.py:532-539

```diff
--- assay:run.py
+++ document:run.py
@@ -0,0 +1,5 @@
+def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+    "Add document-specific system fields (source)."
+    df =df .copy ()
+    df ["source"]="ChEMBL"
+    return df
```

_Only the first 20 hotspots of 58 are shown for module run.py._

_First 20 hotspots of 84 are shown for pair assay ↔ document._

---

## Pair: assay ↔ target

- AST hash: 4e65bc8b94391cb8c868cf6ad530c2c6 ↔ 107171553e4f1c509bba122a3d0ccb96

- Jaccard over tokens: 0.295

### Module run.py

- File status: assay — present, target — present
- AST hash: 6e346915a9c1b06ff585eb00b983cb26 ↔ 8689c415f6f3fc16a44fa879dc47ed50
- Jaccard over tokens: 0.365

Definition                                           | assay signature                                                   | target signature                                     | Side effects                                                                                   | Exceptions                         | Status        
-----------------------------------------------------|-------------------------------------------------------------------|------------------------------------------------------|------------------------------------------------------------------------------------------------|------------------------------------|---------------
ChemblAssayPipeline                                  | —                                                                 | —                                                    | assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning<br>target: ∅          | assay: TypeError(msg)<br>target: ∅ | only in assay 
ChemblAssayPipeline.__init__                         | self, config: PipelineConfig, run_id: str                         | —                                                    | assay: io=∅; logging=∅<br>target: ∅                                                            | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline._add_row_metadata                | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: io=∅; logging=log.debug<br>target: ∅                                                    | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline._build_assay_descriptor          | self: SelfChemblAssayPipeline                                     | —                                                    | assay: io=∅; logging=log.debug, log.info, log.warning<br>target: ∅                             | assay: TypeError(msg)<br>target: ∅ | only in assay 
ChemblAssayPipeline._check_missing_columns           | self, df: pd.DataFrame, log: Any, select_fields: list[str] | None | —                                                    | assay: io=∅; logging=log.debug, log.warning<br>target: ∅                                       | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline._enrich_with_related_data        | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: io=∅; logging=log.debug, log.info, log.warning<br>target: ∅                             | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline._harmonize_identifier_columns    | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: io=∅; logging=log.debug<br>target: ∅                                                    | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline._normalize_data_types            | self, df: pd.DataFrame, schema: Any, log: Any                     | —                                                    | assay: io=∅; logging=log.debug<br>target: ∅                                                    | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline._normalize_identifiers           | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: io=∅; logging=log.debug<br>target: ∅                                                    | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline._normalize_nested_structures     | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: io=∅; logging=log.debug<br>target: ∅                                                    | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline._normalize_string_fields         | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: io=∅; logging=log.debug, log.warning<br>target: ∅                                       | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline._serialize_array_fields          | self, df: pd.DataFrame, log: Any                                  | —                                                    | assay: io=∅; logging=log.debug<br>target: ∅                                                    | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline.chembl_release                   | self                                                              | —                                                    | assay: io=∅; logging=∅<br>target: ∅                                                            | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline.extract                          | self, *args, **kwargs                                             | —                                                    | assay: io=∅; logging=UnifiedLogger.get<br>target: ∅                                            | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline.extract_all                      | self                                                              | —                                                    | assay: io=∅; logging=∅<br>target: ∅                                                            | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline.extract_by_ids                   | self, ids: Sequence[str]                                          | —                                                    | assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning<br>target: ∅          | assay: ∅<br>target: ∅              | only in assay 
ChemblAssayPipeline.transform                        | self, df: pd.DataFrame                                            | —                                                    | assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info<br>target: ∅                       | assay: ∅<br>target: ∅              | only in assay 
ChemblTargetPipeline                                 | —                                                                 | —                                                    | assay: ∅<br>target: io=json.dumps; logging=UnifiedLogger.get, log.debug, log.info, log.warning | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline.__init__                        | —                                                                 | self, config: PipelineConfig, run_id: str            | assay: ∅<br>target: io=∅; logging=∅                                                            | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline._build_target_descriptor        | —                                                                 | self                                                 | assay: ∅<br>target: io=∅; logging=log.info                                                     | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline._enrich_protein_classifications | —                                                                 | self, df: pd.DataFrame, log: Any                     | assay: ∅<br>target: io=json.dumps; logging=log.debug, log.info, log.warning                    | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline._enrich_target_components       | —                                                                 | self, df: pd.DataFrame, log: Any                     | assay: ∅<br>target: io=json.dumps; logging=log.debug, log.info, log.warning                    | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline._harmonize_identifier_columns   | —                                                                 | self, df: pd.DataFrame, log: Any                     | assay: ∅<br>target: io=∅; logging=log.debug                                                    | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline._normalize_data_types           | —                                                                 | self, df: pd.DataFrame, schema: Any | None, log: Any | assay: ∅<br>target: io=∅; logging=log.warning                                                  | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline._normalize_identifiers          | —                                                                 | self, df: pd.DataFrame, log: Any                     | assay: ∅<br>target: io=∅; logging=log.warning                                                  | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline._normalize_string_fields        | —                                                                 | self, df: pd.DataFrame, log: Any                     | assay: ∅<br>target: io=∅; logging=log.debug                                                    | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline.extract                         | —                                                                 | self, *args, **kwargs                                | assay: ∅<br>target: io=∅; logging=UnifiedLogger.get                                            | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline.extract_all                     | —                                                                 | self                                                 | assay: ∅<br>target: io=∅; logging=∅                                                            | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline.extract_by_ids                  | —                                                                 | self, ids: Sequence[str]                             | assay: ∅<br>target: io=∅; logging=UnifiedLogger.get, log.info                                  | assay: ∅<br>target: ∅              | only in target
ChemblTargetPipeline.transform                       | —                                                                 | self, df: pd.DataFrame                               | assay: ∅<br>target: io=∅; logging=UnifiedLogger.get, log.debug, log.info                       | assay: ∅<br>target: ∅              | only in target
__module_block_0                                     | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_1                                     | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | identical     
__module_block_10                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_11                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_12                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_13                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_14                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_15                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_16                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_17                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_18                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_19                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_2                                     | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_20                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: ∅                                                            | assay: ∅<br>target: ∅              | only in assay 
__module_block_21                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: ∅                                                            | assay: ∅<br>target: ∅              | only in assay 
__module_block_22                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: ∅                                                            | assay: ∅<br>target: ∅              | only in assay 
__module_block_26                                    | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: ∅                                                            | assay: ∅<br>target: ∅              | only in assay 
__module_block_3                                     | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_4                                     | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_5                                     | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_6                                     | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_7                                     | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_8                                     | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
__module_block_9                                     | —                                                                 | —                                                    | assay: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | assay: ∅<br>target: ∅              | differs       
_extract_bao_ids_from_classifications                | node: Any                                                         | —                                                    | assay: io=∅; logging=∅<br>target: ∅                                                            | assay: ∅<br>target: ∅              | only in assay 
_iter_classification_mappings                        | node: Any                                                         | —                                                    | assay: io=∅; logging=∅<br>target: ∅                                                            | assay: ∅<br>target: ∅              | only in assay 
_normalize_bao_identifier                            | raw_value: Any                                                    | —                                                    | assay: io=∅; logging=∅<br>target: ∅                                                            | assay: ∅<br>target: ∅              | only in assay 

#### Hotspot 1

- Definition: ChemblAssayPipeline#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:126-952
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,324 +0,0 @@
-class ChemblAssayPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting assay records from the ChEMBL API."
-    actor ="assay_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all assay records from ChEMBL using pagination."
-        descriptor =self ._build_assay_descriptor ()
-        return self .run_extract_all (descriptor )
-    def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-        "Return the descriptor powering the shared extraction template."
-        def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-            if isinstance (pipeline ,ChemblAssayPipeline ):
-                return pipeline
-            msg ="ChemblAssayPipeline instance required"
-            raise TypeError (msg )
-        def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-            chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-            assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-            assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-            assay_pipeline ._chembl_release =assay_client .chembl_release
-            log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-            raw_source =assay_pipeline ._resolve_source_config ("chembl")
-            select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-            log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-            context =ChemblExtractionContext (source_config ,assay_client )
-            context .chembl_client =chembl_client
-            context .select_fields =tuple (select_fields )if select_fields else None
-            context .chembl_release =assay_pipeline ._chembl_release
-            context .extra_filters ={"max_url_length":source_config .max_url_length }
-            return context
-        def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-        def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            if df .empty :
-                return df
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in df .columns or df [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            select_fields =context .select_fields
-            if select_fields :
-                expected_fields =set (select_fields )
-                actual_fields =set (df .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-            return df
-        def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-            return pd .DataFrame ()
-        def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-        return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        self ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        if self .config .cli .dry_run :
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-            return pd .DataFrame ()
-        limit =self .config .cli .limit
-        resolved_select_fields =self ._resolve_select_fields (source_raw )
-        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-        log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-        def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield dict (item )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            if dataframe .empty :
-                return dataframe
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            if context .select_fields :
-                expected_fields =set (context .select_fields )
-                actual_fields =set (dataframe .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-            return dataframe
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw assay data by normalizing identifiers, types, and nested structures."
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._enrich_with_related_data (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._serialize_array_fields (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,AssaySchema ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Serialize array-of-object fields to header+rows format."
-        df =df .copy ()
-        arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-        if arrays_to_serialize :
-            df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-            for column in arrays_to_serialize :
-                if column in df_result .columns :
-                    column_as_string :Series =df_result [column ].astype ("string")
-                    filled_column :Series =column_as_string .copy ()
-                    filled_column [column_as_string .isna ()]=""
-                    empty_mask :Series =filled_column .eq ("")
-                    if bool (empty_mask .any ()):
-                        df_result .loc [empty_mask ,column ]=pd .NA
-            df =df_result
-            log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-        return df
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-            df ["target_chembl_id"]=df ["target_id"]
-            actions .append ("target_id->target_chembl_id")
-        alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize ChEMBL identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-        working_df =df .copy ()
-        string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-        rules ={column :StringRule ()for column in string_fields }
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if "curation_level"not in normalized_df .columns :
-            normalized_df ["curation_level"]=pd .NA
-            log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-        df =df .copy ()
-        if "assay_parameters"in df .columns :
-            log .debug ("validating_assay_parameters_truv")
-            df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-        if "assay_classifications"in df .columns :
-            if "assay_class_id"not in df .columns :
-                df ["assay_class_id"]=pd .NA
-            updated_rows =0
-            classifications_series =df ["assay_classifications"]
-            for row_index ,value in classifications_series .items ():
-                if value is None or value is pd .NA :
-                    continue
-                if isinstance (value ,float )and pd .isna (value ):
-                    continue
-                if isinstance (value ,str ):
-                    continue
-                extracted_ids =_extract_bao_ids_from_classifications (value )
-                if not extracted_ids :
-                    continue
-                joined_ids =";".join (extracted_ids )
-                row_label =cast (Any ,row_index )
-                current_value =df .at [row_label ,"assay_class_id"]
-                if pd .isna (current_value )or current_value !=joined_ids :
-                    df .at [row_label ,"assay_class_id"]=joined_ids
-                    updated_rows +=1
-            if updated_rows >0 :
-                log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-        return df
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_added",value ="assay")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_filled",value ="assay")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-        "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-        df =super ()._normalize_data_types (df ,schema ,log )
-        if "row_index"in df .columns and df ["row_index"].isna ().any ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-        df =df .copy ()
-        optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-        expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-        select_fields_set :set [str ]=set ()
-        if select_fields is not None :
-            select_fields_set =set (select_fields )
-        missing_in_response :list [str ]=[]
-        missing_in_select_fields :list [str ]=[]
-        for column in expected_api_fields :
-            if column not in df .columns :
-                missing_in_response .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-                else :
-                    log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-        missing_columns :list [str ]=[]
-        for column ,version in optional_columns .items ():
-            if column not in df .columns :
-                df [column ]=pd .NA
-                missing_columns .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-                else :
-                    log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-        if missing_in_response or missing_columns :
-            log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-        return df
-    def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-        if df .empty :
-            return df
-        try :
-            source_raw =self ._resolve_source_config ("chembl")
-        except KeyError as exc :
-            log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-            return df
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        chembl_config =getattr (self .config ,"chembl",None )
-        if chembl_config is None :
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-            return df
-        if not isinstance (chembl_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-            return df
-        assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-        if not isinstance (assay_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-            return df
-        enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-        if not isinstance (enrich_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-            return df
-        classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-        if classifications_cfg is not None :
-            log .info ("enrichment_classifications_started")
-            df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-            df =df_with_classifications
-            log .info ("enrichment_classifications_completed")
-            if "assay_class_id"in df .columns :
-                filled_count =int (df ["assay_class_id"].notna ().sum ())
-                total_count =len (df )
-                if filled_count ==0 :
-                    log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-                else :
-                    log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-            else :
-                log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-        else :
-            log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-        parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-        if parameters_cfg is not None :
-            log .info ("enrichment_parameters_started")
-            df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-            df =df_with_parameters
-            log .info ("enrichment_parameters_completed")
-        return df
```

#### Hotspot 2

- Definition: ChemblAssayPipeline.__init__#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:131-133
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,3 +0,0 @@
-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
```

#### Hotspot 3

- Definition: ChemblAssayPipeline._add_row_metadata#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:681-704
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,18 +0,0 @@
-def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_added",value ="assay")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_filled",value ="assay")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Hotspot 4

- Definition: ChemblAssayPipeline._build_assay_descriptor#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:167-323
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,51 +0,0 @@
-def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-    "Return the descriptor powering the shared extraction template."
-    def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-        if isinstance (pipeline ,ChemblAssayPipeline ):
-            return pipeline
-        msg ="ChemblAssayPipeline instance required"
-        raise TypeError (msg )
-    def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        assay_pipeline ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        raw_source =assay_pipeline ._resolve_source_config ("chembl")
-        select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-        log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-        context =ChemblExtractionContext (source_config ,assay_client )
-        context .chembl_client =chembl_client
-        context .select_fields =tuple (select_fields )if select_fields else None
-        context .chembl_release =assay_pipeline ._chembl_release
-        context .extra_filters ={"max_url_length":source_config .max_url_length }
-        return context
-    def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-    def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        if df .empty :
-            return df
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in df .columns or df [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        select_fields =context .select_fields
-        if select_fields :
-            expected_fields =set (select_fields )
-            actual_fields =set (df .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-        return df
-    def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-        return pd .DataFrame ()
-    def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-    return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
```

#### Hotspot 5

- Definition: ChemblAssayPipeline._check_missing_columns#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:720-835
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,31 +0,0 @@
-def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-    "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-    df =df .copy ()
-    optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-    expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-    select_fields_set :set [str ]=set ()
-    if select_fields is not None :
-        select_fields_set =set (select_fields )
-    missing_in_response :list [str ]=[]
-    missing_in_select_fields :list [str ]=[]
-    for column in expected_api_fields :
-        if column not in df .columns :
-            missing_in_response .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-            else :
-                log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-    missing_columns :list [str ]=[]
-    for column ,version in optional_columns .items ():
-        if column not in df .columns :
-            df [column ]=pd .NA
-            missing_columns .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-            else :
-                log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-    if missing_in_response or missing_columns :
-        log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-    return df
```

#### Hotspot 6

- Definition: ChemblAssayPipeline._enrich_with_related_data#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:837-952
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,53 +0,0 @@
-def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-    if df .empty :
-        return df
-    try :
-        source_raw =self ._resolve_source_config ("chembl")
-    except KeyError as exc :
-        log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-        return df
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    chembl_config =getattr (self .config ,"chembl",None )
-    if chembl_config is None :
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-        return df
-    if not isinstance (chembl_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-        return df
-    assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-    if not isinstance (assay_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-        return df
-    enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-    if not isinstance (enrich_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-        return df
-    classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-    if classifications_cfg is not None :
-        log .info ("enrichment_classifications_started")
-        df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-        df =df_with_classifications
-        log .info ("enrichment_classifications_completed")
-        if "assay_class_id"in df .columns :
-            filled_count =int (df ["assay_class_id"].notna ().sum ())
-            total_count =len (df )
-            if filled_count ==0 :
-                log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-            else :
-                log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-        else :
-            log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-    else :
-        log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-    parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-    if parameters_cfg is not None :
-        log .info ("enrichment_parameters_started")
-        df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-        df =df_with_parameters
-        log .info ("enrichment_parameters_completed")
-    return df
```

#### Hotspot 7

- Definition: ChemblAssayPipeline._harmonize_identifier_columns#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:526-548
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,17 +0,0 @@
-def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-    df =df .copy ()
-    actions :list [str ]=[]
-    if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-        df ["assay_chembl_id"]=df ["assay_id"]
-        actions .append ("assay_id->assay_chembl_id")
-    if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-        df ["target_chembl_id"]=df ["target_id"]
-        actions .append ("target_id->target_chembl_id")
-    alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-    if alias_columns :
-        df =df .drop (columns =alias_columns )
-        actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-    if actions :
-        log .debug ("identifier_harmonization",actions =actions )
-    return df
```

#### Hotspot 8

- Definition: ChemblAssayPipeline._normalize_data_types#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:706-718
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,7 +0,0 @@
-def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-    "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-    df =super ()._normalize_data_types (df ,schema ,log )
-    if "row_index"in df .columns and df ["row_index"].isna ().any ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Hotspot 9

- Definition: ChemblAssayPipeline._normalize_identifiers#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:550-577
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,7 +0,0 @@
-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize ChEMBL identifiers with regex validation."
-    rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-    normalized_df ,stats =normalize_identifier_columns (df ,rules )
-    if stats .has_changes :
-        log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-    return normalized_df
```

#### Hotspot 10

- Definition: ChemblAssayPipeline._normalize_nested_structures#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:623-679
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,30 +0,0 @@
-def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-    df =df .copy ()
-    if "assay_parameters"in df .columns :
-        log .debug ("validating_assay_parameters_truv")
-        df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-    if "assay_classifications"in df .columns :
-        if "assay_class_id"not in df .columns :
-            df ["assay_class_id"]=pd .NA
-        updated_rows =0
-        classifications_series =df ["assay_classifications"]
-        for row_index ,value in classifications_series .items ():
-            if value is None or value is pd .NA :
-                continue
-            if isinstance (value ,float )and pd .isna (value ):
-                continue
-            if isinstance (value ,str ):
-                continue
-            extracted_ids =_extract_bao_ids_from_classifications (value )
-            if not extracted_ids :
-                continue
-            joined_ids =";".join (extracted_ids )
-            row_label =cast (Any ,row_index )
-            current_value =df .at [row_label ,"assay_class_id"]
-            if pd .isna (current_value )or current_value !=joined_ids :
-                df .at [row_label ,"assay_class_id"]=joined_ids
-                updated_rows +=1
-        if updated_rows >0 :
-            log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-    return df
```

#### Hotspot 11

- Definition: ChemblAssayPipeline._normalize_string_fields#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:579-621
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,12 +0,0 @@
-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-    working_df =df .copy ()
-    string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-    rules ={column :StringRule ()for column in string_fields }
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if "curation_level"not in normalized_df .columns :
-        normalized_df ["curation_level"]=pd .NA
-        log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    return normalized_df
```

#### Hotspot 12

- Definition: ChemblAssayPipeline._serialize_array_fields#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:504-524
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,17 +0,0 @@
-def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Serialize array-of-object fields to header+rows format."
-    df =df .copy ()
-    arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-    if arrays_to_serialize :
-        df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-        for column in arrays_to_serialize :
-            if column in df_result .columns :
-                column_as_string :Series =df_result [column ].astype ("string")
-                filled_column :Series =column_as_string .copy ()
-                filled_column [column_as_string .isna ()]=""
-                empty_mask :Series =filled_column .eq ("")
-                if bool (empty_mask .any ()):
-                    df_result .loc [empty_mask ,column ]=pd .NA
-        df =df_result
-        log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-    return df
```

#### Hotspot 13

- Definition: ChemblAssayPipeline.chembl_release#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:136-139
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,4 +0,0 @@
-@property
-def chembl_release (self )->str |None :
-    "Return the cached ChEMBL release captured during extraction."
-    return self ._chembl_release
```

#### Hotspot 14

- Definition: ChemblAssayPipeline.extract#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:145-159
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,4 +0,0 @@
-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
```

#### Hotspot 15

- Definition: ChemblAssayPipeline.extract_all#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:161-165
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,4 +0,0 @@
-def extract_all (self )->pd .DataFrame :
-    "Extract all assay records from ChEMBL using pagination."
-    descriptor =self ._build_assay_descriptor ()
-    return self .run_extract_all (descriptor )
```

#### Hotspot 16

- Definition: ChemblAssayPipeline.extract_by_ids#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:325-469
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,42 +0,0 @@
-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-    assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-    self ._chembl_release =assay_client .chembl_release
-    log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-    if self .config .cli .dry_run :
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-        return pd .DataFrame ()
-    limit =self .config .cli .limit
-    resolved_select_fields =self ._resolve_select_fields (source_raw )
-    merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-    log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-    def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield dict (item )
-    def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-        if dataframe .empty :
-            return dataframe
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        if context .select_fields :
-            expected_fields =set (context .select_fields )
-            actual_fields =set (dataframe .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-        return dataframe
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-    return dataframe
```

#### Hotspot 17

- Definition: ChemblAssayPipeline.transform#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:471-498
- target: absent

```diff
--- assay:run.py
+++ target:run.py
@@ -1,21 +0,0 @@
-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw assay data by normalizing identifiers, types, and nested structures."
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-    df =df .copy ()
-    df =self ._harmonize_identifier_columns (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._enrich_with_related_data (df ,log )
-    df =self ._normalize_nested_structures (df ,log )
-    df =self ._serialize_array_fields (df ,log )
-    df =self ._add_row_metadata (df ,log )
-    df =self ._normalize_data_types (df ,AssaySchema ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

#### Hotspot 18

- Definition: ChemblTargetPipeline#1
- assay: absent
- target: src/bioetl/pipelines/chembl/target/run.py:38-783

```diff
--- assay:run.py
+++ target:run.py
@@ -0,0 +1,342 @@
+class ChemblTargetPipeline (ChemblPipelineBase ):
+    "ETL pipeline extracting target records from the ChEMBL API."
+    actor ="target_chembl"
+    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+        super ().__init__ (config ,run_id )
+    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
+        "Fetch target payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        return self ._dispatch_extract_mode (log ,event_name ="chembl_target.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="target_chembl_id")
+    def extract_all (self )->pd .DataFrame :
+        "Extract all target records from ChEMBL using pagination."
+        descriptor =self ._build_target_descriptor ()
+        return self .run_extract_all (descriptor )
+    def _build_target_descriptor (self )->ChemblExtractionDescriptor ["ChemblTargetPipeline"]:
+        "Return the descriptor powering target extraction."
+        def build_context (pipeline :"ChemblTargetPipeline",source_config :TargetSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+            base_url =pipeline ._resolve_base_url (source_config .parameters )
+            http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
+            chembl_client =ChemblClient (http_client )
+            pipeline ._chembl_release =pipeline .fetch_chembl_release (chembl_client ,log )
+            target_client =ChemblTargetClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
+            select_fields =source_config .parameters .select_fields
+            return ChemblExtractionContext (source_config =source_config ,iterator =target_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =pipeline ._chembl_release ,extra_filters ={"batch_size":source_config .batch_size })
+        def empty_frame (_ :"ChemblTargetPipeline",__ :ChemblExtractionContext )->pd .DataFrame :
+            return pd .DataFrame ({"target_chembl_id":pd .Series (dtype ="string")})
+        def dry_run_handler (pipeline :"ChemblTargetPipeline",_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =pipeline ._chembl_release )
+            return pd .DataFrame ()
+        def summary_extra (pipeline :"ChemblTargetPipeline",_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+            return {"limit":pipeline .config .cli .limit }
+        return ChemblExtractionDescriptor [ChemblTargetPipeline ](name ="chembl_target",source_name ="chembl",source_config_factory =TargetSourceConfig .from_source_config ,build_context =build_context ,id_column ="target_chembl_id",summary_event ="chembl_target.extract_summary",sort_by =("target_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
+    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
+        "Extract target records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of target_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted target records.\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        stage_start =time .perf_counter ()
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =TargetSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
+        chembl_client =ChemblClient (http_client )
+        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
+        if self .config .cli .dry_run :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
+            return pd .DataFrame ()
+        batch_size =source_config .batch_size
+        limit =self .config .cli .limit
+        select_fields =source_config .parameters .select_fields
+        target_client =ChemblTargetClient (chembl_client ,batch_size =min (batch_size ,25 ))
+        def fetch_targets (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
+            iterator =target_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
+            for item in iterator :
+                yield dict (item )
+        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="target_chembl_id",fetcher =fetch_targets ,select_fields =select_fields ,batch_size =batch_size ,chunk_size =min (batch_size ,100 ),max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release )
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_target.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
+        return dataframe
+    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
+        "Transform raw target data by normalizing fields and enriching with component/classification data."
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
+        df =df .copy ()
+        df =self ._harmonize_identifier_columns (df ,log )
+        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
+        if df .empty :
+            log .debug ("transform_empty_dataframe")
+            return df
+        log .info ("transform_started",rows =len (df ))
+        df =self ._normalize_identifiers (df ,log )
+        df =serialize_target_arrays (df ,self .config )
+        if not self .config .cli .dry_run :
+            df =self ._enrich_target_components (df ,log )
+        if not self .config .cli .dry_run :
+            df =self ._enrich_protein_classifications (df ,log )
+        df =self ._normalize_string_fields (df ,log )
+        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
+        df =self ._normalize_data_types (df ,TargetSchema ,log )
+        df =self ._order_schema_columns (df ,COLUMN_ORDER )
+        log .info ("transform_completed",rows =len (df ))
+        return df
+    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Harmonize identifier column names."
+        df =df .copy ()
+        actions :list [str ]=[]
+        if "target_id"in df .columns and "target_chembl_id"not in df .columns :
+            df ["target_chembl_id"]=df ["target_id"]
+            actions .append ("target_id->target_chembl_id")
+        alias_columns =[column for column in ("target_id",)if column in df .columns ]
+        if alias_columns :
+            df =df .drop (columns =alias_columns )
+            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
+        if actions :
+            log .debug ("identifier_harmonization",actions =actions )
+        return df
+    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize ChEMBL identifiers with regex validation."
+        rules =[IdentifierRule (name ="target_chembl",columns =["target_chembl_id"],pattern ="^CHEMBL\\d+$")]
+        normalized_df ,stats =normalize_identifier_columns (df ,rules )
+        invalid_info =stats .per_column .get ("target_chembl_id")
+        if invalid_info and invalid_info ["invalid"]>0 :
+            log .warning ("invalid_target_chembl_id",count =invalid_info ["invalid"])
+        return normalized_df
+    def _enrich_target_components (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Enrich targets with component data from /target_component endpoint.\n\n        Only enriches targets where uniprot_accessions or component_count are missing.\n        If data is already present from the main query (via serialize_target_arrays),\n        it will not be overwritten.\n        "
+        df =df .copy ()
+        if df .empty or "target_chembl_id"not in df .columns :
+            return df
+        needs_enrichment =df ["target_chembl_id"].notna ()
+        if "uniprot_accessions"in df .columns :
+            needs_enrichment =needs_enrichment &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
+        if "component_count"in df .columns :
+            needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["component_count"].isna ()|(df ["component_count"]==0 ))
+        target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
+        if not target_ids_to_enrich :
+            log .debug ("enrich_target_components_skipped",reason ="all_data_present")
+            return df
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =TargetSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
+        chembl_client =ChemblClient (http_client )
+        if "target_chembl_id"not in df .columns :
+            return df
+        component_map :dict [str ,list [str ]]={}
+        target_ids_set :set [str ]=set (target_ids_to_enrich )
+        def _is_target (value :object )->bool :
+            if value is None or value is pd .NA :
+                return False
+            if isinstance (value ,Real )and math .isnan (float (value )):
+                return False
+            normalized =str (value ).strip ()
+            if not normalized :
+                return False
+            return normalized in target_ids_set
+        target_membership =df ["target_chembl_id"].map (_is_target )
+        log .info ("enrich_target_components_start",target_count =len (target_ids_to_enrich ))
+        for target_id in target_ids_to_enrich :
+            try :
+                components :list [str ]=[]
+                for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
+                    accession =item .get ("accession")
+                    if isinstance (accession ,str )and accession .strip ():
+                        components .append (accession .strip ())
+                if components :
+                    component_map [target_id ]=components
+            except Exception as exc :
+                log .warning ("target_component_fetch_error",target_chembl_id =target_id ,error =str (exc ))
+        if "uniprot_accessions"in df .columns :
+            mask =target_membership &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
+            if mask .any ():
+                df .loc [mask ,"uniprot_accessions"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
+        if "component_count"in df .columns :
+            mask =target_membership &(df ["component_count"].isna ()|(df ["component_count"]==0 ))
+            if mask .any ():
+                df .loc [mask ,"component_count"]=df .loc [mask ,"target_chembl_id"].map (lambda x :len (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
+        log .info ("enrich_target_components_complete",enriched_count =len (component_map ))
+        return df
+    def _enrich_protein_classifications (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Enrich targets with full protein classification hierarchy.\n\n        Extracts complete protein classification hierarchy with tree nodes and expanded paths l1..l8.\n        Algorithm:\n        1. For each target, get components via /target_component.json\n        2. Filter only PROTEIN components (component_type = 'PROTEIN')\n        3. For each protein component, get classes via /component_class.json \u2192 protein_class_id\n        4. For each protein_class_id, get node metadata via /protein_classification.json\n        5. For each protein_class_id, get expanded path via /protein_family_classification.json \u2192 l1..l8\n        6. Aggregate at TID level: protein_class_list (array) and protein_class_top (min class_level)\n\n        Only enriches targets where protein_class_list or protein_class_top are missing.\n        If data is already present from the main query, it will not be overwritten.\n        "
+        df =df .copy ()
+        if df .empty or "target_chembl_id"not in df .columns :
+            return df
+        needs_enrichment =df ["target_chembl_id"].notna ()
+        if "protein_class_list"in df .columns :
+            needs_enrichment =needs_enrichment &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
+        if "protein_class_top"in df .columns :
+            needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
+        target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
+        if not target_ids_to_enrich :
+            log .debug ("enrich_protein_classifications_skipped",reason ="all_data_present")
+            return df
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =TargetSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
+        chembl_client =ChemblClient (http_client )
+        if "protein_class_list"not in df .columns :
+            df ["protein_class_list"]=pd .NA
+        if "protein_class_top"not in df .columns :
+            df ["protein_class_top"]=pd .NA
+        if "target_chembl_id"not in df .columns :
+            return df
+        classification_list_map :dict [str ,list [dict [str ,Any ]]]={}
+        classification_top_map :dict [str ,dict [str ,Any ]]={}
+        target_ids_set :set [str ]=set (target_ids_to_enrich )
+        def _is_target (value :object )->bool :
+            if value is None or value is pd .NA :
+                return False
+            if isinstance (value ,Real )and math .isnan (float (value )):
+                return False
+            normalized =str (value ).strip ()
+            if not normalized :
+                return False
+            return normalized in target_ids_set
+        target_membership =df ["target_chembl_id"].map (_is_target )
+        log .info ("enrich_protein_classifications_start",target_count =len (target_ids_to_enrich ))
+        for target_id in target_ids_to_enrich :
+            try :
+                component_ids :list [str ]=[]
+                for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
+                    component_id =item .get ("component_id")
+                    if component_id is not None :
+                        component_ids .append (str (component_id ))
+                if not component_ids :
+                    continue
+                protein_component_ids :list [str ]=[]
+                for component_id in component_ids :
+                    try :
+                        for seq_item in chembl_client .paginate ("/component_sequence.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_sequences"):
+                            component_type =seq_item .get ("component_type")
+                            if isinstance (component_type ,str )and component_type .upper ()=="PROTEIN":
+                                protein_component_ids .append (component_id )
+                                break
+                    except Exception as exc :
+                        log .debug ("component_sequence_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
+                if not protein_component_ids :
+                    continue
+                protein_class_ids :set [str ]=set ()
+                for component_id in protein_component_ids :
+                    try :
+                        for class_item in chembl_client .paginate ("/component_class.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_classes"):
+                            protein_class_id =class_item .get ("protein_class_id")
+                            if protein_class_id is not None :
+                                protein_class_ids .add (str (protein_class_id ))
+                    except Exception as exc :
+                        log .debug ("component_class_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
+                if not protein_class_ids :
+                    continue
+                protein_classes :list [dict [str ,Any ]]=[]
+                for protein_class_id in protein_class_ids :
+                    try :
+                        node_metadata :dict [str ,Any ]|None =None
+                        for node_item in chembl_client .paginate ("/protein_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_classifications"):
+                            node_metadata ={"protein_class_id":str (protein_class_id ),"pref_name":node_item .get ("pref_name"),"short_name":node_item .get ("short_name"),"class_level":node_item .get ("class_level"),"parent_id":node_item .get ("parent_id"),"protein_class_desc":node_item .get ("protein_class_desc")}
+                            break
+                        path_levels :list [str |None ]=[None ]*8
+                        try :
+                            for path_item in chembl_client .paginate ("/protein_family_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_family_classifications"):
+                                for i in range (1 ,9 ):
+                                    level_key =f'l{i }'
+                                    level_value =path_item .get (level_key )
+                                    if level_value is not None :
+                                        if isinstance (level_value ,(float ,int )):
+                                            if pd .isna (level_value ):
+                                                path_levels [i -1 ]=None
+                                            else :
+                                                path_levels [i -1 ]=str (level_value )
+                                        else :
+                                            path_levels [i -1 ]=str (level_value )
+                                break
+                        except Exception as exc :
+                            log .debug ("protein_family_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
+                        if node_metadata :
+                            class_obj :dict [str ,Any ]={"protein_class_id":node_metadata ["protein_class_id"],"pref_name":node_metadata .get ("pref_name"),"short_name":node_metadata .get ("short_name"),"class_level":node_metadata .get ("class_level"),"parent_id":node_metadata .get ("parent_id"),"protein_class_desc":node_metadata .get ("protein_class_desc"),"path":[level for level in path_levels if level is not None ]}
+                            protein_classes .append (class_obj )
+                    except Exception as exc :
+                        log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
+                if protein_classes :
+                    seen_ids :set [str ]=set ()
+                    unique_classes :list [dict [str ,Any ]]=[]
+                    for class_obj in protein_classes :
+                        class_id =class_obj .get ("protein_class_id")
+                        if class_id and class_id not in seen_ids :
+                            seen_ids .add (class_id )
+                            unique_classes .append (class_obj )
+                    unique_classes .sort (key =lambda x :(x .get ("class_level")is None ,x .get ("class_level")or 0 ))
+                    classification_list_map [target_id ]=unique_classes
+                    top_class :dict [str ,Any ]|None =None
+                    min_level :int |None =None
+                    for class_obj in unique_classes :
+                        level =class_obj .get ("class_level")
+                        if level is not None :
+                            try :
+                                level_int =int (level )if not isinstance (level ,int )else level
+                                if min_level is None or level_int <min_level :
+                                    min_level =level_int
+                                    top_class =class_obj
+                            except (ValueError ,TypeError ):
+                                continue
+                    if top_class :
+                        classification_top_map [target_id ]=top_class
+            except Exception as exc :
+                log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,error =str (exc ))
+        if "protein_class_list"in df .columns :
+            mask =target_membership &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
+            if mask .any ():
+                df .loc [mask ,"protein_class_list"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_list_map .get (str (x ),[]),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_list_map else pd .NA )
+        if "protein_class_top"in df .columns :
+            mask =target_membership &(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
+            if mask .any ():
+                df .loc [mask ,"protein_class_top"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_top_map .get (str (x ),{}),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_top_map else pd .NA )
+        log .info ("enrich_protein_classifications_complete",enriched_list_count =len (classification_list_map ),enriched_top_count =len (classification_top_map ))
+        return df
+    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize string fields by trimming whitespace."
+        working_df =df .copy ()
+        rules ={"pref_name":StringRule (),"target_type":StringRule (),"organism":StringRule (),"tax_id":StringRule ()}
+        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        if stats .has_changes :
+            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        return normalized_df
+    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any |None ,log :Any )->pd .DataFrame :
+        "Normalize data types to match schema expectations.\n\n        Overrides base implementation to handle component_count and species_group_flag specially.\n        "
+        df =super ()._normalize_data_types (df ,schema ,log )
+        def _coerce_nullable_int (value :object )->object :
+            if value is None or value is pd .NA :
+                return pd .NA
+            if isinstance (value ,bool ):
+                return int (value )
+            if isinstance (value ,Integral ):
+                return int (value )
+            if isinstance (value ,Decimal ):
+                if value .is_nan ()or value %1 !=0 :
+                    return pd .NA
+                return int (value )
+            if isinstance (value ,Real ):
+                float_value =float (value )
+                if not math .isfinite (float_value )or not float_value .is_integer ():
+                    return pd .NA
+                return int (float_value )
+            if isinstance (value ,str ):
+                stripped =value .strip ()
+                if not stripped :
+                    return pd .NA
+                try :
+                    decimal_value =Decimal (stripped )
+                except (InvalidOperation ,ValueError ):
+                    return pd .NA
+                if decimal_value .is_nan ()or decimal_value %1 !=0 :
+                    return pd .NA
+                return int (decimal_value )
+            return pd .NA
+        if "component_count"in df .columns :
+            coerced_component_count =df ["component_count"].map (_coerce_nullable_int )
+            df ["component_count"]=coerced_component_count .astype ("Int64")
+        if "species_group_flag"in df .columns :
+            try :
+                coerced_species_group_flag =df ["species_group_flag"].map (_coerce_nullable_int )
+                df ["species_group_flag"]=coerced_species_group_flag .astype ("Int64")
+            except (ValueError ,TypeError )as exc :
+                log .warning ("species_group_flag_conversion_failed",error =str (exc ))
+        return df
```

#### Hotspot 19

- Definition: ChemblTargetPipeline.__init__#1
- assay: absent
- target: src/bioetl/pipelines/chembl/target/run.py:43-44

```diff
--- assay:run.py
+++ target:run.py
@@ -0,0 +1,2 @@
+def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+    super ().__init__ (config ,run_id )
```

#### Hotspot 20

- Definition: ChemblTargetPipeline._build_target_descriptor#1
- assay: absent
- target: src/bioetl/pipelines/chembl/target/run.py:72-141

```diff
--- assay:run.py
+++ target:run.py
@@ -0,0 +1,19 @@
+def _build_target_descriptor (self )->ChemblExtractionDescriptor ["ChemblTargetPipeline"]:
+    "Return the descriptor powering target extraction."
+    def build_context (pipeline :"ChemblTargetPipeline",source_config :TargetSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+        base_url =pipeline ._resolve_base_url (source_config .parameters )
+        http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
+        chembl_client =ChemblClient (http_client )
+        pipeline ._chembl_release =pipeline .fetch_chembl_release (chembl_client ,log )
+        target_client =ChemblTargetClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
+        select_fields =source_config .parameters .select_fields
+        return ChemblExtractionContext (source_config =source_config ,iterator =target_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =pipeline ._chembl_release ,extra_filters ={"batch_size":source_config .batch_size })
+    def empty_frame (_ :"ChemblTargetPipeline",__ :ChemblExtractionContext )->pd .DataFrame :
+        return pd .DataFrame ({"target_chembl_id":pd .Series (dtype ="string")})
+    def dry_run_handler (pipeline :"ChemblTargetPipeline",_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =pipeline ._chembl_release )
+        return pd .DataFrame ()
+    def summary_extra (pipeline :"ChemblTargetPipeline",_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+        return {"limit":pipeline .config .cli .limit }
+    return ChemblExtractionDescriptor [ChemblTargetPipeline ](name ="chembl_target",source_name ="chembl",source_config_factory =TargetSourceConfig .from_source_config ,build_context =build_context ,id_column ="target_chembl_id",summary_event ="chembl_target.extract_summary",sort_by =("target_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
```

_Only the first 20 hotspots of 56 are shown for module run.py._

_First 20 hotspots of 90 are shown for pair assay ↔ target._

---

## Pair: assay ↔ testitem

- AST hash: 4e65bc8b94391cb8c868cf6ad530c2c6 ↔ 729263c98934cfcb08473bcacfb20e9e

- Jaccard over tokens: 0.259

### Module run.py

- File status: assay — present, testitem — present
- AST hash: 6e346915a9c1b06ff585eb00b983cb26 ↔ 4380f065a3fdcc94928fad4ab20f1d2b
- Jaccard over tokens: 0.317

Definition                                        | assay signature                                                   | testitem signature                                                           | Side effects                                                                                                                                | Exceptions                           | Status          
--------------------------------------------------|-------------------------------------------------------------------|------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------|-----------------
ChemblAssayPipeline                               | —                                                                 | —                                                                            | assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning<br>testitem: ∅                                                     | assay: TypeError(msg)<br>testitem: ∅ | only in assay   
ChemblAssayPipeline.__init__                      | self, config: PipelineConfig, run_id: str                         | —                                                                            | assay: io=∅; logging=∅<br>testitem: ∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline._add_row_metadata             | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: io=∅; logging=log.debug<br>testitem: ∅                                                                                               | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline._build_assay_descriptor       | self: SelfChemblAssayPipeline                                     | —                                                                            | assay: io=∅; logging=log.debug, log.info, log.warning<br>testitem: ∅                                                                        | assay: TypeError(msg)<br>testitem: ∅ | only in assay   
ChemblAssayPipeline._check_missing_columns        | self, df: pd.DataFrame, log: Any, select_fields: list[str] | None | —                                                                            | assay: io=∅; logging=log.debug, log.warning<br>testitem: ∅                                                                                  | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline._enrich_with_related_data     | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: io=∅; logging=log.debug, log.info, log.warning<br>testitem: ∅                                                                        | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline._harmonize_identifier_columns | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: io=∅; logging=log.debug<br>testitem: ∅                                                                                               | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline._normalize_data_types         | self, df: pd.DataFrame, schema: Any, log: Any                     | —                                                                            | assay: io=∅; logging=log.debug<br>testitem: ∅                                                                                               | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline._normalize_identifiers        | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: io=∅; logging=log.debug<br>testitem: ∅                                                                                               | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline._normalize_nested_structures  | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: io=∅; logging=log.debug<br>testitem: ∅                                                                                               | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline._normalize_string_fields      | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: io=∅; logging=log.debug, log.warning<br>testitem: ∅                                                                                  | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline._serialize_array_fields       | self, df: pd.DataFrame, log: Any                                  | —                                                                            | assay: io=∅; logging=log.debug<br>testitem: ∅                                                                                               | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline.chembl_release                | self                                                              | —                                                                            | assay: io=∅; logging=∅<br>testitem: ∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline.extract                       | self, *args, **kwargs                                             | —                                                                            | assay: io=∅; logging=UnifiedLogger.get<br>testitem: ∅                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline.extract_all                   | self                                                              | —                                                                            | assay: io=∅; logging=∅<br>testitem: ∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline.extract_by_ids                | self, ids: Sequence[str]                                          | —                                                                            | assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning<br>testitem: ∅                                                     | assay: ∅<br>testitem: ∅              | only in assay   
ChemblAssayPipeline.transform                     | self, df: pd.DataFrame                                            | —                                                                            | assay: io=∅; logging=UnifiedLogger.get, log.debug, log.info<br>testitem: ∅                                                                  | assay: ∅<br>testitem: ∅              | only in assay   
TestItemChemblPipeline                            | —                                                                 | —                                                                            | assay: ∅<br>testitem: io=status_payload.get; logging=UnifiedLogger.get, bound_log.info, bound_log.warning, log.debug, log.info, log.warning | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.__init__                   | —                                                                 | self, config: PipelineConfig, run_id: str                                    | assay: ∅<br>testitem: io=∅; logging=∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._build_testitem_descriptor | —                                                                 | self: SelfTestitemChemblPipeline                                             | assay: ∅<br>testitem: io=∅; logging=log.debug, log.info                                                                                     | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._check_empty_columns       | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: ∅<br>testitem: io=∅; logging=log.warning                                                                                             | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._deduplicate_molecules     | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: ∅<br>testitem: io=∅; logging=log.info                                                                                                | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._fetch_chembl_release      | —                                                                 | self, client: UnifiedAPIClient | ChemblClient | Any, log: BoundLogger | None | assay: ∅<br>testitem: io=status_payload.get; logging=UnifiedLogger.get, bound_log.info, bound_log.warning                                   | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._flatten_nested_structures | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._normalize_identifiers     | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._normalize_numeric_fields  | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._normalize_string_fields   | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._remove_extra_columns      | —                                                                 | self, df: pd.DataFrame, log: Any                                             | assay: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._schema_column_specs       | —                                                                 | self                                                                         | assay: ∅<br>testitem: io=∅; logging=∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.api_version                | —                                                                 | self                                                                         | assay: ∅<br>testitem: io=∅; logging=∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.augment_metadata           | —                                                                 | self, metadata: Mapping[str, object], df: pd.DataFrame                       | assay: ∅<br>testitem: io=∅; logging=∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.chembl_db_version          | —                                                                 | self                                                                         | assay: ∅<br>testitem: io=∅; logging=∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.extract                    | —                                                                 | self, *args, **kwargs                                                        | assay: ∅<br>testitem: io=∅; logging=UnifiedLogger.get                                                                                       | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.extract_all                | —                                                                 | self                                                                         | assay: ∅<br>testitem: io=∅; logging=∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.extract_by_ids             | —                                                                 | self, ids: Sequence[str]                                                     | assay: ∅<br>testitem: io=∅; logging=UnifiedLogger.get, log.debug, log.info                                                                  | assay: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.transform                  | —                                                                 | self, df: pd.DataFrame                                                       | assay: ∅<br>testitem: io=∅; logging=UnifiedLogger.get, log.debug, log.info                                                                  | assay: ∅<br>testitem: ∅              | only in testitem
__module_block_0                                  | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_1                                  | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | identical       
__module_block_10                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_11                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_12                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_13                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | identical       
__module_block_14                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | identical       
__module_block_15                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | identical       
__module_block_16                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_17                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_18                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_19                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: ∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   
__module_block_2                                  | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_20                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: ∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   
__module_block_21                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: ∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   
__module_block_22                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: ∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   
__module_block_26                                 | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: ∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   
__module_block_3                                  | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_4                                  | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_5                                  | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_6                                  | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_7                                  | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_8                                  | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
__module_block_9                                  | —                                                                 | —                                                                            | assay: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | assay: ∅<br>testitem: ∅              | differs         
_extract_bao_ids_from_classifications             | node: Any                                                         | —                                                                            | assay: io=∅; logging=∅<br>testitem: ∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   
_iter_classification_mappings                     | node: Any                                                         | —                                                                            | assay: io=∅; logging=∅<br>testitem: ∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   
_normalize_bao_identifier                         | raw_value: Any                                                    | —                                                                            | assay: io=∅; logging=∅<br>testitem: ∅                                                                                                       | assay: ∅<br>testitem: ∅              | only in assay   

#### Hotspot 1

- Definition: ChemblAssayPipeline#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:126-952
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,324 +0,0 @@
-class ChemblAssayPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting assay records from the ChEMBL API."
-    actor ="assay_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._chembl_release :str |None =None
-    @property
-    def chembl_release (self )->str |None :
-        "Return the cached ChEMBL release captured during extraction."
-        return self ._chembl_release
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all assay records from ChEMBL using pagination."
-        descriptor =self ._build_assay_descriptor ()
-        return self .run_extract_all (descriptor )
-    def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-        "Return the descriptor powering the shared extraction template."
-        def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-            if isinstance (pipeline ,ChemblAssayPipeline ):
-                return pipeline
-            msg ="ChemblAssayPipeline instance required"
-            raise TypeError (msg )
-        def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-            chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-            assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-            assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-            assay_pipeline ._chembl_release =assay_client .chembl_release
-            log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-            raw_source =assay_pipeline ._resolve_source_config ("chembl")
-            select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-            log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-            context =ChemblExtractionContext (source_config ,assay_client )
-            context .chembl_client =chembl_client
-            context .select_fields =tuple (select_fields )if select_fields else None
-            context .chembl_release =assay_pipeline ._chembl_release
-            context .extra_filters ={"max_url_length":source_config .max_url_length }
-            return context
-        def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-        def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            if df .empty :
-                return df
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in df .columns or df [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            select_fields =context .select_fields
-            if select_fields :
-                expected_fields =set (select_fields )
-                actual_fields =set (df .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-            return df
-        def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-            return pd .DataFrame ()
-        def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            assay_pipeline =_require_assay_pipeline (pipeline )
-            return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-        return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        self ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        if self .config .cli .dry_run :
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-            return pd .DataFrame ()
-        limit =self .config .cli .limit
-        resolved_select_fields =self ._resolve_select_fields (source_raw )
-        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-        log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-        def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield dict (item )
-        def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-            if dataframe .empty :
-                return dataframe
-            for must_field in ("assay_category","assay_group","src_assay_id"):
-                if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                    log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-            if context .select_fields :
-                expected_fields =set (context .select_fields )
-                actual_fields =set (dataframe .columns )
-                missing_in_response =sorted (expected_fields -actual_fields )
-                if missing_in_response :
-                    log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-            dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-            return dataframe
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw assay data by normalizing identifiers, types, and nested structures."
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._enrich_with_related_data (df ,log )
-        df =self ._normalize_nested_structures (df ,log )
-        df =self ._serialize_array_fields (df ,log )
-        df =self ._add_row_metadata (df ,log )
-        df =self ._normalize_data_types (df ,AssaySchema ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Serialize array-of-object fields to header+rows format."
-        df =df .copy ()
-        arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-        if arrays_to_serialize :
-            df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-            for column in arrays_to_serialize :
-                if column in df_result .columns :
-                    column_as_string :Series =df_result [column ].astype ("string")
-                    filled_column :Series =column_as_string .copy ()
-                    filled_column [column_as_string .isna ()]=""
-                    empty_mask :Series =filled_column .eq ("")
-                    if bool (empty_mask .any ()):
-                        df_result .loc [empty_mask ,column ]=pd .NA
-            df =df_result
-            log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-        return df
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-            df ["assay_chembl_id"]=df ["assay_id"]
-            actions .append ("assay_id->assay_chembl_id")
-        if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-            df ["target_chembl_id"]=df ["target_id"]
-            actions .append ("target_id->target_chembl_id")
-        alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize ChEMBL identifiers with regex validation."
-        rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        if stats .has_changes :
-            log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-        return normalized_df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-        working_df =df .copy ()
-        string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-        rules ={column :StringRule ()for column in string_fields }
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if "curation_level"not in normalized_df .columns :
-            normalized_df ["curation_level"]=pd .NA
-            log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-        df =df .copy ()
-        if "assay_parameters"in df .columns :
-            log .debug ("validating_assay_parameters_truv")
-            df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-        if "assay_classifications"in df .columns :
-            if "assay_class_id"not in df .columns :
-                df ["assay_class_id"]=pd .NA
-            updated_rows =0
-            classifications_series =df ["assay_classifications"]
-            for row_index ,value in classifications_series .items ():
-                if value is None or value is pd .NA :
-                    continue
-                if isinstance (value ,float )and pd .isna (value ):
-                    continue
-                if isinstance (value ,str ):
-                    continue
-                extracted_ids =_extract_bao_ids_from_classifications (value )
-                if not extracted_ids :
-                    continue
-                joined_ids =";".join (extracted_ids )
-                row_label =cast (Any ,row_index )
-                current_value =df .at [row_label ,"assay_class_id"]
-                if pd .isna (current_value )or current_value !=joined_ids :
-                    df .at [row_label ,"assay_class_id"]=joined_ids
-                    updated_rows +=1
-            if updated_rows >0 :
-                log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-        return df
-    def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Add required row metadata fields (row_subtype, row_index)."
-        df =df .copy ()
-        if df .empty :
-            return df
-        if "row_subtype"not in df .columns :
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_added",value ="assay")
-        elif df ["row_subtype"].isna ().all ():
-            df ["row_subtype"]="assay"
-            log .debug ("row_subtype_filled",value ="assay")
-        if "row_index"not in df .columns :
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_added",count =len (df ))
-        elif df ["row_index"].isna ().all ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-        "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-        df =super ()._normalize_data_types (df ,schema ,log )
-        if "row_index"in df .columns and df ["row_index"].isna ().any ():
-            df ["row_index"]=range (len (df ))
-            log .debug ("row_index_filled",count =len (df ))
-        return df
-    def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-        df =df .copy ()
-        optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-        expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-        select_fields_set :set [str ]=set ()
-        if select_fields is not None :
-            select_fields_set =set (select_fields )
-        missing_in_response :list [str ]=[]
-        missing_in_select_fields :list [str ]=[]
-        for column in expected_api_fields :
-            if column not in df .columns :
-                missing_in_response .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-                else :
-                    log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-        missing_columns :list [str ]=[]
-        for column ,version in optional_columns .items ():
-            if column not in df .columns :
-                df [column ]=pd .NA
-                missing_columns .append (column )
-                if select_fields is not None and column not in select_fields_set :
-                    missing_in_select_fields .append (column )
-                    log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-                else :
-                    log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-        if missing_in_response or missing_columns :
-            log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-        return df
-    def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-        if df .empty :
-            return df
-        try :
-            source_raw =self ._resolve_source_config ("chembl")
-        except KeyError as exc :
-            log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-            return df
-        source_config =AssaySourceConfig .from_source_config (source_raw )
-        parameters =self ._normalize_parameters (source_config .parameters )
-        base_url =self ._resolve_base_url (parameters )
-        http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        chembl_config =getattr (self .config ,"chembl",None )
-        if chembl_config is None :
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-            return df
-        if not isinstance (chembl_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-            return df
-        assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-        if not isinstance (assay_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-            return df
-        enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-        if not isinstance (enrich_config ,Mapping ):
-            log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-            return df
-        classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-        if classifications_cfg is not None :
-            log .info ("enrichment_classifications_started")
-            df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-            df =df_with_classifications
-            log .info ("enrichment_classifications_completed")
-            if "assay_class_id"in df .columns :
-                filled_count =int (df ["assay_class_id"].notna ().sum ())
-                total_count =len (df )
-                if filled_count ==0 :
-                    log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-                else :
-                    log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-            else :
-                log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-        else :
-            log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-        parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-        if parameters_cfg is not None :
-            log .info ("enrichment_parameters_started")
-            df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-            df =df_with_parameters
-            log .info ("enrichment_parameters_completed")
-        return df
```

#### Hotspot 2

- Definition: ChemblAssayPipeline.__init__#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:131-133
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,3 +0,0 @@
-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._chembl_release :str |None =None
```

#### Hotspot 3

- Definition: ChemblAssayPipeline._add_row_metadata#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:681-704
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,18 +0,0 @@
-def _add_row_metadata (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Add required row metadata fields (row_subtype, row_index)."
-    df =df .copy ()
-    if df .empty :
-        return df
-    if "row_subtype"not in df .columns :
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_added",value ="assay")
-    elif df ["row_subtype"].isna ().all ():
-        df ["row_subtype"]="assay"
-        log .debug ("row_subtype_filled",value ="assay")
-    if "row_index"not in df .columns :
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_added",count =len (df ))
-    elif df ["row_index"].isna ().all ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Hotspot 4

- Definition: ChemblAssayPipeline._build_assay_descriptor#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:167-323
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,51 +0,0 @@
-def _build_assay_descriptor (self :SelfChemblAssayPipeline )->ChemblExtractionDescriptor [SelfChemblAssayPipeline ]:
-    "Return the descriptor powering the shared extraction template."
-    def _require_assay_pipeline (pipeline :ChemblPipelineBase )->ChemblAssayPipeline :
-        if isinstance (pipeline ,ChemblAssayPipeline ):
-            return pipeline
-        msg ="ChemblAssayPipeline instance required"
-        raise TypeError (msg )
-    def build_context (pipeline :SelfChemblAssayPipeline ,source_config :AssaySourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        http_client ,_ =assay_pipeline .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-        chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
-        assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-        assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-        assay_pipeline ._chembl_release =assay_client .chembl_release
-        log .info ("chembl_assay.handshake",chembl_release =assay_pipeline ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-        raw_source =assay_pipeline ._resolve_source_config ("chembl")
-        select_fields =assay_pipeline ._resolve_select_fields (raw_source )
-        log .debug ("chembl_assay.select_fields",fields =select_fields ,fields_count =len (select_fields )if select_fields else 0 )
-        context =ChemblExtractionContext (source_config ,assay_client )
-        context .chembl_client =chembl_client
-        context .select_fields =tuple (select_fields )if select_fields else None
-        context .chembl_release =assay_pipeline ._chembl_release
-        context .extra_filters ={"max_url_length":source_config .max_url_length }
-        return context
-    def empty_frame (_ :SelfChemblAssayPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"assay_chembl_id":pd .Series (dtype ="string")})
-    def post_process (pipeline :SelfChemblAssayPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext ,log :BoundLogger )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        if df .empty :
-            return df
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in df .columns or df [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        select_fields =context .select_fields
-        if select_fields :
-            expected_fields =set (select_fields )
-            actual_fields =set (df .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (select_fields ),received_fields_count =len (actual_fields ),chembl_release =assay_pipeline ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        df =assay_pipeline ._check_missing_columns (df ,log ,select_fields =list (select_fields )if select_fields else None )
-        return df
-    def dry_run_handler (pipeline :SelfChemblAssayPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =assay_pipeline ._chembl_release )
-        return pd .DataFrame ()
-    def summary_extra (pipeline :SelfChemblAssayPipeline ,_ :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        assay_pipeline =_require_assay_pipeline (pipeline )
-        return {"handshake_endpoint":context .source_config .parameters .handshake_endpoint ,"limit":assay_pipeline .config .cli .limit }
-    return ChemblExtractionDescriptor [SelfChemblAssayPipeline ](name ="chembl_assay",source_name ="chembl",source_config_factory =AssaySourceConfig .from_source_config ,build_context =build_context ,id_column ="assay_chembl_id",summary_event ="chembl_assay.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =MUST_HAVE_FIELDS ,post_processors =(post_process ,),sort_by =("assay_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
```

#### Hotspot 5

- Definition: ChemblAssayPipeline._check_missing_columns#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:720-835
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,31 +0,0 @@
-def _check_missing_columns (self ,df :pd .DataFrame ,log :Any ,select_fields :list [str ]|None =None )->pd .DataFrame :
-    "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0434\u043b\u044f \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u043d\u043e\u0441\u0442\u0438 ChEMBL (v34/v35).\n\n        \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 NULL \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f.\n        \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432 select_fields, \u043b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 WARN \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0437 API.\n        \u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442 \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437 \u0441\u0445\u0435\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        select_fields:\n            \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u044b \u0438\u0437 API \u0447\u0435\u0440\u0435\u0437 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 only=.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c\u0438.\n        "
-    df =df .copy ()
-    optional_columns ={"assay_strain":"v34","assay_group":"v35","curation_level":"unknown"}
-    expected_api_fields ={"assay_category","assay_cell_type","assay_group","assay_strain","assay_subcellular_fraction","assay_test_type","assay_tissue","cell_chembl_id","curation_level","src_assay_id","tissue_chembl_id","variant_sequence"}
-    select_fields_set :set [str ]=set ()
-    if select_fields is not None :
-        select_fields_set =set (select_fields )
-    missing_in_response :list [str ]=[]
-    missing_in_select_fields :list [str ]=[]
-    for column in expected_api_fields :
-        if column not in df .columns :
-            missing_in_response .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_field_not_requested",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } not found in API response and was not requested in select_fields')
-            else :
-                log .warning ("missing_field_in_response",column =column ,chembl_release =self ._chembl_release ,message =f'Field {column } was requested but not found in API response')
-    missing_columns :list [str ]=[]
-    for column ,version in optional_columns .items ():
-        if column not in df .columns :
-            df [column ]=pd .NA
-            missing_columns .append (column )
-            if select_fields is not None and column not in select_fields_set :
-                missing_in_select_fields .append (column )
-                log .warning ("missing_column_not_requested",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response and was not requested in select_fields, setting to NULL')
-            else :
-                log .warning ("missing_optional_column",column =column ,version_introduced =version ,chembl_release =self ._chembl_release ,message =f'Column {column } not found in API response, setting to NULL')
-    if missing_in_response or missing_columns :
-        log .debug ("missing_columns_handled",missing_in_response =missing_in_response if missing_in_response else None ,missing_columns =missing_columns if missing_columns else None ,missing_in_select_fields =sorted (missing_in_select_fields )if missing_in_select_fields else None ,chembl_release =self ._chembl_release )
-    return df
```

#### Hotspot 6

- Definition: ChemblAssayPipeline._enrich_with_related_data#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:837-952
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,53 +0,0 @@
-def _enrich_with_related_data (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446 (ASSAY_CLASS_MAP, ASSAY_PARAMETERS).\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 assay.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n\n        Returns\n        -------\n        pd.DataFrame:\n            \u041e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u043d\u044b\u0439 DataFrame \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446.\n        "
-    if df .empty :
-        return df
-    try :
-        source_raw =self ._resolve_source_config ("chembl")
-    except KeyError as exc :
-        log .debug ("enrichment_skipped_missing_source",source ="chembl",message ="Skipping enrichment: source configuration not available",error =str (exc ))
-        return df
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    parameters =self ._normalize_parameters (source_config .parameters )
-    base_url =self ._resolve_base_url (parameters )
-    http_client =self ._client_factory .for_source ("chembl",base_url =base_url )
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    chembl_config =getattr (self .config ,"chembl",None )
-    if chembl_config is None :
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config not found")
-        return df
-    if not isinstance (chembl_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_chembl_config",message ="ChEMBL config is not a Mapping")
-        return df
-    assay_config =cast (Mapping [str ,Any ],chembl_config ).get ("assay")
-    if not isinstance (assay_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_assay_config",message ="Assay config not found")
-        return df
-    enrich_config =cast (Mapping [str ,Any ],assay_config ).get ("enrich")
-    if not isinstance (enrich_config ,Mapping ):
-        log .debug ("enrichment_skipped_no_enrich_config",message ="Enrich config not found")
-        return df
-    classifications_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("classifications")
-    if classifications_cfg is not None :
-        log .info ("enrichment_classifications_started")
-        df_with_classifications :pd .DataFrame =enrich_with_assay_classifications (df ,chembl_client ,cast (Mapping [str ,Any ],classifications_cfg ))
-        df =df_with_classifications
-        log .info ("enrichment_classifications_completed")
-        if "assay_class_id"in df .columns :
-            filled_count =int (df ["assay_class_id"].notna ().sum ())
-            total_count =len (df )
-            if filled_count ==0 :
-                log .warning ("assay_class_id_empty_after_enrichment",total_assays =total_count ,filled_count =0 ,message ="assay_class_id is empty after enrichment. Check if ASSAY_CLASS_MAP contains data for these assays.")
-            else :
-                log .debug ("assay_class_id_enrichment_stats",total_assays =total_count ,filled_count =filled_count ,empty_count =total_count -filled_count )
-        else :
-            log .warning ("assay_class_id_column_missing_after_enrichment",message ="assay_class_id column is missing after enrichment")
-    else :
-        log .warning ("enrichment_classifications_disabled",message ="Enrichment for classifications is not configured. assay_class_id will remain NULL.")
-    parameters_cfg =cast (Mapping [str ,Any ],enrich_config ).get ("parameters")
-    if parameters_cfg is not None :
-        log .info ("enrichment_parameters_started")
-        df_with_parameters :pd .DataFrame =enrich_with_assay_parameters (df ,chembl_client ,cast (Mapping [str ,Any ],parameters_cfg ))
-        df =df_with_parameters
-        log .info ("enrichment_parameters_completed")
-    return df
```

#### Hotspot 7

- Definition: ChemblAssayPipeline._harmonize_identifier_columns#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:526-548
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,17 +0,0 @@
-def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Harmonize identifier column names (e.g., assay_id -> assay_chembl_id)."
-    df =df .copy ()
-    actions :list [str ]=[]
-    if "assay_id"in df .columns and "assay_chembl_id"not in df .columns :
-        df ["assay_chembl_id"]=df ["assay_id"]
-        actions .append ("assay_id->assay_chembl_id")
-    if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-        df ["target_chembl_id"]=df ["target_id"]
-        actions .append ("target_id->target_chembl_id")
-    alias_columns =[column for column in ("assay_id","target_id")if column in df .columns ]
-    if alias_columns :
-        df =df .drop (columns =alias_columns )
-        actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-    if actions :
-        log .debug ("identifier_harmonization",actions =actions )
-    return df
```

#### Hotspot 8

- Definition: ChemblAssayPipeline._normalize_data_types#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:706-718
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,7 +0,0 @@
-def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any ,log :Any )->pd .DataFrame :
-    "Convert data types according to the AssaySchema.\n\n        Overrides base implementation to handle row_index and confidence_score specially.\n        "
-    df =super ()._normalize_data_types (df ,schema ,log )
-    if "row_index"in df .columns and df ["row_index"].isna ().any ():
-        df ["row_index"]=range (len (df ))
-        log .debug ("row_index_filled",count =len (df ))
-    return df
```

#### Hotspot 9

- Definition: ChemblAssayPipeline._normalize_identifiers#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:550-577
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,7 +0,0 @@
-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize ChEMBL identifiers with regex validation."
-    rules =[IdentifierRule (name ="chembl",columns =["assay_chembl_id","target_chembl_id","document_chembl_id","cell_chembl_id","tissue_chembl_id"],pattern ="^CHEMBL\\d+$")]
-    normalized_df ,stats =normalize_identifier_columns (df ,rules )
-    if stats .has_changes :
-        log .debug ("identifiers_normalized",normalized_count =stats .normalized ,invalid_count =stats .invalid ,columns =list (stats .per_column .keys ()))
-    return normalized_df
```

#### Hotspot 10

- Definition: ChemblAssayPipeline._normalize_nested_structures#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:623-679
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,30 +0,0 @@
-def _normalize_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Process nested structures (assay_parameters, assay_classifications).\n\n        \u0412\u0410\u0416\u041d\u041e: \u041d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f assay_class_id.\n        assay_class_id \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c\u0441\u044f \u0438\u0437 ASSAY_CLASS_MAP \u0447\u0435\u0440\u0435\u0437 enrichment.\n        \u0415\u0441\u043b\u0438 enrichment \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c NULL.\n        "
-    df =df .copy ()
-    if "assay_parameters"in df .columns :
-        log .debug ("validating_assay_parameters_truv")
-        df =validate_assay_parameters_truv (df ,column ="assay_parameters",fail_fast =True )
-    if "assay_classifications"in df .columns :
-        if "assay_class_id"not in df .columns :
-            df ["assay_class_id"]=pd .NA
-        updated_rows =0
-        classifications_series =df ["assay_classifications"]
-        for row_index ,value in classifications_series .items ():
-            if value is None or value is pd .NA :
-                continue
-            if isinstance (value ,float )and pd .isna (value ):
-                continue
-            if isinstance (value ,str ):
-                continue
-            extracted_ids =_extract_bao_ids_from_classifications (value )
-            if not extracted_ids :
-                continue
-            joined_ids =";".join (extracted_ids )
-            row_label =cast (Any ,row_index )
-            current_value =df .at [row_label ,"assay_class_id"]
-            if pd .isna (current_value )or current_value !=joined_ids :
-                df .at [row_label ,"assay_class_id"]=joined_ids
-                updated_rows +=1
-        if updated_rows >0 :
-            log .debug ("assay_class_id_extracted_from_classifications",rows_updated =updated_rows )
-    return df
```

#### Hotspot 11

- Definition: ChemblAssayPipeline._normalize_string_fields#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:579-621
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,12 +0,0 @@
-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields (assay_type, assay_category, assay_organism, curation_level).\n\n        \u0412\u0410\u0416\u041d\u041e: \u0412\u0441\u0435 \u043f\u043e\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0437 API-\u043e\u0442\u0432\u0435\u0442\u0430, \u0431\u0435\u0437 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438\u0437 \u0441\u0443\u0440\u0440\u043e\u0433\u0430\u0442\u043e\u0432.\n        - assay_category: \u0438\u0437 ASSAYS.ASSAY_CATEGORY (\u043d\u0435 \u0438\u0437 assay_type \u0438\u043b\u0438 BAO)\n        - assay_strain: \u0438\u0437 ASSAYS.ASSAY_STRAIN (\u043d\u0435 \u0438\u0437 target/organism)\n        - src_assay_id: \u0438\u0437 ASSAYS.SRC_ASSAY_ID (\u043d\u0435 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n        - assay_group: \u0438\u0437 ASSAYS.ASSAY_GROUP\n        - curation_level: \u0438\u0437 \u044f\u0432\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c), \u0438\u043d\u0430\u0447\u0435 NULL\n        "
-    working_df =df .copy ()
-    string_fields =["assay_type","assay_category","assay_organism","assay_strain","src_assay_id","assay_group","curation_level"]
-    rules ={column :StringRule ()for column in string_fields }
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if "curation_level"not in normalized_df .columns :
-        normalized_df ["curation_level"]=pd .NA
-        log .warning ("curation_level_missing",message ="curation_level not found in API response, setting to NULL")
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    return normalized_df
```

#### Hotspot 12

- Definition: ChemblAssayPipeline._serialize_array_fields#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:504-524
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,17 +0,0 @@
-def _serialize_array_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Serialize array-of-object fields to header+rows format."
-    df =df .copy ()
-    arrays_to_serialize :list [str ]=list (self .config .transform .arrays_to_header_rows )
-    if arrays_to_serialize :
-        df_result :pd .DataFrame =serialize_array_fields (df ,arrays_to_serialize )
-        for column in arrays_to_serialize :
-            if column in df_result .columns :
-                column_as_string :Series =df_result [column ].astype ("string")
-                filled_column :Series =column_as_string .copy ()
-                filled_column [column_as_string .isna ()]=""
-                empty_mask :Series =filled_column .eq ("")
-                if bool (empty_mask .any ()):
-                    df_result .loc [empty_mask ,column ]=pd .NA
-        df =df_result
-        log .debug ("array_fields_serialized",columns =arrays_to_serialize )
-    return df
```

#### Hotspot 13

- Definition: ChemblAssayPipeline.chembl_release#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:136-139
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,4 +0,0 @@
-@property
-def chembl_release (self )->str |None :
-    "Return the cached ChEMBL release captured during extraction."
-    return self ._chembl_release
```

#### Hotspot 14

- Definition: ChemblAssayPipeline.extract#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:145-159
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,4 +0,0 @@
-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch assay payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_assay.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="assay_chembl_id")
```

#### Hotspot 15

- Definition: ChemblAssayPipeline.extract_all#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:161-165
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,4 +0,0 @@
-def extract_all (self )->pd .DataFrame :
-    "Extract all assay records from ChEMBL using pagination."
-    descriptor =self ._build_assay_descriptor ()
-    return self .run_extract_all (descriptor )
```

#### Hotspot 16

- Definition: ChemblAssayPipeline.extract_by_ids#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:325-469
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,42 +0,0 @@
-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract assay records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of assay_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted assay records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =AssaySourceConfig .from_source_config (source_raw )
-    http_client ,_ =self .prepare_chembl_client ("chembl",client_name ="chembl_assay_http")
-    chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    assay_client =ChemblAssayClient (chembl_client ,batch_size =source_config .batch_size ,max_url_length =source_config .max_url_length )
-    assay_client .handshake (endpoint =source_config .parameters .handshake_endpoint ,enabled =source_config .parameters .handshake_enabled )
-    self ._chembl_release =assay_client .chembl_release
-    log .info ("chembl_assay.handshake",chembl_release =self ._chembl_release ,handshake_endpoint =source_config .parameters .handshake_endpoint ,handshake_enabled =source_config .parameters .handshake_enabled )
-    if self .config .cli .dry_run :
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_assay.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-        return pd .DataFrame ()
-    limit =self .config .cli .limit
-    resolved_select_fields =self ._resolve_select_fields (source_raw )
-    merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-    log .debug ("chembl_assay.select_fields",fields =merged_select_fields ,fields_count =len (merged_select_fields )if merged_select_fields else 0 )
-    def fetch_assays (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        iterator =assay_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield dict (item )
-    def finalize_dataframe (dataframe :pd .DataFrame ,context :BatchExtractionContext )->pd .DataFrame :
-        if dataframe .empty :
-            return dataframe
-        for must_field in ("assay_category","assay_group","src_assay_id"):
-            if must_field not in dataframe .columns or dataframe [must_field ].isna ().all ():
-                log .warning ("chembl_assay.missing_required_field",field =must_field ,note ="Field not returned by API; check select_fields/only")
-        if context .select_fields :
-            expected_fields =set (context .select_fields )
-            actual_fields =set (dataframe .columns )
-            missing_in_response =sorted (expected_fields -actual_fields )
-            if missing_in_response :
-                log .warning ("assay_missing_fields_in_api_response",missing_fields =missing_in_response ,requested_fields_count =len (context .select_fields ),received_fields_count =len (actual_fields ),chembl_release =self ._chembl_release ,message =f'Fields requested in select_fields but missing in API response: {missing_in_response }')
-        dataframe =self ._check_missing_columns (dataframe ,log ,select_fields =list (context .select_fields )if context .select_fields else None )
-        return dataframe
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="assay_chembl_id",fetcher =fetch_assays ,select_fields =merged_select_fields or None ,batch_size =assay_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None ,"max_url_length":source_config .max_url_length },chembl_release =self ._chembl_release ,finalize =finalize_dataframe )
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_assay.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-    return dataframe
```

#### Hotspot 17

- Definition: ChemblAssayPipeline.transform#1
- assay: src/bioetl/pipelines/chembl/assay/run.py:471-498
- testitem: absent

```diff
--- assay:run.py
+++ testitem:run.py
@@ -1,21 +0,0 @@
-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw assay data by normalizing identifiers, types, and nested structures."
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-    df =df .copy ()
-    df =self ._harmonize_identifier_columns (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._enrich_with_related_data (df ,log )
-    df =self ._normalize_nested_structures (df ,log )
-    df =self ._serialize_array_fields (df ,log )
-    df =self ._add_row_metadata (df ,log )
-    df =self ._normalize_data_types (df ,AssaySchema ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

#### Hotspot 18

- Definition: TestItemChemblPipeline#1
- assay: absent
- testitem: src/bioetl/pipelines/chembl/testitem/run.py:52-807

```diff
--- assay:run.py
+++ testitem:run.py
@@ -0,0 +1,296 @@
+class TestItemChemblPipeline (ChemblPipelineBase ):
+    "ETL pipeline extracting molecule records from the ChEMBL API."
+    actor ="testitem_chembl"
+    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+        super ().__init__ (config ,run_id )
+        self ._chembl_db_version :str |None =None
+        self ._api_version :str |None =None
+    @property
+    def chembl_db_version (self )->str |None :
+        "Return the cached ChEMBL DB version captured during extraction."
+        return self ._chembl_db_version
+    @property
+    def api_version (self )->str |None :
+        "Return the cached ChEMBL API version captured during extraction."
+        return self ._api_version
+    def _fetch_chembl_release (self ,client :UnifiedAPIClient |ChemblClient |Any ,log :BoundLogger |None =None )->str |None :
+        "Capture ChEMBL release and API version from status endpoint."
+        if log is None :
+            bound_log :BoundLogger =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
+        else :
+            bound_log =log
+        request_timestamp =datetime .now (timezone .utc )
+        release_value :str |None =None
+        api_version :str |None =None
+        try :
+            status_payload :dict [str ,Any ]={}
+            handshake_candidate =getattr (client ,"handshake",None )
+            if callable (handshake_candidate ):
+                status_payload =self ._coerce_mapping (handshake_candidate ("/status"))
+            else :
+                get_candidate =getattr (client ,"get",None )
+                if callable (get_candidate ):
+                    response =get_candidate ("/status.json")
+                    json_candidate =getattr (response ,"json",None )
+                    if callable (json_candidate ):
+                        status_payload =self ._coerce_mapping (json_candidate ())
+            if status_payload :
+                release_value =self ._extract_chembl_release (status_payload )
+                api_candidate =status_payload .get ("api_version")
+                if isinstance (api_candidate ,str )and api_candidate .strip ():
+                    api_version =api_candidate
+                bound_log .info ("chembl_testitem.status",chembl_db_version =release_value ,api_version =api_version )
+        except Exception as exc :
+            bound_log .warning ("chembl_testitem.status_failed",error =str (exc ))
+        finally :
+            self ._chembl_db_version =release_value
+            self ._api_version =api_version
+            self .record_extract_metadata (chembl_release =release_value ,requested_at_utc =request_timestamp )
+        return release_value
+    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
+        "Fetch molecule payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        return self ._dispatch_extract_mode (log ,event_name ="chembl_testitem.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="molecule_chembl_id")
+    def extract_all (self )->pd .DataFrame :
+        "Extract all molecule records from ChEMBL using pagination."
+        descriptor =self ._build_testitem_descriptor ()
+        return self .run_extract_all (descriptor )
+    def _build_testitem_descriptor (self :SelfTestitemChemblPipeline )->ChemblExtractionDescriptor [SelfTestitemChemblPipeline ]:
+        "Return the descriptor powering testitem extraction."
+        def build_context (pipeline :SelfTestitemChemblPipeline ,source_config :TestItemSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+            base_url =pipeline ._resolve_base_url (source_config .parameters )
+            http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+            chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
+            pipeline ._fetch_chembl_release (chembl_client ,log )
+            select_fields =source_config .parameters .select_fields
+            log .debug ("chembl_testitem.select_fields",fields =select_fields )
+            testitem_client =ChemblTestitemClient (chembl_client ,batch_size =min (source_config .page_size ,25 ))
+            return ChemblExtractionContext (source_config =source_config ,iterator =testitem_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,page_size =source_config .page_size ,chembl_release =pipeline ._chembl_db_version ,metadata ={"api_version":pipeline ._api_version })
+        def empty_frame (_ :SelfTestitemChemblPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
+            return pd .DataFrame ({"molecule_chembl_id":pd .Series (dtype ="string")})
+        def dry_run_handler (pipeline :SelfTestitemChemblPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =pipeline ._chembl_db_version ,api_version =pipeline ._api_version )
+            return pd .DataFrame ()
+        def summary_extra (pipeline :SelfTestitemChemblPipeline ,_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+            return {"chembl_db_version":pipeline ._chembl_db_version ,"api_version":pipeline ._api_version ,"limit":pipeline .config .cli .limit }
+        return ChemblExtractionDescriptor [SelfTestitemChemblPipeline ](name ="chembl_testitem",source_name ="chembl",source_config_factory =TestItemSourceConfig .from_source_config ,build_context =build_context ,id_column ="molecule_chembl_id",summary_event ="chembl_testitem.extract_summary",must_have_fields =tuple (MUST_HAVE_FIELDS ),default_select_fields =MUST_HAVE_FIELDS ,sort_by =("molecule_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra ,hard_page_size_cap =None )
+    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
+        "Extract molecule records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of molecule_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted molecule records.\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        stage_start =time .perf_counter ()
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =TestItemSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
+        self ._fetch_chembl_release (chembl_client ,log )
+        if self .config .cli .dry_run :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =self ._chembl_db_version ,api_version =self ._api_version )
+            return pd .DataFrame ()
+        page_size =min (source_config .page_size ,25 )
+        limit =self .config .cli .limit
+        resolved_select_fields =source_config .parameters .select_fields
+        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
+        log .debug ("chembl_testitem.select_fields",fields =merged_select_fields )
+        testitem_client =ChemblTestitemClient (chembl_client ,batch_size =page_size )
+        def fetch_testitems (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
+            iterator =testitem_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
+            for item in iterator :
+                yield dict (item )
+        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="molecule_chembl_id",fetcher =fetch_testitems ,select_fields =merged_select_fields or None ,batch_size =page_size ,chunk_size =min (page_size ,25 ),max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None },chembl_release =self ._chembl_release )
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_testitem.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_db_version =self ._chembl_db_version ,api_version =self ._api_version ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
+        return dataframe
+    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
+        "Transform raw molecule data by normalizing fields and extracting nested properties."
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
+        df =df .copy ()
+        if df .empty :
+            log .debug ("transform_empty_dataframe")
+            return df
+        log .info ("transform_started",rows =len (df ))
+        df =transform_testitem (df ,self .config )
+        df =self ._normalize_identifiers (df ,log )
+        df =self ._normalize_string_fields (df ,log )
+        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
+        df =self ._normalize_numeric_fields (df ,log )
+        self ._check_empty_columns (df ,log )
+        df =self ._remove_extra_columns (df ,log )
+        df ["_chembl_db_version"]=self ._chembl_db_version or ""
+        df ["_api_version"]=self ._api_version or ""
+        df =self ._deduplicate_molecules (df ,log )
+        if "molecule_chembl_id"in df .columns :
+            df =df .sort_values ("molecule_chembl_id").reset_index (drop =True )
+        df =self ._order_schema_columns (df ,COLUMN_ORDER )
+        log .info ("transform_completed",rows =len (df ))
+        return df
+    def augment_metadata (self ,metadata :Mapping [str ,object ],df :pd .DataFrame )->Mapping [str ,object ]:
+        "Enrich metadata with ChEMBL versions."
+        enriched =dict (super ().augment_metadata (metadata ,df ))
+        if self ._chembl_db_version :
+            enriched ["chembl_db_version"]=self ._chembl_db_version
+        if self ._api_version :
+            enriched ["api_version"]=self ._api_version
+        return enriched
+    def _flatten_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Flatten nested molecule_structures and molecule_properties into flat columns."
+        if df .empty :
+            return df
+        if "molecule_structures"in df .columns :
+            structures_df =pd .json_normalize (df ["molecule_structures"].tolist ())
+            if "canonical_smiles"in structures_df .columns :
+                df ["canonical_smiles"]=structures_df ["canonical_smiles"]
+            if "standard_inchi_key"in structures_df .columns :
+                df ["standard_inchi_key"]=structures_df ["standard_inchi_key"]
+        if "molecule_properties"in df .columns :
+            properties_df =pd .json_normalize (df ["molecule_properties"].tolist ())
+            property_columns =["full_mwt","mw_freebase","alogp","hbd","hba","psa","aromatic_rings","rtb","num_ro5_violations"]
+            for col in property_columns :
+                if col in properties_df .columns :
+                    df [col ]=properties_df [col ]
+        log .debug ("flatten_nested_structures_completed",columns =list (df .columns ))
+        return df
+    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize ChEMBL identifiers and InChI keys."
+        if df .empty :
+            return df
+        working_df =df .copy ()
+        inchi_key_col ="molecule_structures__standard_inchi_key"if "molecule_structures__standard_inchi_key"in working_df .columns else "standard_inchi_key"
+        rules :dict [str ,StringRule ]={}
+        if "molecule_chembl_id"in working_df .columns :
+            rules ["molecule_chembl_id"]=StringRule ()
+        if inchi_key_col in working_df .columns :
+            rules [inchi_key_col ]=StringRule (uppercase =True )
+        if rules :
+            normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        else :
+            normalized_df =working_df
+            stats =StringStats ()
+        if stats .has_changes :
+            log .debug ("normalize_identifiers_completed",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        else :
+            log .debug ("normalize_identifiers_completed")
+        return normalized_df
+    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize string fields: trim, replace empty strings with NaN."
+        if df .empty :
+            return df
+        working_df =df .copy ()
+        rules ={"pref_name":StringRule (),"molecule_type":StringRule (),"molecule_structures__canonical_smiles":StringRule (),"canonical_smiles":StringRule ()}
+        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        if stats .has_changes :
+            log .debug ("normalize_string_fields_completed",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        else :
+            log .debug ("normalize_string_fields_completed")
+        return normalized_df
+    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
+        specs =dict (super ()._schema_column_specs ())
+        int_columns =["max_phase","first_approval","first_in_class","availability_type","black_box_warning","chirality","dosed_ingredient","inorganic_flag","natural_product","prodrug","therapeutic_flag","molecule_properties__aromatic_rings","molecule_properties__hba","molecule_properties__hba_lipinski","molecule_properties__hbd","molecule_properties__hbd_lipinski","molecule_properties__heavy_atoms","molecule_properties__num_lipinski_ro5_violations","molecule_properties__num_ro5_violations","molecule_properties__ro3_pass","molecule_properties__rtb"]
+        float_columns =["molecule_properties__alogp","molecule_properties__cx_logd","molecule_properties__cx_logp","molecule_properties__cx_most_apka","molecule_properties__cx_most_bpka","molecule_properties__full_mwt","molecule_properties__mw_freebase","molecule_properties__mw_monoisotopic","molecule_properties__psa","molecule_properties__qed_weighted"]
+        for column in int_columns :
+            specs [column ]={"dtype":"Int64","default":pd .NA }
+        for column in float_columns :
+            specs [column ]={"dtype":"Float64","default":pd .NA }
+        for column in COLUMN_ORDER :
+            if column not in specs :
+                specs [column ]={"dtype":"string","default":pd .NA }
+        return specs
+    def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize numeric fields: convert types, replace negative values with None."
+        if df .empty :
+            return df
+        int_ge_zero_columns =["chirality","availability_type","hbd","hba","aromatic_rings","rtb","num_ro5_violations"]
+        int_columns =["max_phase","first_approval","black_box_warning"]
+        boolean_columns =["first_in_class","inorganic_flag","natural_product","prodrug","therapeutic_flag","dosed_ingredient"]
+        for col in int_ge_zero_columns +int_columns +boolean_columns :
+            if col in df .columns :
+                numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                if col in int_ge_zero_columns :
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                elif col in boolean_columns :
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                    numeric_series =numeric_series .where ((numeric_series ==0 )|(numeric_series ==1 )|numeric_series .isna ())
+                df [col ]=numeric_series .astype ("Int64")
+        for col in df .columns :
+            if col .startswith ("molecule_properties__"):
+                prop_name =col .replace ("molecule_properties__","")
+                if prop_name =="ro3_pass":
+                    mask =df [col ].notna ()
+                    if mask .any ():
+                        col_str =df .loc [mask ,col ].astype (str ).str .upper ().str .strip ()
+                        converted =df [col ].copy ()
+                        converted .loc [mask &(col_str =="Y")]=1
+                        converted .loc [mask &(col_str =="N")]=0
+                        other_mask =mask &~col_str .isin (["Y","N"])
+                        if other_mask .any ():
+                            numeric_vals =pd .to_numeric (df .loc [other_mask ,col ],errors ="coerce")
+                            valid_numeric =numeric_vals .notna ()&numeric_vals .isin ([0 ,1 ])
+                            converted .loc [other_mask ]=pd .NA
+                            if valid_numeric .any ():
+                                valid_indices =numeric_vals .index [valid_numeric ]
+                                converted .loc [valid_indices ]=numeric_vals .loc [valid_indices ]
+                        df [col ]=converted
+                    numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                    numeric_series =numeric_series .where ((numeric_series ==0 )|(numeric_series ==1 )|numeric_series .isna ())
+                    df [col ]=numeric_series .astype ("Int64")
+                elif prop_name in ["aromatic_rings","hba","hba_lipinski","hbd","hbd_lipinski","heavy_atoms","num_lipinski_ro5_violations","num_ro5_violations","rtb"]:
+                    numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                    df [col ]=numeric_series .astype ("Int64")
+        log .debug ("normalize_numeric_fields_completed")
+        return df
+    def _check_empty_columns (self ,df :pd .DataFrame ,log :Any )->None :
+        "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043f\u0443\u0441\u0442\u044b\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n        \u041b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u0435, \u0435\u0441\u043b\u0438 \u043f\u0440\u043e\u0446\u0435\u043d\u0442 \u043f\u0443\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u043e \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u043c \u043f\u043e\u043b\u044f\u043c > 95%.\n        \u042d\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443 \u0441 select_fields \u0438\u043b\u0438 flatten_objects.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        "
+        if df .empty :
+            return
+        key_fields =["molecule_chembl_id","pref_name","molecule_type","availability_type","chirality","first_approval","first_in_class","indication_class","helm_notation","molecule_properties__cx_logp","molecule_properties__cx_logd","molecule_properties__mw_monoisotopic","molecule_properties__hba_lipinski","molecule_properties__hbd_lipinski","molecule_properties__molecular_species","molecule_properties__num_lipinski_ro5_violations"]
+        available_key_fields =[col for col in key_fields if col in df .columns ]
+        if not available_key_fields :
+            return
+        empty_percentages :dict [str ,float ]={}
+        for col in available_key_fields :
+            if len (df )>0 :
+                empty_count =df [col ].isna ().sum ()
+                empty_percentage =empty_count /len (df )*100.0
+                empty_percentages [col ]=empty_percentage
+        highly_empty_fields ={col :pct for col ,pct in empty_percentages .items ()if pct >95.0 }
+        if highly_empty_fields :
+            log .warning ("highly_empty_columns_detected",empty_fields =highly_empty_fields ,message =f'Fields with >95% empty values detected: {highly_empty_fields }. This may indicate missing fields in select_fields or flatten_objects configuration.')
+    def _remove_extra_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Remove columns that are not in the schema."
+        if df .empty :
+            return df
+        from bioetl .schemas .testitem import COLUMN_ORDER
+        schema_columns =set (COLUMN_ORDER )
+        existing_columns =set (df .columns )
+        extra_columns =existing_columns -schema_columns
+        if extra_columns :
+            df =df .drop (columns =list (extra_columns ))
+            log .debug ("remove_extra_columns_completed",removed_columns =list (extra_columns ),remaining_columns =list (df .columns ))
+        return df
+    def _deduplicate_molecules (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Deduplicate molecules by standard_inchi_key (fallback to molecule_chembl_id)."
+        if df .empty :
+            return df
+        rows_before =len (df )
+        inchi_key_col ="molecule_structures__standard_inchi_key"if "molecule_structures__standard_inchi_key"in df .columns else "standard_inchi_key"
+        canonical_smiles_col ="molecule_structures__canonical_smiles"if "molecule_structures__canonical_smiles"in df .columns else "canonical_smiles"
+        full_mwt_col ="molecule_properties__full_mwt"if "molecule_properties__full_mwt"in df .columns else "full_mwt"
+        alogp_col ="molecule_properties__alogp"if "molecule_properties__alogp"in df .columns else "alogp"
+        if inchi_key_col in df .columns :
+            completeness_cols =[canonical_smiles_col ,full_mwt_col ,alogp_col ]
+            available_completeness =[col for col in completeness_cols if col in df .columns ]
+            if available_completeness :
+                df ["_completeness"]=df [available_completeness ].notna ().sum (axis =1 )
+                df =df .sort_values (["_completeness",canonical_smiles_col ],ascending =[False ,False ],na_position ="last")
+                df =df .drop (columns =["_completeness"])
+            df =df .drop_duplicates (subset =[inchi_key_col ],keep ="first")
+        else :
+            df =df .drop_duplicates (subset =["molecule_chembl_id"],keep ="first")
+        rows_after =len (df )
+        dropped =rows_before -rows_after
+        if dropped >0 :
+            log .info ("deduplicate_molecules_completed",rows_before =rows_before ,rows_after =rows_after ,dropped =dropped )
+        return df
```

#### Hotspot 19

- Definition: TestItemChemblPipeline.__init__#1
- assay: absent
- testitem: src/bioetl/pipelines/chembl/testitem/run.py:57-60

```diff
--- assay:run.py
+++ testitem:run.py
@@ -0,0 +1,4 @@
+def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+    super ().__init__ (config ,run_id )
+    self ._chembl_db_version :str |None =None
+    self ._api_version :str |None =None
```

#### Hotspot 20

- Definition: TestItemChemblPipeline._build_testitem_descriptor#1
- assay: absent
- testitem: src/bioetl/pipelines/chembl/testitem/run.py:151-234

```diff
--- assay:run.py
+++ testitem:run.py
@@ -0,0 +1,20 @@
+def _build_testitem_descriptor (self :SelfTestitemChemblPipeline )->ChemblExtractionDescriptor [SelfTestitemChemblPipeline ]:
+    "Return the descriptor powering testitem extraction."
+    def build_context (pipeline :SelfTestitemChemblPipeline ,source_config :TestItemSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+        base_url =pipeline ._resolve_base_url (source_config .parameters )
+        http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+        chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
+        pipeline ._fetch_chembl_release (chembl_client ,log )
+        select_fields =source_config .parameters .select_fields
+        log .debug ("chembl_testitem.select_fields",fields =select_fields )
+        testitem_client =ChemblTestitemClient (chembl_client ,batch_size =min (source_config .page_size ,25 ))
+        return ChemblExtractionContext (source_config =source_config ,iterator =testitem_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,page_size =source_config .page_size ,chembl_release =pipeline ._chembl_db_version ,metadata ={"api_version":pipeline ._api_version })
+    def empty_frame (_ :SelfTestitemChemblPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
+        return pd .DataFrame ({"molecule_chembl_id":pd .Series (dtype ="string")})
+    def dry_run_handler (pipeline :SelfTestitemChemblPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =pipeline ._chembl_db_version ,api_version =pipeline ._api_version )
+        return pd .DataFrame ()
+    def summary_extra (pipeline :SelfTestitemChemblPipeline ,_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+        return {"chembl_db_version":pipeline ._chembl_db_version ,"api_version":pipeline ._api_version ,"limit":pipeline .config .cli .limit }
+    return ChemblExtractionDescriptor [SelfTestitemChemblPipeline ](name ="chembl_testitem",source_name ="chembl",source_config_factory =TestItemSourceConfig .from_source_config ,build_context =build_context ,id_column ="molecule_chembl_id",summary_event ="chembl_testitem.extract_summary",must_have_fields =tuple (MUST_HAVE_FIELDS ),default_select_fields =MUST_HAVE_FIELDS ,sort_by =("molecule_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra ,hard_page_size_cap =None )
```

_Only the first 20 hotspots of 59 are shown for module run.py._

_First 20 hotspots of 88 are shown for pair assay ↔ testitem._

---

## Pair: document ↔ target

- AST hash: 3774d930e14eb5befd7fdffc255cb346 ↔ 107171553e4f1c509bba122a3d0ccb96

- Jaccard over tokens: 0.298

### Module run.py

- File status: document — present, target — present
- AST hash: 3fbedfe7f15a107384e19a0da1698398 ↔ 8689c415f6f3fc16a44fa879dc47ed50
- Jaccard over tokens: 0.342

Definition                                           | document signature                        | target signature                                     | Side effects                                                                                      | Exceptions                            | Status          
-----------------------------------------------------|-------------------------------------------|------------------------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------|-----------------
ChemblDocumentPipeline                               | —                                         | —                                                    | document: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning<br>target: ∅          | document: TypeError(msg)<br>target: ∅ | only in document
ChemblDocumentPipeline.__init__                      | self, config: PipelineConfig, run_id: str | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._add_system_fields            | self, df: pd.DataFrame, log: Any          | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._build_document_descriptor    | self: SelfChemblDocumentPipeline          | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: TypeError(msg)<br>target: ∅ | only in document
ChemblDocumentPipeline._check_document_id_uniqueness | self, df: pd.DataFrame, log: Any          | —                                                    | document: io=∅; logging=log.warning<br>target: ∅                                                  | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._coerce_mapping               | payload: Any                              | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._enrich_document_terms        | self, df: pd.DataFrame                    | —                                                    | document: io=∅; logging=UnifiedLogger.get, log.warning<br>target: ∅                               | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._extract_nested_fields        | self, record: dict[str, Any]              | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._normalize_authors            | authors: Any, separator: str              | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._normalize_doi                | doi: str | None                           | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._normalize_identifiers        | self, df: pd.DataFrame, log: Any          | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._normalize_journal            | value: Any, max_len: int                  | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._normalize_numeric_fields     | self, df: pd.DataFrame, log: Any          | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._normalize_string_fields      | self, df: pd.DataFrame, log: Any          | —                                                    | document: io=∅; logging=log.debug<br>target: ∅                                                    | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._schema_column_specs          | self                                      | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline._should_enrich_document_terms | self                                      | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline.extract                       | self, *args, **kwargs                     | —                                                    | document: io=∅; logging=UnifiedLogger.get<br>target: ∅                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline.extract_all                   | self                                      | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline.extract_by_ids                | self, ids: Sequence[str]                  | —                                                    | document: io=∅; logging=UnifiedLogger.get, log.info<br>target: ∅                                  | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline.transform                     | self, df: pd.DataFrame                    | —                                                    | document: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning<br>target: ∅          | document: ∅<br>target: ∅              | only in document
ChemblDocumentPipeline.validate                      | self, df: pd.DataFrame                    | —                                                    | document: io=∅; logging=UnifiedLogger.get, log.debug, log.info<br>target: ∅                       | document: ∅<br>target: ∅              | only in document
ChemblTargetPipeline                                 | —                                         | —                                                    | document: ∅<br>target: io=json.dumps; logging=UnifiedLogger.get, log.debug, log.info, log.warning | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline.__init__                        | —                                         | self, config: PipelineConfig, run_id: str            | document: ∅<br>target: io=∅; logging=∅                                                            | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline._build_target_descriptor        | —                                         | self                                                 | document: ∅<br>target: io=∅; logging=log.info                                                     | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline._enrich_protein_classifications | —                                         | self, df: pd.DataFrame, log: Any                     | document: ∅<br>target: io=json.dumps; logging=log.debug, log.info, log.warning                    | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline._enrich_target_components       | —                                         | self, df: pd.DataFrame, log: Any                     | document: ∅<br>target: io=json.dumps; logging=log.debug, log.info, log.warning                    | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline._harmonize_identifier_columns   | —                                         | self, df: pd.DataFrame, log: Any                     | document: ∅<br>target: io=∅; logging=log.debug                                                    | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline._normalize_data_types           | —                                         | self, df: pd.DataFrame, schema: Any | None, log: Any | document: ∅<br>target: io=∅; logging=log.warning                                                  | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline._normalize_identifiers          | —                                         | self, df: pd.DataFrame, log: Any                     | document: ∅<br>target: io=∅; logging=log.warning                                                  | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline._normalize_string_fields        | —                                         | self, df: pd.DataFrame, log: Any                     | document: ∅<br>target: io=∅; logging=log.debug                                                    | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline.extract                         | —                                         | self, *args, **kwargs                                | document: ∅<br>target: io=∅; logging=UnifiedLogger.get                                            | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline.extract_all                     | —                                         | self                                                 | document: ∅<br>target: io=∅; logging=∅                                                            | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline.extract_by_ids                  | —                                         | self, ids: Sequence[str]                             | document: ∅<br>target: io=∅; logging=UnifiedLogger.get, log.info                                  | document: ∅<br>target: ∅              | only in target  
ChemblTargetPipeline.transform                       | —                                         | self, df: pd.DataFrame                               | document: ∅<br>target: io=∅; logging=UnifiedLogger.get, log.debug, log.info                       | document: ∅<br>target: ∅              | only in target  
__module_block_0                                     | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_1                                     | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | identical       
__module_block_10                                    | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_11                                    | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_12                                    | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_13                                    | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_14                                    | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_15                                    | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_16                                    | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_17                                    | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_18                                    | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_19                                    | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_2                                     | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_20                                    | —                                         | —                                                    | document: io=∅; logging=∅<br>target: ∅                                                            | document: ∅<br>target: ∅              | only in document
__module_block_3                                     | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | identical       
__module_block_4                                     | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | identical       
__module_block_5                                     | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_6                                     | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_7                                     | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_8                                     | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         
__module_block_9                                     | —                                         | —                                                    | document: io=∅; logging=∅<br>target: io=∅; logging=∅                                              | document: ∅<br>target: ∅              | differs         

#### Hotspot 1

- Definition: ChemblDocumentPipeline#1
- document: src/bioetl/pipelines/chembl/document/run.py:60-654
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,324 +0,0 @@
-class ChemblDocumentPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting document records from the ChEMBL API."
-    actor ="document_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch document payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_document.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="document_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all document records from ChEMBL using pagination."
-        return self .run_extract_all (self ._build_document_descriptor ())
-    def _build_document_descriptor (self :SelfChemblDocumentPipeline )->ChemblExtractionDescriptor [SelfChemblDocumentPipeline ]:
-        "Return the descriptor powering the shared extraction routine."
-        def _require_document_pipeline (pipeline :ChemblPipelineBase )->ChemblDocumentPipeline :
-            if isinstance (pipeline ,ChemblDocumentPipeline ):
-                return pipeline
-            msg ="ChemblDocumentPipeline instance required"
-            raise TypeError (msg )
-        def build_context (pipeline :SelfChemblDocumentPipeline ,source_config :Any ,log :BoundLogger )->ChemblExtractionContext :
-            document_pipeline =_require_document_pipeline (pipeline )
-            typed_source_config =source_config if isinstance (source_config ,DocumentSourceConfig )else DocumentSourceConfig .from_source_config (cast (Any ,source_config ))
-            base_url =document_pipeline ._resolve_base_url (typed_source_config .parameters )
-            http_client ,_ =document_pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-            chembl_client =ChemblClient (http_client )
-            document_client =ChemblDocumentClient (chembl_client ,batch_size =min (typed_source_config .batch_size ,25 ))
-            document_pipeline ._chembl_release =document_pipeline .fetch_chembl_release (chembl_client ,log )
-            select_fields =document_pipeline ._resolve_select_fields (cast (SourceConfig ,cast (Any ,typed_source_config )),default_fields =API_DOCUMENT_FIELDS )
-            context =ChemblExtractionContext (typed_source_config ,document_client )
-            context .chembl_client =chembl_client
-            context .select_fields =tuple (select_fields )if select_fields else None
-            context .chembl_release =document_pipeline ._chembl_release
-            return context
-        def empty_frame (_ :SelfChemblDocumentPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"document_chembl_id":pd .Series (dtype ="string")})
-        def record_transform (pipeline :SelfChemblDocumentPipeline ,payload :Mapping [str ,Any ],_ :ChemblExtractionContext )->Mapping [str ,Any ]:
-            document_pipeline =_require_document_pipeline (pipeline )
-            return document_pipeline ._extract_nested_fields (dict (payload ))
-        def summary_extra (pipeline :SelfChemblDocumentPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            _require_document_pipeline (pipeline )
-            page_size =context .page_size or 0
-            pages =0
-            if page_size >0 :
-                total_rows =int (df .shape [0 ])
-                pages =(total_rows +page_size -1 )//page_size
-            return {"pages":pages }
-        return ChemblExtractionDescriptor [SelfChemblDocumentPipeline ](name ="chembl_document",source_name ="chembl",source_config_factory =DocumentSourceConfig .from_source_config ,build_context =build_context ,id_column ="document_chembl_id",summary_event ="chembl_document.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =API_DOCUMENT_FIELDS ,record_transform =record_transform ,sort_by =("document_chembl_id",),empty_frame_factory =empty_frame ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract document records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of document_chembl_id values to extract (as strings).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted document records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =DocumentSourceConfig .from_source_config (source_raw )
-        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-        chembl_client =ChemblClient (http_client )
-        document_client =ChemblDocumentClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        resolved_select_fields =self ._resolve_select_fields (source_raw ,default_fields =list (API_DOCUMENT_FIELDS ))
-        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-        limit =self .config .cli .limit
-        def fetch_documents (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            if "original_paginate"not in context .extra :
-                original_paginate =chembl_client .paginate
-                def counted_paginate (*args :Any ,**kwargs :Any )->Any :
-                    context .increment_api_calls ()
-                    return original_paginate (*args ,**kwargs )
-                chembl_client .paginate =counted_paginate
-                context .extra ["original_paginate"]=original_paginate
-            iterator =document_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield self ._extract_nested_fields (dict (item ))
-        def finalize_context (context :BatchExtractionContext )->None :
-            original =context .extra .pop ("original_paginate",None )
-            if original is not None :
-                chembl_client .paginate =original
-            api_calls_value =context .stats .api_calls if context .stats .api_calls is not None else 0
-            override ={"batches":context .stats .batches ,"api_calls":api_calls_value ,"cache_hits":context .stats .cache_hits }
-            context .extra ["stats_attribute_override"]=override
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="document_chembl_id",fetcher =fetch_documents ,select_fields =merged_select_fields or None ,batch_size =document_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":merged_select_fields }if merged_select_fields else None ,chembl_release =self ._chembl_release ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats")
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_document.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =stats .batches ,api_calls =stats .api_calls ,cache_hits =stats .cache_hits )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw document data by normalizing fields and identifiers."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_numeric_fields (df ,log )
-        if self ._should_enrich_document_terms ():
-            df =self ._enrich_document_terms (df )
-        df =self ._add_system_fields (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if "document_chembl_id"in df .columns and df ["document_chembl_id"].duplicated ().any ():
-            initial_count =len (df )
-            df =df .sort_values (by =list (df .columns )).drop_duplicates (subset =["document_chembl_id"],keep ="first")
-            deduped_count =len (df )
-            if deduped_count <initial_count :
-                log .warning ("document_deduplication_applied",initial_count =initial_count ,deduped_count =deduped_count ,removed_count =initial_count -deduped_count )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against DocumentSchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        self ._check_document_id_uniqueness (df ,log )
-        validated =super ().validate (df )
-        log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =self .config .validation .coerce )
-        return validated
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize identifier fields (DOI, PMID)."
-        df =df .copy ()
-        if "doi"in df .columns :
-            df ["doi_clean"]=df ["doi"].apply (self ._normalize_doi )
-        if "pubmed_id"in df .columns :
-            df ["pubmed_id"]=pd .to_numeric (df ["pubmed_id"],errors ="coerce").astype ("Int64")
-        return df
-    @staticmethod
-    def _normalize_doi (doi :str |None )->str :
-        "Normalize DOI by removing prefixes and validating format."
-        if not doi :
-            return ""
-        if not isinstance (doi ,str ):
-            return ""
-        doi =doi .strip ().lower ()
-        for prefix in ["doi:","https://doi.org/","http://dx.doi.org/","http://doi.org/"]:
-            if doi .startswith (prefix ):
-                doi =doi [len (prefix ):]
-        doi =doi .strip ()
-        doi_pattern =re .compile ("^10\\.\\d{4,9}/\\S+$")
-        if doi_pattern .match (doi ):
-            return doi
-        return ""
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields (title, abstract, journal, authors)."
-        working_df =df .copy ()
-        rules ={"title":StringRule (max_length =1000 ),"abstract":StringRule (max_length =5000 )}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        if "journal"in normalized_df .columns :
-            journal_series :pd .Series [Any ]=normalized_df ["journal"]
-            normalized_df ["journal"]=journal_series .map (lambda value :self ._normalize_journal (value ))
-        if "authors"in normalized_df .columns :
-            def _to_author_tuple (item :object )->tuple [str ,int ]|None :
-                if not isinstance (item ,tuple ):
-                    return None
-                tuple_item =cast (tuple [object ,...],item )
-                if len (tuple_item )!=2 :
-                    return None
-                name_raw ,count_raw =tuple_item
-                if not isinstance (name_raw ,str ):
-                    return None
-                name_value :str =name_raw
-                if isinstance (count_raw ,Integral ):
-                    count_value =int (count_raw )
-                elif isinstance (count_raw ,Real ):
-                    float_value =float (count_raw )
-                    if not float_value .is_integer ():
-                        return None
-                    count_value =int (float_value )
-                else :
-                    return None
-                if count_value <0 :
-                    return None
-                return (name_value ,count_value )
-            def _author_name_from_tuple (data :tuple [str ,int ]|None )->str :
-                return data [0 ]if data is not None else ""
-            def _author_count_from_tuple (data :tuple [str ,int ]|None )->int :
-                return data [1 ]if data is not None else 0
-            authors_series :pd .Series [Any ]=normalized_df ["authors"]
-            normalized_result =authors_series .apply (self ._normalize_authors )
-            normalized_tuples =normalized_result .apply (_to_author_tuple )
-            normalized_df ["authors"]=normalized_tuples .apply (_author_name_from_tuple )
-            normalized_df ["authors_count"]=normalized_tuples .apply (_author_count_from_tuple )
-        return normalized_df
-    @staticmethod
-    def _normalize_journal (value :Any ,max_len :int =255 )->str :
-        "Trim and collapse whitespace for journal name."
-        if pd .isna (value ):
-            return ""
-        text =str (value )
-        text =re .sub ("\\s+"," ",text ).strip ()
-        return text [:max_len ]if len (text )>max_len else text
-    @staticmethod
-    def _normalize_authors (authors :Any ,separator :str =", ")->tuple [str ,int ]:
-        "Normalize author separators and count."
-        if pd .isna (authors ):
-            return ("",0 )
-        text =str (authors ).strip ()
-        text =re .sub (";",",",text )
-        text =re .sub ("\\s+"," ",text )
-        if not text :
-            return ("",0 )
-        parts =text .split (",")
-        parts =[p .strip ()for p in parts if p .strip ()]
-        return (separator .join (parts ),len (parts ))
-    def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize numeric fields (year)."
-        df =df .copy ()
-        if "year"in df .columns :
-            def _coerce_year (value :object )->int |None :
-                if value is None or value is pd .NA :
-                    return None
-                if isinstance (value ,Integral ):
-                    year_int =int (value )
-                elif isinstance (value ,Real ):
-                    float_value =float (value )
-                    if not float_value .is_integer ():
-                        return None
-                    year_int =int (float_value )
-                elif isinstance (value ,str ):
-                    stripped =value .strip ()
-                    if not stripped :
-                        return None
-                    if not stripped .isdigit ():
-                        return None
-                    year_int =int (stripped )
-                else :
-                    return None
-                if 1500 <=year_int <=2100 :
-                    return year_int
-                return None
-            normalized_year =df ["year"].apply (_coerce_year )
-            df ["year"]=normalized_year .astype ("Int64")
-        return df
-    def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Add document-specific system fields (source)."
-        df =df .copy ()
-        df ["source"]="ChEMBL"
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        specs ["source"]={"default":"ChEMBL"}
-        specs ["authors_count"]={"default":0 ,"dtype":"Int64"}
-        hashing_config =self .config .determinism .hashing
-        business_key_column =hashing_config .business_key_column
-        row_hash_column =hashing_config .row_hash_column
-        if business_key_column :
-            specs [business_key_column ]={"default":""}
-        if row_hash_column :
-            specs [row_hash_column ]={"default":""}
-        return specs
-    def _check_document_id_uniqueness (self ,df :pd .DataFrame ,log :Any )->None :
-        "Check that document_chembl_id is unique."
-        if df .empty :
-            return
-        if "document_chembl_id"not in df .columns :
-            return
-        duplicates =df ["document_chembl_id"].duplicated ()
-        if duplicates .any ():
-            duplicate_ids =df [df ["document_chembl_id"].duplicated ()]["document_chembl_id"].unique ().tolist ()
-            log .warning ("document_id_duplicates",duplicate_count =duplicates .sum (),duplicate_ids =duplicate_ids [:10 ])
-    def _should_enrich_document_terms (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 document_term \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            document_section :Any =chembl_section .get ("document")
-            if not isinstance (document_section ,Mapping ):
-                return False
-            document_section =cast (Mapping [str ,Any ],document_section )
-            enrich_section :Any =document_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            document_term_section :Any =enrich_section .get ("document_term")
-            if not isinstance (document_term_section ,Mapping ):
-                return False
-            document_term_section =cast (Mapping [str ,Any ],document_term_section )
-            enabled :Any =document_term_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_document_terms (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                document_section :Any =chembl_section .get ("document")
-                if isinstance (document_section ,Mapping ):
-                    document_section =cast (Mapping [str ,Any ],document_section )
-                    enrich_section :Any =document_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        document_term_section :Any =enrich_section .get ("document_term")
-                        if isinstance (document_term_section ,Mapping ):
-                            document_term_section =cast (Mapping [str ,Any ],document_term_section )
-                            enrich_cfg =dict (document_term_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =DocumentSourceConfig .from_source_config (source_raw )
-        api_client ,_ =self .prepare_chembl_client ("chembl",base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters ))),client_name ="chembl_enrichment_client")
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_document_terms (df ,chembl_client ,enrich_cfg )
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested objects in document records."
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        "Coerce payload to dictionary mapping."
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
```

#### Hotspot 2

- Definition: ChemblDocumentPipeline.__init__#1
- document: src/bioetl/pipelines/chembl/document/run.py:65-67
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,3 +0,0 @@
-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
```

#### Hotspot 3

- Definition: ChemblDocumentPipeline._add_system_fields#1
- document: src/bioetl/pipelines/chembl/document/run.py:532-539
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,5 +0,0 @@
-def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Add document-specific system fields (source)."
-    df =df .copy ()
-    df ["source"]="ChEMBL"
-    return df
```

#### Hotspot 4

- Definition: ChemblDocumentPipeline._build_document_descriptor#1
- document: src/bioetl/pipelines/chembl/document/run.py:90-178
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,35 +0,0 @@
-def _build_document_descriptor (self :SelfChemblDocumentPipeline )->ChemblExtractionDescriptor [SelfChemblDocumentPipeline ]:
-    "Return the descriptor powering the shared extraction routine."
-    def _require_document_pipeline (pipeline :ChemblPipelineBase )->ChemblDocumentPipeline :
-        if isinstance (pipeline ,ChemblDocumentPipeline ):
-            return pipeline
-        msg ="ChemblDocumentPipeline instance required"
-        raise TypeError (msg )
-    def build_context (pipeline :SelfChemblDocumentPipeline ,source_config :Any ,log :BoundLogger )->ChemblExtractionContext :
-        document_pipeline =_require_document_pipeline (pipeline )
-        typed_source_config =source_config if isinstance (source_config ,DocumentSourceConfig )else DocumentSourceConfig .from_source_config (cast (Any ,source_config ))
-        base_url =document_pipeline ._resolve_base_url (typed_source_config .parameters )
-        http_client ,_ =document_pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-        chembl_client =ChemblClient (http_client )
-        document_client =ChemblDocumentClient (chembl_client ,batch_size =min (typed_source_config .batch_size ,25 ))
-        document_pipeline ._chembl_release =document_pipeline .fetch_chembl_release (chembl_client ,log )
-        select_fields =document_pipeline ._resolve_select_fields (cast (SourceConfig ,cast (Any ,typed_source_config )),default_fields =API_DOCUMENT_FIELDS )
-        context =ChemblExtractionContext (typed_source_config ,document_client )
-        context .chembl_client =chembl_client
-        context .select_fields =tuple (select_fields )if select_fields else None
-        context .chembl_release =document_pipeline ._chembl_release
-        return context
-    def empty_frame (_ :SelfChemblDocumentPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"document_chembl_id":pd .Series (dtype ="string")})
-    def record_transform (pipeline :SelfChemblDocumentPipeline ,payload :Mapping [str ,Any ],_ :ChemblExtractionContext )->Mapping [str ,Any ]:
-        document_pipeline =_require_document_pipeline (pipeline )
-        return document_pipeline ._extract_nested_fields (dict (payload ))
-    def summary_extra (pipeline :SelfChemblDocumentPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        _require_document_pipeline (pipeline )
-        page_size =context .page_size or 0
-        pages =0
-        if page_size >0 :
-            total_rows =int (df .shape [0 ])
-            pages =(total_rows +page_size -1 )//page_size
-        return {"pages":pages }
-    return ChemblExtractionDescriptor [SelfChemblDocumentPipeline ](name ="chembl_document",source_name ="chembl",source_config_factory =DocumentSourceConfig .from_source_config ,build_context =build_context ,id_column ="document_chembl_id",summary_event ="chembl_document.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =API_DOCUMENT_FIELDS ,record_transform =record_transform ,sort_by =("document_chembl_id",),empty_frame_factory =empty_frame ,summary_extra =summary_extra )
```

#### Hotspot 5

- Definition: ChemblDocumentPipeline._check_document_id_uniqueness#1
- document: src/bioetl/pipelines/chembl/document/run.py:557-572
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,10 +0,0 @@
-def _check_document_id_uniqueness (self ,df :pd .DataFrame ,log :Any )->None :
-    "Check that document_chembl_id is unique."
-    if df .empty :
-        return
-    if "document_chembl_id"not in df .columns :
-        return
-    duplicates =df ["document_chembl_id"].duplicated ()
-    if duplicates .any ():
-        duplicate_ids =df [df ["document_chembl_id"].duplicated ()]["document_chembl_id"].unique ().tolist ()
-        log .warning ("document_id_duplicates",duplicate_count =duplicates .sum (),duplicate_ids =duplicate_ids [:10 ])
```

#### Hotspot 6

- Definition: ChemblDocumentPipeline._coerce_mapping#1
- document: src/bioetl/pipelines/chembl/document/run.py:650-654
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,6 +0,0 @@
-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    "Coerce payload to dictionary mapping."
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Hotspot 7

- Definition: ChemblDocumentPipeline._enrich_document_terms#1
- document: src/bioetl/pipelines/chembl/document/run.py:597-641
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,24 +0,0 @@
-def _enrich_document_terms (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            document_section :Any =chembl_section .get ("document")
-            if isinstance (document_section ,Mapping ):
-                document_section =cast (Mapping [str ,Any ],document_section )
-                enrich_section :Any =document_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    document_term_section :Any =enrich_section .get ("document_term")
-                    if isinstance (document_term_section ,Mapping ):
-                        document_term_section =cast (Mapping [str ,Any ],document_term_section )
-                        enrich_cfg =dict (document_term_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =DocumentSourceConfig .from_source_config (source_raw )
-    api_client ,_ =self .prepare_chembl_client ("chembl",base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters ))),client_name ="chembl_enrichment_client")
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_document_terms (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 8

- Definition: ChemblDocumentPipeline._extract_nested_fields#1
- document: src/bioetl/pipelines/chembl/document/run.py:643-647
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,3 +0,0 @@
-def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-    "Extract fields from nested objects in document records."
-    return record
```

#### Hotspot 9

- Definition: ChemblDocumentPipeline._normalize_authors#1
- document: src/bioetl/pipelines/chembl/document/run.py:484-495
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,13 +0,0 @@
-@staticmethod
-def _normalize_authors (authors :Any ,separator :str =", ")->tuple [str ,int ]:
-    "Normalize author separators and count."
-    if pd .isna (authors ):
-        return ("",0 )
-    text =str (authors ).strip ()
-    text =re .sub (";",",",text )
-    text =re .sub ("\\s+"," ",text )
-    if not text :
-        return ("",0 )
-    parts =text .split (",")
-    parts =[p .strip ()for p in parts if p .strip ()]
-    return (separator .join (parts ),len (parts ))
```

#### Hotspot 10

- Definition: ChemblDocumentPipeline._normalize_doi#1
- document: src/bioetl/pipelines/chembl/document/run.py:392-408
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,16 +0,0 @@
-@staticmethod
-def _normalize_doi (doi :str |None )->str :
-    "Normalize DOI by removing prefixes and validating format."
-    if not doi :
-        return ""
-    if not isinstance (doi ,str ):
-        return ""
-    doi =doi .strip ().lower ()
-    for prefix in ["doi:","https://doi.org/","http://dx.doi.org/","http://doi.org/"]:
-        if doi .startswith (prefix ):
-            doi =doi [len (prefix ):]
-    doi =doi .strip ()
-    doi_pattern =re .compile ("^10\\.\\d{4,9}/\\S+$")
-    if doi_pattern .match (doi ):
-        return doi
-    return ""
```

#### Hotspot 11

- Definition: ChemblDocumentPipeline._normalize_identifiers#1
- document: src/bioetl/pipelines/chembl/document/run.py:377-389
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,8 +0,0 @@
-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize identifier fields (DOI, PMID)."
-    df =df .copy ()
-    if "doi"in df .columns :
-        df ["doi_clean"]=df ["doi"].apply (self ._normalize_doi )
-    if "pubmed_id"in df .columns :
-        df ["pubmed_id"]=pd .to_numeric (df ["pubmed_id"],errors ="coerce").astype ("Int64")
-    return df
```

#### Hotspot 12

- Definition: ChemblDocumentPipeline._normalize_journal#1
- document: src/bioetl/pipelines/chembl/document/run.py:475-481
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,8 +0,0 @@
-@staticmethod
-def _normalize_journal (value :Any ,max_len :int =255 )->str :
-    "Trim and collapse whitespace for journal name."
-    if pd .isna (value ):
-        return ""
-    text =str (value )
-    text =re .sub ("\\s+"," ",text ).strip ()
-    return text [:max_len ]if len (text )>max_len else text
```

#### Hotspot 13

- Definition: ChemblDocumentPipeline._normalize_numeric_fields#1
- document: src/bioetl/pipelines/chembl/document/run.py:497-530
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,29 +0,0 @@
-def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize numeric fields (year)."
-    df =df .copy ()
-    if "year"in df .columns :
-        def _coerce_year (value :object )->int |None :
-            if value is None or value is pd .NA :
-                return None
-            if isinstance (value ,Integral ):
-                year_int =int (value )
-            elif isinstance (value ,Real ):
-                float_value =float (value )
-                if not float_value .is_integer ():
-                    return None
-                year_int =int (float_value )
-            elif isinstance (value ,str ):
-                stripped =value .strip ()
-                if not stripped :
-                    return None
-                if not stripped .isdigit ():
-                    return None
-                year_int =int (stripped )
-            else :
-                return None
-            if 1500 <=year_int <=2100 :
-                return year_int
-            return None
-        normalized_year =df ["year"].apply (_coerce_year )
-        df ["year"]=normalized_year .astype ("Int64")
-    return df
```

#### Hotspot 14

- Definition: ChemblDocumentPipeline._normalize_string_fields#1
- document: src/bioetl/pipelines/chembl/document/run.py:410-472
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,43 +0,0 @@
-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields (title, abstract, journal, authors)."
-    working_df =df .copy ()
-    rules ={"title":StringRule (max_length =1000 ),"abstract":StringRule (max_length =5000 )}
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    if "journal"in normalized_df .columns :
-        journal_series :pd .Series [Any ]=normalized_df ["journal"]
-        normalized_df ["journal"]=journal_series .map (lambda value :self ._normalize_journal (value ))
-    if "authors"in normalized_df .columns :
-        def _to_author_tuple (item :object )->tuple [str ,int ]|None :
-            if not isinstance (item ,tuple ):
-                return None
-            tuple_item =cast (tuple [object ,...],item )
-            if len (tuple_item )!=2 :
-                return None
-            name_raw ,count_raw =tuple_item
-            if not isinstance (name_raw ,str ):
-                return None
-            name_value :str =name_raw
-            if isinstance (count_raw ,Integral ):
-                count_value =int (count_raw )
-            elif isinstance (count_raw ,Real ):
-                float_value =float (count_raw )
-                if not float_value .is_integer ():
-                    return None
-                count_value =int (float_value )
-            else :
-                return None
-            if count_value <0 :
-                return None
-            return (name_value ,count_value )
-        def _author_name_from_tuple (data :tuple [str ,int ]|None )->str :
-            return data [0 ]if data is not None else ""
-        def _author_count_from_tuple (data :tuple [str ,int ]|None )->int :
-            return data [1 ]if data is not None else 0
-        authors_series :pd .Series [Any ]=normalized_df ["authors"]
-        normalized_result =authors_series .apply (self ._normalize_authors )
-        normalized_tuples =normalized_result .apply (_to_author_tuple )
-        normalized_df ["authors"]=normalized_tuples .apply (_author_name_from_tuple )
-        normalized_df ["authors_count"]=normalized_tuples .apply (_author_count_from_tuple )
-    return normalized_df
```

#### Hotspot 15

- Definition: ChemblDocumentPipeline._schema_column_specs#1
- document: src/bioetl/pipelines/chembl/document/run.py:541-555
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,12 +0,0 @@
-def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-    specs =dict (super ()._schema_column_specs ())
-    specs ["source"]={"default":"ChEMBL"}
-    specs ["authors_count"]={"default":0 ,"dtype":"Int64"}
-    hashing_config =self .config .determinism .hashing
-    business_key_column =hashing_config .business_key_column
-    row_hash_column =hashing_config .row_hash_column
-    if business_key_column :
-        specs [business_key_column ]={"default":""}
-    if row_hash_column :
-        specs [row_hash_column ]={"default":""}
-    return specs
```

#### Hotspot 16

- Definition: ChemblDocumentPipeline._should_enrich_document_terms#1
- document: src/bioetl/pipelines/chembl/document/run.py:574-595
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,22 +0,0 @@
-def _should_enrich_document_terms (self )->bool :
-    "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 document_term \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-    if not self .config .chembl :
-        return False
-    try :
-        chembl_section =self .config .chembl
-        document_section :Any =chembl_section .get ("document")
-        if not isinstance (document_section ,Mapping ):
-            return False
-        document_section =cast (Mapping [str ,Any ],document_section )
-        enrich_section :Any =document_section .get ("enrich")
-        if not isinstance (enrich_section ,Mapping ):
-            return False
-        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-        document_term_section :Any =enrich_section .get ("document_term")
-        if not isinstance (document_term_section ,Mapping ):
-            return False
-        document_term_section =cast (Mapping [str ,Any ],document_term_section )
-        enabled :Any =document_term_section .get ("enabled")
-        return bool (enabled )if enabled is not None else False
-    except (AttributeError ,KeyError ,TypeError ):
-        return False
```

#### Hotspot 17

- Definition: ChemblDocumentPipeline.extract#1
- document: src/bioetl/pipelines/chembl/document/run.py:69-83
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,4 +0,0 @@
-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch document payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_document.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="document_chembl_id")
```

#### Hotspot 18

- Definition: ChemblDocumentPipeline.extract_all#1
- document: src/bioetl/pipelines/chembl/document/run.py:85-88
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,3 +0,0 @@
-def extract_all (self )->pd .DataFrame :
-    "Extract all document records from ChEMBL using pagination."
-    return self .run_extract_all (self ._build_document_descriptor ())
```

#### Hotspot 19

- Definition: ChemblDocumentPipeline.extract_by_ids#1
- document: src/bioetl/pipelines/chembl/document/run.py:180-280
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,36 +0,0 @@
-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract document records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of document_chembl_id values to extract (as strings).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted document records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =DocumentSourceConfig .from_source_config (source_raw )
-    base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-    http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-    chembl_client =ChemblClient (http_client )
-    document_client =ChemblDocumentClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-    self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-    resolved_select_fields =self ._resolve_select_fields (source_raw ,default_fields =list (API_DOCUMENT_FIELDS ))
-    merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-    limit =self .config .cli .limit
-    def fetch_documents (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        if "original_paginate"not in context .extra :
-            original_paginate =chembl_client .paginate
-            def counted_paginate (*args :Any ,**kwargs :Any )->Any :
-                context .increment_api_calls ()
-                return original_paginate (*args ,**kwargs )
-            chembl_client .paginate =counted_paginate
-            context .extra ["original_paginate"]=original_paginate
-        iterator =document_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield self ._extract_nested_fields (dict (item ))
-    def finalize_context (context :BatchExtractionContext )->None :
-        original =context .extra .pop ("original_paginate",None )
-        if original is not None :
-            chembl_client .paginate =original
-        api_calls_value =context .stats .api_calls if context .stats .api_calls is not None else 0
-        override ={"batches":context .stats .batches ,"api_calls":api_calls_value ,"cache_hits":context .stats .cache_hits }
-        context .extra ["stats_attribute_override"]=override
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="document_chembl_id",fetcher =fetch_documents ,select_fields =merged_select_fields or None ,batch_size =document_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":merged_select_fields }if merged_select_fields else None ,chembl_release =self ._chembl_release ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats")
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_document.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =stats .batches ,api_calls =stats .api_calls ,cache_hits =stats .cache_hits )
-    return dataframe
```

#### Hotspot 20

- Definition: ChemblDocumentPipeline.transform#1
- document: src/bioetl/pipelines/chembl/document/run.py:282-336
- target: absent

```diff
--- document:run.py
+++ target:run.py
@@ -1,24 +0,0 @@
-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw document data by normalizing fields and identifiers."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-    df =df .copy ()
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._normalize_numeric_fields (df ,log )
-    if self ._should_enrich_document_terms ():
-        df =self ._enrich_document_terms (df )
-    df =self ._add_system_fields (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if "document_chembl_id"in df .columns and df ["document_chembl_id"].duplicated ().any ():
-        initial_count =len (df )
-        df =df .sort_values (by =list (df .columns )).drop_duplicates (subset =["document_chembl_id"],keep ="first")
-        deduped_count =len (df )
-        if deduped_count <initial_count :
-            log .warning ("document_deduplication_applied",initial_count =initial_count ,deduped_count =deduped_count ,removed_count =initial_count -deduped_count )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

_Only the first 20 hotspots of 52 are shown for module run.py._

_First 20 hotspots of 86 are shown for pair document ↔ target._

---

## Pair: document ↔ testitem

- AST hash: 3774d930e14eb5befd7fdffc255cb346 ↔ 729263c98934cfcb08473bcacfb20e9e

- Jaccard over tokens: 0.307

### Module run.py

- File status: document — present, testitem — present
- AST hash: 3fbedfe7f15a107384e19a0da1698398 ↔ 4380f065a3fdcc94928fad4ab20f1d2b
- Jaccard over tokens: 0.346

Definition                                           | document signature                        | testitem signature                                                           | Side effects                                                                                                                                   | Exceptions                              | Status          
-----------------------------------------------------|-------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------|-----------------
ChemblDocumentPipeline                               | —                                         | —                                                                            | document: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning<br>testitem: ∅                                                     | document: TypeError(msg)<br>testitem: ∅ | only in document
ChemblDocumentPipeline.__init__                      | self, config: PipelineConfig, run_id: str | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._add_system_fields            | self, df: pd.DataFrame, log: Any          | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._build_document_descriptor    | self: SelfChemblDocumentPipeline          | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: TypeError(msg)<br>testitem: ∅ | only in document
ChemblDocumentPipeline._check_document_id_uniqueness | self, df: pd.DataFrame, log: Any          | —                                                                            | document: io=∅; logging=log.warning<br>testitem: ∅                                                                                             | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._coerce_mapping               | payload: Any                              | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._enrich_document_terms        | self, df: pd.DataFrame                    | —                                                                            | document: io=∅; logging=UnifiedLogger.get, log.warning<br>testitem: ∅                                                                          | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._extract_nested_fields        | self, record: dict[str, Any]              | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._normalize_authors            | authors: Any, separator: str              | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._normalize_doi                | doi: str | None                           | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._normalize_identifiers        | self, df: pd.DataFrame, log: Any          | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._normalize_journal            | value: Any, max_len: int                  | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._normalize_numeric_fields     | self, df: pd.DataFrame, log: Any          | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._normalize_string_fields      | self, df: pd.DataFrame, log: Any          | —                                                                            | document: io=∅; logging=log.debug<br>testitem: ∅                                                                                               | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._schema_column_specs          | self                                      | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline._should_enrich_document_terms | self                                      | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline.extract                       | self, *args, **kwargs                     | —                                                                            | document: io=∅; logging=UnifiedLogger.get<br>testitem: ∅                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline.extract_all                   | self                                      | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline.extract_by_ids                | self, ids: Sequence[str]                  | —                                                                            | document: io=∅; logging=UnifiedLogger.get, log.info<br>testitem: ∅                                                                             | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline.transform                     | self, df: pd.DataFrame                    | —                                                                            | document: io=∅; logging=UnifiedLogger.get, log.debug, log.info, log.warning<br>testitem: ∅                                                     | document: ∅<br>testitem: ∅              | only in document
ChemblDocumentPipeline.validate                      | self, df: pd.DataFrame                    | —                                                                            | document: io=∅; logging=UnifiedLogger.get, log.debug, log.info<br>testitem: ∅                                                                  | document: ∅<br>testitem: ∅              | only in document
TestItemChemblPipeline                               | —                                         | —                                                                            | document: ∅<br>testitem: io=status_payload.get; logging=UnifiedLogger.get, bound_log.info, bound_log.warning, log.debug, log.info, log.warning | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.__init__                      | —                                         | self, config: PipelineConfig, run_id: str                                    | document: ∅<br>testitem: io=∅; logging=∅                                                                                                       | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._build_testitem_descriptor    | —                                         | self: SelfTestitemChemblPipeline                                             | document: ∅<br>testitem: io=∅; logging=log.debug, log.info                                                                                     | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._check_empty_columns          | —                                         | self, df: pd.DataFrame, log: Any                                             | document: ∅<br>testitem: io=∅; logging=log.warning                                                                                             | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._deduplicate_molecules        | —                                         | self, df: pd.DataFrame, log: Any                                             | document: ∅<br>testitem: io=∅; logging=log.info                                                                                                | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._fetch_chembl_release         | —                                         | self, client: UnifiedAPIClient | ChemblClient | Any, log: BoundLogger | None | document: ∅<br>testitem: io=status_payload.get; logging=UnifiedLogger.get, bound_log.info, bound_log.warning                                   | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._flatten_nested_structures    | —                                         | self, df: pd.DataFrame, log: Any                                             | document: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._normalize_identifiers        | —                                         | self, df: pd.DataFrame, log: Any                                             | document: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._normalize_numeric_fields     | —                                         | self, df: pd.DataFrame, log: Any                                             | document: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._normalize_string_fields      | —                                         | self, df: pd.DataFrame, log: Any                                             | document: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._remove_extra_columns         | —                                         | self, df: pd.DataFrame, log: Any                                             | document: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline._schema_column_specs          | —                                         | self                                                                         | document: ∅<br>testitem: io=∅; logging=∅                                                                                                       | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.api_version                   | —                                         | self                                                                         | document: ∅<br>testitem: io=∅; logging=∅                                                                                                       | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.augment_metadata              | —                                         | self, metadata: Mapping[str, object], df: pd.DataFrame                       | document: ∅<br>testitem: io=∅; logging=∅                                                                                                       | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.chembl_db_version             | —                                         | self                                                                         | document: ∅<br>testitem: io=∅; logging=∅                                                                                                       | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.extract                       | —                                         | self, *args, **kwargs                                                        | document: ∅<br>testitem: io=∅; logging=UnifiedLogger.get                                                                                       | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.extract_all                   | —                                         | self                                                                         | document: ∅<br>testitem: io=∅; logging=∅                                                                                                       | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.extract_by_ids                | —                                         | self, ids: Sequence[str]                                                     | document: ∅<br>testitem: io=∅; logging=UnifiedLogger.get, log.debug, log.info                                                                  | document: ∅<br>testitem: ∅              | only in testitem
TestItemChemblPipeline.transform                     | —                                         | self, df: pd.DataFrame                                                       | document: ∅<br>testitem: io=∅; logging=UnifiedLogger.get, log.debug, log.info                                                                  | document: ∅<br>testitem: ∅              | only in testitem
__module_block_0                                     | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         
__module_block_1                                     | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | identical       
__module_block_10                                    | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         
__module_block_11                                    | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | identical       
__module_block_12                                    | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         
__module_block_13                                    | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | identical       
__module_block_14                                    | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | identical       
__module_block_15                                    | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | identical       
__module_block_16                                    | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         
__module_block_17                                    | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         
__module_block_18                                    | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         
__module_block_19                                    | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
__module_block_2                                     | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         
__module_block_20                                    | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: ∅                                                                                                       | document: ∅<br>testitem: ∅              | only in document
__module_block_3                                     | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | identical       
__module_block_4                                     | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | identical       
__module_block_5                                     | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         
__module_block_6                                     | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         
__module_block_7                                     | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         
__module_block_8                                     | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         
__module_block_9                                     | —                                         | —                                                                            | document: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | document: ∅<br>testitem: ∅              | differs         

#### Hotspot 1

- Definition: ChemblDocumentPipeline#1
- document: src/bioetl/pipelines/chembl/document/run.py:60-654
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,324 +0,0 @@
-class ChemblDocumentPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting document records from the ChEMBL API."
-    actor ="document_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-        self ._last_batch_extract_stats :dict [str ,Any ]|None =None
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch document payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_document.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="document_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all document records from ChEMBL using pagination."
-        return self .run_extract_all (self ._build_document_descriptor ())
-    def _build_document_descriptor (self :SelfChemblDocumentPipeline )->ChemblExtractionDescriptor [SelfChemblDocumentPipeline ]:
-        "Return the descriptor powering the shared extraction routine."
-        def _require_document_pipeline (pipeline :ChemblPipelineBase )->ChemblDocumentPipeline :
-            if isinstance (pipeline ,ChemblDocumentPipeline ):
-                return pipeline
-            msg ="ChemblDocumentPipeline instance required"
-            raise TypeError (msg )
-        def build_context (pipeline :SelfChemblDocumentPipeline ,source_config :Any ,log :BoundLogger )->ChemblExtractionContext :
-            document_pipeline =_require_document_pipeline (pipeline )
-            typed_source_config =source_config if isinstance (source_config ,DocumentSourceConfig )else DocumentSourceConfig .from_source_config (cast (Any ,source_config ))
-            base_url =document_pipeline ._resolve_base_url (typed_source_config .parameters )
-            http_client ,_ =document_pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-            chembl_client =ChemblClient (http_client )
-            document_client =ChemblDocumentClient (chembl_client ,batch_size =min (typed_source_config .batch_size ,25 ))
-            document_pipeline ._chembl_release =document_pipeline .fetch_chembl_release (chembl_client ,log )
-            select_fields =document_pipeline ._resolve_select_fields (cast (SourceConfig ,cast (Any ,typed_source_config )),default_fields =API_DOCUMENT_FIELDS )
-            context =ChemblExtractionContext (typed_source_config ,document_client )
-            context .chembl_client =chembl_client
-            context .select_fields =tuple (select_fields )if select_fields else None
-            context .chembl_release =document_pipeline ._chembl_release
-            return context
-        def empty_frame (_ :SelfChemblDocumentPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"document_chembl_id":pd .Series (dtype ="string")})
-        def record_transform (pipeline :SelfChemblDocumentPipeline ,payload :Mapping [str ,Any ],_ :ChemblExtractionContext )->Mapping [str ,Any ]:
-            document_pipeline =_require_document_pipeline (pipeline )
-            return document_pipeline ._extract_nested_fields (dict (payload ))
-        def summary_extra (pipeline :SelfChemblDocumentPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-            _require_document_pipeline (pipeline )
-            page_size =context .page_size or 0
-            pages =0
-            if page_size >0 :
-                total_rows =int (df .shape [0 ])
-                pages =(total_rows +page_size -1 )//page_size
-            return {"pages":pages }
-        return ChemblExtractionDescriptor [SelfChemblDocumentPipeline ](name ="chembl_document",source_name ="chembl",source_config_factory =DocumentSourceConfig .from_source_config ,build_context =build_context ,id_column ="document_chembl_id",summary_event ="chembl_document.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =API_DOCUMENT_FIELDS ,record_transform =record_transform ,sort_by =("document_chembl_id",),empty_frame_factory =empty_frame ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract document records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of document_chembl_id values to extract (as strings).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted document records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =DocumentSourceConfig .from_source_config (source_raw )
-        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-        chembl_client =ChemblClient (http_client )
-        document_client =ChemblDocumentClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        resolved_select_fields =self ._resolve_select_fields (source_raw ,default_fields =list (API_DOCUMENT_FIELDS ))
-        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-        limit =self .config .cli .limit
-        def fetch_documents (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            if "original_paginate"not in context .extra :
-                original_paginate =chembl_client .paginate
-                def counted_paginate (*args :Any ,**kwargs :Any )->Any :
-                    context .increment_api_calls ()
-                    return original_paginate (*args ,**kwargs )
-                chembl_client .paginate =counted_paginate
-                context .extra ["original_paginate"]=original_paginate
-            iterator =document_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield self ._extract_nested_fields (dict (item ))
-        def finalize_context (context :BatchExtractionContext )->None :
-            original =context .extra .pop ("original_paginate",None )
-            if original is not None :
-                chembl_client .paginate =original
-            api_calls_value =context .stats .api_calls if context .stats .api_calls is not None else 0
-            override ={"batches":context .stats .batches ,"api_calls":api_calls_value ,"cache_hits":context .stats .cache_hits }
-            context .extra ["stats_attribute_override"]=override
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="document_chembl_id",fetcher =fetch_documents ,select_fields =merged_select_fields or None ,batch_size =document_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":merged_select_fields }if merged_select_fields else None ,chembl_release =self ._chembl_release ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats")
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_document.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =stats .batches ,api_calls =stats .api_calls ,cache_hits =stats .cache_hits )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw document data by normalizing fields and identifiers."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-        df =df .copy ()
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._normalize_numeric_fields (df ,log )
-        if self ._should_enrich_document_terms ():
-            df =self ._enrich_document_terms (df )
-        df =self ._add_system_fields (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if "document_chembl_id"in df .columns and df ["document_chembl_id"].duplicated ().any ():
-            initial_count =len (df )
-            df =df .sort_values (by =list (df .columns )).drop_duplicates (subset =["document_chembl_id"],keep ="first")
-            deduped_count =len (df )
-            if deduped_count <initial_count :
-                log .warning ("document_deduplication_applied",initial_count =initial_count ,deduped_count =deduped_count ,removed_count =initial_count -deduped_count )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def validate (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Validate payload against DocumentSchema with detailed error handling."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.validate')
-        if df .empty :
-            log .debug ("validate_empty_dataframe")
-            return df
-        if self .config .validation .strict :
-            allowed_columns =set (COLUMN_ORDER )
-            extra_columns =[column for column in df .columns if column not in allowed_columns ]
-            if extra_columns :
-                log .debug ("drop_extra_columns_before_validation",extras =extra_columns )
-                df =df .drop (columns =extra_columns )
-        log .info ("validate_started",rows =len (df ))
-        self ._check_document_id_uniqueness (df ,log )
-        validated =super ().validate (df )
-        log .info ("validate_completed",rows =len (validated ),schema =self .config .validation .schema_out ,strict =self .config .validation .strict ,coerce =self .config .validation .coerce )
-        return validated
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize identifier fields (DOI, PMID)."
-        df =df .copy ()
-        if "doi"in df .columns :
-            df ["doi_clean"]=df ["doi"].apply (self ._normalize_doi )
-        if "pubmed_id"in df .columns :
-            df ["pubmed_id"]=pd .to_numeric (df ["pubmed_id"],errors ="coerce").astype ("Int64")
-        return df
-    @staticmethod
-    def _normalize_doi (doi :str |None )->str :
-        "Normalize DOI by removing prefixes and validating format."
-        if not doi :
-            return ""
-        if not isinstance (doi ,str ):
-            return ""
-        doi =doi .strip ().lower ()
-        for prefix in ["doi:","https://doi.org/","http://dx.doi.org/","http://doi.org/"]:
-            if doi .startswith (prefix ):
-                doi =doi [len (prefix ):]
-        doi =doi .strip ()
-        doi_pattern =re .compile ("^10\\.\\d{4,9}/\\S+$")
-        if doi_pattern .match (doi ):
-            return doi
-        return ""
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields (title, abstract, journal, authors)."
-        working_df =df .copy ()
-        rules ={"title":StringRule (max_length =1000 ),"abstract":StringRule (max_length =5000 )}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        if "journal"in normalized_df .columns :
-            journal_series :pd .Series [Any ]=normalized_df ["journal"]
-            normalized_df ["journal"]=journal_series .map (lambda value :self ._normalize_journal (value ))
-        if "authors"in normalized_df .columns :
-            def _to_author_tuple (item :object )->tuple [str ,int ]|None :
-                if not isinstance (item ,tuple ):
-                    return None
-                tuple_item =cast (tuple [object ,...],item )
-                if len (tuple_item )!=2 :
-                    return None
-                name_raw ,count_raw =tuple_item
-                if not isinstance (name_raw ,str ):
-                    return None
-                name_value :str =name_raw
-                if isinstance (count_raw ,Integral ):
-                    count_value =int (count_raw )
-                elif isinstance (count_raw ,Real ):
-                    float_value =float (count_raw )
-                    if not float_value .is_integer ():
-                        return None
-                    count_value =int (float_value )
-                else :
-                    return None
-                if count_value <0 :
-                    return None
-                return (name_value ,count_value )
-            def _author_name_from_tuple (data :tuple [str ,int ]|None )->str :
-                return data [0 ]if data is not None else ""
-            def _author_count_from_tuple (data :tuple [str ,int ]|None )->int :
-                return data [1 ]if data is not None else 0
-            authors_series :pd .Series [Any ]=normalized_df ["authors"]
-            normalized_result =authors_series .apply (self ._normalize_authors )
-            normalized_tuples =normalized_result .apply (_to_author_tuple )
-            normalized_df ["authors"]=normalized_tuples .apply (_author_name_from_tuple )
-            normalized_df ["authors_count"]=normalized_tuples .apply (_author_count_from_tuple )
-        return normalized_df
-    @staticmethod
-    def _normalize_journal (value :Any ,max_len :int =255 )->str :
-        "Trim and collapse whitespace for journal name."
-        if pd .isna (value ):
-            return ""
-        text =str (value )
-        text =re .sub ("\\s+"," ",text ).strip ()
-        return text [:max_len ]if len (text )>max_len else text
-    @staticmethod
-    def _normalize_authors (authors :Any ,separator :str =", ")->tuple [str ,int ]:
-        "Normalize author separators and count."
-        if pd .isna (authors ):
-            return ("",0 )
-        text =str (authors ).strip ()
-        text =re .sub (";",",",text )
-        text =re .sub ("\\s+"," ",text )
-        if not text :
-            return ("",0 )
-        parts =text .split (",")
-        parts =[p .strip ()for p in parts if p .strip ()]
-        return (separator .join (parts ),len (parts ))
-    def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize numeric fields (year)."
-        df =df .copy ()
-        if "year"in df .columns :
-            def _coerce_year (value :object )->int |None :
-                if value is None or value is pd .NA :
-                    return None
-                if isinstance (value ,Integral ):
-                    year_int =int (value )
-                elif isinstance (value ,Real ):
-                    float_value =float (value )
-                    if not float_value .is_integer ():
-                        return None
-                    year_int =int (float_value )
-                elif isinstance (value ,str ):
-                    stripped =value .strip ()
-                    if not stripped :
-                        return None
-                    if not stripped .isdigit ():
-                        return None
-                    year_int =int (stripped )
-                else :
-                    return None
-                if 1500 <=year_int <=2100 :
-                    return year_int
-                return None
-            normalized_year =df ["year"].apply (_coerce_year )
-            df ["year"]=normalized_year .astype ("Int64")
-        return df
-    def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Add document-specific system fields (source)."
-        df =df .copy ()
-        df ["source"]="ChEMBL"
-        return df
-    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-        specs =dict (super ()._schema_column_specs ())
-        specs ["source"]={"default":"ChEMBL"}
-        specs ["authors_count"]={"default":0 ,"dtype":"Int64"}
-        hashing_config =self .config .determinism .hashing
-        business_key_column =hashing_config .business_key_column
-        row_hash_column =hashing_config .row_hash_column
-        if business_key_column :
-            specs [business_key_column ]={"default":""}
-        if row_hash_column :
-            specs [row_hash_column ]={"default":""}
-        return specs
-    def _check_document_id_uniqueness (self ,df :pd .DataFrame ,log :Any )->None :
-        "Check that document_chembl_id is unique."
-        if df .empty :
-            return
-        if "document_chembl_id"not in df .columns :
-            return
-        duplicates =df ["document_chembl_id"].duplicated ()
-        if duplicates .any ():
-            duplicate_ids =df [df ["document_chembl_id"].duplicated ()]["document_chembl_id"].unique ().tolist ()
-            log .warning ("document_id_duplicates",duplicate_count =duplicates .sum (),duplicate_ids =duplicate_ids [:10 ])
-    def _should_enrich_document_terms (self )->bool :
-        "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 document_term \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-        if not self .config .chembl :
-            return False
-        try :
-            chembl_section =self .config .chembl
-            document_section :Any =chembl_section .get ("document")
-            if not isinstance (document_section ,Mapping ):
-                return False
-            document_section =cast (Mapping [str ,Any ],document_section )
-            enrich_section :Any =document_section .get ("enrich")
-            if not isinstance (enrich_section ,Mapping ):
-                return False
-            enrich_section =cast (Mapping [str ,Any ],enrich_section )
-            document_term_section :Any =enrich_section .get ("document_term")
-            if not isinstance (document_term_section ,Mapping ):
-                return False
-            document_term_section =cast (Mapping [str ,Any ],document_term_section )
-            enabled :Any =document_term_section .get ("enabled")
-            return bool (enabled )if enabled is not None else False
-        except (AttributeError ,KeyError ,TypeError ):
-            return False
-    def _enrich_document_terms (self ,df :pd .DataFrame )->pd .DataFrame :
-        "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term."
-        log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-        enrich_cfg :dict [str ,Any ]={}
-        try :
-            if self .config .chembl :
-                chembl_section =self .config .chembl
-                document_section :Any =chembl_section .get ("document")
-                if isinstance (document_section ,Mapping ):
-                    document_section =cast (Mapping [str ,Any ],document_section )
-                    enrich_section :Any =document_section .get ("enrich")
-                    if isinstance (enrich_section ,Mapping ):
-                        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                        document_term_section :Any =enrich_section .get ("document_term")
-                        if isinstance (document_term_section ,Mapping ):
-                            document_term_section =cast (Mapping [str ,Any ],document_term_section )
-                            enrich_cfg =dict (document_term_section )
-        except (AttributeError ,KeyError ,TypeError )as exc :
-            log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =DocumentSourceConfig .from_source_config (source_raw )
-        api_client ,_ =self .prepare_chembl_client ("chembl",base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters ))),client_name ="chembl_enrichment_client")
-        chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-        return enrich_with_document_terms (df ,chembl_client ,enrich_cfg )
-    def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-        "Extract fields from nested objects in document records."
-        return record
-    @staticmethod
-    def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-        "Coerce payload to dictionary mapping."
-        if isinstance (payload ,Mapping ):
-            return cast (dict [str ,Any ],payload )
-        return {}
```

#### Hotspot 2

- Definition: ChemblDocumentPipeline.__init__#1
- document: src/bioetl/pipelines/chembl/document/run.py:65-67
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,3 +0,0 @@
-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
-    self ._last_batch_extract_stats :dict [str ,Any ]|None =None
```

#### Hotspot 3

- Definition: ChemblDocumentPipeline._add_system_fields#1
- document: src/bioetl/pipelines/chembl/document/run.py:532-539
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,5 +0,0 @@
-def _add_system_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Add document-specific system fields (source)."
-    df =df .copy ()
-    df ["source"]="ChEMBL"
-    return df
```

#### Hotspot 4

- Definition: ChemblDocumentPipeline._build_document_descriptor#1
- document: src/bioetl/pipelines/chembl/document/run.py:90-178
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,35 +0,0 @@
-def _build_document_descriptor (self :SelfChemblDocumentPipeline )->ChemblExtractionDescriptor [SelfChemblDocumentPipeline ]:
-    "Return the descriptor powering the shared extraction routine."
-    def _require_document_pipeline (pipeline :ChemblPipelineBase )->ChemblDocumentPipeline :
-        if isinstance (pipeline ,ChemblDocumentPipeline ):
-            return pipeline
-        msg ="ChemblDocumentPipeline instance required"
-        raise TypeError (msg )
-    def build_context (pipeline :SelfChemblDocumentPipeline ,source_config :Any ,log :BoundLogger )->ChemblExtractionContext :
-        document_pipeline =_require_document_pipeline (pipeline )
-        typed_source_config =source_config if isinstance (source_config ,DocumentSourceConfig )else DocumentSourceConfig .from_source_config (cast (Any ,source_config ))
-        base_url =document_pipeline ._resolve_base_url (typed_source_config .parameters )
-        http_client ,_ =document_pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-        chembl_client =ChemblClient (http_client )
-        document_client =ChemblDocumentClient (chembl_client ,batch_size =min (typed_source_config .batch_size ,25 ))
-        document_pipeline ._chembl_release =document_pipeline .fetch_chembl_release (chembl_client ,log )
-        select_fields =document_pipeline ._resolve_select_fields (cast (SourceConfig ,cast (Any ,typed_source_config )),default_fields =API_DOCUMENT_FIELDS )
-        context =ChemblExtractionContext (typed_source_config ,document_client )
-        context .chembl_client =chembl_client
-        context .select_fields =tuple (select_fields )if select_fields else None
-        context .chembl_release =document_pipeline ._chembl_release
-        return context
-    def empty_frame (_ :SelfChemblDocumentPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"document_chembl_id":pd .Series (dtype ="string")})
-    def record_transform (pipeline :SelfChemblDocumentPipeline ,payload :Mapping [str ,Any ],_ :ChemblExtractionContext )->Mapping [str ,Any ]:
-        document_pipeline =_require_document_pipeline (pipeline )
-        return document_pipeline ._extract_nested_fields (dict (payload ))
-    def summary_extra (pipeline :SelfChemblDocumentPipeline ,df :pd .DataFrame ,context :ChemblExtractionContext )->Mapping [str ,Any ]:
-        _require_document_pipeline (pipeline )
-        page_size =context .page_size or 0
-        pages =0
-        if page_size >0 :
-            total_rows =int (df .shape [0 ])
-            pages =(total_rows +page_size -1 )//page_size
-        return {"pages":pages }
-    return ChemblExtractionDescriptor [SelfChemblDocumentPipeline ](name ="chembl_document",source_name ="chembl",source_config_factory =DocumentSourceConfig .from_source_config ,build_context =build_context ,id_column ="document_chembl_id",summary_event ="chembl_document.extract_summary",must_have_fields =MUST_HAVE_FIELDS ,default_select_fields =API_DOCUMENT_FIELDS ,record_transform =record_transform ,sort_by =("document_chembl_id",),empty_frame_factory =empty_frame ,summary_extra =summary_extra )
```

#### Hotspot 5

- Definition: ChemblDocumentPipeline._check_document_id_uniqueness#1
- document: src/bioetl/pipelines/chembl/document/run.py:557-572
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,10 +0,0 @@
-def _check_document_id_uniqueness (self ,df :pd .DataFrame ,log :Any )->None :
-    "Check that document_chembl_id is unique."
-    if df .empty :
-        return
-    if "document_chembl_id"not in df .columns :
-        return
-    duplicates =df ["document_chembl_id"].duplicated ()
-    if duplicates .any ():
-        duplicate_ids =df [df ["document_chembl_id"].duplicated ()]["document_chembl_id"].unique ().tolist ()
-        log .warning ("document_id_duplicates",duplicate_count =duplicates .sum (),duplicate_ids =duplicate_ids [:10 ])
```

#### Hotspot 6

- Definition: ChemblDocumentPipeline._coerce_mapping#1
- document: src/bioetl/pipelines/chembl/document/run.py:650-654
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,6 +0,0 @@
-@staticmethod
-def _coerce_mapping (payload :Any )->dict [str ,Any ]:
-    "Coerce payload to dictionary mapping."
-    if isinstance (payload ,Mapping ):
-        return cast (dict [str ,Any ],payload )
-    return {}
```

#### Hotspot 7

- Definition: ChemblDocumentPipeline._enrich_document_terms#1
- document: src/bioetl/pipelines/chembl/document/run.py:597-641
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,24 +0,0 @@
-def _enrich_document_terms (self ,df :pd .DataFrame )->pd .DataFrame :
-    "\u041e\u0431\u043e\u0433\u0430\u0442\u0438\u0442\u044c DataFrame \u043f\u043e\u043b\u044f\u043c\u0438 \u0438\u0437 document_term."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.enrich')
-    enrich_cfg :dict [str ,Any ]={}
-    try :
-        if self .config .chembl :
-            chembl_section =self .config .chembl
-            document_section :Any =chembl_section .get ("document")
-            if isinstance (document_section ,Mapping ):
-                document_section =cast (Mapping [str ,Any ],document_section )
-                enrich_section :Any =document_section .get ("enrich")
-                if isinstance (enrich_section ,Mapping ):
-                    enrich_section =cast (Mapping [str ,Any ],enrich_section )
-                    document_term_section :Any =enrich_section .get ("document_term")
-                    if isinstance (document_term_section ,Mapping ):
-                        document_term_section =cast (Mapping [str ,Any ],document_term_section )
-                        enrich_cfg =dict (document_term_section )
-    except (AttributeError ,KeyError ,TypeError )as exc :
-        log .warning ("enrichment_config_error",error =str (exc ),message ="Using default enrichment config")
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =DocumentSourceConfig .from_source_config (source_raw )
-    api_client ,_ =self .prepare_chembl_client ("chembl",base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters ))),client_name ="chembl_enrichment_client")
-    chembl_client =ChemblClient (api_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
-    return enrich_with_document_terms (df ,chembl_client ,enrich_cfg )
```

#### Hotspot 8

- Definition: ChemblDocumentPipeline._extract_nested_fields#1
- document: src/bioetl/pipelines/chembl/document/run.py:643-647
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,3 +0,0 @@
-def _extract_nested_fields (self ,record :dict [str ,Any ])->dict [str ,Any ]:
-    "Extract fields from nested objects in document records."
-    return record
```

#### Hotspot 9

- Definition: ChemblDocumentPipeline._normalize_authors#1
- document: src/bioetl/pipelines/chembl/document/run.py:484-495
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,13 +0,0 @@
-@staticmethod
-def _normalize_authors (authors :Any ,separator :str =", ")->tuple [str ,int ]:
-    "Normalize author separators and count."
-    if pd .isna (authors ):
-        return ("",0 )
-    text =str (authors ).strip ()
-    text =re .sub (";",",",text )
-    text =re .sub ("\\s+"," ",text )
-    if not text :
-        return ("",0 )
-    parts =text .split (",")
-    parts =[p .strip ()for p in parts if p .strip ()]
-    return (separator .join (parts ),len (parts ))
```

#### Hotspot 10

- Definition: ChemblDocumentPipeline._normalize_doi#1
- document: src/bioetl/pipelines/chembl/document/run.py:392-408
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,16 +0,0 @@
-@staticmethod
-def _normalize_doi (doi :str |None )->str :
-    "Normalize DOI by removing prefixes and validating format."
-    if not doi :
-        return ""
-    if not isinstance (doi ,str ):
-        return ""
-    doi =doi .strip ().lower ()
-    for prefix in ["doi:","https://doi.org/","http://dx.doi.org/","http://doi.org/"]:
-        if doi .startswith (prefix ):
-            doi =doi [len (prefix ):]
-    doi =doi .strip ()
-    doi_pattern =re .compile ("^10\\.\\d{4,9}/\\S+$")
-    if doi_pattern .match (doi ):
-        return doi
-    return ""
```

#### Hotspot 11

- Definition: ChemblDocumentPipeline._normalize_identifiers#1
- document: src/bioetl/pipelines/chembl/document/run.py:377-389
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,8 +0,0 @@
-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize identifier fields (DOI, PMID)."
-    df =df .copy ()
-    if "doi"in df .columns :
-        df ["doi_clean"]=df ["doi"].apply (self ._normalize_doi )
-    if "pubmed_id"in df .columns :
-        df ["pubmed_id"]=pd .to_numeric (df ["pubmed_id"],errors ="coerce").astype ("Int64")
-    return df
```

#### Hotspot 12

- Definition: ChemblDocumentPipeline._normalize_journal#1
- document: src/bioetl/pipelines/chembl/document/run.py:475-481
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,8 +0,0 @@
-@staticmethod
-def _normalize_journal (value :Any ,max_len :int =255 )->str :
-    "Trim and collapse whitespace for journal name."
-    if pd .isna (value ):
-        return ""
-    text =str (value )
-    text =re .sub ("\\s+"," ",text ).strip ()
-    return text [:max_len ]if len (text )>max_len else text
```

#### Hotspot 13

- Definition: ChemblDocumentPipeline._normalize_numeric_fields#1
- document: src/bioetl/pipelines/chembl/document/run.py:497-530
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,29 +0,0 @@
-def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize numeric fields (year)."
-    df =df .copy ()
-    if "year"in df .columns :
-        def _coerce_year (value :object )->int |None :
-            if value is None or value is pd .NA :
-                return None
-            if isinstance (value ,Integral ):
-                year_int =int (value )
-            elif isinstance (value ,Real ):
-                float_value =float (value )
-                if not float_value .is_integer ():
-                    return None
-                year_int =int (float_value )
-            elif isinstance (value ,str ):
-                stripped =value .strip ()
-                if not stripped :
-                    return None
-                if not stripped .isdigit ():
-                    return None
-                year_int =int (stripped )
-            else :
-                return None
-            if 1500 <=year_int <=2100 :
-                return year_int
-            return None
-        normalized_year =df ["year"].apply (_coerce_year )
-        df ["year"]=normalized_year .astype ("Int64")
-    return df
```

#### Hotspot 14

- Definition: ChemblDocumentPipeline._normalize_string_fields#1
- document: src/bioetl/pipelines/chembl/document/run.py:410-472
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,43 +0,0 @@
-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields (title, abstract, journal, authors)."
-    working_df =df .copy ()
-    rules ={"title":StringRule (max_length =1000 ),"abstract":StringRule (max_length =5000 )}
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    if "journal"in normalized_df .columns :
-        journal_series :pd .Series [Any ]=normalized_df ["journal"]
-        normalized_df ["journal"]=journal_series .map (lambda value :self ._normalize_journal (value ))
-    if "authors"in normalized_df .columns :
-        def _to_author_tuple (item :object )->tuple [str ,int ]|None :
-            if not isinstance (item ,tuple ):
-                return None
-            tuple_item =cast (tuple [object ,...],item )
-            if len (tuple_item )!=2 :
-                return None
-            name_raw ,count_raw =tuple_item
-            if not isinstance (name_raw ,str ):
-                return None
-            name_value :str =name_raw
-            if isinstance (count_raw ,Integral ):
-                count_value =int (count_raw )
-            elif isinstance (count_raw ,Real ):
-                float_value =float (count_raw )
-                if not float_value .is_integer ():
-                    return None
-                count_value =int (float_value )
-            else :
-                return None
-            if count_value <0 :
-                return None
-            return (name_value ,count_value )
-        def _author_name_from_tuple (data :tuple [str ,int ]|None )->str :
-            return data [0 ]if data is not None else ""
-        def _author_count_from_tuple (data :tuple [str ,int ]|None )->int :
-            return data [1 ]if data is not None else 0
-        authors_series :pd .Series [Any ]=normalized_df ["authors"]
-        normalized_result =authors_series .apply (self ._normalize_authors )
-        normalized_tuples =normalized_result .apply (_to_author_tuple )
-        normalized_df ["authors"]=normalized_tuples .apply (_author_name_from_tuple )
-        normalized_df ["authors_count"]=normalized_tuples .apply (_author_count_from_tuple )
-    return normalized_df
```

#### Hotspot 15

- Definition: ChemblDocumentPipeline._schema_column_specs#1
- document: src/bioetl/pipelines/chembl/document/run.py:541-555
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,12 +0,0 @@
-def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
-    specs =dict (super ()._schema_column_specs ())
-    specs ["source"]={"default":"ChEMBL"}
-    specs ["authors_count"]={"default":0 ,"dtype":"Int64"}
-    hashing_config =self .config .determinism .hashing
-    business_key_column =hashing_config .business_key_column
-    row_hash_column =hashing_config .row_hash_column
-    if business_key_column :
-        specs [business_key_column ]={"default":""}
-    if row_hash_column :
-        specs [row_hash_column ]={"default":""}
-    return specs
```

#### Hotspot 16

- Definition: ChemblDocumentPipeline._should_enrich_document_terms#1
- document: src/bioetl/pipelines/chembl/document/run.py:574-595
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,22 +0,0 @@
-def _should_enrich_document_terms (self )->bool :
-    "\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u043e \u043b\u0438 \u043e\u0431\u043e\u0433\u0430\u0449\u0435\u043d\u0438\u0435 document_term \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0435."
-    if not self .config .chembl :
-        return False
-    try :
-        chembl_section =self .config .chembl
-        document_section :Any =chembl_section .get ("document")
-        if not isinstance (document_section ,Mapping ):
-            return False
-        document_section =cast (Mapping [str ,Any ],document_section )
-        enrich_section :Any =document_section .get ("enrich")
-        if not isinstance (enrich_section ,Mapping ):
-            return False
-        enrich_section =cast (Mapping [str ,Any ],enrich_section )
-        document_term_section :Any =enrich_section .get ("document_term")
-        if not isinstance (document_term_section ,Mapping ):
-            return False
-        document_term_section =cast (Mapping [str ,Any ],document_term_section )
-        enabled :Any =document_term_section .get ("enabled")
-        return bool (enabled )if enabled is not None else False
-    except (AttributeError ,KeyError ,TypeError ):
-        return False
```

#### Hotspot 17

- Definition: ChemblDocumentPipeline.extract#1
- document: src/bioetl/pipelines/chembl/document/run.py:69-83
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,4 +0,0 @@
-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch document payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_document.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="document_chembl_id")
```

#### Hotspot 18

- Definition: ChemblDocumentPipeline.extract_all#1
- document: src/bioetl/pipelines/chembl/document/run.py:85-88
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,3 +0,0 @@
-def extract_all (self )->pd .DataFrame :
-    "Extract all document records from ChEMBL using pagination."
-    return self .run_extract_all (self ._build_document_descriptor ())
```

#### Hotspot 19

- Definition: ChemblDocumentPipeline.extract_by_ids#1
- document: src/bioetl/pipelines/chembl/document/run.py:180-280
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,36 +0,0 @@
-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract document records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of document_chembl_id values to extract (as strings).\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted document records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =DocumentSourceConfig .from_source_config (source_raw )
-    base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-    http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_document_client")
-    chembl_client =ChemblClient (http_client )
-    document_client =ChemblDocumentClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-    self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-    resolved_select_fields =self ._resolve_select_fields (source_raw ,default_fields =list (API_DOCUMENT_FIELDS ))
-    merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
-    limit =self .config .cli .limit
-    def fetch_documents (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        if "original_paginate"not in context .extra :
-            original_paginate =chembl_client .paginate
-            def counted_paginate (*args :Any ,**kwargs :Any )->Any :
-                context .increment_api_calls ()
-                return original_paginate (*args ,**kwargs )
-            chembl_client .paginate =counted_paginate
-            context .extra ["original_paginate"]=original_paginate
-        iterator =document_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield self ._extract_nested_fields (dict (item ))
-    def finalize_context (context :BatchExtractionContext )->None :
-        original =context .extra .pop ("original_paginate",None )
-        if original is not None :
-            chembl_client .paginate =original
-        api_calls_value =context .stats .api_calls if context .stats .api_calls is not None else 0
-        override ={"batches":context .stats .batches ,"api_calls":api_calls_value ,"cache_hits":context .stats .cache_hits }
-        context .extra ["stats_attribute_override"]=override
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="document_chembl_id",fetcher =fetch_documents ,select_fields =merged_select_fields or None ,batch_size =document_client .batch_size ,max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":merged_select_fields }if merged_select_fields else None ,chembl_release =self ._chembl_release ,finalize_context =finalize_context ,stats_attribute ="_last_batch_extract_stats")
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_document.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,batches =stats .batches ,api_calls =stats .api_calls ,cache_hits =stats .cache_hits )
-    return dataframe
```

#### Hotspot 20

- Definition: ChemblDocumentPipeline.transform#1
- document: src/bioetl/pipelines/chembl/document/run.py:282-336
- testitem: absent

```diff
--- document:run.py
+++ testitem:run.py
@@ -1,24 +0,0 @@
-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw document data by normalizing fields and identifiers."
-    log =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.transform')
-    df =df .copy ()
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._normalize_numeric_fields (df ,log )
-    if self ._should_enrich_document_terms ():
-        df =self ._enrich_document_terms (df )
-    df =self ._add_system_fields (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if "document_chembl_id"in df .columns and df ["document_chembl_id"].duplicated ().any ():
-        initial_count =len (df )
-        df =df .sort_values (by =list (df .columns )).drop_duplicates (subset =["document_chembl_id"],keep ="first")
-        deduped_count =len (df )
-        if deduped_count <initial_count :
-            log .warning ("document_deduplication_applied",initial_count =initial_count ,deduped_count =deduped_count ,removed_count =initial_count -deduped_count )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

_Only the first 20 hotspots of 54 are shown for module run.py._

_First 20 hotspots of 80 are shown for pair document ↔ testitem._

---

## Pair: target ↔ testitem

- AST hash: 107171553e4f1c509bba122a3d0ccb96 ↔ 729263c98934cfcb08473bcacfb20e9e

- Jaccard over tokens: 0.302

### Module run.py

- File status: target — present, testitem — present
- AST hash: 8689c415f6f3fc16a44fa879dc47ed50 ↔ 4380f065a3fdcc94928fad4ab20f1d2b
- Jaccard over tokens: 0.335

Definition                                           | target signature                                     | testitem signature                                                           | Side effects                                                                                                                                 | Exceptions               | Status          
-----------------------------------------------------|------------------------------------------------------|------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|--------------------------|-----------------
ChemblTargetPipeline                                 | —                                                    | —                                                                            | target: io=json.dumps; logging=UnifiedLogger.get, log.debug, log.info, log.warning<br>testitem: ∅                                            | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline.__init__                        | self, config: PipelineConfig, run_id: str            | —                                                                            | target: io=∅; logging=∅<br>testitem: ∅                                                                                                       | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline._build_target_descriptor        | self                                                 | —                                                                            | target: io=∅; logging=log.info<br>testitem: ∅                                                                                                | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline._enrich_protein_classifications | self, df: pd.DataFrame, log: Any                     | —                                                                            | target: io=json.dumps; logging=log.debug, log.info, log.warning<br>testitem: ∅                                                               | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline._enrich_target_components       | self, df: pd.DataFrame, log: Any                     | —                                                                            | target: io=json.dumps; logging=log.debug, log.info, log.warning<br>testitem: ∅                                                               | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline._harmonize_identifier_columns   | self, df: pd.DataFrame, log: Any                     | —                                                                            | target: io=∅; logging=log.debug<br>testitem: ∅                                                                                               | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline._normalize_data_types           | self, df: pd.DataFrame, schema: Any | None, log: Any | —                                                                            | target: io=∅; logging=log.warning<br>testitem: ∅                                                                                             | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline._normalize_identifiers          | self, df: pd.DataFrame, log: Any                     | —                                                                            | target: io=∅; logging=log.warning<br>testitem: ∅                                                                                             | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline._normalize_string_fields        | self, df: pd.DataFrame, log: Any                     | —                                                                            | target: io=∅; logging=log.debug<br>testitem: ∅                                                                                               | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline.extract                         | self, *args, **kwargs                                | —                                                                            | target: io=∅; logging=UnifiedLogger.get<br>testitem: ∅                                                                                       | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline.extract_all                     | self                                                 | —                                                                            | target: io=∅; logging=∅<br>testitem: ∅                                                                                                       | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline.extract_by_ids                  | self, ids: Sequence[str]                             | —                                                                            | target: io=∅; logging=UnifiedLogger.get, log.info<br>testitem: ∅                                                                             | target: ∅<br>testitem: ∅ | only in target  
ChemblTargetPipeline.transform                       | self, df: pd.DataFrame                               | —                                                                            | target: io=∅; logging=UnifiedLogger.get, log.debug, log.info<br>testitem: ∅                                                                  | target: ∅<br>testitem: ∅ | only in target  
TestItemChemblPipeline                               | —                                                    | —                                                                            | target: ∅<br>testitem: io=status_payload.get; logging=UnifiedLogger.get, bound_log.info, bound_log.warning, log.debug, log.info, log.warning | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline.__init__                      | —                                                    | self, config: PipelineConfig, run_id: str                                    | target: ∅<br>testitem: io=∅; logging=∅                                                                                                       | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline._build_testitem_descriptor    | —                                                    | self: SelfTestitemChemblPipeline                                             | target: ∅<br>testitem: io=∅; logging=log.debug, log.info                                                                                     | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline._check_empty_columns          | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: ∅<br>testitem: io=∅; logging=log.warning                                                                                             | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline._deduplicate_molecules        | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: ∅<br>testitem: io=∅; logging=log.info                                                                                                | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline._fetch_chembl_release         | —                                                    | self, client: UnifiedAPIClient | ChemblClient | Any, log: BoundLogger | None | target: ∅<br>testitem: io=status_payload.get; logging=UnifiedLogger.get, bound_log.info, bound_log.warning                                   | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline._flatten_nested_structures    | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline._normalize_identifiers        | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline._normalize_numeric_fields     | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline._normalize_string_fields      | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline._remove_extra_columns         | —                                                    | self, df: pd.DataFrame, log: Any                                             | target: ∅<br>testitem: io=∅; logging=log.debug                                                                                               | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline._schema_column_specs          | —                                                    | self                                                                         | target: ∅<br>testitem: io=∅; logging=∅                                                                                                       | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline.api_version                   | —                                                    | self                                                                         | target: ∅<br>testitem: io=∅; logging=∅                                                                                                       | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline.augment_metadata              | —                                                    | self, metadata: Mapping[str, object], df: pd.DataFrame                       | target: ∅<br>testitem: io=∅; logging=∅                                                                                                       | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline.chembl_db_version             | —                                                    | self                                                                         | target: ∅<br>testitem: io=∅; logging=∅                                                                                                       | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline.extract                       | —                                                    | self, *args, **kwargs                                                        | target: ∅<br>testitem: io=∅; logging=UnifiedLogger.get                                                                                       | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline.extract_all                   | —                                                    | self                                                                         | target: ∅<br>testitem: io=∅; logging=∅                                                                                                       | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline.extract_by_ids                | —                                                    | self, ids: Sequence[str]                                                     | target: ∅<br>testitem: io=∅; logging=UnifiedLogger.get, log.debug, log.info                                                                  | target: ∅<br>testitem: ∅ | only in testitem
TestItemChemblPipeline.transform                     | —                                                    | self, df: pd.DataFrame                                                       | target: ∅<br>testitem: io=∅; logging=UnifiedLogger.get, log.debug, log.info                                                                  | target: ∅<br>testitem: ∅ | only in testitem
__module_block_0                                     | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_1                                     | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | identical       
__module_block_10                                    | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_11                                    | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_12                                    | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_13                                    | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_14                                    | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_15                                    | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_16                                    | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_17                                    | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_18                                    | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_19                                    | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: ∅                                                                                                       | target: ∅<br>testitem: ∅ | only in target  
__module_block_2                                     | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_3                                     | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | identical       
__module_block_4                                     | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | identical       
__module_block_5                                     | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_6                                     | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_7                                     | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | identical       
__module_block_8                                     | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         
__module_block_9                                     | —                                                    | —                                                                            | target: io=∅; logging=∅<br>testitem: io=∅; logging=∅                                                                                         | target: ∅<br>testitem: ∅ | differs         

#### Hotspot 1

- Definition: ChemblTargetPipeline#1
- target: src/bioetl/pipelines/chembl/target/run.py:38-783
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,342 +0,0 @@
-class ChemblTargetPipeline (ChemblPipelineBase ):
-    "ETL pipeline extracting target records from the ChEMBL API."
-    actor ="target_chembl"
-    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-        super ().__init__ (config ,run_id )
-    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-        "Fetch target payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        return self ._dispatch_extract_mode (log ,event_name ="chembl_target.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="target_chembl_id")
-    def extract_all (self )->pd .DataFrame :
-        "Extract all target records from ChEMBL using pagination."
-        descriptor =self ._build_target_descriptor ()
-        return self .run_extract_all (descriptor )
-    def _build_target_descriptor (self )->ChemblExtractionDescriptor ["ChemblTargetPipeline"]:
-        "Return the descriptor powering target extraction."
-        def build_context (pipeline :"ChemblTargetPipeline",source_config :TargetSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-            base_url =pipeline ._resolve_base_url (source_config .parameters )
-            http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
-            chembl_client =ChemblClient (http_client )
-            pipeline ._chembl_release =pipeline .fetch_chembl_release (chembl_client ,log )
-            target_client =ChemblTargetClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-            select_fields =source_config .parameters .select_fields
-            return ChemblExtractionContext (source_config =source_config ,iterator =target_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =pipeline ._chembl_release ,extra_filters ={"batch_size":source_config .batch_size })
-        def empty_frame (_ :"ChemblTargetPipeline",__ :ChemblExtractionContext )->pd .DataFrame :
-            return pd .DataFrame ({"target_chembl_id":pd .Series (dtype ="string")})
-        def dry_run_handler (pipeline :"ChemblTargetPipeline",_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =pipeline ._chembl_release )
-            return pd .DataFrame ()
-        def summary_extra (pipeline :"ChemblTargetPipeline",_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
-            return {"limit":pipeline .config .cli .limit }
-        return ChemblExtractionDescriptor [ChemblTargetPipeline ](name ="chembl_target",source_name ="chembl",source_config_factory =TargetSourceConfig .from_source_config ,build_context =build_context ,id_column ="target_chembl_id",summary_event ="chembl_target.extract_summary",sort_by =("target_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
-    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-        "Extract target records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of target_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted target records.\n        "
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-        stage_start =time .perf_counter ()
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =TargetSourceConfig .from_source_config (source_raw )
-        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
-        chembl_client =ChemblClient (http_client )
-        self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-        if self .config .cli .dry_run :
-            duration_ms =(time .perf_counter ()-stage_start )*1000.0
-            log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-            return pd .DataFrame ()
-        batch_size =source_config .batch_size
-        limit =self .config .cli .limit
-        select_fields =source_config .parameters .select_fields
-        target_client =ChemblTargetClient (chembl_client ,batch_size =min (batch_size ,25 ))
-        def fetch_targets (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-            iterator =target_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-            for item in iterator :
-                yield dict (item )
-        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="target_chembl_id",fetcher =fetch_targets ,select_fields =select_fields ,batch_size =batch_size ,chunk_size =min (batch_size ,100 ),max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release )
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_target.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-        return dataframe
-    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-        "Transform raw target data by normalizing fields and enriching with component/classification data."
-        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-        df =df .copy ()
-        df =self ._harmonize_identifier_columns (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        if df .empty :
-            log .debug ("transform_empty_dataframe")
-            return df
-        log .info ("transform_started",rows =len (df ))
-        df =self ._normalize_identifiers (df ,log )
-        df =serialize_target_arrays (df ,self .config )
-        if not self .config .cli .dry_run :
-            df =self ._enrich_target_components (df ,log )
-        if not self .config .cli .dry_run :
-            df =self ._enrich_protein_classifications (df ,log )
-        df =self ._normalize_string_fields (df ,log )
-        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-        df =self ._normalize_data_types (df ,TargetSchema ,log )
-        df =self ._order_schema_columns (df ,COLUMN_ORDER )
-        log .info ("transform_completed",rows =len (df ))
-        return df
-    def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Harmonize identifier column names."
-        df =df .copy ()
-        actions :list [str ]=[]
-        if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-            df ["target_chembl_id"]=df ["target_id"]
-            actions .append ("target_id->target_chembl_id")
-        alias_columns =[column for column in ("target_id",)if column in df .columns ]
-        if alias_columns :
-            df =df .drop (columns =alias_columns )
-            actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-        if actions :
-            log .debug ("identifier_harmonization",actions =actions )
-        return df
-    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize ChEMBL identifiers with regex validation."
-        rules =[IdentifierRule (name ="target_chembl",columns =["target_chembl_id"],pattern ="^CHEMBL\\d+$")]
-        normalized_df ,stats =normalize_identifier_columns (df ,rules )
-        invalid_info =stats .per_column .get ("target_chembl_id")
-        if invalid_info and invalid_info ["invalid"]>0 :
-            log .warning ("invalid_target_chembl_id",count =invalid_info ["invalid"])
-        return normalized_df
-    def _enrich_target_components (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Enrich targets with component data from /target_component endpoint.\n\n        Only enriches targets where uniprot_accessions or component_count are missing.\n        If data is already present from the main query (via serialize_target_arrays),\n        it will not be overwritten.\n        "
-        df =df .copy ()
-        if df .empty or "target_chembl_id"not in df .columns :
-            return df
-        needs_enrichment =df ["target_chembl_id"].notna ()
-        if "uniprot_accessions"in df .columns :
-            needs_enrichment =needs_enrichment &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
-        if "component_count"in df .columns :
-            needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["component_count"].isna ()|(df ["component_count"]==0 ))
-        target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
-        if not target_ids_to_enrich :
-            log .debug ("enrich_target_components_skipped",reason ="all_data_present")
-            return df
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =TargetSourceConfig .from_source_config (source_raw )
-        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
-        chembl_client =ChemblClient (http_client )
-        if "target_chembl_id"not in df .columns :
-            return df
-        component_map :dict [str ,list [str ]]={}
-        target_ids_set :set [str ]=set (target_ids_to_enrich )
-        def _is_target (value :object )->bool :
-            if value is None or value is pd .NA :
-                return False
-            if isinstance (value ,Real )and math .isnan (float (value )):
-                return False
-            normalized =str (value ).strip ()
-            if not normalized :
-                return False
-            return normalized in target_ids_set
-        target_membership =df ["target_chembl_id"].map (_is_target )
-        log .info ("enrich_target_components_start",target_count =len (target_ids_to_enrich ))
-        for target_id in target_ids_to_enrich :
-            try :
-                components :list [str ]=[]
-                for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
-                    accession =item .get ("accession")
-                    if isinstance (accession ,str )and accession .strip ():
-                        components .append (accession .strip ())
-                if components :
-                    component_map [target_id ]=components
-            except Exception as exc :
-                log .warning ("target_component_fetch_error",target_chembl_id =target_id ,error =str (exc ))
-        if "uniprot_accessions"in df .columns :
-            mask =target_membership &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
-            if mask .any ():
-                df .loc [mask ,"uniprot_accessions"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
-        if "component_count"in df .columns :
-            mask =target_membership &(df ["component_count"].isna ()|(df ["component_count"]==0 ))
-            if mask .any ():
-                df .loc [mask ,"component_count"]=df .loc [mask ,"target_chembl_id"].map (lambda x :len (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
-        log .info ("enrich_target_components_complete",enriched_count =len (component_map ))
-        return df
-    def _enrich_protein_classifications (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Enrich targets with full protein classification hierarchy.\n\n        Extracts complete protein classification hierarchy with tree nodes and expanded paths l1..l8.\n        Algorithm:\n        1. For each target, get components via /target_component.json\n        2. Filter only PROTEIN components (component_type = 'PROTEIN')\n        3. For each protein component, get classes via /component_class.json \u2192 protein_class_id\n        4. For each protein_class_id, get node metadata via /protein_classification.json\n        5. For each protein_class_id, get expanded path via /protein_family_classification.json \u2192 l1..l8\n        6. Aggregate at TID level: protein_class_list (array) and protein_class_top (min class_level)\n\n        Only enriches targets where protein_class_list or protein_class_top are missing.\n        If data is already present from the main query, it will not be overwritten.\n        "
-        df =df .copy ()
-        if df .empty or "target_chembl_id"not in df .columns :
-            return df
-        needs_enrichment =df ["target_chembl_id"].notna ()
-        if "protein_class_list"in df .columns :
-            needs_enrichment =needs_enrichment &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
-        if "protein_class_top"in df .columns :
-            needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
-        target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
-        if not target_ids_to_enrich :
-            log .debug ("enrich_protein_classifications_skipped",reason ="all_data_present")
-            return df
-        source_raw =self ._resolve_source_config ("chembl")
-        source_config =TargetSourceConfig .from_source_config (source_raw )
-        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
-        chembl_client =ChemblClient (http_client )
-        if "protein_class_list"not in df .columns :
-            df ["protein_class_list"]=pd .NA
-        if "protein_class_top"not in df .columns :
-            df ["protein_class_top"]=pd .NA
-        if "target_chembl_id"not in df .columns :
-            return df
-        classification_list_map :dict [str ,list [dict [str ,Any ]]]={}
-        classification_top_map :dict [str ,dict [str ,Any ]]={}
-        target_ids_set :set [str ]=set (target_ids_to_enrich )
-        def _is_target (value :object )->bool :
-            if value is None or value is pd .NA :
-                return False
-            if isinstance (value ,Real )and math .isnan (float (value )):
-                return False
-            normalized =str (value ).strip ()
-            if not normalized :
-                return False
-            return normalized in target_ids_set
-        target_membership =df ["target_chembl_id"].map (_is_target )
-        log .info ("enrich_protein_classifications_start",target_count =len (target_ids_to_enrich ))
-        for target_id in target_ids_to_enrich :
-            try :
-                component_ids :list [str ]=[]
-                for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
-                    component_id =item .get ("component_id")
-                    if component_id is not None :
-                        component_ids .append (str (component_id ))
-                if not component_ids :
-                    continue
-                protein_component_ids :list [str ]=[]
-                for component_id in component_ids :
-                    try :
-                        for seq_item in chembl_client .paginate ("/component_sequence.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_sequences"):
-                            component_type =seq_item .get ("component_type")
-                            if isinstance (component_type ,str )and component_type .upper ()=="PROTEIN":
-                                protein_component_ids .append (component_id )
-                                break
-                    except Exception as exc :
-                        log .debug ("component_sequence_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
-                if not protein_component_ids :
-                    continue
-                protein_class_ids :set [str ]=set ()
-                for component_id in protein_component_ids :
-                    try :
-                        for class_item in chembl_client .paginate ("/component_class.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_classes"):
-                            protein_class_id =class_item .get ("protein_class_id")
-                            if protein_class_id is not None :
-                                protein_class_ids .add (str (protein_class_id ))
-                    except Exception as exc :
-                        log .debug ("component_class_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
-                if not protein_class_ids :
-                    continue
-                protein_classes :list [dict [str ,Any ]]=[]
-                for protein_class_id in protein_class_ids :
-                    try :
-                        node_metadata :dict [str ,Any ]|None =None
-                        for node_item in chembl_client .paginate ("/protein_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_classifications"):
-                            node_metadata ={"protein_class_id":str (protein_class_id ),"pref_name":node_item .get ("pref_name"),"short_name":node_item .get ("short_name"),"class_level":node_item .get ("class_level"),"parent_id":node_item .get ("parent_id"),"protein_class_desc":node_item .get ("protein_class_desc")}
-                            break
-                        path_levels :list [str |None ]=[None ]*8
-                        try :
-                            for path_item in chembl_client .paginate ("/protein_family_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_family_classifications"):
-                                for i in range (1 ,9 ):
-                                    level_key =f'l{i }'
-                                    level_value =path_item .get (level_key )
-                                    if level_value is not None :
-                                        if isinstance (level_value ,(float ,int )):
-                                            if pd .isna (level_value ):
-                                                path_levels [i -1 ]=None
-                                            else :
-                                                path_levels [i -1 ]=str (level_value )
-                                        else :
-                                            path_levels [i -1 ]=str (level_value )
-                                break
-                        except Exception as exc :
-                            log .debug ("protein_family_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
-                        if node_metadata :
-                            class_obj :dict [str ,Any ]={"protein_class_id":node_metadata ["protein_class_id"],"pref_name":node_metadata .get ("pref_name"),"short_name":node_metadata .get ("short_name"),"class_level":node_metadata .get ("class_level"),"parent_id":node_metadata .get ("parent_id"),"protein_class_desc":node_metadata .get ("protein_class_desc"),"path":[level for level in path_levels if level is not None ]}
-                            protein_classes .append (class_obj )
-                    except Exception as exc :
-                        log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
-                if protein_classes :
-                    seen_ids :set [str ]=set ()
-                    unique_classes :list [dict [str ,Any ]]=[]
-                    for class_obj in protein_classes :
-                        class_id =class_obj .get ("protein_class_id")
-                        if class_id and class_id not in seen_ids :
-                            seen_ids .add (class_id )
-                            unique_classes .append (class_obj )
-                    unique_classes .sort (key =lambda x :(x .get ("class_level")is None ,x .get ("class_level")or 0 ))
-                    classification_list_map [target_id ]=unique_classes
-                    top_class :dict [str ,Any ]|None =None
-                    min_level :int |None =None
-                    for class_obj in unique_classes :
-                        level =class_obj .get ("class_level")
-                        if level is not None :
-                            try :
-                                level_int =int (level )if not isinstance (level ,int )else level
-                                if min_level is None or level_int <min_level :
-                                    min_level =level_int
-                                    top_class =class_obj
-                            except (ValueError ,TypeError ):
-                                continue
-                    if top_class :
-                        classification_top_map [target_id ]=top_class
-            except Exception as exc :
-                log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,error =str (exc ))
-        if "protein_class_list"in df .columns :
-            mask =target_membership &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
-            if mask .any ():
-                df .loc [mask ,"protein_class_list"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_list_map .get (str (x ),[]),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_list_map else pd .NA )
-        if "protein_class_top"in df .columns :
-            mask =target_membership &(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
-            if mask .any ():
-                df .loc [mask ,"protein_class_top"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_top_map .get (str (x ),{}),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_top_map else pd .NA )
-        log .info ("enrich_protein_classifications_complete",enriched_list_count =len (classification_list_map ),enriched_top_count =len (classification_top_map ))
-        return df
-    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-        "Normalize string fields by trimming whitespace."
-        working_df =df .copy ()
-        rules ={"pref_name":StringRule (),"target_type":StringRule (),"organism":StringRule (),"tax_id":StringRule ()}
-        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-        if stats .has_changes :
-            log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-        return normalized_df
-    def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any |None ,log :Any )->pd .DataFrame :
-        "Normalize data types to match schema expectations.\n\n        Overrides base implementation to handle component_count and species_group_flag specially.\n        "
-        df =super ()._normalize_data_types (df ,schema ,log )
-        def _coerce_nullable_int (value :object )->object :
-            if value is None or value is pd .NA :
-                return pd .NA
-            if isinstance (value ,bool ):
-                return int (value )
-            if isinstance (value ,Integral ):
-                return int (value )
-            if isinstance (value ,Decimal ):
-                if value .is_nan ()or value %1 !=0 :
-                    return pd .NA
-                return int (value )
-            if isinstance (value ,Real ):
-                float_value =float (value )
-                if not math .isfinite (float_value )or not float_value .is_integer ():
-                    return pd .NA
-                return int (float_value )
-            if isinstance (value ,str ):
-                stripped =value .strip ()
-                if not stripped :
-                    return pd .NA
-                try :
-                    decimal_value =Decimal (stripped )
-                except (InvalidOperation ,ValueError ):
-                    return pd .NA
-                if decimal_value .is_nan ()or decimal_value %1 !=0 :
-                    return pd .NA
-                return int (decimal_value )
-            return pd .NA
-        if "component_count"in df .columns :
-            coerced_component_count =df ["component_count"].map (_coerce_nullable_int )
-            df ["component_count"]=coerced_component_count .astype ("Int64")
-        if "species_group_flag"in df .columns :
-            try :
-                coerced_species_group_flag =df ["species_group_flag"].map (_coerce_nullable_int )
-                df ["species_group_flag"]=coerced_species_group_flag .astype ("Int64")
-            except (ValueError ,TypeError )as exc :
-                log .warning ("species_group_flag_conversion_failed",error =str (exc ))
-        return df
```

#### Hotspot 2

- Definition: ChemblTargetPipeline.__init__#1
- target: src/bioetl/pipelines/chembl/target/run.py:43-44
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,2 +0,0 @@
-def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
-    super ().__init__ (config ,run_id )
```

#### Hotspot 3

- Definition: ChemblTargetPipeline._build_target_descriptor#1
- target: src/bioetl/pipelines/chembl/target/run.py:72-141
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,19 +0,0 @@
-def _build_target_descriptor (self )->ChemblExtractionDescriptor ["ChemblTargetPipeline"]:
-    "Return the descriptor powering target extraction."
-    def build_context (pipeline :"ChemblTargetPipeline",source_config :TargetSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
-        base_url =pipeline ._resolve_base_url (source_config .parameters )
-        http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
-        chembl_client =ChemblClient (http_client )
-        pipeline ._chembl_release =pipeline .fetch_chembl_release (chembl_client ,log )
-        target_client =ChemblTargetClient (chembl_client ,batch_size =min (source_config .batch_size ,25 ))
-        select_fields =source_config .parameters .select_fields
-        return ChemblExtractionContext (source_config =source_config ,iterator =target_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,chembl_release =pipeline ._chembl_release ,extra_filters ={"batch_size":source_config .batch_size })
-    def empty_frame (_ :"ChemblTargetPipeline",__ :ChemblExtractionContext )->pd .DataFrame :
-        return pd .DataFrame ({"target_chembl_id":pd .Series (dtype ="string")})
-    def dry_run_handler (pipeline :"ChemblTargetPipeline",_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =pipeline ._chembl_release )
-        return pd .DataFrame ()
-    def summary_extra (pipeline :"ChemblTargetPipeline",_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
-        return {"limit":pipeline .config .cli .limit }
-    return ChemblExtractionDescriptor [ChemblTargetPipeline ](name ="chembl_target",source_name ="chembl",source_config_factory =TargetSourceConfig .from_source_config ,build_context =build_context ,id_column ="target_chembl_id",summary_event ="chembl_target.extract_summary",sort_by =("target_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra )
```

#### Hotspot 4

- Definition: ChemblTargetPipeline._enrich_protein_classifications#1
- target: src/bioetl/pipelines/chembl/target/run.py:418-708
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,136 +0,0 @@
-def _enrich_protein_classifications (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Enrich targets with full protein classification hierarchy.\n\n        Extracts complete protein classification hierarchy with tree nodes and expanded paths l1..l8.\n        Algorithm:\n        1. For each target, get components via /target_component.json\n        2. Filter only PROTEIN components (component_type = 'PROTEIN')\n        3. For each protein component, get classes via /component_class.json \u2192 protein_class_id\n        4. For each protein_class_id, get node metadata via /protein_classification.json\n        5. For each protein_class_id, get expanded path via /protein_family_classification.json \u2192 l1..l8\n        6. Aggregate at TID level: protein_class_list (array) and protein_class_top (min class_level)\n\n        Only enriches targets where protein_class_list or protein_class_top are missing.\n        If data is already present from the main query, it will not be overwritten.\n        "
-    df =df .copy ()
-    if df .empty or "target_chembl_id"not in df .columns :
-        return df
-    needs_enrichment =df ["target_chembl_id"].notna ()
-    if "protein_class_list"in df .columns :
-        needs_enrichment =needs_enrichment &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
-    if "protein_class_top"in df .columns :
-        needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
-    target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
-    if not target_ids_to_enrich :
-        log .debug ("enrich_protein_classifications_skipped",reason ="all_data_present")
-        return df
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =TargetSourceConfig .from_source_config (source_raw )
-    base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-    http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
-    chembl_client =ChemblClient (http_client )
-    if "protein_class_list"not in df .columns :
-        df ["protein_class_list"]=pd .NA
-    if "protein_class_top"not in df .columns :
-        df ["protein_class_top"]=pd .NA
-    if "target_chembl_id"not in df .columns :
-        return df
-    classification_list_map :dict [str ,list [dict [str ,Any ]]]={}
-    classification_top_map :dict [str ,dict [str ,Any ]]={}
-    target_ids_set :set [str ]=set (target_ids_to_enrich )
-    def _is_target (value :object )->bool :
-        if value is None or value is pd .NA :
-            return False
-        if isinstance (value ,Real )and math .isnan (float (value )):
-            return False
-        normalized =str (value ).strip ()
-        if not normalized :
-            return False
-        return normalized in target_ids_set
-    target_membership =df ["target_chembl_id"].map (_is_target )
-    log .info ("enrich_protein_classifications_start",target_count =len (target_ids_to_enrich ))
-    for target_id in target_ids_to_enrich :
-        try :
-            component_ids :list [str ]=[]
-            for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
-                component_id =item .get ("component_id")
-                if component_id is not None :
-                    component_ids .append (str (component_id ))
-            if not component_ids :
-                continue
-            protein_component_ids :list [str ]=[]
-            for component_id in component_ids :
-                try :
-                    for seq_item in chembl_client .paginate ("/component_sequence.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_sequences"):
-                        component_type =seq_item .get ("component_type")
-                        if isinstance (component_type ,str )and component_type .upper ()=="PROTEIN":
-                            protein_component_ids .append (component_id )
-                            break
-                except Exception as exc :
-                    log .debug ("component_sequence_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
-            if not protein_component_ids :
-                continue
-            protein_class_ids :set [str ]=set ()
-            for component_id in protein_component_ids :
-                try :
-                    for class_item in chembl_client .paginate ("/component_class.json",params ={"component_id":component_id },page_size =25 ,items_key ="component_classes"):
-                        protein_class_id =class_item .get ("protein_class_id")
-                        if protein_class_id is not None :
-                            protein_class_ids .add (str (protein_class_id ))
-                except Exception as exc :
-                    log .debug ("component_class_fetch_error",target_chembl_id =target_id ,component_id =component_id ,error =str (exc ))
-            if not protein_class_ids :
-                continue
-            protein_classes :list [dict [str ,Any ]]=[]
-            for protein_class_id in protein_class_ids :
-                try :
-                    node_metadata :dict [str ,Any ]|None =None
-                    for node_item in chembl_client .paginate ("/protein_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_classifications"):
-                        node_metadata ={"protein_class_id":str (protein_class_id ),"pref_name":node_item .get ("pref_name"),"short_name":node_item .get ("short_name"),"class_level":node_item .get ("class_level"),"parent_id":node_item .get ("parent_id"),"protein_class_desc":node_item .get ("protein_class_desc")}
-                        break
-                    path_levels :list [str |None ]=[None ]*8
-                    try :
-                        for path_item in chembl_client .paginate ("/protein_family_classification.json",params ={"protein_classification_id":protein_class_id },page_size =25 ,items_key ="protein_family_classifications"):
-                            for i in range (1 ,9 ):
-                                level_key =f'l{i }'
-                                level_value =path_item .get (level_key )
-                                if level_value is not None :
-                                    if isinstance (level_value ,(float ,int )):
-                                        if pd .isna (level_value ):
-                                            path_levels [i -1 ]=None
-                                        else :
-                                            path_levels [i -1 ]=str (level_value )
-                                    else :
-                                        path_levels [i -1 ]=str (level_value )
-                            break
-                    except Exception as exc :
-                        log .debug ("protein_family_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
-                    if node_metadata :
-                        class_obj :dict [str ,Any ]={"protein_class_id":node_metadata ["protein_class_id"],"pref_name":node_metadata .get ("pref_name"),"short_name":node_metadata .get ("short_name"),"class_level":node_metadata .get ("class_level"),"parent_id":node_metadata .get ("parent_id"),"protein_class_desc":node_metadata .get ("protein_class_desc"),"path":[level for level in path_levels if level is not None ]}
-                        protein_classes .append (class_obj )
-                except Exception as exc :
-                    log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,protein_class_id =protein_class_id ,error =str (exc ))
-            if protein_classes :
-                seen_ids :set [str ]=set ()
-                unique_classes :list [dict [str ,Any ]]=[]
-                for class_obj in protein_classes :
-                    class_id =class_obj .get ("protein_class_id")
-                    if class_id and class_id not in seen_ids :
-                        seen_ids .add (class_id )
-                        unique_classes .append (class_obj )
-                unique_classes .sort (key =lambda x :(x .get ("class_level")is None ,x .get ("class_level")or 0 ))
-                classification_list_map [target_id ]=unique_classes
-                top_class :dict [str ,Any ]|None =None
-                min_level :int |None =None
-                for class_obj in unique_classes :
-                    level =class_obj .get ("class_level")
-                    if level is not None :
-                        try :
-                            level_int =int (level )if not isinstance (level ,int )else level
-                            if min_level is None or level_int <min_level :
-                                min_level =level_int
-                                top_class =class_obj
-                        except (ValueError ,TypeError ):
-                            continue
-                if top_class :
-                    classification_top_map [target_id ]=top_class
-        except Exception as exc :
-            log .warning ("protein_classification_fetch_error",target_chembl_id =target_id ,error =str (exc ))
-    if "protein_class_list"in df .columns :
-        mask =target_membership &(df ["protein_class_list"].isna ()|(df ["protein_class_list"]==""))
-        if mask .any ():
-            df .loc [mask ,"protein_class_list"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_list_map .get (str (x ),[]),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_list_map else pd .NA )
-    if "protein_class_top"in df .columns :
-        mask =target_membership &(df ["protein_class_top"].isna ()|(df ["protein_class_top"]==""))
-        if mask .any ():
-            df .loc [mask ,"protein_class_top"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (classification_top_map .get (str (x ),{}),ensure_ascii =False ,sort_keys =True )if pd .notna (x )and str (x )in classification_top_map else pd .NA )
-    log .info ("enrich_protein_classifications_complete",enriched_list_count =len (classification_list_map ),enriched_top_count =len (classification_top_map ))
-    return df
```

#### Hotspot 5

- Definition: ChemblTargetPipeline._enrich_target_components#1
- target: src/bioetl/pipelines/chembl/target/run.py:312-416
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,55 +0,0 @@
-def _enrich_target_components (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Enrich targets with component data from /target_component endpoint.\n\n        Only enriches targets where uniprot_accessions or component_count are missing.\n        If data is already present from the main query (via serialize_target_arrays),\n        it will not be overwritten.\n        "
-    df =df .copy ()
-    if df .empty or "target_chembl_id"not in df .columns :
-        return df
-    needs_enrichment =df ["target_chembl_id"].notna ()
-    if "uniprot_accessions"in df .columns :
-        needs_enrichment =needs_enrichment &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
-    if "component_count"in df .columns :
-        needs_enrichment =needs_enrichment |df ["target_chembl_id"].notna ()&(df ["component_count"].isna ()|(df ["component_count"]==0 ))
-    target_ids_to_enrich :list [str ]=df .loc [needs_enrichment ,"target_chembl_id"].dropna ().unique ().tolist ()
-    if not target_ids_to_enrich :
-        log .debug ("enrich_target_components_skipped",reason ="all_data_present")
-        return df
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =TargetSourceConfig .from_source_config (source_raw )
-    base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-    http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url )
-    chembl_client =ChemblClient (http_client )
-    if "target_chembl_id"not in df .columns :
-        return df
-    component_map :dict [str ,list [str ]]={}
-    target_ids_set :set [str ]=set (target_ids_to_enrich )
-    def _is_target (value :object )->bool :
-        if value is None or value is pd .NA :
-            return False
-        if isinstance (value ,Real )and math .isnan (float (value )):
-            return False
-        normalized =str (value ).strip ()
-        if not normalized :
-            return False
-        return normalized in target_ids_set
-    target_membership =df ["target_chembl_id"].map (_is_target )
-    log .info ("enrich_target_components_start",target_count =len (target_ids_to_enrich ))
-    for target_id in target_ids_to_enrich :
-        try :
-            components :list [str ]=[]
-            for item in chembl_client .paginate ("/target_component.json",params ={"target_chembl_id":target_id },page_size =25 ,items_key ="target_components"):
-                accession =item .get ("accession")
-                if isinstance (accession ,str )and accession .strip ():
-                    components .append (accession .strip ())
-            if components :
-                component_map [target_id ]=components
-        except Exception as exc :
-            log .warning ("target_component_fetch_error",target_chembl_id =target_id ,error =str (exc ))
-    if "uniprot_accessions"in df .columns :
-        mask =target_membership &(df ["uniprot_accessions"].isna ()|(df ["uniprot_accessions"]==""))
-        if mask .any ():
-            df .loc [mask ,"uniprot_accessions"]=df .loc [mask ,"target_chembl_id"].map (lambda x :json .dumps (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
-    if "component_count"in df .columns :
-        mask =target_membership &(df ["component_count"].isna ()|(df ["component_count"]==0 ))
-        if mask .any ():
-            df .loc [mask ,"component_count"]=df .loc [mask ,"target_chembl_id"].map (lambda x :len (component_map .get (str (x ),[]))if pd .notna (x )else pd .NA )
-    log .info ("enrich_target_components_complete",enriched_count =len (component_map ))
-    return df
```

#### Hotspot 6

- Definition: ChemblTargetPipeline._harmonize_identifier_columns#1
- target: src/bioetl/pipelines/chembl/target/run.py:272-289
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,14 +0,0 @@
-def _harmonize_identifier_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Harmonize identifier column names."
-    df =df .copy ()
-    actions :list [str ]=[]
-    if "target_id"in df .columns and "target_chembl_id"not in df .columns :
-        df ["target_chembl_id"]=df ["target_id"]
-        actions .append ("target_id->target_chembl_id")
-    alias_columns =[column for column in ("target_id",)if column in df .columns ]
-    if alias_columns :
-        df =df .drop (columns =alias_columns )
-        actions .append (f"dropped_aliases:{",".join (alias_columns )}")
-    if actions :
-        log .debug ("identifier_harmonization",actions =actions )
-    return df
```

#### Hotspot 7

- Definition: ChemblTargetPipeline._normalize_data_types#1
- target: src/bioetl/pipelines/chembl/target/run.py:732-783
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,41 +0,0 @@
-def _normalize_data_types (self ,df :pd .DataFrame ,schema :Any |None ,log :Any )->pd .DataFrame :
-    "Normalize data types to match schema expectations.\n\n        Overrides base implementation to handle component_count and species_group_flag specially.\n        "
-    df =super ()._normalize_data_types (df ,schema ,log )
-    def _coerce_nullable_int (value :object )->object :
-        if value is None or value is pd .NA :
-            return pd .NA
-        if isinstance (value ,bool ):
-            return int (value )
-        if isinstance (value ,Integral ):
-            return int (value )
-        if isinstance (value ,Decimal ):
-            if value .is_nan ()or value %1 !=0 :
-                return pd .NA
-            return int (value )
-        if isinstance (value ,Real ):
-            float_value =float (value )
-            if not math .isfinite (float_value )or not float_value .is_integer ():
-                return pd .NA
-            return int (float_value )
-        if isinstance (value ,str ):
-            stripped =value .strip ()
-            if not stripped :
-                return pd .NA
-            try :
-                decimal_value =Decimal (stripped )
-            except (InvalidOperation ,ValueError ):
-                return pd .NA
-            if decimal_value .is_nan ()or decimal_value %1 !=0 :
-                return pd .NA
-            return int (decimal_value )
-        return pd .NA
-    if "component_count"in df .columns :
-        coerced_component_count =df ["component_count"].map (_coerce_nullable_int )
-        df ["component_count"]=coerced_component_count .astype ("Int64")
-    if "species_group_flag"in df .columns :
-        try :
-            coerced_species_group_flag =df ["species_group_flag"].map (_coerce_nullable_int )
-            df ["species_group_flag"]=coerced_species_group_flag .astype ("Int64")
-        except (ValueError ,TypeError )as exc :
-            log .warning ("species_group_flag_conversion_failed",error =str (exc ))
-    return df
```

#### Hotspot 8

- Definition: ChemblTargetPipeline._normalize_identifiers#1
- target: src/bioetl/pipelines/chembl/target/run.py:291-310
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,8 +0,0 @@
-def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize ChEMBL identifiers with regex validation."
-    rules =[IdentifierRule (name ="target_chembl",columns =["target_chembl_id"],pattern ="^CHEMBL\\d+$")]
-    normalized_df ,stats =normalize_identifier_columns (df ,rules )
-    invalid_info =stats .per_column .get ("target_chembl_id")
-    if invalid_info and invalid_info ["invalid"]>0 :
-        log .warning ("invalid_target_chembl_id",count =invalid_info ["invalid"])
-    return normalized_df
```

#### Hotspot 9

- Definition: ChemblTargetPipeline._normalize_string_fields#1
- target: src/bioetl/pipelines/chembl/target/run.py:710-730
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,8 +0,0 @@
-def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
-    "Normalize string fields by trimming whitespace."
-    working_df =df .copy ()
-    rules ={"pref_name":StringRule (),"target_type":StringRule (),"organism":StringRule (),"tax_id":StringRule ()}
-    normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
-    if stats .has_changes :
-        log .debug ("string_fields_normalized",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
-    return normalized_df
```

#### Hotspot 10

- Definition: ChemblTargetPipeline.extract#1
- target: src/bioetl/pipelines/chembl/target/run.py:50-64
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,4 +0,0 @@
-def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
-    "Fetch target payloads from ChEMBL using the configured HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    return self ._dispatch_extract_mode (log ,event_name ="chembl_target.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="target_chembl_id")
```

#### Hotspot 11

- Definition: ChemblTargetPipeline.extract_all#1
- target: src/bioetl/pipelines/chembl/target/run.py:66-70
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,4 +0,0 @@
-def extract_all (self )->pd .DataFrame :
-    "Extract all target records from ChEMBL using pagination."
-    descriptor =self ._build_target_descriptor ()
-    return self .run_extract_all (descriptor )
```

#### Hotspot 12

- Definition: ChemblTargetPipeline.extract_by_ids#1
- target: src/bioetl/pipelines/chembl/target/run.py:143-222
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,26 +0,0 @@
-def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
-    "Extract target records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of target_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted target records.\n        "
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
-    stage_start =time .perf_counter ()
-    source_raw =self ._resolve_source_config ("chembl")
-    source_config =TargetSourceConfig .from_source_config (source_raw )
-    base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
-    http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_target_http")
-    chembl_client =ChemblClient (http_client )
-    self ._chembl_release =self .fetch_chembl_release (chembl_client ,log )
-    if self .config .cli .dry_run :
-        duration_ms =(time .perf_counter ()-stage_start )*1000.0
-        log .info ("chembl_target.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_release =self ._chembl_release )
-        return pd .DataFrame ()
-    batch_size =source_config .batch_size
-    limit =self .config .cli .limit
-    select_fields =source_config .parameters .select_fields
-    target_client =ChemblTargetClient (chembl_client ,batch_size =min (batch_size ,25 ))
-    def fetch_targets (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
-        iterator =target_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
-        for item in iterator :
-            yield dict (item )
-    dataframe ,stats =self .run_batched_extraction (ids ,id_column ="target_chembl_id",fetcher =fetch_targets ,select_fields =select_fields ,batch_size =batch_size ,chunk_size =min (batch_size ,100 ),max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (select_fields )if select_fields else None },chembl_release =self ._chembl_release )
-    duration_ms =(time .perf_counter ()-stage_start )*1000.0
-    log .info ("chembl_target.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_release =self ._chembl_release ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
-    return dataframe
```

#### Hotspot 13

- Definition: ChemblTargetPipeline.transform#1
- target: src/bioetl/pipelines/chembl/target/run.py:224-266
- testitem: absent

```diff
--- target:run.py
+++ testitem:run.py
@@ -1,22 +0,0 @@
-def transform (self ,df :pd .DataFrame )->pd .DataFrame :
-    "Transform raw target data by normalizing fields and enriching with component/classification data."
-    log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
-    df =df .copy ()
-    df =self ._harmonize_identifier_columns (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    if df .empty :
-        log .debug ("transform_empty_dataframe")
-        return df
-    log .info ("transform_started",rows =len (df ))
-    df =self ._normalize_identifiers (df ,log )
-    df =serialize_target_arrays (df ,self .config )
-    if not self .config .cli .dry_run :
-        df =self ._enrich_target_components (df ,log )
-    if not self .config .cli .dry_run :
-        df =self ._enrich_protein_classifications (df ,log )
-    df =self ._normalize_string_fields (df ,log )
-    df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
-    df =self ._normalize_data_types (df ,TargetSchema ,log )
-    df =self ._order_schema_columns (df ,COLUMN_ORDER )
-    log .info ("transform_completed",rows =len (df ))
-    return df
```

#### Hotspot 14

- Definition: TestItemChemblPipeline#1
- target: absent
- testitem: src/bioetl/pipelines/chembl/testitem/run.py:52-807

```diff
--- target:run.py
+++ testitem:run.py
@@ -0,0 +1,296 @@
+class TestItemChemblPipeline (ChemblPipelineBase ):
+    "ETL pipeline extracting molecule records from the ChEMBL API."
+    actor ="testitem_chembl"
+    def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+        super ().__init__ (config ,run_id )
+        self ._chembl_db_version :str |None =None
+        self ._api_version :str |None =None
+    @property
+    def chembl_db_version (self )->str |None :
+        "Return the cached ChEMBL DB version captured during extraction."
+        return self ._chembl_db_version
+    @property
+    def api_version (self )->str |None :
+        "Return the cached ChEMBL API version captured during extraction."
+        return self ._api_version
+    def _fetch_chembl_release (self ,client :UnifiedAPIClient |ChemblClient |Any ,log :BoundLogger |None =None )->str |None :
+        "Capture ChEMBL release and API version from status endpoint."
+        if log is None :
+            bound_log :BoundLogger =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
+        else :
+            bound_log =log
+        request_timestamp =datetime .now (timezone .utc )
+        release_value :str |None =None
+        api_version :str |None =None
+        try :
+            status_payload :dict [str ,Any ]={}
+            handshake_candidate =getattr (client ,"handshake",None )
+            if callable (handshake_candidate ):
+                status_payload =self ._coerce_mapping (handshake_candidate ("/status"))
+            else :
+                get_candidate =getattr (client ,"get",None )
+                if callable (get_candidate ):
+                    response =get_candidate ("/status.json")
+                    json_candidate =getattr (response ,"json",None )
+                    if callable (json_candidate ):
+                        status_payload =self ._coerce_mapping (json_candidate ())
+            if status_payload :
+                release_value =self ._extract_chembl_release (status_payload )
+                api_candidate =status_payload .get ("api_version")
+                if isinstance (api_candidate ,str )and api_candidate .strip ():
+                    api_version =api_candidate
+                bound_log .info ("chembl_testitem.status",chembl_db_version =release_value ,api_version =api_version )
+        except Exception as exc :
+            bound_log .warning ("chembl_testitem.status_failed",error =str (exc ))
+        finally :
+            self ._chembl_db_version =release_value
+            self ._api_version =api_version
+            self .record_extract_metadata (chembl_release =release_value ,requested_at_utc =request_timestamp )
+        return release_value
+    def extract (self ,*args :object ,**kwargs :object )->pd .DataFrame :
+        "Fetch molecule payloads from ChEMBL using the unified HTTP client.\n\n        Checks for input_file in config.cli.input_file and calls extract_by_ids()\n        if present, otherwise calls extract_all().\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        return self ._dispatch_extract_mode (log ,event_name ="chembl_testitem.extract_mode",batch_callback =self .extract_by_ids ,full_callback =self .extract_all ,id_column_name ="molecule_chembl_id")
+    def extract_all (self )->pd .DataFrame :
+        "Extract all molecule records from ChEMBL using pagination."
+        descriptor =self ._build_testitem_descriptor ()
+        return self .run_extract_all (descriptor )
+    def _build_testitem_descriptor (self :SelfTestitemChemblPipeline )->ChemblExtractionDescriptor [SelfTestitemChemblPipeline ]:
+        "Return the descriptor powering testitem extraction."
+        def build_context (pipeline :SelfTestitemChemblPipeline ,source_config :TestItemSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+            base_url =pipeline ._resolve_base_url (source_config .parameters )
+            http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+            chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
+            pipeline ._fetch_chembl_release (chembl_client ,log )
+            select_fields =source_config .parameters .select_fields
+            log .debug ("chembl_testitem.select_fields",fields =select_fields )
+            testitem_client =ChemblTestitemClient (chembl_client ,batch_size =min (source_config .page_size ,25 ))
+            return ChemblExtractionContext (source_config =source_config ,iterator =testitem_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,page_size =source_config .page_size ,chembl_release =pipeline ._chembl_db_version ,metadata ={"api_version":pipeline ._api_version })
+        def empty_frame (_ :SelfTestitemChemblPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
+            return pd .DataFrame ({"molecule_chembl_id":pd .Series (dtype ="string")})
+        def dry_run_handler (pipeline :SelfTestitemChemblPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =pipeline ._chembl_db_version ,api_version =pipeline ._api_version )
+            return pd .DataFrame ()
+        def summary_extra (pipeline :SelfTestitemChemblPipeline ,_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+            return {"chembl_db_version":pipeline ._chembl_db_version ,"api_version":pipeline ._api_version ,"limit":pipeline .config .cli .limit }
+        return ChemblExtractionDescriptor [SelfTestitemChemblPipeline ](name ="chembl_testitem",source_name ="chembl",source_config_factory =TestItemSourceConfig .from_source_config ,build_context =build_context ,id_column ="molecule_chembl_id",summary_event ="chembl_testitem.extract_summary",must_have_fields =tuple (MUST_HAVE_FIELDS ),default_select_fields =MUST_HAVE_FIELDS ,sort_by =("molecule_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra ,hard_page_size_cap =None )
+    def extract_by_ids (self ,ids :Sequence [str ])->pd .DataFrame :
+        "Extract molecule records by a specific list of IDs using batch extraction.\n\n        Parameters\n        ----------\n        ids:\n            Sequence of molecule_chembl_id values to extract.\n\n        Returns\n        -------\n        pd.DataFrame:\n            DataFrame containing extracted molecule records.\n        "
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("extract"))
+        stage_start =time .perf_counter ()
+        source_raw =self ._resolve_source_config ("chembl")
+        source_config =TestItemSourceConfig .from_source_config (source_raw )
+        base_url =self ._resolve_base_url (cast (Mapping [str ,Any ],dict (source_config .parameters )))
+        http_client ,_ =self .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+        chembl_client =ChemblClient (http_client ,load_meta_store =self .load_meta_store ,job_id =self .run_id ,operator =self .pipeline_code )
+        self ._fetch_chembl_release (chembl_client ,log )
+        if self .config .cli .dry_run :
+            duration_ms =(time .perf_counter ()-stage_start )*1000.0
+            log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =self ._chembl_db_version ,api_version =self ._api_version )
+            return pd .DataFrame ()
+        page_size =min (source_config .page_size ,25 )
+        limit =self .config .cli .limit
+        resolved_select_fields =source_config .parameters .select_fields
+        merged_select_fields =self ._merge_select_fields (resolved_select_fields ,MUST_HAVE_FIELDS )
+        log .debug ("chembl_testitem.select_fields",fields =merged_select_fields )
+        testitem_client =ChemblTestitemClient (chembl_client ,batch_size =page_size )
+        def fetch_testitems (batch_ids :Sequence [str ],context :BatchExtractionContext )->Iterable [Mapping [str ,Any ]]:
+            iterator =testitem_client .iterate_by_ids (batch_ids ,select_fields =context .select_fields or None )
+            for item in iterator :
+                yield dict (item )
+        dataframe ,stats =self .run_batched_extraction (ids ,id_column ="molecule_chembl_id",fetcher =fetch_testitems ,select_fields =merged_select_fields or None ,batch_size =page_size ,chunk_size =min (page_size ,25 ),max_batch_size =25 ,limit =limit ,metadata_filters ={"select_fields":list (merged_select_fields )if merged_select_fields else None },chembl_release =self ._chembl_release )
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_testitem.extract_by_ids_summary",rows =int (dataframe .shape [0 ]),requested =len (ids ),duration_ms =duration_ms ,chembl_db_version =self ._chembl_db_version ,api_version =self ._api_version ,limit =limit ,batches =stats .batches ,api_calls =stats .api_calls )
+        return dataframe
+    def transform (self ,df :pd .DataFrame )->pd .DataFrame :
+        "Transform raw molecule data by normalizing fields and extracting nested properties."
+        log =UnifiedLogger .get (__name__ ).bind (component =self ._component_for_stage ("transform"))
+        df =df .copy ()
+        if df .empty :
+            log .debug ("transform_empty_dataframe")
+            return df
+        log .info ("transform_started",rows =len (df ))
+        df =transform_testitem (df ,self .config )
+        df =self ._normalize_identifiers (df ,log )
+        df =self ._normalize_string_fields (df ,log )
+        df =self ._ensure_schema_columns (df ,COLUMN_ORDER ,log )
+        df =self ._normalize_numeric_fields (df ,log )
+        self ._check_empty_columns (df ,log )
+        df =self ._remove_extra_columns (df ,log )
+        df ["_chembl_db_version"]=self ._chembl_db_version or ""
+        df ["_api_version"]=self ._api_version or ""
+        df =self ._deduplicate_molecules (df ,log )
+        if "molecule_chembl_id"in df .columns :
+            df =df .sort_values ("molecule_chembl_id").reset_index (drop =True )
+        df =self ._order_schema_columns (df ,COLUMN_ORDER )
+        log .info ("transform_completed",rows =len (df ))
+        return df
+    def augment_metadata (self ,metadata :Mapping [str ,object ],df :pd .DataFrame )->Mapping [str ,object ]:
+        "Enrich metadata with ChEMBL versions."
+        enriched =dict (super ().augment_metadata (metadata ,df ))
+        if self ._chembl_db_version :
+            enriched ["chembl_db_version"]=self ._chembl_db_version
+        if self ._api_version :
+            enriched ["api_version"]=self ._api_version
+        return enriched
+    def _flatten_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Flatten nested molecule_structures and molecule_properties into flat columns."
+        if df .empty :
+            return df
+        if "molecule_structures"in df .columns :
+            structures_df =pd .json_normalize (df ["molecule_structures"].tolist ())
+            if "canonical_smiles"in structures_df .columns :
+                df ["canonical_smiles"]=structures_df ["canonical_smiles"]
+            if "standard_inchi_key"in structures_df .columns :
+                df ["standard_inchi_key"]=structures_df ["standard_inchi_key"]
+        if "molecule_properties"in df .columns :
+            properties_df =pd .json_normalize (df ["molecule_properties"].tolist ())
+            property_columns =["full_mwt","mw_freebase","alogp","hbd","hba","psa","aromatic_rings","rtb","num_ro5_violations"]
+            for col in property_columns :
+                if col in properties_df .columns :
+                    df [col ]=properties_df [col ]
+        log .debug ("flatten_nested_structures_completed",columns =list (df .columns ))
+        return df
+    def _normalize_identifiers (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize ChEMBL identifiers and InChI keys."
+        if df .empty :
+            return df
+        working_df =df .copy ()
+        inchi_key_col ="molecule_structures__standard_inchi_key"if "molecule_structures__standard_inchi_key"in working_df .columns else "standard_inchi_key"
+        rules :dict [str ,StringRule ]={}
+        if "molecule_chembl_id"in working_df .columns :
+            rules ["molecule_chembl_id"]=StringRule ()
+        if inchi_key_col in working_df .columns :
+            rules [inchi_key_col ]=StringRule (uppercase =True )
+        if rules :
+            normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        else :
+            normalized_df =working_df
+            stats =StringStats ()
+        if stats .has_changes :
+            log .debug ("normalize_identifiers_completed",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        else :
+            log .debug ("normalize_identifiers_completed")
+        return normalized_df
+    def _normalize_string_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize string fields: trim, replace empty strings with NaN."
+        if df .empty :
+            return df
+        working_df =df .copy ()
+        rules ={"pref_name":StringRule (),"molecule_type":StringRule (),"molecule_structures__canonical_smiles":StringRule (),"canonical_smiles":StringRule ()}
+        normalized_df ,stats =normalize_string_columns (working_df ,rules ,copy =False )
+        if stats .has_changes :
+            log .debug ("normalize_string_fields_completed",columns =list (stats .per_column .keys ()),rows_processed =stats .processed )
+        else :
+            log .debug ("normalize_string_fields_completed")
+        return normalized_df
+    def _schema_column_specs (self )->Mapping [str ,Mapping [str ,Any ]]:
+        specs =dict (super ()._schema_column_specs ())
+        int_columns =["max_phase","first_approval","first_in_class","availability_type","black_box_warning","chirality","dosed_ingredient","inorganic_flag","natural_product","prodrug","therapeutic_flag","molecule_properties__aromatic_rings","molecule_properties__hba","molecule_properties__hba_lipinski","molecule_properties__hbd","molecule_properties__hbd_lipinski","molecule_properties__heavy_atoms","molecule_properties__num_lipinski_ro5_violations","molecule_properties__num_ro5_violations","molecule_properties__ro3_pass","molecule_properties__rtb"]
+        float_columns =["molecule_properties__alogp","molecule_properties__cx_logd","molecule_properties__cx_logp","molecule_properties__cx_most_apka","molecule_properties__cx_most_bpka","molecule_properties__full_mwt","molecule_properties__mw_freebase","molecule_properties__mw_monoisotopic","molecule_properties__psa","molecule_properties__qed_weighted"]
+        for column in int_columns :
+            specs [column ]={"dtype":"Int64","default":pd .NA }
+        for column in float_columns :
+            specs [column ]={"dtype":"Float64","default":pd .NA }
+        for column in COLUMN_ORDER :
+            if column not in specs :
+                specs [column ]={"dtype":"string","default":pd .NA }
+        return specs
+    def _normalize_numeric_fields (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Normalize numeric fields: convert types, replace negative values with None."
+        if df .empty :
+            return df
+        int_ge_zero_columns =["chirality","availability_type","hbd","hba","aromatic_rings","rtb","num_ro5_violations"]
+        int_columns =["max_phase","first_approval","black_box_warning"]
+        boolean_columns =["first_in_class","inorganic_flag","natural_product","prodrug","therapeutic_flag","dosed_ingredient"]
+        for col in int_ge_zero_columns +int_columns +boolean_columns :
+            if col in df .columns :
+                numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                if col in int_ge_zero_columns :
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                elif col in boolean_columns :
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                    numeric_series =numeric_series .where ((numeric_series ==0 )|(numeric_series ==1 )|numeric_series .isna ())
+                df [col ]=numeric_series .astype ("Int64")
+        for col in df .columns :
+            if col .startswith ("molecule_properties__"):
+                prop_name =col .replace ("molecule_properties__","")
+                if prop_name =="ro3_pass":
+                    mask =df [col ].notna ()
+                    if mask .any ():
+                        col_str =df .loc [mask ,col ].astype (str ).str .upper ().str .strip ()
+                        converted =df [col ].copy ()
+                        converted .loc [mask &(col_str =="Y")]=1
+                        converted .loc [mask &(col_str =="N")]=0
+                        other_mask =mask &~col_str .isin (["Y","N"])
+                        if other_mask .any ():
+                            numeric_vals =pd .to_numeric (df .loc [other_mask ,col ],errors ="coerce")
+                            valid_numeric =numeric_vals .notna ()&numeric_vals .isin ([0 ,1 ])
+                            converted .loc [other_mask ]=pd .NA
+                            if valid_numeric .any ():
+                                valid_indices =numeric_vals .index [valid_numeric ]
+                                converted .loc [valid_indices ]=numeric_vals .loc [valid_indices ]
+                        df [col ]=converted
+                    numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                    numeric_series =numeric_series .where ((numeric_series ==0 )|(numeric_series ==1 )|numeric_series .isna ())
+                    df [col ]=numeric_series .astype ("Int64")
+                elif prop_name in ["aromatic_rings","hba","hba_lipinski","hbd","hbd_lipinski","heavy_atoms","num_lipinski_ro5_violations","num_ro5_violations","rtb"]:
+                    numeric_series =pd .to_numeric (df [col ],errors ="coerce")
+                    numeric_series =numeric_series .where (numeric_series >=0 )
+                    df [col ]=numeric_series .astype ("Int64")
+        log .debug ("normalize_numeric_fields_completed")
+        return df
+    def _check_empty_columns (self ,df :pd .DataFrame ,log :Any )->None :
+        "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043f\u0443\u0441\u0442\u044b\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n        \u041b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u0435, \u0435\u0441\u043b\u0438 \u043f\u0440\u043e\u0446\u0435\u043d\u0442 \u043f\u0443\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u043e \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u043c \u043f\u043e\u043b\u044f\u043c > 95%.\n        \u042d\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443 \u0441 select_fields \u0438\u043b\u0438 flatten_objects.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        "
+        if df .empty :
+            return
+        key_fields =["molecule_chembl_id","pref_name","molecule_type","availability_type","chirality","first_approval","first_in_class","indication_class","helm_notation","molecule_properties__cx_logp","molecule_properties__cx_logd","molecule_properties__mw_monoisotopic","molecule_properties__hba_lipinski","molecule_properties__hbd_lipinski","molecule_properties__molecular_species","molecule_properties__num_lipinski_ro5_violations"]
+        available_key_fields =[col for col in key_fields if col in df .columns ]
+        if not available_key_fields :
+            return
+        empty_percentages :dict [str ,float ]={}
+        for col in available_key_fields :
+            if len (df )>0 :
+                empty_count =df [col ].isna ().sum ()
+                empty_percentage =empty_count /len (df )*100.0
+                empty_percentages [col ]=empty_percentage
+        highly_empty_fields ={col :pct for col ,pct in empty_percentages .items ()if pct >95.0 }
+        if highly_empty_fields :
+            log .warning ("highly_empty_columns_detected",empty_fields =highly_empty_fields ,message =f'Fields with >95% empty values detected: {highly_empty_fields }. This may indicate missing fields in select_fields or flatten_objects configuration.')
+    def _remove_extra_columns (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Remove columns that are not in the schema."
+        if df .empty :
+            return df
+        from bioetl .schemas .testitem import COLUMN_ORDER
+        schema_columns =set (COLUMN_ORDER )
+        existing_columns =set (df .columns )
+        extra_columns =existing_columns -schema_columns
+        if extra_columns :
+            df =df .drop (columns =list (extra_columns ))
+            log .debug ("remove_extra_columns_completed",removed_columns =list (extra_columns ),remaining_columns =list (df .columns ))
+        return df
+    def _deduplicate_molecules (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+        "Deduplicate molecules by standard_inchi_key (fallback to molecule_chembl_id)."
+        if df .empty :
+            return df
+        rows_before =len (df )
+        inchi_key_col ="molecule_structures__standard_inchi_key"if "molecule_structures__standard_inchi_key"in df .columns else "standard_inchi_key"
+        canonical_smiles_col ="molecule_structures__canonical_smiles"if "molecule_structures__canonical_smiles"in df .columns else "canonical_smiles"
+        full_mwt_col ="molecule_properties__full_mwt"if "molecule_properties__full_mwt"in df .columns else "full_mwt"
+        alogp_col ="molecule_properties__alogp"if "molecule_properties__alogp"in df .columns else "alogp"
+        if inchi_key_col in df .columns :
+            completeness_cols =[canonical_smiles_col ,full_mwt_col ,alogp_col ]
+            available_completeness =[col for col in completeness_cols if col in df .columns ]
+            if available_completeness :
+                df ["_completeness"]=df [available_completeness ].notna ().sum (axis =1 )
+                df =df .sort_values (["_completeness",canonical_smiles_col ],ascending =[False ,False ],na_position ="last")
+                df =df .drop (columns =["_completeness"])
+            df =df .drop_duplicates (subset =[inchi_key_col ],keep ="first")
+        else :
+            df =df .drop_duplicates (subset =["molecule_chembl_id"],keep ="first")
+        rows_after =len (df )
+        dropped =rows_before -rows_after
+        if dropped >0 :
+            log .info ("deduplicate_molecules_completed",rows_before =rows_before ,rows_after =rows_after ,dropped =dropped )
+        return df
```

#### Hotspot 15

- Definition: TestItemChemblPipeline.__init__#1
- target: absent
- testitem: src/bioetl/pipelines/chembl/testitem/run.py:57-60

```diff
--- target:run.py
+++ testitem:run.py
@@ -0,0 +1,4 @@
+def __init__ (self ,config :PipelineConfig ,run_id :str )->None :
+    super ().__init__ (config ,run_id )
+    self ._chembl_db_version :str |None =None
+    self ._api_version :str |None =None
```

#### Hotspot 16

- Definition: TestItemChemblPipeline._build_testitem_descriptor#1
- target: absent
- testitem: src/bioetl/pipelines/chembl/testitem/run.py:151-234

```diff
--- target:run.py
+++ testitem:run.py
@@ -0,0 +1,20 @@
+def _build_testitem_descriptor (self :SelfTestitemChemblPipeline )->ChemblExtractionDescriptor [SelfTestitemChemblPipeline ]:
+    "Return the descriptor powering testitem extraction."
+    def build_context (pipeline :SelfTestitemChemblPipeline ,source_config :TestItemSourceConfig ,log :BoundLogger )->ChemblExtractionContext :
+        base_url =pipeline ._resolve_base_url (source_config .parameters )
+        http_client ,_ =pipeline .prepare_chembl_client ("chembl",base_url =base_url ,client_name ="chembl_testitem_http")
+        chembl_client =ChemblClient (http_client ,load_meta_store =pipeline .load_meta_store ,job_id =pipeline .run_id ,operator =pipeline .pipeline_code )
+        pipeline ._fetch_chembl_release (chembl_client ,log )
+        select_fields =source_config .parameters .select_fields
+        log .debug ("chembl_testitem.select_fields",fields =select_fields )
+        testitem_client =ChemblTestitemClient (chembl_client ,batch_size =min (source_config .page_size ,25 ))
+        return ChemblExtractionContext (source_config =source_config ,iterator =testitem_client ,chembl_client =chembl_client ,select_fields =list (select_fields )if select_fields else None ,page_size =source_config .page_size ,chembl_release =pipeline ._chembl_db_version ,metadata ={"api_version":pipeline ._api_version })
+    def empty_frame (_ :SelfTestitemChemblPipeline ,__ :ChemblExtractionContext )->pd .DataFrame :
+        return pd .DataFrame ({"molecule_chembl_id":pd .Series (dtype ="string")})
+    def dry_run_handler (pipeline :SelfTestitemChemblPipeline ,_ :ChemblExtractionContext ,log :BoundLogger ,stage_start :float )->pd .DataFrame :
+        duration_ms =(time .perf_counter ()-stage_start )*1000.0
+        log .info ("chembl_testitem.extract_skipped",dry_run =True ,duration_ms =duration_ms ,chembl_db_version =pipeline ._chembl_db_version ,api_version =pipeline ._api_version )
+        return pd .DataFrame ()
+    def summary_extra (pipeline :SelfTestitemChemblPipeline ,_ :pd .DataFrame ,__ :ChemblExtractionContext )->Mapping [str ,Any ]:
+        return {"chembl_db_version":pipeline ._chembl_db_version ,"api_version":pipeline ._api_version ,"limit":pipeline .config .cli .limit }
+    return ChemblExtractionDescriptor [SelfTestitemChemblPipeline ](name ="chembl_testitem",source_name ="chembl",source_config_factory =TestItemSourceConfig .from_source_config ,build_context =build_context ,id_column ="molecule_chembl_id",summary_event ="chembl_testitem.extract_summary",must_have_fields =tuple (MUST_HAVE_FIELDS ),default_select_fields =MUST_HAVE_FIELDS ,sort_by =("molecule_chembl_id",),empty_frame_factory =empty_frame ,dry_run_handler =dry_run_handler ,summary_extra =summary_extra ,hard_page_size_cap =None )
```

#### Hotspot 17

- Definition: TestItemChemblPipeline._check_empty_columns#1
- target: absent
- testitem: src/bioetl/pipelines/chembl/testitem/run.py:664-724

```diff
--- target:run.py
+++ testitem:run.py
@@ -0,0 +1,17 @@
+def _check_empty_columns (self ,df :pd .DataFrame ,log :Any )->None :
+    "\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043f\u0443\u0441\u0442\u044b\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n        \u041b\u043e\u0433\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u0435, \u0435\u0441\u043b\u0438 \u043f\u0440\u043e\u0446\u0435\u043d\u0442 \u043f\u0443\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u043e \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u043c \u043f\u043e\u043b\u044f\u043c > 95%.\n        \u042d\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443 \u0441 select_fields \u0438\u043b\u0438 flatten_objects.\n\n        Parameters\n        ----------\n        df:\n            DataFrame \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n        log:\n            UnifiedLogger \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n        "
+    if df .empty :
+        return
+    key_fields =["molecule_chembl_id","pref_name","molecule_type","availability_type","chirality","first_approval","first_in_class","indication_class","helm_notation","molecule_properties__cx_logp","molecule_properties__cx_logd","molecule_properties__mw_monoisotopic","molecule_properties__hba_lipinski","molecule_properties__hbd_lipinski","molecule_properties__molecular_species","molecule_properties__num_lipinski_ro5_violations"]
+    available_key_fields =[col for col in key_fields if col in df .columns ]
+    if not available_key_fields :
+        return
+    empty_percentages :dict [str ,float ]={}
+    for col in available_key_fields :
+        if len (df )>0 :
+            empty_count =df [col ].isna ().sum ()
+            empty_percentage =empty_count /len (df )*100.0
+            empty_percentages [col ]=empty_percentage
+    highly_empty_fields ={col :pct for col ,pct in empty_percentages .items ()if pct >95.0 }
+    if highly_empty_fields :
+        log .warning ("highly_empty_columns_detected",empty_fields =highly_empty_fields ,message =f'Fields with >95% empty values detected: {highly_empty_fields }. This may indicate missing fields in select_fields or flatten_objects configuration.')
```

#### Hotspot 18

- Definition: TestItemChemblPipeline._deduplicate_molecules#1
- target: absent
- testitem: src/bioetl/pipelines/chembl/testitem/run.py:748-807

```diff
--- target:run.py
+++ testitem:run.py
@@ -0,0 +1,24 @@
+def _deduplicate_molecules (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+    "Deduplicate molecules by standard_inchi_key (fallback to molecule_chembl_id)."
+    if df .empty :
+        return df
+    rows_before =len (df )
+    inchi_key_col ="molecule_structures__standard_inchi_key"if "molecule_structures__standard_inchi_key"in df .columns else "standard_inchi_key"
+    canonical_smiles_col ="molecule_structures__canonical_smiles"if "molecule_structures__canonical_smiles"in df .columns else "canonical_smiles"
+    full_mwt_col ="molecule_properties__full_mwt"if "molecule_properties__full_mwt"in df .columns else "full_mwt"
+    alogp_col ="molecule_properties__alogp"if "molecule_properties__alogp"in df .columns else "alogp"
+    if inchi_key_col in df .columns :
+        completeness_cols =[canonical_smiles_col ,full_mwt_col ,alogp_col ]
+        available_completeness =[col for col in completeness_cols if col in df .columns ]
+        if available_completeness :
+            df ["_completeness"]=df [available_completeness ].notna ().sum (axis =1 )
+            df =df .sort_values (["_completeness",canonical_smiles_col ],ascending =[False ,False ],na_position ="last")
+            df =df .drop (columns =["_completeness"])
+        df =df .drop_duplicates (subset =[inchi_key_col ],keep ="first")
+    else :
+        df =df .drop_duplicates (subset =["molecule_chembl_id"],keep ="first")
+    rows_after =len (df )
+    dropped =rows_before -rows_after
+    if dropped >0 :
+        log .info ("deduplicate_molecules_completed",rows_before =rows_before ,rows_after =rows_after ,dropped =dropped )
+    return df
```

#### Hotspot 19

- Definition: TestItemChemblPipeline._fetch_chembl_release#1
- target: absent
- testitem: src/bioetl/pipelines/chembl/testitem/run.py:72-123

```diff
--- target:run.py
+++ testitem:run.py
@@ -0,0 +1,34 @@
+def _fetch_chembl_release (self ,client :UnifiedAPIClient |ChemblClient |Any ,log :BoundLogger |None =None )->str |None :
+    "Capture ChEMBL release and API version from status endpoint."
+    if log is None :
+        bound_log :BoundLogger =UnifiedLogger .get (__name__ ).bind (component =f'{self .pipeline_code }.extract')
+    else :
+        bound_log =log
+    request_timestamp =datetime .now (timezone .utc )
+    release_value :str |None =None
+    api_version :str |None =None
+    try :
+        status_payload :dict [str ,Any ]={}
+        handshake_candidate =getattr (client ,"handshake",None )
+        if callable (handshake_candidate ):
+            status_payload =self ._coerce_mapping (handshake_candidate ("/status"))
+        else :
+            get_candidate =getattr (client ,"get",None )
+            if callable (get_candidate ):
+                response =get_candidate ("/status.json")
+                json_candidate =getattr (response ,"json",None )
+                if callable (json_candidate ):
+                    status_payload =self ._coerce_mapping (json_candidate ())
+        if status_payload :
+            release_value =self ._extract_chembl_release (status_payload )
+            api_candidate =status_payload .get ("api_version")
+            if isinstance (api_candidate ,str )and api_candidate .strip ():
+                api_version =api_candidate
+            bound_log .info ("chembl_testitem.status",chembl_db_version =release_value ,api_version =api_version )
+    except Exception as exc :
+        bound_log .warning ("chembl_testitem.status_failed",error =str (exc ))
+    finally :
+        self ._chembl_db_version =release_value
+        self ._api_version =api_version
+        self .record_extract_metadata (chembl_release =release_value ,requested_at_utc =request_timestamp )
+    return release_value
```

#### Hotspot 20

- Definition: TestItemChemblPipeline._flatten_nested_structures#1
- target: absent
- testitem: src/bioetl/pipelines/chembl/testitem/run.py:397-434

```diff
--- target:run.py
+++ testitem:run.py
@@ -0,0 +1,18 @@
+def _flatten_nested_structures (self ,df :pd .DataFrame ,log :Any )->pd .DataFrame :
+    "Flatten nested molecule_structures and molecule_properties into flat columns."
+    if df .empty :
+        return df
+    if "molecule_structures"in df .columns :
+        structures_df =pd .json_normalize (df ["molecule_structures"].tolist ())
+        if "canonical_smiles"in structures_df .columns :
+            df ["canonical_smiles"]=structures_df ["canonical_smiles"]
+        if "standard_inchi_key"in structures_df .columns :
+            df ["standard_inchi_key"]=structures_df ["standard_inchi_key"]
+    if "molecule_properties"in df .columns :
+        properties_df =pd .json_normalize (df ["molecule_properties"].tolist ())
+        property_columns =["full_mwt","mw_freebase","alogp","hbd","hba","psa","aromatic_rings","rtb","num_ro5_violations"]
+        for col in property_columns :
+            if col in properties_df .columns :
+                df [col ]=properties_df [col ]
+    log .debug ("flatten_nested_structures_completed",columns =list (df .columns ))
+    return df
```

_Only the first 20 hotspots of 48 are shown for module run.py._

_First 20 hotspots of 66 are shown for pair target ↔ testitem._

---
